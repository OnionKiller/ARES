Reaction	Sources	Question	Document	Answer	Context_Relevance
no_reaction	['https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html']	how do i deploy a private link enabled databricks workspace in AWS	      Enable AWS PrivateLink ========================     This article explains how to use AWS PrivateLink to enable private connectivity between users and their Databricks workspaces and between clusters on the data plane and core services on the control plane within the Databricks workspace infrastructure.        Important       This article mentions the term  *data plane*   , which is the compute layer of the Databricks platform. In the context of this article, data plane refers to the Classic data plane in your AWS account. By contrast, the serverless data plane that supports  [serverless SQL warehouses (Public Preview)](../../../sql/admin/serverless.html)  runs in the Databricks AWS account. To learn more, see  [Serverless compute](../../../serverless-compute/index.html)  .         Overview ----------     AWS PrivateLink provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network. Databricks workspaces on the E2 version of the platform support PrivateLink connections for two connection types:     * **Front-end (user to workspace)**   : A front-end PrivateLink connection allows users to connect to the Databricks web application, REST API, and Databricks Connect API over a VPC interface endpoint. * **Back-end (data plane to control plane)**   : Databricks Runtime clusters in a customer-managed VPC (the  [data plane](../../../getting-started/overview.html)  ) connect to a Databricks workspace’s core services (the  [control plane](../../../getting-started/overview.html)  ) in the Databricks cloud account. Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay. This PrivateLink connection type involves two different VPC interface endpoints because of the two different destination services.     You can implement both front-end and back-end PrivateLink connections or just one of them. This article discusses how to configure either one or both PrivateLink connection types. If you implement PrivateLink for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means Databricks rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.       To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects.       To create configuration objects and create (or update) a workspace, this article describes how to  [use the account console](#account-console)  or  [use the Account API](#account-api)  .       The following table describes important terminology.          |   Terminology   |   Description   | | --- | --- | |   AWS PrivateLink   |   An AWS technology that provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network.   | |   Front-end PrivateLink   |   The PrivateLink connection for users to connect to the Databricks web application, REST API, and Databricks Connect API.   | |   Back-end PrivateLink   |   The PrivateLink connection for the  [data plane](../../../getting-started/overview.html)  in your AWS account to connect to the Databricks  [control plane](../../../getting-started/overview.html)  ).   | |   AWS VPC endpoint service   |   An AWS VPC endpoint service is a PrivateLink-powered service. Each Databricks control plane (typically one per region) publishes two AWS VPC endpoint services for PrivateLink. The workspace VPC endpoint service applies to both a Databricks front-end PrivateLink connection or the Databricks back-end PrivateLink connection for REST APIs. Databricks publishes another VPC endpoint service for its  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay.   | |   AWS VPC endpoint   |   An  [AWS VPC interface endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html)   enables private connections between your VPC and VPC endpoint services powered by AWS PrivateLink. You must create AWS VPC interface endpoints and then register them with Databricks. Registering a VPC endpoint creates a Databricks-specific object called a VPC endpoint registration that references the AWS VPC endpoint.   | |   Databricks network configuration   |   A Databricks object that describes the important information about a  [Customer-managed VPC](customer-managed-vpc.html)  . If you implement any PrivateLink connection (front-end or back-end), your workspace must use a customer-managed VPC. For PrivateLink back-end support only, your network configuration needs an extra property that identifies the VPC endpoints for the back-end connection.   | |   Databricks private access settings object   |   A Databricks object that describes a workspace’s PrivateLink connectivity. You must attach a private access settings object to the workspace during workspace creation, whether using front-end, back-end, or both. It expresses your intent to use AWS PrivateLink with your workspace. It controls your settings for the front-end use case of AWS PrivateLink for public network access. It controls which VPC endpoints are permitted to access your workspace.   | |   Databricks workspace configuration object   |   A Databricks object that describes a workspace. To enable PrivateLink, this object must reference Databricks private access settings object. For back-end PrivateLink, the workspace must also have a Databricks network configuration object with two extra fields that specify which VPC endpoint registrations to use, one for control plane’s secure cluster connectivity relay and the other connects to the workspace to access REST APIs.   |      ###   Updates of existing PrivateLink configuration objects     This article focuses on the main two use cases of creating a new workspace or enabling PrivateLink on a workspace. You also can make other configuration changes to related objects using the UI or API:     * You can upgrade a workspace’s PrivateLink support to add support for front-end, back-end, or both types of connectivity. Add a private access settings object (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ) or update a private access settings object (  [UI](../../account-settings-e2/private-access-settings.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/replace-private-access-settings)   ). To do so, create a new network configuration with new settings, for example for a new VPC or different PrivateLink support settings, and then update the workspace to use the new network configuration. Note that you cannot remove (downgrade) any existing front-end or back-end PrivateLink support on a workspace. * Add or update a workspace’s registered VPC endpoints by creating a new network configuration object with registered VPC endpoints and then update the workspace’s network configuration (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ). * For more information about what kinds of workspace fields can be changed on failed or running workspaces, see information about this task by using the  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   .     Note that not all related objects can be updated. Where update is not possible, create new objects and set their parent objects to reference the new objects. The following rules apply both to the account console UI and the Account API:           |   Object   |   Can be created   |   Can be updated   | | --- | --- | --- | |   Workspace configurations   |   Yes   |   Yes   | |   Private access settings   |   Yes   |   Yes   | |   Network configurations   |   Yes   |  **No**   | |   VPC endpoint registrations   |   Yes   |  **No**   |     To update CIDR ranges on an existing VPC, see  [Updating CIDRs](customer-managed-vpc.html#update-cidr)  .       ###   Network flow     The following diagram shows the network flow in a typical implementation.      ![PrivateLink network architecture](../../../_images/privatelink-network.png)             Requirements --------------    **Databricks account**     * Your Databricks account is on the E2 version of the platform. * Your Databricks account is on the  [Enterprise pricing tier](https://databricks.com/product/aws-pricing)   . * You have your Databricks account ID. Get your account ID from the  [account console](../../account-settings/index.html#account-id)  .    **Databricks workspace**     * Your workspace must be in an  [AWS region that supports the E2 version of the platform](../../../resources/supported-regions.html)  . However, the  `us-west-1`  region does not support PrivateLink even for workspaces on the E2 version of the platform. * Your Databricks workspace must use  [Customer-managed VPC](customer-managed-vpc.html)  to add any PrivateLink connection (even a front-end-only connection). Note that you cannot update an existing workspace with a Databricks-managed VPC and change it to use a customer-managed VPC. * If you implement the back-end PrivateLink connection, your Databricks workspace must use  [Secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  , which is the default for new workspaces on the E2 version of the platform. To add back-end PrivateLink to an older existing workspace that does not use secure cluster connectivity, contact your Databricks representative.    **AWS account permissions**     * If you are the user who sets up PrivateLink, you must have all necessary AWS permissions to provision a Databricks workspace and to provision new VPC endpoints for your workspace.    **Network architecture**     * To implement the front-end PrivateLink connection to access the workspace from your on-premises network, add private connectivity from the on-premises network to an AWS VPC using either Direct Connect or VPN. * For guidance for other network objects, see  [Step 1: Configure AWS network objects](#create-vpc)  .         Step 1: Configure AWS network objects ---------------------------------------         You can use the AWS Management Console to create these objects or automate the process with tools such as the  [Terraform provider for networks](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/mws_networks)   .       To configure a VPC, subnets, and security groups:     1. Set up a VPC for your workspace if you haven’t already done so. You may re-use a VPC from another workspace, but you must create separate subnets for each workspace. Every workspace requires at least two private subnets.      1. To create a VPC, see   [Customer-managed VPC](customer-managed-vpc.html)   . If you are updating a workspace for PrivateLink rather than creating a new workspace, note that the workspace must already be using a customer-managed VPC.  2. On your VPC, ensure that you enable both of the settings   **DNS Hostnames**    and   **DNS resolution**    .  3. Ensure that the network ACLs for the subnets have   **bidirectional**    (outbound and inbound) rules that allow TCP access to 0.0.0.0/0 for these ports:          * 443: for Databricks infrastructure, cloud data sources, and library repositories   * 3306: for the metastore   * 6666: for PrivateLink   * 2443: only for use with compliance security profile     Important            If your	To deploy a private link enabled Databricks workspace in AWS, you need to use AWS PrivateLink to enable private connectivity between users and their Databricks workspaces and between clusters on the data plane and core services on the control plane within the Databricks workspace infrastructure. Databricks workspaces on the E2 version of the platform support PrivateLink connections for two connection types: front-end (user to workspace) and back-end (data plane to control plane). To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects. To create configuration objects and create (or update) a workspace, you can use the account console or use the Account API. The requirements include having a Databricks account on the E2 version of the platform, using a customer-managed VPC, and having the necessary AWS permissions to provision a Databricks workspace and to provision new VPC endpoints for your workspace. To configure a VPC, subnets, and security groups, you can use the AWS Management Console or automate the process with tools such as the Terraform provider for networks.	1
no_reaction	['https://docs.databricks.com/data-sharing/index.html']	Tell me about Delta Sharing streaming	      Delta Sharing ===============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data.       There are three components to Delta Sharing:     * **[Providers](#providers)**      A provider is an entity which has made data available for sharing. * **[Shares](#shares)**      A share defines a logical grouping for the tables you intend to share. * **[Recipients](#recipients)**      A recipient identifies an organization with which you want to share any number of shares.     For a detailed guide on how to use Delta Sharing see  [Share data securely using Delta Sharing](../../data-sharing/index.html)  .        Providers -----------    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.1 and above       A data provider is an object representing the organization in the real world who shares the data. A provider contains shares which further contain the shared data. Once a provider has added you as a  **[Recipients](#recipients)**  you can     * Customize the local name of the provider using  [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html)  . * List the sets of data shared with you with  [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html)  . * Create a catalog using shares of interest to you with  [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html)  .    ###   Examples      ``` -- Change the data provider name locally. > ALTER PROVIDER `Center for Disease Control` RENAME TO cdc;  -- List the shares the provider has granted you access too. > SHOW SHARES IN PROVIDER cdc;  vaccinedata  -- Make the share accessible locally as a catalog. > CREATE CATALOG cdcdata USING cdc.vaccinedata;  -- Use the data. > USE CATALOG cdcdata; > SELECT COUNT(\*) FROM information\_schema.tables;  10  ```      ###   Related articles   * [DESCRIBE PROVIDER](sql-ref-syntax-aux-describe-provider.html) * [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html) * [DROP PROVIDER](sql-ref-syntax-ddl-drop-provider.html) * [SHOW PROVIDERS](sql-ref-syntax-aux-show-providers.html) * [COMMENT ON PROVIDER](sql-ref-syntax-ddl-comment.html) * [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html) * [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Shares --------     A share is a container instantiated with the  [CREATE SHARE](sql-ref-syntax-ddl-create-share.html)  command. Once created you can iteratively register a collection of existing tables defined within the metastore using the  [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html)  command. You can register tables under their original name, qualified by their original schema, or provide alternate exposed names.       You must be a metastore admin or account admin to create, alter, and drop shares.        ###   Examples      ``` -- Create share `customer\_share` only if share with same name doesn't exist, with a comment. > CREATE SHARE IF NOT EXISTS customer\_share COMMENT 'This is customer share';  -- Add 2 tables to the share. -- Expose my\_schema.tab1 a different name. -- Expose only two partitions of other\_schema.tab2 > ALTER SHARE customer\_share ADD TABLE my\_schema.tab1 AS their\_schema.tab1; > ALTER SHARE customer\_share ADD TABLE other\_schema.tab2 PARTITION (c1 = 5), (c1 = 7);  -- List the content of the share > SHOW ALL IN SHARE customer\_share;  name type shared\_object added\_at added\_by comment partitions  ----------------- ---- ---------------------- ---------------------------- -------------------------- ------- -----------------  other\_schema.tab2 TABLE main.other\_schema.tab2 2022-01-01T00:00:01.000+0000 alwaysworks@databricks.com NULL  their\_schema.tab1 TABLE main.myschema.tab2 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com NULL (c1 = 5), (c1 = 7)  ```        ###   Related articles   * [Change data feed](../../delta/delta-change-data-feed.html) * [CREATE SHARE](sql-ref-syntax-ddl-create-share.html) * [DESCRIBE SHARE](sql-ref-syntax-aux-describe-share.html) * [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html) * [DROP SHARE](sql-ref-syntax-ddl-drop-share.html) * [SHOW SHARES](sql-ref-syntax-aux-show-shares.html) * [SHOW ALL IN SHARE](sql-ref-syntax-aux-show-all-in-share.html) * [COMMENT ON SHARE](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Recipients ------------     A recipient is an object you create using  [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html)  to represent an organization which you want to allow access shares. When you create a recipient Databricks SQL generates an activation link you can send to the organization. To retrieve the activation link after creation you use  [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html)  .       Once a recipient has been created you can give it  `SELECT`  privileges on shares of your choice using  [GRANT ON SHARE](security-grant-share.html)  .       You must be a metastore administrator to create recipients, drop recipients, and grant access to shares.        ###   Examples      ``` -- Create a recipient. > CREATE RECIPIENT IF NOT EXISTS other\_org COMMENT 'other.org';  -- Retrieve the activation link to send to other.org > DESCRIBE RECIPIENT other\_org;  name created\_at created\_by comment activation\_link active\_token\_id active\_token\_expiration\_time rotated\_token\_id rotated\_token\_expiration\_time  --------- ---------------------------- -------------------------- --------- --------------- ------------------------------------ ---------------------------- ---------------- -----------------------------  other\_org 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com other.org https://.... 0160c81f-5262-40bb-9b03-3ee12e6d98d7 9999-12-31T23:59:59.999+0000 NULL NULL  -- Choose shares that other.org has access to > GRANT SELECT ON SHARE customer\_share TO RECIPIENT other\_org;  ```        ###   Related articles   * [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html) * [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html) * [ALTER RECIPIENT](sql-ref-syntax-ddl-alter-recipient.html) * [DROP RECIPIENT](sql-ref-syntax-ddl-drop-recipient.html) * [SHOW RECIPIENTS](sql-ref-syntax-aux-show-recipients.html) * [COMMENT ON RECIPIENT](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)                                                       	Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data. There are three components to Delta Sharing: Providers, Shares, and Recipients.	0
no_reaction	['https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-alter-table-add-constraint.html']	Does Delta support Foreign Key constraints	    Constraints on Databricks ===========================     Databricks supports standard SQL constraint management clauses. Constraints fall into two categories:     * Enforced contraints ensure that the quality and integrity of data added to a table is automatically verified. * Informational primary key and foreign key constraints encode relationships between fields in tables and are not enforced.     All constraints on Databricks require Delta Lake.       Delta Live Tables has a similar concept known as expectations. See  [Manage data quality with Delta Live Tables](../delta-live-tables/expectations.html)  .        Enforced constraints on Databricks ------------------------------------     When a constraint is violated, the transaction fails with an error. Two types of constraints are supported:     * `NOT     NULL`  : indicates that values in specific columns cannot be null. * `CHECK`  : indicates that a specified boolean expression must be true for each input row.      Important       Adding a constraint automatically upgrades the table writer protocol version if the previous writer version was less than 3. See  [How does Databricks manage Delta Lake feature compatibility?](../delta/feature-compatibility.html)  to understand table protocol versioning and what it means to upgrade the protocol version.            Set a  `NOT     NULL`  constraint in Databricks -------------------------------------------------      Note     * `SET     NOT     NULL`  is available in Databricks Runtime 7.4 and above.      You specify  `NOT     NULL`  constraints in the schema when you create a table. You drop or add  `NOT     NULL`  constraints using the  `ALTER     TABLE     ALTER     COLUMN`  command.        ``` CREATE TABLE people10m (  id INT NOT NULL,  firstName STRING,  middleName STRING NOT NULL,  lastName STRING,  gender STRING,  birthDate TIMESTAMP,  ssn STRING,  salary INT ) USING DELTA;  ALTER TABLE people10m ALTER COLUMN middleName DROP NOT NULL; ALTER TABLE people10m ALTER COLUMN ssn SET NOT NULL;  ```      Before adding a  `NOT     NULL`  constraint to a table, Databricks verifies that all existing rows satisfy the constraint.       If you specify a  `NOT     NULL`  constraint on a column nested within a struct, the parent struct must also be not null. Columns nested within array or map types do not accept  `NOT     NULL`  constraints.       See  [CREATE TABLE [USING]](../sql/language-manual/sql-ref-syntax-ddl-create-table-using.html)  and  [ALTER TABLE ALTER COLUMN](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html)  .           Set a  `CHECK`  constraint in Databricks ------------------------------------------      Note     * Available in Databricks Runtime 7.4 and above. * In Databricks Runtime 7.3 LTS you can write to tables with  `CHECK`  constraints defined but you cannot create  `CHECK`  constraints.      You manage  `CHECK`  constraints using the  `ALTER     TABLE     ADD     CONSTRAINT`  and  `ALTER     TABLE     DROP     CONSTRAINT`  commands.  `ALTER     TABLE     ADD     CONSTRAINT`  verifies that all existing rows satisfy the constraint before adding it to the table.        ``` CREATE TABLE people10m (  id INT,  firstName STRING,  middleName STRING,  lastName STRING,  gender STRING,  birthDate TIMESTAMP,  ssn STRING,  salary INT ) USING DELTA;  ALTER TABLE people10m ADD CONSTRAINT dateWithinRange CHECK (birthDate > '1900-01-01'); ALTER TABLE people10m DROP CONSTRAINT dateWithinRange;  ```      See  [ALTER TABLE ADD CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html#add-constraint)  and  [ALTER TABLE DROP CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html#drop-constraint)  .      `CHECK`  constraints are exposed as table properties in the output of the  `DESCRIBE     DETAIL`  and  `SHOW     TBLPROPERTIES`  commands.        ``` ALTER TABLE people10m ADD CONSTRAINT validIds CHECK (id > 1 and id < 99999999);  DESCRIBE DETAIL people10m;  SHOW TBLPROPERTIES people10m;  ```          Declare primary key and foreign key relationships ---------------------------------------------------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .         Note     * Primary key and foreign key constraints are available in Databricks Runtime 11.1 and above. * Primary key and foreign key constraints require Unity Catalog and Delta Lake.      You can use primary key and foreign key relationships on fields in Unity Catalog tables. Primary and foreign keys are informational only and are not enforced. Foreign keys must reference a primary key in another table.       You can declare primary keys and foreign keys as part of the table specification clause during table creation. This clause is not allowed during CTAS statements. You can also add constraints to existing tables.        ``` CREATE TABLE T(pk1 INTEGER NOT NULL, pk2 INTEGER NOT NULL,  CONSTRAINT t\_pk PRIMARY KEY(pk1, pk2)); CREATE TABLE S(pk INTEGER NOT NULL PRIMARY KEY,  fk1 INTEGER, fk2 INTEGER,  CONSTRAINT s\_t\_fk FOREIGN KEY(fk1, fk2) REFERENCES T);  ```      You can query the  `information\_schema`  or use  `DESCRIBE`  to get details about how constraints are applied across a given catalog.       See:     * [ALTER TABLE](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html) * [ADD CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table-add-constraint.html) * [DROP CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table-drop-constraint.html) * [CONSTRAINT clause](../sql/language-manual/sql-ref-syntax-ddl-create-table-constraint.html)                                                      	Yes, Delta supports Foreign Key constraints. However, it's important to note that foreign key constraints in Delta are informational only and are not enforced. Foreign keys must reference a primary key in another table.	0
thumb_down	['https://kb.databricks.com/sql/to_json-results-in-cannot-use-null-as-map-key-error']	Hi Assitant. how do I make to_json SQL function to preserve null fields from passed struct?	"Title: to_json() results in Cannot use null as map key error  Description: You must filter or replace null values in your input data before using to_json().  Answer: Problem You are using to_json() to convert data to JSON and you get a Cannot use null as map key error: RuntimeException: Cannot use null as map key. Cause to_json() function does not support using null values as the input map keys. This example code causes the Cannot use null as map key error when run, because of the null value used as a map key in the fourth line. %sql select   to_json(     map(       1, 'Databricks',       2, 'Map',       3, 'Error',       null, 'Data'     )   ) as json; Solution  You should filter out any null values present in the input data before running to_json(), or use nvl() to replace all of the null values with non-null values. Filter null values Consider this example DataFrame: +---+----------+-------+ | Id|     Value|address| +---+----------+-------+ |  1|Databricks|   null| |  2|       Map|   null| |  3|     Error|    xyz| +---+----------+-------+ There are two null values in the example. Attempting to use to_json() on this DataFrame will return an error. We can filter the null data by showing only the rows that have non-null values. For example, filtering with df.filter(""address is not null"").show() returns: +---+-----+-------+ | Id|Value|address| +---+-----+-------+ |  3|Error|    xyz| +---+-----+-------+ This filtered DataFrame does not contain any null values, so it can now be used as an input with to_json(). Replace null values with replacements If you cannot filter out the null values, you can use nvl() to replace the null values with non-null values.  The sample code originally had a null value as the map key for the fourth line. Since that results in an error, nvl() is used in this updated example to substitute 4 for the null value. %sql select   to_json(     map(       1, 'Databricks',       2, 'Map',       3, 'Error', nvl(null, 4), 'Data'     ) ) as JSON;"	To preserve null fields from a passed struct in the to_json SQL function, you should filter out any null values present in the input data before running to_json(), or use nvl() to replace all of the null values with non-null values. If you cannot filter out the null values, you can use nvl() to replace the null values with non-null values.	1
no_reaction	['https://docs.databricks.com/ingestion/auto-loader/index.html']	autoloader with scala? can you give me an example ?	"      Databricks Runtime 7.0 (Unsupported) ======================================     Databricks released this image in June 2020.       The following release notes provide information about Databricks Runtime 7.0, powered by Apache Spark 3.0.        New features --------------     Databricks Runtime 7.0 includes the following new features:     * **Scala 2.12**       Databricks Runtime 7.0 upgrades Scala from 2.11.12 to 2.12.10. The change list between Scala 2.12 and 2.11 is in the  [Scala 2.12.0 release notes](https://github.com/scala/scala/releases/tag/v2.12.0)   . * **Auto Loader (Public Preview)**   , released in Databricks Runtime 6.4, has been improved in Databricks Runtime 7.0       Auto Loader gives you a more efficient way to process new data files incrementally as they arrive on a cloud blob store during ETL. This is an improvement over file-based structured streaming, which identifies new files by repeatedly listing the cloud directory and tracking the files that have been seen, and can be very inefficient as the directory grows. Auto Loader is also more convenient and effective than file-notification-based structured streaming, which requires that you manually configure file-notification services on the cloud and doesn’t let you backfill existing files. For details, see  [What is Auto Loader?](../../ingestion/auto-loader/index.html)  .   * **`COPY     INTO`  (Public Preview)**   , which lets you load data into Delta Lake with idempotent retries, has been improved in Databricks Runtime 7.0       Released as a Public Preview in Databricks Runtime 6.4, the  `COPY     INTO`  SQL command lets you load data into Delta Lake with idempotent retries. To load data into Delta Lake today you have to use Apache Spark DataFrame APIs. If there are failures during loads, you have to handle them effectively. The new  `COPY     INTO`  command provides a familiar declarative interface to load data in SQL. The command keeps track of previously loaded files and you safely re-run it in case of failures. For details, see  [COPY INTO](../../sql/language-manual/delta-copy-into.html)  .       Improvements --------------   * More Amazon Kinesis concurrent streams:       The Amazon Kinesis Structured Streaming source uses  `ListShards`  by default to get the list of shards in a Kinesis stream. This requires additional IAM permissions to successfully run your stream. In previous versions of Databricks Runtime,  `DescribeStream`  was used by default.  `ListShards`  has a significantly higher API limit than  `DescribeStream`  (100 requests per second per stream for  `ListShards`  versus 10 requests per second across your entire AWS account for  `DescribeStream`  ). This change will allow users to run more than 10 concurrent Kinesis streams with Structured Streaming in Databricks.   * Azure Synapse (formerly SQL Data Warehouse) connector supports the  `COPY`  statement.       The main benefit of  `COPY`  is that lower privileged users can write data to Azure Synapse without needing strict  `CONTROL`  permissions on Azure Synapse. * The  `%matplotlib     inline`  magic command is no longer required to  [display Matplolib](../../visualizations/matplotlib.html)  objects inline in notebook cells. They are always displayed inline by default. * [Matplolib](../../visualizations/matplotlib.html)  figures are now rendered with  `transparent=False`  , so that user-specified backgrounds are not lost. This behavior can be overridden by setting Spark configuration  `spark.databricks.workspace.matplotlib.transparent     true`  . * When running Structured Streaming production jobs on High Concurrency mode clusters, restarts of a job would occasionally fail, because the previously running job wasn’t terminated properly. Databricks Runtime 6.3 introduced the ability to set the SQL configuration  `spark.sql.streaming.stopActiveRunOnRestart     true`  on your cluster to ensure that the previous run stops. This configuration is set by default in Databricks Runtime 7.0.    ###   Major library changes    ####   Python packages     Major Python packages upgraded:     * boto3 1.9.162 -> 1.12.0 * matplotlib 3.0.3 -> 3.1.3 * numpy 1.16.2 -> 1.18.1 * pandas 0.24.2 -> 1.0.1 * pip 19.0.3 -> 20.0.2 * pyarrow 0.13.0 -> 0.15.1 * psycopg2 2.7.6 -> 2.8.4 * scikit-learn 0.20.3 -> 0.22.1 * scipy 1.2.1 -> 1.4.1 * seaborn 0.9.0 -> 0.10.0     Python packages removed:     * boto (use boto3) * pycurl      Note       The Python environment in Databricks Runtime 7.0 uses Python 3.7, which is different from the installed Ubuntu system Python:  `/usr/bin/python`  and  `/usr/bin/python2`  are linked to Python 2.7 and  `/usr/bin/python3`  is linked to Python 3.6.        ####   R packages     R packages added:     * broom * highr * isoband * knitr * markdown * modelr * reprex * rmarkdown * rvest * selectr * tidyverse * tinytex * xfun     R packages removed:     * abind * bitops * car * carData * doMC * gbm * h2o * littler * lme4 * mapproj * maps * maptools * MatrixModels * minqa * mvtnorm * nloptr * openxlsx * pbkrtest * pkgKitten * quantreg * R.methodsS3 * R.oo * R.utils * RcppEigen * RCurl * rio * sp * SparseM * statmod * zip     ####   Java and Scala libraries   * AWS SDK (aws-java-sdk) upgraded to 1.11.655. * Amazon Kinesis Client upgraded to 1.12.0   * Apache Hive version used for handling Hive user-defined functions and Hive SerDes upgraded to 2.3. * Previously Azure Storage and Key Vault jars were packaged as part of Databricks Runtime, which would prevent you from using different versions of those libraries attached to clusters. Classes under  `com.microsoft.azure.storage`  and  `com.microsoft.azure.keyvault`  are no longer on the class path in Databricks Runtime. If you depend on either of those class paths, you must now attach Azure Storage SDK or Azure Key Vault SDK to your clusters.         Behavior changes ------------------     This section lists behavior changes from Databricks Runtime 6.6 to Databricks Runtime 7.0. You should be aware of these as you migrate workloads from lower Databricks Runtime releases to Databricks Runtime 7.0 and above.      ###   Spark behavior changes     Because Databricks Runtime 7.0 is the first Databricks Runtime built on Spark 3.0, there are many changes that you should be aware of when you migrate workloads from Databricks Runtime 5.5 LTS or 6.x, which are built on Spark 2.4. These changes are listed in the “Behavior changes” section of each functional area in the  [Apache Spark](#spark)  section of this release notes article:     * [Behavior changes for Spark core, Spark SQL, and Structured Streaming](#behavior-changes-for-spark-core-spark-sql-and-structured-streaming) * [Behavior changes for MLlib](#behavior-changes-for-mllib) * [Behavior changes for SparkR](#behavior-changes-for-sparkr)     ###   Other behavior changes   * The upgrade to Scala 2.12 involves the following changes:      + Package cell serialization is handled differently. The following example illustrates the behavior change and how to handle it.            Running   `foo.bar.MyObjectInPackageCell.run()`   as defined in the following package cell will trigger the error   `java.lang.NoClassDefFoundError:        Could        not        initialize        class        foo.bar.MyObjectInPackageCell$`              ```  package foo.bar    case class MyIntStruct(int: Int)    import org.apache.spark.sql.SparkSession  import org.apache.spark.sql.functions.\_  import org.apache.spark.sql.Column    object MyObjectInPackageCell extends Serializable {     // Because SparkSession cannot be created in Spark executors,   // the following line triggers the error   // Could not initialize class foo.bar.MyObjectInPackageCell$   val spark = SparkSession.builder.getOrCreate()     def foo: Int => Option[MyIntStruct] = (x: Int) => Some(MyIntStruct(100))     val theUDF = udf(foo)     val df = {   val myUDFInstance = theUDF(col(""id""))   spark.range(0, 1, 1, 1).withColumn(""u"", myUDFInstance)   }     def run(): Unit = {   df.collect().foreach(println)   }  }    ```           To work around this error, you can wrap   `MyObjectInPackageCell`   inside a serializable class.  + Certain cases using   `DataStreamWriter.foreachBatch`   will require a source code update.  This change is due to the fact that Scala 2.12 has automatic conversion from lambda expressions to SAM types and can cause ambiguity.            For example, the following Scala code can’t compile:               ```  streams   .writeStream   .foreachBatch { (df, id) => myFunc(df, id) }    ```           To fix the compilation error, change   `foreachBatch        {        (df,        id)        =>        myFunc(df,        id)        }`   to   `foreachBatch(myFunc        \_)`   or use the Java API explicitly:   `foreachBatch(new        VoidFunction2        ...)`   .   * With the AWS SDK upgrade to 1.11.655, the use of  `org.apache.hadoop.fs.s3native.NativeS3FileSystem`  requires AWS Signature v4 and bucket endpoint setup. A 403 Forbidden error may be thrown if a user has configured AWS Signature v2 to sign requests to S3 with the S3N file system or a user accesses an S3 path that contains “+” characters and uses the legacy S3N file system (for example s3n://bucket/path/+file).   * Because the Apache Hive version used for handling Hive user-defined functions and Hive SerDes is upgraded to 2.3, two changes are required:      + Hive’s   `SerDe`   interface is replaced by an abstract class   `AbstractSerDe`   .  For any custom Hive   `SerDe`   implementation, migrating to   `AbstractSerDe`   is required.  + Setting   `spark.sql.hive.metastore.jars`   to   `builtin`   means that the Hive 2.3 metastore client will be used to access"	Yes, Databricks Runtime 7.0 includes Scala 2.12. Here is an example of using Scala 2.12 in Databricks Runtime 7.0:   ``` val list = List(1, 2, 3, 4, 5) val filteredList = list.filter(_ % 2 == 0) println(filteredList) ```  This code will print out a list of even numbers from the original list.	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.groupby.GroupBy.sum.html']	What's the best syntax in python for grouping a dataframe on customer, and summing cost while also counting the number of transactions for each customer?	"   pyspark.pandas.groupby.GroupBy.sum  [¶](#pyspark-pandas-groupby-groupby-sum ""Permalink to this headline"") ===========================================================================================================  `GroupBy.`  `sum`    (   *numeric\_only     :     Optional    [    bool    ]     =     True*  ,  *min\_count     :     int     =     0*   )    → FrameLike  [[source]](../../../_modules/pyspark/pandas/groupby.html#GroupBy.sum) [¶](#pyspark.pandas.groupby.GroupBy.sum ""Permalink to this definition"")    Compute sum of group values     New in version 3.3.0.     Parameters    **numeric\_only**    bool, default False     Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. It takes no effect since only numeric columns can be support here.     New in version 3.4.0.    **min\_count**    int, default 0     The required number of valid values to perform the operation. If fewer than min\_count non-NA values are present the result will be NA.     New in version 3.4.0.     See also    [`pyspark.pandas.Series.groupby`](pyspark.pandas.Series.groupby.html#pyspark.pandas.Series.groupby ""pyspark.pandas.Series.groupby"")  [`pyspark.pandas.DataFrame.groupby`](pyspark.pandas.DataFrame.groupby.html#pyspark.pandas.DataFrame.groupby ""pyspark.pandas.DataFrame.groupby"")   Notes     There is a behavior difference between pandas-on-Spark and pandas:    * when there is a non-numeric aggregation column, it will be ignored     even if    numeric\_only    is False.   Examples    ``` >>> df = ps.DataFrame({""A"": [1, 2, 1, 2], ""B"": [True, False, False, True], ...                    ""C"": [3, 4, 3, 4], ""D"": [""a"", ""a"", ""b"", ""a""]})  ```  ``` >>> df.groupby(""A"").sum().sort\_index()  B C A 1 1 6 2 1 8  ```  ``` >>> df.groupby(""D"").sum().sort\_index()  A B C D a 5 2 11 b 1 0 3  ```  ``` >>> df.groupby(""D"").sum(min\_count=3).sort\_index()  A B C D a 5.0 2.0 11.0 b NaN NaN NaN  ```  [pyspark.pandas.groupby.GroupBy.std](pyspark.pandas.groupby.GroupBy.std.html ""previous page"")  [pyspark.pandas.groupby.GroupBy.var](pyspark.pandas.groupby.GroupBy.var.html ""next page"")   "	The best syntax in Python for grouping a dataframe on customer, and summing cost while also counting the number of transactions for each customer can be achieved using the following code:  ``` df.groupby('customer').agg({'cost': 'sum', 'transaction': 'count'}) ```  This will group the dataframe by the 'customer' column, and then aggregate the 'cost' column by summing it and the 'transaction' column by counting it for each customer.	0
no_reaction	['https://docs.databricks.com/data/manage-external-locations.html']	Can you use external locations without unity catalog	    External locations ====================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime  ![check marked yes](../../_images/check.png)  Unity Catalog only       Unity Catalog and the built-in Databricks Hive metastore use default locations for managed tables. Unity Catalog introduces several new securable objects to grant privileges to data in cloud object storage.     * **[storage credential](sql-ref-storage-credentials.html)**      A Unity Catalog object used to abstract long term credentials from cloud storage providers. * **[external location](#)**      A Unity Catalog object used to associate a cloud object storage URI with a  [storage credential](sql-ref-storage-credentials.html)  . * **[external table](sql-ref-external-tables.html)**      A Unity Catalog  [table](sql-ref-syntax-ddl-create-table-using.html)  created in a Unity Catalog-managed  [external location](#)  .      External location -------------------     An external location is a securable object that combines a storage path with a storage credential that authorizes access to that path.       An external location’s creator is its initial owner. An external location’s owner can modify the external location’s name, URI, and storage credential.       After an external location is created, you can grant access to it to account-level  [principals](sql-ref-principal.html)  (users and groups).       A user or group with permission to use an external location can access any storage path within the location’s path without direct access to the storage credential.       To further refine access control you can use  [GRANT](security-grant.html)  on  [external tables](sql-ref-external-tables.html)  to encapsulate access to individual files within an external location.      [External location names](sql-ref-names.html#location-name)  are unqualified and must be unique within the metastore.       The storage path of any external location cannot be contained within another external location’s storage path, or within an external table’s storage path using an explicit storage credential.        Warning       If a schema (database) is registered in your workspace-level Hive metastore, dropping that schema using the  `CASCADE`  option causes all files in that schema location to be deleted recursively, regardless of the table type (managed or external).       If the schema is registered to a Unity Catalog metastore, the files for Unity Catalog  [managed tables](../../data-governance/unity-catalog/create-tables.html#managed-table)  are deleted recursively. However, the files for  [external tables](../../data-governance/unity-catalog/create-tables.html#external-table)  are not deleted. You must manage those files using the cloud storage provider directly.       Therefore, to avoid accidental data loss, you should never register a schema in a Hive metastore to a location with existing data. Nor should you create new external tables in a location managed by Hive metastore schemas or containing Unity Catalog managed tables.          Graphical Representation of relationships -------------------------------------------     The following diagram describes the relationship between:     * storage credentials * external locations * external tables * storage paths * IAM entities * Azure service accounts    ![External location ER diagram](../../_images/external-location-er-diagram.png)        Examples ----------      ``` -- Grant `finance` user permission to create external location on `my\_aws\_storage\_cred` storage credential, and then create an external location on the specific path to which `my\_aws\_storage\_cred` has access > GRANT CREATE EXTERNAL LOCATION ON STORAGE CREDENTIAL `my\_aws\_storage\_cred` TO `finance` > CREATE EXTERNAL LOCATION finance\_loc URL 's3://depts/finance'  WITH (CREDENTIAL my\_aws\_storage\_cred)  COMMENT 'finance';  -- Grant read, write, and create table access to the finance location to `finance` user > GRANT READ FILES, WRITE FILES, CREATE EXTERNAL TABLE ON EXTERNAL LOCATION `finance\_loc` TO `finance`;  -- `finance` can read from any storage path under 's3://depts/finance' but nowhere else > SELECT count(1) FROM `delta`.`s3://depts/finance/forecast\_delta\_table`;  100 -- 's3://depts/hr/' is not under external location `finance\_loc` so `finance` cannot read it > SELECT count(1) FROM `delta`.`s3://depts/hr/employees\_delta\_table`;  Error  -- `finance` can create an external table over specific object within the `finance\_loc` location > CREATE TABLE main.default.sec\_filings LOCATION 's3://depts/finance/sec\_filings';  -- Cannot list files under an external table with a user that doesn't have SELECT permission on it > LIST 's3://depts/finance/sec\_filings'  Error > LIST 's3://depts/finance/sec\_filings/\_delta\_log'  Error  ```        Related articles ------------------   * [Create a storage credential (CLI)](../../dev-tools/cli/unity-catalog-cli.html#create-a-storage-credential) * [ALTER STORAGE CREDENTIAL](sql-ref-syntax-ddl-alter-credential.html) * [ALTER TABLE](sql-ref-syntax-ddl-alter-table.html) * [CREATE LOCATION](sql-ref-syntax-ddl-create-location.html) * [DESCRIBE STORAGE CREDENTIAL](sql-ref-syntax-aux-describe-credential.html) * [DESCRIBE TABLE](sql-ref-syntax-aux-describe-table.html) * [DROP STORAGE CREDENTIAL](sql-ref-syntax-ddl-drop-credential.html) * [DROP TABLE](sql-ref-syntax-ddl-drop-table.html) * [SHOW STORAGE CREDENTIALS](sql-ref-syntax-aux-show-credentials.html) * [SHOW TABLES](sql-ref-syntax-aux-show-tables.html) * [GRANT](security-grant.html) * [REVOKE](security-revoke.html)                                                      	Yes, you can use external locations without Unity Catalog.	0
no_reaction	['https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-alter-share.html']	whats the syntax doe altering shares	    ALTER SHARE =============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Adds or removes tables to or from the share. Renames a share. Transfers the ownership of a share to a new  [principal](sql-ref-principal.html)  .        Syntax --------      ``` ALTER SHARE share\_name   { alter\_table |     REMOVE TABLE clause |     RENAME TO to\_share\_name |     [ SET ] OWNER TO principal }  alter\_table   { ADD [ TABLE ] table\_name [ COMMENT comment ]         [ PARTITION clause ] [ AS table\_share\_name ]         [ WITH HISTORY | WITHOUT HISTORY ] }  ```      For  ![check marked yes](../../_images/check.png)  Databricks SQL and  ![check marked yes](../../_images/check.png)  Databricks Runtime between 11.1 and 12.0 you must specify  `WITH     CHANGE     DATA     FEED     [     START     VERSION     version     ]     ]`  instead of  `WITH     HISTORY`  . This clause will be deprecated.         Parameters ------------   * **[share\_name](sql-ref-names.html#share-name)**      The name of the share to be altered. * **`alter\_table`**      Adds a table or partitions of a table to the share. To run this statement, you must be the owner of the share and have  `SELECT`  privilege on the table.      + **`ADD        [        TABLE        ]`  [table\_name](sql-ref-names.html#table-name)**           Identifies the table to be added. The table must not reside in the   `hive\_metastore`   .  If the table cannot be found Databricks raises a   [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)   error.  + **`COMMENT        comment`**           An optional string literal attached to the table share as a comment.  + **[PARTITION clause](sql-ref-partition.html#partition)**           One or to more partitions of the table to be added. The partition keys must match the partitioning of the table and be associated with values.  If no   `PARTITION        clause`   is present   `ADD        TABLE`   adds the entire table.            To partition by reference to a   [recipient properties](../../data-sharing/create-recipient.html#properties)   , use the syntax:               ```  PARTITION (column\_name = CURRENT\_RECPIENT().<property\_key>)    ```           Partitioning by reference to recipient properties   **Applies to:**   ![check marked yes](../../_images/check.png)   Databricks Runtime 12.2 and above.  + **`AS`  [table\_share\_name](sql-ref-names.html#table-name)**           Optionally exposes the table under a different name. The name can be qualified with a database (schema) name.  If no   `table\_share\_name`   is specified the table will be known under its own name.  + **`WITH        HISTORY`**   or   **`WITHOUT        HISTORY`**           Applies to:   ![check marked yes](../../_images/check.png)   Databricks SQL   ![check marked yes](../../_images/check.png)   Databricks Runtime 12.1 and above            When   `WITH        HISTORY`   is specified, share the table with full history, allowing recipients to perform time travel queries.  The shared table can then be referenced using   [VERSION AS OF and TIMESTAMP AS OF](sql-ref-names.html#table-name)   .            A shared table can be referenced by the   [table\_changes() function](functions/table_changes.html)   to view the history of changes to it, if “delta.enableChangeDataFeed” is set to “true” on the source table and history is shared.            The default behavior is   `WITHOUT        HISTORY`   . * **`REMOVE     [     TABLE     ]` [table\_name](sql-ref-names.html#table-name)**      Remove the table identified by  `table\_name`  from the share. To run this statement, you must be the owner of the share. * **`RENAME     TO` [to\_share\_name](sql-ref-names.html#share-name)**     **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.1 and above       Renames the share. The name must be unique among all shares in the metastore. To run this statement, you must be the owner of the share and have  `CREATE     SHARE`  privilege on the metastore. * **`[     SET     ]     OWNER     TO` [principal](sql-ref-principal.html)**      Transfers ownership of the share to  `principal`  . To run this statement, you must be the owner of the share.      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  [SQL warehouse version 2022.35](../release-notes/index.html)  or higher  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.2 and above      `SET`  is allowed as an optional keyword.       Examples ----------      ``` -- Creates a share named `some_share`. > CREATE SHARE some_share;  -- Add a table to the share. > ALTER SHARE some_share      ADD TABLE my_schema.my_tab          COMMENT 'some comment'          PARTITION(c1_int = 5, c2_date LIKE '2021%')          AS shared_schema.shared_tab;  — Share a table with history > ALTER SHARE share ADD TABLE table1 WITH HISTORY; > ALTER SHARE share ADD TABLE table2 WITHOUT HISTORY; > SHOW ALL IN SHARE share;   Name    type   ... history_sharing  ...   ------  ------ ... ----------------   Table1  TABLE  ... ENABLED          ...   Table2  TABLE  ... DISABLED         ...  -- Remove the table again > ALTER SHARE some_share     REMOVE TABLE shared_schema.shared_tab;  -- Rename a share > ALTER SHARE some_share     RENAME TO new_share;  -- Change ownership of the share > ALTER SHARE some_share     OWNER TO `alf@melmak.et`  ```        Related articles ------------------   * [Change data feed](../../delta/delta-change-data-feed.html) * [CREATE SHARE](sql-ref-syntax-ddl-create-share.html) * [DESCRIBE SHARE](sql-ref-syntax-aux-describe-share.html) * [DESCRIBE HISTORY](delta-describe-history.html) * [DROP SHARE](sql-ref-syntax-ddl-drop-share.html) * [SHOW SHARES](sql-ref-syntax-aux-show-shares.html) * [SHOW ALL IN SHARE](sql-ref-syntax-aux-show-all-in-share.html) * [COMMENT ON SHARE](sql-ref-syntax-ddl-comment.html)                                                      	The syntax for altering shares in Databricks SQL is as follows:  ``` ALTER SHARE share_name   { alter_table |     REMOVE TABLE clause |     RENAME TO to_share_name |     [ SET ] OWNER TO principal }  alter_table   { ADD [ TABLE ] table_name [ COMMENT comment ]         [ PARTITION clause ] [ AS table_share_name ]         [ WITH HISTORY | WITHOUT HISTORY ] } ```  Where `share_name` is the name of the share to be altered. The `alter_table` clause adds a table or partitions of a table to the share. The `REMOVE TABLE` clause removes the table identified by `table_name` from the share. The `RENAME TO` clause renames the share. The `[SET] OWNER TO` clause transfers ownership of the share to `principal`.	1
no_reaction	['https://kb.databricks.com/sql/gen-unique-increasing-values']	how do I get unique values of a column	"   pyspark.pandas.Index.unique  [¶](#pyspark-pandas-index-unique ""Permalink to this headline"") =============================================================================================  `Index.`  `unique`    (   *level     :     Union[int, Any, Tuple[Any, …], None]     =     None*   )    → pyspark.pandas.indexes.base.Index  [[source]](../../../_modules/pyspark/pandas/indexes/base.html#Index.unique) [¶](#pyspark.pandas.Index.unique ""Permalink to this definition"")    Return unique values in the index.     Be aware the order of unique values might be different than pandas.Index.unique     Parameters    **level**    int or str, optional, default is None     Returns     Index without duplicates     See also    [`Series.unique`](pyspark.pandas.Series.unique.html#pyspark.pandas.Series.unique ""pyspark.pandas.Series.unique"")  [`groupby.SeriesGroupBy.unique`](pyspark.pandas.groupby.SeriesGroupBy.unique.html#pyspark.pandas.groupby.SeriesGroupBy.unique ""pyspark.pandas.groupby.SeriesGroupBy.unique"")   Examples    ``` >>> ps.DataFrame({'a': ['a', 'b', 'c']}, index=[1, 1, 3]).index.unique().sort\_values() Int64Index([1, 3], dtype='int64')  ```  ``` >>> ps.DataFrame({'a': ['a', 'b', 'c']}, index=['d', 'e', 'e']).index.unique().sort\_values() Index(['d', 'e'], dtype='object')  ```   MultiIndex    ``` >>> ps.MultiIndex.from\_tuples([(""A"", ""X""), (""A"", ""Y""), (""A"", ""X"")]).unique() ...  MultiIndex([('A', 'X'),  ('A', 'Y')],  )  ```  [pyspark.pandas.Index.take](pyspark.pandas.Index.take.html ""previous page"")  [pyspark.pandas.Index.nunique](pyspark.pandas.Index.nunique.html ""next page"")   "	To get unique values of a column in PySpark, you can use the `distinct()` method on the DataFrame column. For example, if you have a DataFrame `df` with a column named `col`, you can get the unique values of `col` using `df.select('col').distinct()`. Alternatively, you can use the `dropDuplicates()` method on the DataFrame, like this: `df.dropDuplicates(['col'])`.	1
no_reaction	['https://docs.databricks.com/runtime/mlruntime.html']	Create a cluster with the Databricks Runtime for Machine Learning.	      Introduction to Databricks Runtime for Machine Learning =========================================================     Databricks Runtime for Machine Learning (Databricks Runtime ML) provides pre-built machine learning infrastructure that is integrated with all of the capabilities of the Databricks workspace. Each version of Databricks Runtime ML is built on the corresponding version of Databricks Runtime. For example, Databricks Runtime 11.3 LTS for Machine Learning is built on Databricks Runtime 11.3 LTS.       For details about the capabilities of each version of Databricks Runtime ML, including the full list of included libraries, see the  [release notes](../release-notes/runtime/releases.html)  .          Why use Databricks Runtime for Machine Learning? --------------------------------------------------     Databricks Runtime ML automates the creation of a cluster optimized for machine learning. Some of the advantages of using Databricks Runtime ML clusters include:     * Built-in popular machine learning libraries, such as TensorFlow, PyTorch, Keras, and XGBoost. * Built-in distributed training libraries, such as Horovod. * Compatible versions of installed libraries. * Pre-configured GPU support including drivers and supporting libraries. * Faster cluster creation.     With Databricks, you can use any library to create the logic to train your model. The preconfigured Databricks Runtime ML makes it possible to easily scale common machine learning and deep learning steps.       Databricks Runtime ML also includes all of the capabilities of the Databricks workspace, such as:     * Data exploration, management, and governance. * Cluster creation and management. * Library and environment management. * Code management with Databricks Repos. * Automation support including Delta Live Tables, Databricks Jobs, and APIs. * Integrated MLflow for model development tracking, model deployment and serving, and real-time inference.     For complete information about using Databricks for machine learning and deep learning, see  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .        Warning       If you require HIPAA compliance, see  [HIPAA compliance features](../security/privacy/hipaa.html)  .            Tutorial: Databricks Runtime for Machine Learning ---------------------------------------------------     This tutorial is designed for new users of Databricks Runtime ML. It takes about 10 minutes to work through, and shows a complete end-to-end example of loading tabular data, training a model, distributed hyperparameter tuning, and model inference. It also illustrates how to use the MLflow API and MLflow Model Registry.        ###   Databricks tutorial notebook     [Open notebook in new tab](/_extras/notebooks/source/mlflow/mlflow-end-to-end-example.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                      Libraries included in Databricks Runtime ML ---------------------------------------------     The Databricks Runtime ML includes a variety of popular ML libraries. The libraries are updated with each release to include new features and fixes.       Databricks has designated a subset of the supported libraries as top-tier libraries. For these libraries, Databricks provides a faster update cadence, updating to the latest package releases with each runtime release (barring dependency conflicts). Databricks also provides advanced support, testing, and embedded optimizations for top-tier libraries.       For a full list of top-tier and other provided libraries, see the release notes for each runtime:     * [Databricks Runtime 12.2 LTS for Machine Learning](../release-notes/runtime/12.2ml.html) * [Databricks Runtime 12.1 for Machine Learning](../release-notes/runtime/12.1ml.html) * [Databricks Runtime 12.0 for Machine Learning](../release-notes/runtime/12.0ml.html) * [Databricks Runtime 11.3 LTS for Machine Learning](../release-notes/runtime/11.3ml.html) * [Databricks Runtime 11.2 for Machine Learning](../release-notes/runtime/11.2ml.html) * [Databricks Runtime 10.4 LTS for Machine Learning](../release-notes/runtime/10.4ml.html) * [Databricks Runtime 9.1 LTS for Machine Learning](../release-notes/runtime/9.1ml.html) * [Databricks Runtime 7.3 LTS for Machine Learning (Unsupported)](../release-notes/runtime/7.3ml.html)     For release notes for unsupported Databricks Runtime ML runtimes, see  [Unsupported releases](../release-notes/runtime/releases.html#unsupported-releases)  .           Create a cluster using Databricks Runtime ML ----------------------------------------------     When you  [create a cluster](../clusters/configure.html)  , select a Databricks Runtime ML version from the Databricks Runtime Version drop-down. Both CPU and GPU-enabled ML runtimes are available.      ![Select Databricks Runtime ML](../_images/mlruntime-dbr-dropdown.png)    If you  [select a cluster from the drop-down menu in the notebook](../notebooks/notebook-ui.html#attach-a-notebook-to-a-cluster)  , the Databricks Runtime version appears at the right of the cluster name:      ![View Databricks Runtime ML version](../_images/cluster-attach.png)    If you select a GPU-enabled ML runtime, you are prompted to select a compatible  **Driver Type**   and  **Worker Type**   . Incompatible instance types are grayed out in the drop-downs. GPU-enabled instance types are listed under the  **GPU-Accelerated**   label.        Important     * Libraries in your workspace that  [automatically install into all clusters](../libraries/cluster-libraries.html#install-libraries)  can conflict with the libraries included in Databricks Runtime ML. Before you create a cluster with Databricks Runtime ML, clear the  **Install automatically on all clusters**   checkbox for conflicting libraries. See  [the release notes](../release-notes/runtime/releases.html)  for a list of libraries that are included with each version of Databricks Runtime ML. * To access data in Unity Catalog for machine learning workflows, you must use a  [single user cluster](../data-governance/unity-catalog/compute.html#access-mode)  . User Isolation clusters are not compatible with Databricks Runtime ML.          Manage Python packages ------------------------     Databricks Runtime ML differs from Databricks Runtime in how you manage Python packages.       In Databricks Runtime 9.0 ML and above, the  [virtualenv](https://virtualenv.pypa.io/en/latest/)   package manager is used to install Python packages. All Python packages are installed inside a single environment:  `/databricks/python3`  .       In Databricks Runtime 8.4 ML and below, the  [Conda](https://conda.io/docs/)   package manager is used to install Python packages. All Python packages are installed inside a single environment:  `/databricks/python2`  on clusters using Python 2 and  `/databricks/python3`  on clusters using Python 3. Switching (or activating) Conda environments is not supported.       For information on managing Python libraries, see  [Libraries](../libraries/index.html)  .           Support for automated machine learning ----------------------------------------     Databricks Runtime ML includes tools to automate the model development process and help you efficiently find the best performing model.     * [AutoML](../machine-learning/automl/index.html)  automatically creates, tunes, and evaluates a set of models and creates a Python notebook with the source code for each run so you can review, reproduce, and modify the code. * [Managed MLflow](../mlflow/index.html)  manages the end-to-end model lifecycle, including tracking experimental runs, deploying and sharing models, and maintaining a centralized model registry. * [Hyperopt](../machine-learning/automl-hyperparam-tuning/index.html#hyperopt-overview)  , augmented with the  `SparkTrials`  class, automates and distributes ML model parameter tuning.       Limitations -------------     Databricks Runtime ML is not supported on:     * [TableACLs clusters](../data-governance/table-acls/index.html) * [“Shared” UC clusters](../data-governance/unity-catalog/compute.html#what-is-cluster-access-mode) * Clusters with  `spark.databricks.pyspark.enableProcessIsolation     config`  set to  `true`  .                                                      	To create a cluster with the Databricks Runtime for Machine Learning, you need to select a Databricks Runtime ML version from the Databricks Runtime Version drop-down when creating a cluster. Both CPU and GPU-enabled ML runtimes are available. If you select a GPU-enabled ML runtime, you are prompted to select a compatible Driver Type and Worker Type. Incompatible instance types are grayed out in the drop-downs. GPU-enabled instance types are listed under the GPU-Accelerated label.	1
no_reaction	['https://docs.databricks.com/administration-guide/account-settings/index.html']	how do i get the account id in databricks sql	    Manage your Databricks account ================================     The Databricks account configurations are managed by  [account admins](../index.html)  . The articles in this section cover tasks performed by account admins.       As an account admin, you can manage your Databricks account using the  [account console](#account-console)  , the  [Account API](../workspace/create-workspace-api.html)  , or  [Automation templates](../workspace/templates.html)  .            Manage your account using the Databricks account console ----------------------------------------------------------     The account console is where account admins manage Databricks account-level configurations.       Tasks performed through the account console include  [workspace creation](../workspace/create-workspace.html)  ,  [user management](../users-groups/users.html)  ,  [subscription management](account.html)  , and configuring cloud resources (such as  [credentials](../account-settings-e2/credentials.html)  ,  [storage](../account-settings-e2/storage.html)  , and  [networks](../account-settings-e2/networks.html)  ).      ###   Access the account console     Users can access the account console at  <https://accounts.cloud.databricks.com>  . Account admins can also access the account console from within a workspace:     1. Click your email address at the top of the Databricks workspace UI. 2. Select  **Manage Account**   .     ###   Change the account console language settings     The account console is available in multiple languages. To change the account console language, select  **Settings**   then go to the  **Language settings**   tab.            Locate your account ID ------------------------     To retrieve your account ID, go to the  [account console](#account-console)  and click the down arrow next to your username in the upper right corner. In the dropdown menu you can view and copy your  **Account ID**   .           Manage email preferences --------------------------     Databricks can occasionally send emails with personalized product and feature recommendations based on your use of Databricks. These messages may include information to help users get started with Databricks or learn about new features and previews.       You can manage whether you receive these emails in the account console:     1. Log in to the  [account console](#account-console)  and click the  **Settings**   icon in the sidebar. 2. In the  **My preferences**   section, click the  **Instructional product and feature emails**   toggle.     You can also manage your promotional email communications by clicking  **Manage**   under  **Promotional email communications**   or by going to the  [Marketing preference center](https://preferences.databricks.com/)   . Non-admin users can update this setting by clicking the  **My preferences**   link next to their workspace in the account console.                                                        	To retrieve your account ID in Databricks SQL, you can go to the account console and click the down arrow next to your username in the upper right corner. In the dropdown menu, you can view and copy your Account ID.	1
no_reaction	['https://docs.databricks.com/external-data/parquet.html']	How can I convert my files to parquet files?	    Parquet file ==============    [Apache Parquet](https://parquet.apache.org/)   is a columnar file format that provides optimizations to speed up queries. It is a far more efficient file format than  [CSV](csv.html)  or  [JSON](json.html)  .       For more information, see  [Parquet Files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#parquet(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#parquet(path:String):Unit)     The following notebook shows how to read and write data to Parquet files.        ###   Reading Parquet files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-parquet-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 	To convert files to Parquet files, you can use the supported write options in Apache Spark, which include the DataFrameWriter.parquet() method in Python and Scala. For more information on how to read and write data to Parquet files, you can refer to the provided notebook in the context.	1
no_reaction	['https://docs.databricks.com/archive/spark-3.x-migration/index.html']	How do I migrate to Spark 3.x?	      Migrate Apache Spark to 3.x =============================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [Databricks runtimes](../../runtime/index.html)  .        New Databricks Runtime versions include updated versions of Apache Spark. Apache Spark 3.0 was released in June 2020. The reference articles below are maintained in this archive for users who are upgrading to Apache Spark 3.x.      * [Data skipping index](dataskipping-index.html) * [Dates and timestamps](dates-timestamps.html) * [Deep Learning Pipelines migration guide](deep-learning-pipelines.html)                                                      	Based on the context, to migrate to Spark 3.x, you can refer to the reference articles provided in the archive for users who are upgrading to Apache Spark 3.x. These articles include information on data skipping index, dates and timestamps, and deep learning pipelines migration guide. However, it is important to note that this documentation has been retired and might not be updated. It is recommended to refer to the latest Databricks runtimes for updated versions of Apache Spark.	0
no_reaction	['https://docs.databricks.com/delta/data-skipping.html']	What is the difference between partitioning and zordering?	    Partitions ============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       A partition is composed of a subset of rows in a table that share the same value for a predefined subset of columns called the  *partitioning columns*   . Using partitions can speed up queries against the table as well as data manipulation.       To use partitions, you define the set of partitioning column when you create a table by including the  [PARTITIONED BY](#partitioned-by)  clause.       When inserting or manipulating rows in a table Databricks automatically dispatches rows into the appropriate partitions.       You can also specify the partition directly using a  [PARTITION](#partition)  clause.       This syntax is also available for tables that don’t use Delta Lake format, to DROP, ADD or RENAME partitions quickly by using the ALTER TABLE statement.        PARTITIONED BY ----------------     The  `PARTITIONED     BY`  clause specified a list of columns along which the new table is partitioned.      ###   Syntax      ``` PARTITIONED BY ( { partition\_column [ column\_type ] } [, ...] )  ```      ###   Parameters   * **[partition\_column](sql-ref-identifiers.html)**      An identifier may reference a  `column\_identifier`  in the table. If you specify more than one column there must be no duplicates. If you reference all columns in the table’s  `column\_specification`  an error is raised. * **[column\_type](sql-ref-datatypes.html)**      Unless the  `partition\_column`  refers to a  `column\_identifier`  in the table’s  `column\_specification`  ,  `column\_type`  defines the data type of the  `partition\_column`  .       Not all  [data types](sql-ref-datatypes.html)  supported by Databricks SQL are supported by all data sources.     ###   Notes     Unless you define a Delta Lake table partitioning columns referencing the columns in the column specification are always moved to the end of the table.          PARTITION -----------     You use the  `PARTITION`  clause to identify a partition to be queried or manipulated.       A partition is identified by naming all its columns and associating each with a value. You need not specify them in a specific order.       Unless you are adding a new partition to an existing table you may omit columns or values to indicate that the operation applies to the all matching partitions matching the subset of columns.        ``` PARTITION ( { partition\_column  [ = partition\_value | LIKE pattern ] } [ , ... ] )  ```       ###   Parameters   * **[partition\_column](sql-ref-identifiers.html)**      A column named as a partition column of the table. You may not specify the same column twice. * **`=     partition\_value`**      A literal of a data type matching the type of the partition column. If you omit a partition value the specification will match all values for this partition column. * **`LIKE     pattern`**      This form is only allowed in  [ALTER SHARE ADD TABLE](sql-ref-syntax-ddl-alter-share.html)  .       Matches the string representation of  `partition\_column`  to  `pattern`  .  `pattern`  must be a string literal as used in  [LIKE](functions/like.html)  .        Examples ----------      ``` -- Use the PARTTIONED BY clause in a table definition > CREATE TABLE student(university STRING,  major STRING,  name STRING)  PARTITIONED BY(university, major)  > CREATE TABLE professor(name STRING)  PARTITIONED BY(university STRING,  department STRING);  -- Use the PARTITION specification to INSERT into a table > INSERT INTO student  PARTITION(university= 'TU Kaiserslautern') (major, name)  SELECT major, name FROM freshmen;  -- Use the partition specification to add and drop a partition > CREATE TABLE log(date DATE, id INT, event STRING)  USING CSV LOCATION 'dbfs:/log'  PARTITIONED BY (date);  > ALTER TABLE log ADD PARTITION(date = DATE'2021-09-10');  > ALTER TABLE log DROP PARTITION(date = DATE'2021-09-10');  -- Drop all partitions from the named university, independent of the major. > ALTER TABLE student DROP PARTITION(university = 'TU Kaiserslautern');  ```                                                       	Partitioning is the process of dividing a table into smaller, more manageable parts called partitions, based on a predefined subset of columns called partitioning columns. Z-ordering is a technique used to optimize data access by grouping related data together in the same partition based on their spatial proximity.	0
no_reaction	['https://docs.databricks.com/dev-tools/cli/dlt-cli.html']	sample code for delta live table	"      Delta Live Tables CLI =======================     You run Databricks Delta Live Tables CLI subcommands by appending them to  `databricks     pipelines`  . These subcommands call the  [Delta Live Tables API](../../delta-live-tables/api-guide.html)  .        ``` databricks pipelines -h  ```       ``` Usage: databricks pipelines [OPTIONS] COMMAND [ARGS]...    Utility to interact with Databricks Delta Live Tables Pipelines.  Options:   -v, --version   0.17.0   --debug         Debug Mode. Shows full stack trace on error.   --profile TEXT  CLI connection profile to use. The default profile is                   ""DEFAULT"".    -h, --help      Show this message and exit.  Commands:   create  Creates a pipeline.   delete  Deletes the pipeline and cancels any active updates.   deploy  [Deprecated] This command is deprecated, use create and edit           commands instead.  Creates or edits a pipeline specified by the           pipeline settings.    edit    Edits a pipeline.   get     Gets a pipeline's current settings and status.   list    Lists all pipelines and their statuses.   reset   [Deprecated] Use the ""start --full-refresh"" command instead. Resets           a pipeline so that data can be reprocessed from the beginning.    run     [Deprecated] Use the ""start"" command instead. Starts a pipeline           update.    start   Starts a pipeline update.   stop    Stops the pipeline by cancelling any active update.  ```       Create a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     create     --help`  .      ###   Usage      ``` databricks pipelines create --settings create-pipeline.json  ```     `create-pipeline.json`  :        ``` {  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 5  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_analysis"",  ""continuous"": false }  ```       ``` Successfully created pipeline: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003 with ID: 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Edit a pipeline -----------------     To display usage documentation, run  `databricks     pipelines     edit     --help`  .        ###   Usage      ``` databricks pipelines edit --settings edit-pipeline.json  ```     `edit-pipeline.json`  :        ``` {  ""id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 3  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_quickstart\_data"",  ""continuous"": false }  ```       ``` Successfully edited pipeline settings: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Delete a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     delete     --help`  .        ###   Usage      ``` databricks pipelines delete --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003 deleted  ```         Get information about a pipeline ----------------------------------     To display usage documentation, run  `databricks     pipelines     get     --help`  .        ###   Usage      ``` databricks pipelines get --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` {  ""pipeline\_id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""spec"": {  ""id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 5  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_analysis"",  ""continuous"": false  },  ""state"": ""IDLE"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""creator\_user\_name"": ""username@databricks.com"",  ""last\_modified"": 1641600056265 }  ```         List all pipelines and information on their status ----------------------------------------------------     To display usage documentation, run  `databricks     pipelines     list     --help`  .        ###   Usage      ``` databricks pipelines list  ```       ``` [  {  ""pipeline\_id"": ""8a93e1b4-7024-11ec-90d6-0242ac120003"",  ""state"": ""IDLE"",  ""name"": ""Retail sales pipeline (SQL)"",  ""latest\_updates"": [  {  ""update\_id"": ""9df03924-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T16:18:57.982Z""  },  {  ""update\_id"": ""a57e601c-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-28T18:19:30.371Z""  }  ],  ""creator\_user\_name"": ""user@databricks.com""  },  {  ""pipeline\_id"": ""b46e2670-7024-11ec-90d6-0242ac120003"",  ""state"": ""IDLE"",  ""name"": ""DLT quickstart (Python)"",  ""latest\_updates"": [  {  ""update\_id"": ""bcd8fa2e-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-12-16T18:19:25.827Z""  },  {  ""update\_id"": ""c2c7a2c8-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T22:22:32.586Z""  },  {  ""update\_id"": ""c88d2818-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T22:16:18.258Z""  }  ],  ""creator\_user\_name"": ""user@databricks.com""  } ]  ```         Start a pipeline update -------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Started an update c6462e2e-beb7-4c24-a9d3-962eaa19781b for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```        Start a pipeline update with full refresh -------------------------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ###   Usage      ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003 --full-refresh  ```       ``` Started an update ed885d3d-4fc0-44fd-b5dd-59471c789327 with full refresh for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Stop a pipeline update ------------------------     To display usage documentation, run  `databricks     pipelines     stop     --help`  .        ###   Usage      ``` databricks pipelines stop --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Stopped pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```                                                        "	This document provides information on how to use the Delta Live Tables CLI to create, edit, delete, get information about, list, start, and stop pipelines. It also includes usage examples for each of these subcommands.	1
no_reaction	['https://docs.databricks.com/machine-learning/feature-store/time-series.html']	Does FeatureStoreClient().score_batch support multidimentional predictions? I have a pyfunc model that I can use to get predictions. It takes time series data with context information at each date, and produces a string of predictions. For example:     The data is set up like below (temp/pressure/output are different than my input data columns)  date,sales,temperature,pressure,output 01-09-2020,100,101,5000,10 01-10-2020,120,91,4000,24 01-11-2020,50,110,6000,30 let's say the model is trained using a window size of 60 and prediction inverval of 14. then in the model, you provide provide 60 records, and the prediction will get 14 predictions starting the last date+1 in your prediction dataset.     the return is just of the form  date,prediction 01-12-2022,81 01-13-2022,60 01-14-2022,111 ... with N records (in our example 14). it works brilliantly if I augment the data myself and work with the predict function.     Does working with the feature store support this? score batch doesn't seem to be able to return arbitrary/different shaped data. I could try making the data wide, but then that would defeat the purpose of trying to use the feature store.     don't know if I'm making sense.	"   pyspark.ml.functions.predict\_batch\_udf  [¶](#pyspark-ml-functions-predict-batch-udf ""Permalink to this headline"") =====================================================================================================================  `pyspark.ml.functions.`  `predict_batch_udf`    (   *make\_predict\_fn     :     Callable    [     ]     ,    PredictBatchFunction    ]*  ,  *\**  ,  *return\_type     :     DataType*  ,  *batch\_size     :     int*  ,  *input\_tensor\_shapes     :     Optional    [    Union    [    List    [    Optional    [    List    [    int    ]     ]     ]     ,    Mapping    [    int    ,    List    [    int    ]     ]     ]     ]     =     None*   )    → UserDefinedFunctionLike  [[source]](../../_modules/pyspark/ml/functions.html#predict_batch_udf) [¶](#pyspark.ml.functions.predict_batch_udf ""Permalink to this definition"")    Given a function which loads a model and returns a    predict    function for inference over a batch of numpy inputs, returns a Pandas UDF wrapper for inference over a Spark DataFrame.     The returned Pandas UDF does the following on each DataFrame partition:    * calls the    make\_predict\_fn    to load the model and cache its    predict    function. * batches the input records as numpy arrays and invokes    predict    on each batch.   Note: this assumes that the    make\_predict\_fn    encapsulates all of the necessary dependencies for running the model, or the Spark executor environment already satisfies all runtime requirements.     For the conversion of the Spark DataFrame to numpy arrays, there is a one-to-one mapping between the input arguments of the    predict    function (returned by the    make\_predict\_fn    ) and the input columns sent to the Pandas UDF (returned by the    predict\_batch\_udf    ) at runtime. Each input column will be converted as follows:    * scalar column -> 1-dim np.ndarray * tensor column + tensor shape -> N-dim np.ndarray   Note that any tensor columns in the Spark DataFrame must be represented as a flattened one-dimensional array, and multiple scalar columns can be combined into a single tensor column using the standard  [`pyspark.sql.functions.array()`](../pyspark.sql/api/pyspark.sql.functions.array.html#pyspark.sql.functions.array ""pyspark.sql.functions.array"")  function.     New in version 3.4.0.     Parameters    **make\_predict\_fn**    callable     Function which is responsible for loading a model and returning a  `PredictBatchFunction`  which takes one or more numpy arrays as input and returns one of the following:    * a numpy array (for a single output) * a dictionary of named numpy arrays (for multiple outputs) * a row-oriented list of dictionaries (for multiple outputs).   For a dictionary of named numpy arrays, the arrays can only be one or two dimensional, since higher dimensional arrays are not supported. For a row-oriented list of dictionaries, each element in the dictionary must be either a scalar or one-dimensional array.    **return\_type**   [`pyspark.sql.types.DataType`](../pyspark.sql/api/pyspark.sql.types.DataType.html#pyspark.sql.types.DataType ""pyspark.sql.types.DataType"")  or str.     Spark SQL datatype for the expected output:    * Scalar (e.g. IntegerType, FloatType) –> 1-dim numpy array. * ArrayType –> 2-dim numpy array. * StructType –> dict with keys matching struct fields. * StructType –> list of dict with keys matching struct fields, for models like the  [Huggingface pipeline for sentiment analysis](https://huggingface.co/docs/transformers/quicktour#pipeline-usage)   .  **batch\_size**    int     Batch size to use for inference. This is typically a limitation of the model and/or available hardware resources and is usually smaller than the Spark partition size.    **input\_tensor\_shapes**    list, dict, optional.     A list of ints or a dictionary of ints (key) and list of ints (value). Input tensor shapes for models with tensor inputs. This can be a list of shapes, where each shape is a list of integers or None (for scalar inputs). Alternatively, this can be represented by a “sparse” dictionary, where the keys are the integer indices of the inputs, and the values are the shapes. Each tensor input value in the Spark DataFrame must be represented as a single column containing a flattened 1-D array. The provided    input\_tensor\_shapes    will be used to reshape the flattened array into the expected tensor shape. For the list form, the order of the tensor shapes must match the order of the selected DataFrame columns. The batch dimension (typically -1 or None in the first dimension) should not be included, since it will be determined by the batch\_size argument. Tabular datasets with scalar-valued columns should not provide this argument.     Returns    `UserDefinedFunctionLike`   A Pandas UDF for model inference on a Spark DataFrame.     Examples     For a pre-trained TensorFlow MNIST model with two-dimensional input images represented as a flattened tensor value stored in a single Spark DataFrame column of type    array<float>    .    ``` from pyspark.ml.functions import predict\_batch\_udf  def make\_mnist\_fn():     # load/init happens once per python worker     import tensorflow as tf     model = tf.keras.models.load\_model('/path/to/mnist\_model')      # predict on batches of tasks/partitions, using cached model     def predict(inputs: np.ndarray) -> np.ndarray:         # inputs.shape = [batch\_size, 784], see input\_tensor\_shapes         # outputs.shape = [batch\_size, 10], see return\_type         return model.predict(inputs)      return predict  mnist\_udf = predict\_batch\_udf(make\_mnist\_fn,                               return\_type=ArrayType(FloatType()),                               batch\_size=100,                               input\_tensor\_shapes=[[784]])  df = spark.read.parquet(""/path/to/mnist\_data"") df.show(5) # +--------------------+ # | data| # +--------------------+ # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # +--------------------+  df.withColumn(""preds"", mnist\_udf(""data"")).show(5) # +--------------------+--------------------+ # | data| preds| # +--------------------+--------------------+ # |[0.0, 0.0, 0.0, 0...|[-13.511008, 8.84...| # |[0.0, 0.0, 0.0, 0...|[-5.3957458, -2.2...| # |[0.0, 0.0, 0.0, 0...|[-7.2014456, -8.8...| # |[0.0, 0.0, 0.0, 0...|[-19.466187, -13....| # |[0.0, 0.0, 0.0, 0...|[-5.7757926, -7.8...| # +--------------------+--------------------+  ```   To demonstrate usage with different combinations of input and output types, the following examples just use simple mathematical transforms as the models.    * Single scalar column     Input DataFrame has a single scalar column, which will be passed to the    predict    function as a 1-D numpy array.    ``` >>> import numpy as np >>> import pandas as pd >>> from pyspark.ml.functions import predict\_batch\_udf >>> from pyspark.sql.types import FloatType >>> >>> df = spark.createDataFrame(pd.DataFrame(np.arange(100))) >>> df.show(5) +---+ | 0| +---+ | 0| | 1| | 2| | 3| | 4| +---+ only showing top 5 rows  ```  ``` >>> def make\_times\_two\_fn(): ...     def predict(inputs: np.ndarray) -> np.ndarray: ...         # inputs.shape = [batch\_size] ...         # outputs.shape = [batch\_size] ...         return inputs \* 2 ...     return predict >>> >>> times\_two\_udf = predict\_batch\_udf(make\_times\_two\_fn, ...                                   return\_type=FloatType(), ...                                   batch\_size=10) >>> >>> df = spark.createDataFrame(pd.DataFrame(np.arange(100))) >>> df.withColumn(""x2"", times\_two\_udf(""0"")).show(5) +---+---+ | 0| x2| +---+---+ | 0|0.0| | 1|2.0| | 2|4.0| | 3|6.0| | 4|8.0| +---+---+ only showing top 5 rows  ``` * Multiple scalar columns     Input DataFrame has muliple columns of scalar values. If the user-provided    predict    function expects a single input, then the user must combine the multiple columns into a single tensor using    pyspark.sql.functions.array    .    ``` >>> import numpy as np >>> import pandas as pd >>> from pyspark.ml.functions import predict\_batch\_udf >>> from pyspark.sql.functions import array >>> >>> data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4) >>> pdf = pd.DataFrame(data, columns=['a','b','c','d']) >>> df = spark.create"	Based on the context, it seems that `score_batch` does not support multidimensional predictions. The user has a pyfunc model that produces a string of predictions for time series data with context information at each date. The model is trained using a window size of 60 and prediction interval of 14. The return is of the form date, prediction with N records (in the example 14). The user is wondering if working with the feature store supports this, but `score_batch` doesn't seem to be able to return arbitrary/different shaped data. The user could try making the data wide, but then that would defeat the purpose of trying to use the feature store.	0
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/enable-workspaces.html']	How do I use unity catalog to share metastores between workspaces?	      Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .                                                      	To use Unity Catalog to share metastores between workspaces, you need to create a metastore in Unity Catalog and link it to workspaces. To create a metastore, you must be a Databricks account admin, and your Databricks account must be on the Premium plan or above. You also need to configure a storage bucket and IAM role in AWS, and create the metastore in the same region as the workspaces you want to use to access the data. Once you have created the metastore, you can link workspaces to it. For more information about linking workspaces to metastores, see Enable a workspace for Unity Catalog.	1
no_reaction	['https://docs.databricks.com/ingestion/auto-loader/index.html']	autoloader	      What is Auto Loader? ======================     Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup.            How does Auto Loader work? ----------------------------     Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage. Auto Loader can load data files from AWS S3 (  `s3://`  ), Azure Data Lake Storage Gen2 (ADLS Gen2,  `abfss://`  ), Google Cloud Storage (GCS,  `gs://`  ), Azure Blob Storage (  `wasbs://`  ), ADLS Gen1 (  `adl://`  ), and Databricks File System (DBFS,  `dbfs:/`  ). Auto Loader can ingest  `JSON`  ,  `CSV`  ,  `PARQUET`  ,  `AVRO`  ,  `ORC`  ,  `TEXT`  , and  `BINARYFILE`  file formats.       Auto Loader provides a Structured Streaming source called  `cloudFiles`  . Given an input directory path on the cloud file storage, the  `cloudFiles`  source automatically processes new files as they arrive, with the option of also processing existing files in that directory. Auto Loader has support for both Python and SQL in Delta Live Tables.       You can use Auto Loader to process billions of files to migrate or backfill a table. Auto Loader scales to support near real-time ingestion of millions of files per hour.           How does Auto Loader track ingestion progress? ------------------------------------------------     As files are discovered, their metadata is persisted in a scalable key-value store (RocksDB) in the  *checkpoint location*   of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once.       In case of failures, Auto Loader can resume from where it left off by information stored in the checkpoint location and continue to provide exactly-once guarantees when writing data into Delta Lake. You don’t need to maintain or manage any state yourself to achieve fault tolerance or exactly-once semantics.           Tutorial: ingesting data with Databricks Auto Loader ------------------------------------------------------     Databricks recommends Auto Loader in Delta Live Tables for incremental data ingestion. Delta Live Tables extends functionality in Apache Spark Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline.       Databricks recommends Auto Loader whenever you use Apache Spark Structured Streaming to ingest data from cloud object storage. APIs are available in Python and Scala.       To get started using Auto Loader, see:     * [Using Auto Loader in Delta Live Tables](dlt.html) * [Run your first ETL workload on Databricks](../../getting-started/etl-quick-start.html)     For examples of commonly used patterns, see:     * [Common data loading patterns](patterns.html)       Configure Auto Loader options -------------------------------     You can tune Auto Loader based on data volume, variety, and velocity.     * [Configure schema inference and evolution in Auto Loader](schema.html) * [Configure Auto Loader for production workloads](production.html)     For a full list of Auto Loader options, see:     * [Auto Loader options](options.html)     If you encounter unexpected performance, see the  [FAQ](faq.html)  .         Configure Auto Loader file detection modes --------------------------------------------     Auto Loader supports two  [file detection modes](file-detection-modes.html)  . See:     * [What is Auto Loader directory listing mode?](directory-listing-mode.html) * [What is Auto Loader file notification mode?](file-notification-mode.html)         Benefits of Auto Loader over using Structured Streaming directly on files ---------------------------------------------------------------------------     In Apache Spark, you can read files incrementally using  `spark.readStream.format(fileFormat).load(directory)`  . Auto Loader provides the following benefits over the file source:     * Scalability: Auto Loader can discover billions of files efficiently. Backfills can be performed asynchronously to avoid wasting any compute resources. * Performance: The cost of discovering files with Auto Loader scales with the number of files that are being ingested instead of the number of directories that the files may land in. See  [What is Auto Loader directory listing mode?](directory-listing-mode.html)  . * Schema inference and evolution support: Auto Loader can detect schema drifts, notify you when schema changes happen, and rescue data that would have been otherwise ignored or lost. See  [How does Auto Loader schema inference work?](schema.html#schema-inference)  . * Cost: Auto Loader uses native cloud APIs to get lists of files that exist in storage. In addition, Auto Loader’s file notification mode can help reduce your cloud costs further by avoiding directory listing altogether. Auto Loader can automatically set up file notification services on storage to make file discovery much cheaper.                                                      	What is Auto Loader and how does it work?	1
no_reaction	['https://docs.databricks.com/dev-tools/api/index.html']	Where can I download API specs for all Databricks APIs?	"    Databricks REST API reference ===============================     Databricks has three REST APIs that perform different tasks:     * [2.0](2.0/index.html)  and  [2.1](2.1/index.html)  for general administration * [1.2](1.2/index.html)  for running commands directly on Databricks             For the latest version of all REST APIs, see  [REST API (latest)](latest/index.html)  . You can also jump directly to the REST API home pages for each version:  [2.1](2.1/index.html)  ,  [2.0](2.0/index.html)  , or  [1.2](1.2/index.html)  .        Important       To access Databricks REST APIs, you must  [authenticate](latest/authentication.html)  .        * [REST API Explorer (Beta)](/api-explorer?http://) * Account API  [2.0](latest/account.html) * Clusters API  [2.0](latest/clusters.html) * Cluster Policies API  [2.0](latest/policies.html) * Data Lineage API  [2.0](../../data-governance/unity-catalog/data-lineage.html) * Databricks SQL Queries, Dashboards, and Alerts API  [2.0](../../sql/api/queries-dashboards.html) * Databricks SQL Query History API  [2.0](../../sql/api/query-history.html) * Databricks SQL Statement Execution API  [2.0](../../sql/api/statements.html) * Databricks SQL Warehouses API  [2.0](../../sql/api/sql-endpoints.html) * DBFS API  [2.0](latest/dbfs.html) * Delta Live Tables API  [2.0](../../delta-live-tables/api-guide.html) * Git Credentials API  [2.0](latest/gitcredentials.html) * Global Init Scripts API  [2.0](latest/global-init-scripts.html) * Groups API  [2.0](latest/groups.html) * Instance Pools API  [2.0](latest/instance-pools.html) * Instance Profiles API  [2.0](latest/instance-profiles.html) * IP Access List API  [2.0](latest/ip-access-list.html) * Jobs API  [2.1](latest/jobs.html)  ,  [2.0](2.0/jobs.html) * Libraries API  [2.0](latest/libraries.html) * MLflow API  [2.0](latest/mlflow.html) * Permissions API  [2.0](latest/permissions.html) * Repos API  [2.0](latest/repos.html) * SCIM API  [2.0](latest/scim/index.html) * Secrets API  [2.0](latest/secrets.html) * Serving Endpoints API  [2.0](latest/endpoints.html) * Token API  [2.0](latest/tokens.html) * Token Management API  [2.0](latest/token-management.html) * Unity Catalog API  [2.1](https://api-docs.databricks.com/rest/latest/unity-catalog-api-specification-2-1.html)   ,  [2.0](https://api-docs.databricks.com/rest/latest/unity-catalog-api-specification-2-0.html) * Workspace API  [2.0](latest/workspace.html) * API  [1.2](1.2/index.html)      Authentication ----------------     For information about authenticating to the REST API, see  [Authentication using Databricks personal access tokens](latest/authentication.html)  . For API examples, see  [API examples](latest/examples.html)  .         Rate limits -------------     To ensure high quality of service under heavy load, Databricks enforces rate limits for all REST API calls. Limits are set per endpoint and per workspace to ensure fair usage and high availability.       Requests that exceed the rate limit return a  [429 response status code](https://developer.mozilla.org/docs/Web/HTTP/Status/429)   .       For information on rate limits for API requests, see  [API rate limits](../../resources/limits.html#limits-api-rate-limits)  .         Parse output --------------     It can be useful to parse out parts of the JSON output. Databricks recommends the utility  `jq`  for parsing JSON. You can install  `jq`  on Linux through  [jq Releases](https://github.com/stedolan/jq/releases/latest)   , on macOS using Homebrew with  `brew     install     jq`  , or on Windows using Chocolatey with  `choco     install     jq`  . For more information on  `jq`  , see the  [jq Manual](https://stedolan.github.io/jq/manual/)   .       This example lists the names and IDs of available clusters in the specified workspace. This example uses a  [.netrc file](latest/authentication.html#netrc)  .        ``` curl --netrc -X GET https://abc-d1e2345f-a6b2.cloud.databricks.com/api/2.0/clusters/list \ | jq '[ .clusters[] | { id: .cluster\_id, name: .cluster\_name } ]'  ```       ``` [  {  ""id"": ""1234-567890-batch123"",  ""name"": ""My Cluster 1""  },  {  ""id"": ""2345-678901-rigs234"",  ""name"": ""My Cluster 2""  } ]  ```        Compatibility ---------------     Responses for the same API version will not remove any field from the JSON output. However, the API might add new fields to the JSON output without incrementing the API version. Your programmatic workflows must be aware of these additions and ignore unknown fields.       Some  `STRING`  fields (which contain error and descriptive messaging intended to be consumed by the UI) are unstructured, and you should not depend on the format of these fields in programmatic workflows.         Use curl to invoke the Databricks REST API --------------------------------------------    [curl](https://curl.se/)   is a popular tool for transferring data to and from servers. This section provides specific information about using curl to invoke the Databricks REST API.      ###   Invoke a GET using a query string     While most API calls require that you specify a JSON body, for  `GET`  calls you can specify a query string by appending it after  `?`  and surrounding the URL in quotes. If you use  `curl`  , you can specify  `--get`  (or  `-G`  ) and  `--data`  (or  `-d`  ) along with the query string; you do not need to surround the URL or the query string in quotes.       In the following examples, replace  `abc-d1e2345f-a6b2.cloud.databricks.com`  with the  [workspace URL](../../workspace/workspace-details.html#workspace-url)  of your Databricks deployment.       This example prints information about the specified cluster. This example uses a  [.netrc file](latest/authentication.html#netrc)  .       Using  `?`  :        ``` curl --netrc 'https://abc-d1e2345f-a6b2.cloud.databricks.com/api/2.0/clusters/get?cluster\_id=1234-567890-patch123'  ```      Using  `--get`  and  `--data`  :        ``` curl --netrc --get \ https://abc-d1e2345f-a6b2.cloud.databricks.com/api/2.0/clusters/get \ --data cluster\_id=1234-567890-patch123  ```       ``` {  ""cluster\_id"": ""1234-567890-patch123"",  ""spark\_context\_id"": 123456789012345678,  ""cluster\_name"": ""job-239-run-1"",  ""spark\_version"": ""8.1.x-scala2.12"",  ... }  ```      This example lists the contents of the DBFS root. This example uses a  [.netrc file](latest/authentication.html#netrc)  .        ``` curl --netrc --get \ https://abc-d1e2345f-a6b2.cloud.databricks.com/api/2.0/dbfs/list \ --data path=/  ```       ``` ""files"": [  {  ""path"": ""/tmp"",  ""is\_dir"": true,  ""file\_size"": 0,  ""modification\_time"": 1547078156000  },  {  ""path"": ""/my\_file.txt"",  ""is\_dir"": false,  ""file\_size"": 40,  ""modification\_time"": 1541374426000  },  ... ]  ```           Use Python to invoke the Databricks REST API ----------------------------------------------     To call the Databricks REST API with Python, you can use the  [Databricks CLI package](https://github.com/databricks/databricks-cli)   as a library. This package is written in Python and enables you to call the Databricks REST API through Python classes that closely model the Databricks REST API request and response payloads. For more information, see  [Call the Databricks REST API with Python](../python-api.html)  .       Direct use of the Python  `requests`  library is another approach. However, you would need to work at a lower level, manually providing the necessary headers, handling errors, and other related low-level coding tasks.      [requests](https://docs.python-requests.org/)   is a popular library for making HTTP requests in Python. This example uses the  `requests`  library to list information about the specified Databricks cluster. This example uses a  [.netrc file](latest/authentication.html#netrc)  .        ``` import requests import json  instance\_id = 'dbc-a1b2345c-d6e7.cloud.databricks.com'  api\_version = '/api/2.0' api\_command = '/clusters/get' url = f""https://{instance\_id}{api\_version}{api\_command}""  params = {   'cluster\_id': '1234-567890-batch123' }  response = requests.get(   url = url,   params = params )  print(json.dumps(json.loads(response.text), indent = 2))  ```       ``` {  ""cluster\_id"": ""1234-567890-batch123"",  ""spark\_context\_id"": 1234567890123456789,  ... }  ```          Use Postman to invoke the Databricks REST API -----------------------------------------------   1. In the  [Postman app](https://www.postman.com/downloads/)   , create a new HTTP request (  **File**   >  **New**   >  **HTTP Request**   ). 2. In the HTTP verb drop-down list, select the verb that matches the REST API operation you want to call. For example, to list information about a Databricks cluster, select  **GET**   . 3. For  **Enter request URL**   , begin by entering  `https://<databricks-instance-name>`  , where  `<databricks-instance-name"	You can download API specs for all Databricks APIs from the Databricks REST API reference page. The reference page provides a list of all REST APIs, including the latest version of all REST APIs, and you can jump directly to the REST API home pages for each version. You can find the link to the reference page in the context.	1
no_reaction	['https://docs.databricks.com/ingestion/auto-loader/schema.html']	databricks autoloader schema inference	"      Configure schema inference and evolution in Auto Loader =========================================================     You can configure Auto Loader to automatically detect the schema of loaded data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time.       Auto Loader can also “rescue” data that was unexpected (for example, of differing data types) in a JSON blob column, that you can choose to access later using the  [semi-structured data access APIs](../../optimizations/semi-structured.html)  .       The following formats are supported for schema inference and evolution:          |   File format   |   Supported versions   | | --- | --- | |  `JSON`  |   Databricks Runtime 8.2 and above   | |  `CSV`  |   Databricks Runtime 8.3 and above   | |  `Avro`  |   Databricks Runtime 10.2 and above   | |  `Parquet`  |   Databricks Runtime 11.1 and above   | |  `ORC`  |   Unsupported   | |  `Text`  |   Not applicable (fixed-schema)   | |  `Binaryfile`  |   Not applicable (fixed-schema)   |      Syntax for schema inference and evolution -------------------------------------------     Specifying a target directory for the option  `cloudFiles.schemaLocation`  enables schema inference and evolution. You can choose to use the same directory you specify for the  `checkpointLocation`  . If you use  [Delta Live Tables](../../delta-live-tables/index.html)  , Databricks manages schema location and other checkpoint information automatically.        Note       If you have more than one source data location being loaded into the target table, each Auto Loader ingestion workload requires a separate streaming checkpoint.        The following example uses  `parquet`  for the  `cloudFiles.format`  . Use  `csv`  ,  `avro`  , or  `json`  for other file sources. All other settings for read and write stay the same for the default behaviors for each format.         ``` (spark.readStream.format(""cloudFiles"")   .option(""cloudFiles.format"", ""parquet"")   # The schema location directory keeps track of your data schema over time   .option(""cloudFiles.schemaLocation"", ""<path\_to\_checkpoint>"")   .load(""<path\_to\_source\_data>"")   .writeStream   .option(""checkpointLocation"", ""<path\_to\_checkpoint>"")   .start(""<path\_to\_target"") )  ```       ``` spark.readStream.format(""cloudFiles"")  .option(""cloudFiles.format"", ""parquet"")  // The schema location directory keeps track of your data schema over time  .option(""cloudFiles.schemaLocation"", ""<path\_to\_checkpoint>"")  .load(""<path\_to\_source\_data>"")  .writeStream  .option(""checkpointLocation"", ""<path\_to\_checkpoint>"")  .start(""<path\_to\_target"")  ```             How does Auto Loader schema inference work? ---------------------------------------------     To infer the schema when first reading data, Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first. Auto Loader stores the schema information in a directory  `\_schemas`  at the configured  `cloudfFiles.schemaLocation`  to track schema changes to the input data over time.        Note       To change the size of the sample that’s used you can set the SQL configurations:        ``` spark.databricks.cloudFiles.schemaInference.sampleSize.numBytes  ```      (byte string, for example  `10gb`  )       and        ``` spark.databricks.cloudFiles.schemaInference.sampleSize.numFiles  ```      (integer)        By default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don’t encode data types (JSON and CSV), Auto Loader infers all columns as strings (including nested fields in JSON files). For formats with typed schema (Parquet and Avro), Auto Loader samples a subset of files and merges the schemas of individual files. This behavior is summarized in the following table:          |   File format   |   Default inferred data type   | | --- | --- | |  `JSON`  |   String   | |  `CSV`  |   String   | |  `Avro`  |   Types encoded in Avro schema   | |  `Parquet`  |   Types encoded in Parquet schema   |     The Apache Spark DataFrameReader uses different behavior for schema inference, selecting data types for columns in JSON and CSV sources based on sample data. To enable this behavior with Auto Loader, set the option  `cloudFiles.inferColumnTypes`  to  `true`  .        Note       When inferring schema for CSV data, Auto Loader assumes that the files contain headers. If your CSV files do not contain headers, provide the option  `.option(""header"",     ""false"")`  . In addition, Auto Loader merges the schemas of all the files in the sample to come up with a global schema. Auto Loader can then read each file according to its header and parse the CSV correctly.         Note       When a column has different data types in two Parquet files, Auto Loader attempts to  `upcast`  one type to the other. If upcasting is not possible, data inference fails. See the following table for examples:           |   Type 1   |   Type 2   |   Upcast type   | | --- | --- | --- | |  `int`  |  `double`  |  `double`  | |  `double`  |  `string`  |  `string`  | |  `int`  |  `struct`  |   Inference fails   |     After merging data types on inference, files containing records of the unselected type are loaded to the  [rescued data column](#rescue)  , because the data type is different from the inferred schema.              How does Auto Loader schema evolution work? ---------------------------------------------     Auto Loader detects the addition of new columns as it processes your data. When Auto Loader detects a new column, the stream stops with an  `UnknownFieldException`  . Before your stream throws this error, Auto Loader performs schema inference on the latest micro-batch of data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged.       Databricks recommends configuring Auto Loader streams with  [workflows](../../workflows/index.html)  to restart automatically after such schema changes.       Auto Loader supports the following modes for schema evolution, which you set in the option  `cloudFiles.schemaEvolutionMode`  :          |   Mode   |   Behavior on reading new column   | | --- | --- | |  `addNewColumns`  (default)   |   Stream fails. New columns are added to the schema. Existing columns do not evolve data types.   | |  `rescue`  |   Schema is never evolved and stream does not fail due to schema changes. All new columns are recorded in the  [rescued data column](#rescue)  .   | |  `failOnNewColumns`  |   Stream fails. Stream does not restart unless the provided schema is updated, or the offending data file is removed.   | |  `none`  |   Does not evolve the schema, new columns are ignored, and data is not rescued unless the  `rescuedDataColumn`  option is set. Stream does not fail due to schema changes.   |         How do partitions work with Auto Loader? ------------------------------------------     Auto Loader attempts to infer partition columns from the underlying directory structure of the data if the data is laid out in Hive style partitioning. For example, the file path  `base\_path/event=click/date=2021-04-01/f0.json`  results in the inference of  `date`  and  `event`  as partition columns. If the underlying directory structure contains conflicting Hive partitions or doesn’t contain Hive style partitioning, partition columns are ignored.       Binary file (  `binaryFile`  ) and  `text`  file formats have fixed data schemas, but support partition column inference. Databricks recommends setting  `cloudFiles.schemaLocation`  for these file formats. This avoids any potential errors or information loss and prevents inference of partitions columns each time an Auto Loader begins.       Partition columns are not considered for schema evolution. If you had an initial directory structure like  `base\_path/event=click/date=2021-04-01/f0.json`  , and then start receiving new files as  `base\_path/event=click/date=2021-04-01/hour=01/f1.json`  , Auto Loader ignores the hour column. To capture information for new partition columns, set  `cloudFiles.partitionColumns`  to  `event,date,hour`  .        Note       The option  `cloudFiles.partitionColumns`  takes a comma-separated list of column names. Only columns that exist as  `key=value`  pairs in your directory structure are parsed.            What is the rescued data column? ----------------------------------     When Auto Loader infers the schema, a rescued data column is automatically added to your schema as  `\_rescued\_data`  . You can rename the column or include it in cases where you provide a schema by setting the option  `rescuedDataColumn`  .       The rescued data column ensures that columns that don’t match with the schema are rescued instead of being dropped. The rescued data column contains any data that isn’t parsed for the following reasons:     * The column is missing from the schema. * Type mismatches. * Case mismatches.     The rescued data column contains a JSON containing the rescued columns and the source file path of the record.        Note       The JSON and CSV parsers support three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records are dropped or throw errors, such as incomplete or malformed JSON or CSV. If you use  `badRecordsPath`  when parsing JSON or CSV, data type mismatches are not considered as bad records when using the  `rescuedDataColumn`  . Only incomplete and malformed JSON or CSV records are stored in  `badRecordsPath`  .            Change case-sensitive behavior --------------------------------     Unless case sensitivity is enabled, the columns  `abc`  ,  `Abc`  , and  `ABC`  are considered the same column for the purposes of schema inference. The case that is chosen is arbitrary and depends on the sampled data. You can use  [schema hints](#schema-hints)  to enforce which case should be used. Once a selection has been made and the schema is inferred, Auto Loader does not consider the casing variants that were not selected consistent with the schema.       When  [rescued data column](#rescue)  is enabled, fields named in a case other than that of the schema are loaded to the  `\_rescued\_data`  column. Change this behavior by setting the option  `readerCaseSensitive`  to false, in which case Auto Loader reads data in a case-insensitive way.           Override schema inference with schema hints ---------------------------------------------     You can use schema hints to enforce the schema information that you know and expect on an inferred schema. When you know that a column is of a specific data type, or if you want to choose a more general data type (for example,"	Auto Loader can be configured to automatically detect the schema of loaded data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time. Auto Loader supports schema inference and evolution for JSON, CSV, Avro, Parquet, Binaryfile, and Text file formats. To enable schema inference and evolution, you need to specify a target directory for the option `cloudFiles.schemaLocation`. When first reading data, Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first, to infer the schema. Auto Loader detects the addition of new columns as it processes your data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged. Auto Loader supports different modes for schema evolution, which you set in the option `cloudFiles.schemaEvolutionMode`. You can also use schema hints to enforce the schema information that you know and expect on an inferred schema.	1
no_reaction	['https://docs.databricks.com/delta-live-tables/index.html']	How use delta live tables optimal with different source files for unrelated tables?	    Release 2023.01 =================    **January 11 - 17, 2023**       These features and improvements were released with the 2023.01 release of Delta Live Tables.          Databricks Runtime versions used by this release --------------------------------------------------    **Channel:**     * CURRENT (default): Databricks Runtime 11.0.12 * PREVIEW: Databricks Runtime 11.3.3       New features and improvements in this release -----------------------------------------------   * A new Delta Live Tables on-boarding widget (available in new workspaces) guides users who want to create pipelines using data from an S3 bucket in AWS.       Bug fixes in this release ---------------------------   * This release fixes a bug that can cause failures loading a pipeline in the Delta Live Tables UI when the pipeline has zero events. This can occur when a pipeline has existed long enough without updates being run that all events have been purged.                                                      	I don't think the provided context contains information relevant to the question. Can you please provide more context or clarify the question?	0
no_reaction	['https://docs.databricks.com/integrations/jdbc-odbc-bi.html']	how do connect to informatica from Databricks using Informatica's ODBC connector?	      Configure the Databricks ODBC and JDBC drivers ================================================     This article describes how to configure the Databricks ODBC and JDBC drivers to connect your tools or clients to Databricks. For tool or client specific connection instructions, see  [Technology partners](index.html)  .        Requirements --------------     Before you start, you need to make sure you have the appropriate permissions to connect to Databricks, to prepare your credentials and to retrieve the connection details.      ###   Permission requirements     There are two  [permissions](../security/auth-authz/access-control/cluster-acl.html)  you may need when you connect to a Databricks  [cluster](../clusters/index.html)  :     * **Can Attach To**   permission to connect to the running cluster. * **Can Restart**   permission to automatically trigger the cluster to start if its state is terminated when connecting.     To access a Databricks SQL  [warehouse](../sql/admin/create-sql-warehouse.html)  , you need  **Can Use**  [permission](../security/auth-authz/access-control/sql-endpoint-acl.html)  . The Databricks SQL warehouse automatically starts if it was stopped.         ###   Authentication requirements     The Databricks ODBC and JDBC drivers support authentication by using a personal access token or your Databricks username and password.     * For instructions about how to generate a token, see  [Databricks personal access tokens](../dev-tools/auth.html#pat)  . * Username and password authentication is possible only if  [single sign-on](../administration-guide/users-groups/single-sign-on/index.html)  is disabled.       ###   Retrieve the connection details     This section explains how to retrieve the connection details that you need to connect to Databricks.        ####   Get connection details for a cluster     To establish connections to many external data sources, developer tools, or technology partners, you must provide connection details for your cluster. To retrieve connection details, do the following:     1. Log in to your Databricks workspace and go to the Data Science & Engineering or Databricks Machine Learning persona-based environment.       To change the persona, click the icon below the Databricks logo  ![Databricks logo](../_images/databricks-logo.png)  , then select  **Data Science & Engineering**   or  **Machine Learning**   . 2. Click  ![compute icon](../_images/clusters-icon.png) **Compute**   in the sidebar. 3. Choose a cluster to connect to. 4. Navigate to  **Advanced Options**   . 5. Click on the  **JDBC/ODBC**   tab. 6. Copy the connection details.           ####   Get connection details for a SQL warehouse     To establish connections to many external data sources, developer tools, or technology partners, you must provide connection details for your SQL warehouse. To retrieve connection details, do the following:     1. Log in to your Databricks workspace and go to the SQL persona-based environment.       To change the persona, click the icon below the Databricks logo  ![Databricks logo](../_images/databricks-logo.png)  , then select  **SQL**   . 2. Click  ![Endpoints Icon](../_images/warehouses-icon.png) **SQL Warehouses**   in the sidebar. 3. Choose a warehouse to connect to. 4. Navigate to the  **Connection Details**   tab. 5. Copy the connection details.       Alternatively, you can click the icon for one of the displayed technology partners or developer tools and follow the onscreen steps to connect using your SQL warehouse’s connection details.           ODBC driver -------------     This section presents the steps to configure your ODBC driver to connect to Databricks.      ###   Download the ODBC driver     Review the license agreement for the  [Databricks ODBC driver](https://databricks.com/jdbc-odbc-driver-license)   before installing the software.       Some tools and clients require you to install the Databricks ODBC driver to set up a connection to Databricks, while others embed the driver and do not require separate installation. For example, to use Tableau Desktop, the ODBC driver needs to be installed, while recent Power BI Desktop releases include the driver preinstalled and no action is needed.       Go to the  [Databricks ODBC driver](https://databricks.com/spark/odbc-drivers-download)   download page to download the driver for your operating system.       After you download the appropriate installer of the driver for your platform, use the following instructions to install and configure the driver:     * [Install and configure the ODBC driver for Windows](#odbc-windows) * [Install and configure the ODBC driver for Mac](#odbc-mac) * [Install and configure the ODBC driver for Linux](#odbc-linux) * [ODBC configuration and connection parameters](#odbc-parameters)       ###   Install and configure the ODBC driver for Windows     In Windows, you can set up a Data Source Name (DSN) configuration to connect your ODBC client application to Databricks. To set up a DSN configuration, use the Windows  [ODBC Data Source Administrator](https://learn.microsoft.com/sql/database-engine/configure-windows/open-the-odbc-data-source-administrator)   .     1. Download the latest driver version for Windows, if you haven’t already done so. See  [Download the ODBC driver](#download-the-odbc-driver)  . 2. Double-click on the dowloaded  `.msi`  file to install the driver. The installation directory is  `C:\Program     Files\Simba     Spark     ODBC     Driver`  . 3. From the Start menu, search for  **ODBC Data Sources**   to launch the  **ODBC Data Source Administrator**   . 4. Navigate to the  **Drivers**   tab to verify that the driver (Simba Spark ODBC Driver) is installed. 5. Go to the  **User DSN**   or  **System DSN**   tab and click the  **Add**   button. 6. Select the  **Simba Spark ODBC Driver**   from the list of installed drivers. 7. Choose a Data Source Name and set the mandatory  [ODBC configuration and connection parameters](#odbc-parameters)  . See also  [ODBC driver capabilities](#odbc-driver-capabilities)  for more driver configurations. 8. Click  **OK**   to create the DSN.       ###   Install and configure the ODBC driver for macOS     In macOS, you can set up a Data Source Name (DSN) configuration to connect your ODBC client application to Databricks. To set up a DSN on macOS, use the  [ODBC Manager](http://www.odbcmanager.net)   .     1. [Install ODBC Manager by using Homebrew](https://formulae.brew.sh/cask/odbc-manager)   , or  [download the ODBC Manager](http://www.odbcmanager.net)   and then double-click on the downloaded  `.dmg`  file to install it. 2. Download the latest driver version for macOS, if you haven’t already done so. See  [Download the ODBC driver](#download-the-odbc-driver)  . 3. Double-click on the dowloaded  `.dmg`  file to install the driver. The installation directory is  `/Library/simba/spark`  . 4. Start the ODBC Manager. 5. Navigate to the  **Drivers**   tab to verify that the driver (Simba Spark ODBC Driver) is installed. 6. Go to the  **User DSN**   or  **System DSN**   tab and click the  **Add**   button. 7. Select the  **Simba Spark ODBC Driver**   from the list of installed drivers. 8. Click  **Add**   . 9. Choose a Data Source Name and create key-value pairs to set the mandatory  [ODBC configuration and connection parameters](#odbc-parameters)  . See also  [ODBC driver capabilities](#odbc-driver-capabilities)  for more driver configurations. 10. Click  **OK**   to create the DSN.       ###   Install and configure the ODBC driver for Linux     In Linux, you can set up a Data Source Name (DSN) configuration to connect your ODBC client application to Databricks. To set up a DSN on Linux, use the  [unixODBC](http://www.unixodbc.org/)   Driver Manager.     1. Install  [unixODBC](http://www.unixodbc.org/)   . 2. Download the latest driver version for Linux, if you haven’t already done so. See  [Download the ODBC driver](#download-the-odbc-driver)  . 3. Install the ODBC driver.      * RPM package:               ```  sudo yum --nogpgcheck localinstall simbaspark_<version>.rpm    ```  * DEB package:               ```  sudo dpkg -i simbaspark_<version>.deb    ```  The installation directory is  `/opt/simba/spark`  . 4. Locate the  `odbc.ini`  driver configuration file that corresponds to  `SYSTEM     DATA     SOURCES`  :        ``` odbcinst -j  ``` 5. In a text editor, open the  `odbc.ini`  configuration file. 6. Create an  `[ODBC     Data     Sources]`  section:        ``` [ODBC Data Sources] Databricks=Databricks ODBC Connector  ``` 7. Create another section with the same name as your DSN and specify the configuration parameters as key-value pairs. See the mandatory  [ODBC configuration and connection parameters](#odbc-parameters)  for more information about how to set the configuration parameters. See also  [ODBC driver capabilities](#odbc-driver-capabilities)  for more driver configurations.        ``` [Databricks] Driver=<path-to-driver> <key>=<value> ...  ```       ###   ODBC configuration and connection parameters     This section presents the mandatory (unless otherwise specified) configuration and connection parameters for the ODBC driver.          |   Configuration   |   Description   | | --- | --- | |   Driver   |   Driver installation path   | |   Host   |   Databricks server hostname   | |   Port   |   443   | |   HTTPPath   |   Databricks compute resources URL   | |   SSL   |   1   | |   ThriftTransport   |   2   | |   Schema (optional)   |   Default schema   | |   Catalog (optional)   |   Default catalog   |      ####   Authentication parameters     To authenticate by using a personal access token, set the following configurations:          |   Configuration   |   Description   | | --- | --- | |   AuthMech   |   3   | |   UID   |   token   | |   PWD   |   Databricks personal access token for your workspace user   |     If you want to use your Databricks credentials, then set UID and PWD to your username and password, respectively.       Define the connection string to use in your application as follows:        ``` Driver=<path-to-driver>;Host=<server-hostname>;Port=443;HTTPPath=<http-path>;ThriftTransport=2;SSL=1;AuthMech=3;UID=token;PWD=<personal-access-token>  ```      Set the HOST and HTTPPath configurations to	To connect to Databricks using Informatica's ODBC connector, you need to configure the Databricks ODBC driver and retrieve the connection details. The HOST and HTTPPath configurations should be set to the Databricks server hostname and compute resources URL, respectively.	0
no_reaction	['https://docs.databricks.com/serverless-compute/index.html']	Whats is serverless?	    Serverless compute ====================     With the serverless compute version of the Databricks platform architecture, the compute layer exists in your Databricks account rather than your AWS account.            Databricks SQL Serverless ---------------------------    [Databricks SQL Serverless](../sql/admin/index.html)  supports serverless compute. Admins can create serverless SQL warehouses (formerly SQL endpoints) that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute clusters in your Databricks account. Use them with Databricks SQL queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.       Databricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it’s connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.       If serverless SQL warehouses are enabled for your account, note the following:     * New SQL warehouses are serverless by default when you create them from the  [UI](../sql/admin/create-sql-warehouse.html#create-warehouse)  . New SQL warehouses are not serverless by default when you create them using the  [API](../sql/api/sql-endpoints.html#create-warehouse)  , which requires that you explicitly specify serverless. You can also create new pro or classic SQL warehouses using either method. For more information about warehouse type defaults, see  [What are the warehouse type defaults?](../sql/index.html#warehouse-type-defaults)  . * You can  [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](../sql/admin/create-sql-warehouse.html#convert-to-serverless)  or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic. * This feature only affects Databricks SQL. It does not affect how Databricks Runtime clusters work with notebooks and jobs in the Data Science & Engineering or Databricks Machine Learning workspace environments. Databricks Runtime clusters always run in the classic data plane in your AWS account. See  [Serverless quotas](#compare)  .     If your account needs updated  [terms of use](../sql/admin/serverless.html#accept-terms)  , workspace admins are prompted in the Databricks SQL UI.       If your workspace has an AWS instance profile,  [you might need to update the trust relationship](../sql/admin/serverless.html#aws-instance-profile-setup)  to support serverless compute, depending on how and when it was created.       For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .         Model Serving ---------------     Model Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your Databricks account. The <ServerlessCompute> resources run as Databricks AWS resources in what is known as the serverless data plane.       In contrast, the legacy model serving architecture is a single-node cluster that runs in your AWS account within the classic data plane.     * **Easy configuration and compute resource management**   : Databricks automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration. * **High availability and scalability**   : Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests. * **Dashboards**   : Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.     For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .       Before you can use serverless Model Serving, you must  [enable serverless compute for your account](../machine-learning/model-serving/migrate-model-serving.html)  .                   Serverless quotas -------------------     Serverless quotas are a safety measure for serverless compute. Serverless quotas restrict how many serverless compute resources a customer can have at any given time. The quota is enforced at the regional level for all workspaces in your account. Quotas are enforced only for  [serverless SQL warehouses](../sql/admin/serverless.html)  . See  [Serverless quotas](serverless-quotas.html)  .         Compare serverless compute to other Databricks architectures --------------------------------------------------------------     Databricks operates out of a  *control plane*   and a  *data plane*   :     * The control plane includes the backend services that Databricks manages in its own AWS account. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest. * The data plane is where data is processed by clusters of compute resources.     There are important differences between the  *classic data plane*   (the original Databricks platform architecture) and the  *serverless data plane*   :     * For a classic data plane, Databricks compute resources run in your AWS account. Clusters perform distributed data analysis using queries (in Databricks SQL) or notebooks (in the Data Science & Engineering or Databricks Machine Learning environments):      + New clusters are created within each workspace’s virtual network in the customer’s AWS account.  + A classic data plane has natural isolation because it runs in each customer’s own AWS account. * For a serverless data plane, Databricks compute resources run in a compute layer within your Databricks account:      + The serverless data plane is used for serverless SQL warehouses and Model Serving. Enabling serverless compute does not change how Databricks Runtime clusters work in the Data Science & Engineering or Databricks Machine Learning environments.  + To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different Databricks customer workspaces and additional network controls between clusters of the same customer.     Databricks creates a serverless data plane in the same AWS region as your workspace’s classic data plane.       Worker nodes are private, which means they do not have public IP addresses.       For communication between the Databricks control plane and the serverless data plane:     * For Databricks SQL Serverless, the communication uses private connectivity. * For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.     When reading or writing to AWS S3 buckets in the same region as your workspace,  [serverless SQL warehouses](../sql/admin/serverless.html)  now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace’s root S3 bucket in your AWS account and to other S3 data sources in the same region.       The following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.      ![Compare classic and serverless data plane for Databricks SQL](../_images/serverless-compute-data-plane-comparison-dbsql-aws.png)   ![Compare classic and serverless data plane for Model Serving](../_images/serverless-compute-data-plane-comparison-models-aws.png)    For more information about secure cluster connectivity, which is mentioned in the diagram, see  [Secure cluster connectivity](../security/network/secure-cluster-connectivity.html)  .       The table below summarizes differences between serverless compute and the classic data plane architecture of Databricks, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about serverless compute security, or if you have questions about items in this table, contact your Databricks representative.               |   Item   |   Serverless data plane (AWS only)   |   Classic data plane (AWS and Azure)   | | --- | --- | --- | |  **Location of control plane resources**   |   Databricks cloud account   |   Databricks cloud account   | |  **Location of data plane compute resources**   |   Serverless data plane (VPC in the Databricks AWS account)   |   Classic data plane (VPC in the customer’s cloud provider account)   | |  **Data plane compute resources**   |   Databricks-managed Kubernetes (EKS) clusters   |   Databricks-managed standalone VMs   | |  **Customer access to data plane**   |   Access through Databricks control plane   | * **AWS**   : Direct access in customer’s AWS account. Additional indirect access through Databricks control plane. * **Azure**   : Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.  | |  **Who pays for unassigned VMs for Databricks SQL?**   |   Databricks   |   Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.   | |  **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**   |   Customer pays based on DBUs until Auto Stop stops the SQL warehouse.   |   Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.   | |  **Virtual private network (VPC) for data plane**   |   VPC in the customer’s Databricks account, with network boundaries between workspaces and between clusters.   | * **AWS**   : Exclusive, the VPC is in the customer’s account. * **Azure**   : Exclusive, the VNet is in the customer’s account.  | |  **OS image**   |   Databricks-modified cloud-managed Amazon-linux2   |   Databricks-managed Ubuntu or CentOS   | |  **Technology that manages default egress from the VPC**   |   Databricks-created AWS internet gateway   |   Default internet gateway or load balancer provided by the cloud   | |  **Customize VPC and firewall settings**   |   No   |   Yes   | |  **Customize CIDR ranges**   |   No   |   Yes   | |  **Public IPs**   |   No   |   Secure cluster connectivity  * When disabled, one public IP for each VM. * When enabled (the default), no public IPs for VMs.  | |  **Container-level network isolation for Databricks Runtime clusters**   |   Uses Kubernetes network policy   |   Uses Databricks-managed  `iptable`  rules   | |  **VM-level network isolation for Databricks Runtime clusters**   |   Security group isolation   |   Security group and isolation of VPC (AWS) or VNet (Azure)   | |  **VM isolation**   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   | |  **Communication between control plane and data plane**   |   For Databricks SQL Serverless, communication uses private connectivity. For Model Serving, communication uses direct mTLS encrypted communication with the connection initiated from the control plane with access limited to control plane IP addresses.   |   Secure cluster connectivity  * When enabled (the default for AWS E2 and Azure): Individual VMs connect to the SCC relay in the control plane during cluster creation * When disabled: Control plane connects to individual VMs using public IPs.  | |  **Credential for initial deployment**   |   Databricks internal IAM roles   | * **AWS**   : IAM roles provided by customers. * **Azure**   : None required.  | |  **Credential for regular data plane operations**   |   Databricks invokes  `sts:AssumeRole`  on customer-provided IAM role.   | * **AWS**   : VMs run with instance profiles that are provided by	Serverless compute is a version of the Databricks platform architecture where the compute layer exists in your Databricks account rather than your AWS account. It allows for serverless SQL warehouses and model serving, which autoscale and provide high availability and scalability. Serverless quotas are in place to restrict how many serverless compute resources a customer can have at any given time. The serverless data plane is used for serverless SQL warehouses and model serving, while Databricks Runtime clusters always run in the classic data plane in your AWS account.	1
no_reaction	['https://spark.apache.org/docs/latest/graphx-programming-guide.html']	How to use spark graphX?	"   p    [org](../../../index.html)   .  [apache](../../index.html)   .  [spark](../index.html)    graphx   [**](../../../../org/apache/spark/graphx/index.html ""Permalink"") ============================================================================  ####    package     graphx   ALPHA COMPONENT    GraphX is a graph processing framework built on top of Spark.     Source    [package.scala](https://github.com/apache/spark/tree/v3.4.0/graphx/src/main/scala/org/apache/spark/graphx/package.scala)    Linear Supertypes    [AnyRef](../../../../scala/index.html#AnyRef=Object)   ,    Any    **   **    Ordering   1. Alphabetic 2. By Inheritance   Inherited      1. graphx 2. AnyRef 3. Any  1. Hide All 2. Show All   Visibility   1. Public 2. All  ###   Type Members  1. [**](../../../../org/apache/spark/graphx/Edge.html ""Permalink"")   case class    [Edge](Edge.html ""A single directed edge consisting of a source id, target id, and the data associated with the edge."")   [    ED    ]     (    srcId:  [VertexId](#VertexId=Long)   =    0     ,    dstId:  [VertexId](#VertexId=Long)   =    0     ,    attr:    ED    =    null.asInstanceOf[ED]     )     extends    Serializable    with    Product     A single directed edge consisting of a source id, target id, and the data associated with the edge.     A single directed edge consisting of a source id, target id, and the data associated with the edge.     ED     type of the edge attribute     srcId     The vertex id of the source vertex     dstId     The vertex id of the target vertex     attr     The attribute associated with the edge 2. [**](../../../../org/apache/spark/graphx/EdgeContext.html ""Permalink"")   abstract     class    [EdgeContext](EdgeContext.html ""Represents an edge along with its neighboring vertices and allows sending messages along the edge."")   [    VD    ,    ED    ,    A    ]     extends  [AnyRef](../../../../scala/index.html#AnyRef=Object)    Represents an edge along with its neighboring vertices and allows sending messages along the edge.     Represents an edge along with its neighboring vertices and allows sending messages along the edge. Used in  [Graph#aggregateMessages](Graph.html#aggregateMessages[A](sendMsg:org.apache.spark.graphx.EdgeContext[VD,ED,A]=>Unit,mergeMsg:(A,A)=>A,tripletFields:org.apache.spark.graphx.TripletFields)(implicitevidence$11:scala.reflect.ClassTag[A]):org.apache.spark.graphx.VertexRDD[A])   . 3. [**](../../../../org/apache/spark/graphx/EdgeDirection.html ""Permalink"")   class    [EdgeDirection](EdgeDirection.html ""The direction of a directed edge relative to a vertex."")   extends    Serializable     The direction of a directed edge relative to a vertex. 4. [**](../../../../org/apache/spark/graphx/EdgeRDD.html ""Permalink"")   abstract     class    [EdgeRDD](EdgeRDD.html ""EdgeRDD[ED, VD] extends RDD[Edge[ED]] by storing the edges in columnar format on each partition for performance."")   [    ED    ]     extends  [RDD](../rdd/RDD.html)   [  [Edge](Edge.html)   [    ED    ]]    `EdgeRDD[ED, VD]`   extends  `RDD[Edge[ED]]`   by storing the edges in columnar format on each partition for performance.    `EdgeRDD[ED, VD]`   extends  `RDD[Edge[ED]]`   by storing the edges in columnar format on each partition for performance. It may additionally store the vertex attributes associated with each edge to provide the triplet view. Shipping of the vertex attributes is managed by  `impl.ReplicatedVertexView`   . 5. [**](../../../../org/apache/spark/graphx/EdgeTriplet.html ""Permalink"")   class    [EdgeTriplet](EdgeTriplet.html ""An edge triplet represents an edge along with the vertex attributes of its neighboring vertices."")   [    VD    ,    ED    ]     extends  [Edge](Edge.html)   [    ED    ]     An edge triplet represents an edge along with the vertex attributes of its neighboring vertices.     An edge triplet represents an edge along with the vertex attributes of its neighboring vertices.     VD     the type of the vertex attribute.     ED     the type of the edge attribute 6. [**](../../../../org/apache/spark/graphx/Graph.html ""Permalink"")   abstract     class    [Graph](Graph.html ""The Graph abstractly represents a graph with arbitrary objects associated with vertices and edges."")   [    VD    ,    ED    ]     extends    Serializable     The Graph abstractly represents a graph with arbitrary objects associated with vertices and edges.     The Graph abstractly represents a graph with arbitrary objects associated with vertices and edges. The graph provides basic operations to access and manipulate the data associated with vertices and edges as well as the underlying structure. Like Spark RDDs, the graph is a functional data-structure in which mutating operations return new graphs.     VD     the vertex attribute type     ED     the edge attribute type     Note    [GraphOps](GraphOps.html)   contains additional convenience operations and graph algorithms. 7. [**](../../../../org/apache/spark/graphx/GraphOps.html ""Permalink"")   class    [GraphOps](GraphOps.html ""Contains additional functionality for Graph."")   [    VD    ,    ED    ]     extends    Serializable     Contains additional functionality for  [Graph](Graph.html)   .     Contains additional functionality for  [Graph](Graph.html)   . All operations are expressed in terms of the efficient GraphX API. This class is implicitly constructed for each Graph object.     VD     the vertex attribute type     ED     the edge attribute type 8. [**](../../../../org/apache/spark/graphx/index.html#PartitionID=Int ""Permalink"")   type     PartitionID     =    Int     Integer identifier of a graph partition.     Integer identifier of a graph partition. Must be less than 2^30. 9. [**](../../../../org/apache/spark/graphx/PartitionStrategy.html ""Permalink"")   trait    [PartitionStrategy](PartitionStrategy.html ""Represents the way edges are assigned to edge partitions based on their source and destination vertex IDs."")   extends    Serializable     Represents the way edges are assigned to edge partitions based on their source and destination vertex IDs. 10. [**](../../../../org/apache/spark/graphx/TripletFields.html ""Permalink"")   class    [TripletFields](TripletFields.html)   extends    Serializable 11. [**](../../../../org/apache/spark/graphx/index.html#VertexId=Long ""Permalink"")   type     VertexId     =    Long     A 64-bit vertex identifier that uniquely identifies a vertex within a graph.     A 64-bit vertex identifier that uniquely identifies a vertex within a graph. It does not need to follow any ordering or any constraints other than uniqueness. 12. [**](../../../../org/apache/spark/graphx/VertexRDD.html ""Permalink"")   abstract     class    [VertexRDD](VertexRDD.html ""Extends RDD[(VertexId, VD)] by ensuring that there is only one entry for each vertex and by pre-indexing the entries for fast, efficient joins."")   [    VD    ]     extends  [RDD](../rdd/RDD.html)   [(  [VertexId](#VertexId=Long)   ,    VD    )]     Extends  `RDD[(VertexId, VD)]`   by ensuring that there is only one entry for each vertex and by pre-indexing the entries for fast, efficient joins.     Extends  `RDD[(VertexId, VD)]`   by ensuring that there is only one entry for each vertex and by pre-indexing the entries for fast, efficient joins. Two VertexRDDs with the same index can be joined efficiently. All operations except  [reindex](VertexRDD.html#reindex():org.apache.spark.graphx.VertexRDD[VD])   preserve the index. To construct a  `VertexRDD`   , use the  [VertexRDD object](VertexRDD$.html)   .     Additionally, stores routing information to enable joining the vertex attributes with an  [EdgeRDD](EdgeRDD.html)   .     VD     the vertex attribute associated with each vertex in the set.     Example:    1. Construct a   `VertexRDD`    from a plain RDD:           ```  // Construct an initial vertex set  val someData: RDD[(VertexId, SomeType)] = loadData(someFile)  val vset = VertexRDD(someData)  // If there were redundant values in someData we would use a reduceFunc  val vset2 = VertexRDD(someData, reduceFunc)  // Finally we can use the VertexRDD to index another dataset  val otherData: RDD[(VertexId, OtherType)] = loadData(otherFile)  val vset3 = vset2.innerJoin(otherData) { (vid, a, b) => b }  // Now we can construct very fast joins between the two sets  val vset4: VertexRDD[(SomeType, OtherType)] = vset.leftJoin(vset3)  ```  ###   Value Members  1. [**](../../../../org/apache/spark/graphx/Edge$.html ""Permalink"")   object    [Edge](Edge$.html)   extends    Serializable 2. [**](../../../../org/apache/spark/graphx/EdgeContext$.html ""Permalink"")   object    [EdgeContext](EdgeContext$.html) 3. [**](../../../../org/apache/spark/graphx/EdgeDirection$.html ""Permalink"")   object    [EdgeDirection](EdgeDirection$.html ""A set of EdgeDirections."")   extends    Serializable     A set of  [EdgeDirection](EdgeDirection.html)   s. 4. [**](../../../../org/apache/spark/graphx/EdgeRDD$.html ""Permalink"")   object    [EdgeRDD](Edge"	To use Spark GraphX, you need to import the GraphX package, which is a graph processing framework built on top of Spark. The package provides classes such as Edge, EdgeContext, EdgeDirection, EdgeRDD, EdgeTriplet, Graph, GraphOps, PartitionStrategy, TripletFields, and VertexRDD. You can use these classes to represent and manipulate graphs and their attributes.	1
no_reaction	['https://docs.databricks.com/data-governance/table-acls/index.html']	i dont want users to access hive metastore iin a UC enables workspace. How do I restrict	"      Work with Unity Catalog and the legacy Hive metastore =======================================================     This article explains how to use a Unity Catalog metastore with the per-workspace Hive metastore in Databricks.       If your workspace was in service before it was enabled for Unity Catalog, it likely has a Hive metastore that contains data that you want to continue to use. Databricks recommends that you  [migrate the tables managed by the Hive metastore to the Unity Catalog metastore](migrate.html)  , but if you choose not to, this article explains how to work with data managed by both metastores.          Query the Hive metastore in Unity Catalog -------------------------------------------     The Unity Catalog metastore is additive, meaning it can be used with the per-workspace Hive metastore in Databricks. The Hive metastore appears as a top-level catalog called  `hive\_metastore`  in the three-level namespace.       For example, you can refer to a table called  `sales\_raw`  in the  `sales`  schema in the legacy Hive metastore by using the following notation:         ``` SELECT \* from hive\_metastore.sales.sales\_raw;  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       ``` library(SparkR)  display(tableToDF(""hive\_metastore.sales.sales\_raw""))  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       You can also specify the catalog and schema with a  `USE`  statement:         ``` USE hive\_metastore.sales; SELECT \* from sales\_raw;  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```       ``` library(SparkR)  sql(""USE hive\_metastore.sales"") display(tableToDF(""sales\_raw""))  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```           Access control in Unity Catalog and the Hive metastore --------------------------------------------------------     If you configured  [table access control](../table-acls/index.html)  on the Hive metastore, Databricks continues to enforce those access controls for data in the  `hive\_metastore`  catalog for clusters running in the shared access mode. The Unity Catalog access model differs slightly from legacy access controls, like no  `DENY`  statements. The Hive metastore is a workspace-level object. Permissions defined within the  `hive\_metastore`  catalog always refer to the local users and groups in the workspace. See  [Differences from table access control](#differences-from-table-access-control)  .         Differences from table access control ---------------------------------------     Unity Catalog has the following key differences from using  [table access controls](../table-acls/index.html)  in the legacy Hive metastore in each workspace.       The access control model in Unity Catalog has the following differences from table access control:     * **Account groups**   : Access control policies in Unity Catalog are applied to account groups, while access control policies for the Hive metastore are applied to workspace-local groups. See  [Special considerations for groups](../../administration-guide/users-groups/index.html#special-groups)  . * **`USE     CATALOG`  and  `USE     SCHEMA`  permissions are required on the catalog and schema for all operations on objects inside the catalog or schema**   : Regardless of a principal’s privileges on a table, the principal must also have the  `USE     CATALOG`  privilege on its parent catalog to access the schema and the  `USE     SCHEMA`  privilege to access objects within the schema. With workspace-level table access controls, on the other hand, granting  `USAGE`  on the root catalog automatically grants  `USAGE`  on all databases, but  `USAGE`  on the root catalog is not required. * **Views**   : In Unity Catalog, the owner of a view does not need to be an owner of the view’s referenced tables and views. Having the  `SELECT`  privilege is sufficient, along with  `USE     SCHEMA`  on the views’ parent schema and  `USE     CATALOG`  on the parent catalog. With workspace-level table access controls, a view’s owner needs to be an owner of all referenced tables and views. * **No support for  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  s**   : In Unity Catalog, there is no concept of an  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  permission. These permissions could be used to circumvent access control restrictions by allowing an unprivileged user to run privileged code.         Joins between Unity Catalog and Hive metastore objects --------------------------------------------------------     By using three-level namespace notation, you can join data in a Unity Catalog metastore with data in the legacy Hive metastore.        Note       A join with data in the legacy Hive metastore will only work on the workspace where that data resides. Trying to run such a join in another workspace results in an error. Databricks recommends that you  [upgrade](migrate.html)  legacy tables and views to Unity Catalog.        The following example joins results from the  `sales\_current`  table in the legacy Hive metastore with the  `sales\_historical`  table in the Unity Catalog metastore when the  `order\_id`  fields are equal.         ``` SELECT \* FROM hive\_metastore.sales.sales\_current JOIN main.shared\_sales.sales\_historical ON hive\_metastore.sales.sales\_current.order\_id = main.shared\_sales.sales\_historical.order\_id;  ```       ``` dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(   other = dfHistorical,   on = dfCurrent.order\_id == dfHistorical.order\_id ))  ```       ``` library(SparkR)  dfCurrent = tableToDF(""hive\_metastore.sales.sales\_current"") dfHistorical = tableToDF(""main.shared\_sales.sales\_historical"")  display(join(  x = dfCurrent,  y = dfHistorical,  joinExpr = dfCurrent$order\_id == dfHistorical$order\_id))  ```       ``` val dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") val dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(  right = dfHistorical,  joinExprs = dfCurrent(""order\_id"") === dfHistorical(""order\_id"") ))  ```         Default catalog -----------------     If you omit the top-level catalog name when you perform data operations, and there is no  `USE     CATALOG`  statement, the default catalog is assumed. Of course, if you are supplying the catalog name in another way, such as setting it on a JDBC string, then that catalog is assumed instead of the default catalog when you omit the catalog name in data operations.       By default, the default catalog is  `hive\_metastore`  . To configure a different default catalog for a workspace, you can set a Spark configuration or run a  [Databricks CLI](../../dev-tools/cli/unity-catalog-cli.html)  command:         Set the  `spark.databricks.sql.initial.catalog.name`  value.         This CLI command updates the default catalog only for SQL warehouses. To specify the default catalog for clusters, use the Spark config.        ``` databricks unity-catalog metastores assign --workspace-id 1234567890123456 \  --metastore-id 12a345b6-7890-1cd2-3456-e789f0a12b34 \  --default-catalog-name my_catalog  ```        If your Databricks workspace used the per-workspace Hive metastore before the workspace was enabled for Unity Catalog, Databricks recommends that you keep the default catalog value as  `hive\_metastore`  so that your existing code can operate on current Hive metastore data without any change.        Warning       Changing the default catalog can break existing data operations that depend on it.          Cluster instance profile --------------------------     When using the Hive metastore alongside Unity Catalog, the instance profile on the cluster is used to access Hive metastore data but not data registered in Unity Catalog.       If users access paths that are outside Unity Catalog (such as a path not registered as a table or external location) then the instance profile on the cluster is used.           Upgrade legacy tables to Unity Catalog ----------------------------------------     Tables in the Hive metastore do not benefit from the full set of security and governance features that Unity Catalog introduces, such as built-in auditing and access control. Databricks recommends that you  [upgrade your legacy tables](migrate.html)  by adding them to Unity Catalog.                                                        "	"To restrict users from accessing the Hive metastore in a Unity Catalog enabled workspace, you can configure table access control on the Hive metastore. However, if you choose not to migrate the tables managed by the Hive metastore to the Unity Catalog metastore, you can still query the Hive metastore in Unity Catalog by using the three-level namespace notation. Access control policies in Unity Catalog are applied to account groups, while access control policies for the Hive metastore are applied to workspace-local groups. See the ""Access control in Unity Catalog and the Hive metastore"" section for more information."	1
thumb_down	['https://docs.databricks.com/external-data/snowflake.html']	why should I use Databricks over Snowflake?	      Query federation for Snowflake in Databricks SQL ==================================================      Preview       This feature is  [Experimental](../release-notes/release-types.html)  and has no support for Unity Catalog. Experimental features are provided as-is and are not supported by Databricks through customer technical support channels.        Databricks SQL supports read-only query federation to Snowflake on serverless and pro SQL warehouses.          Connecting to Snowflake with Databricks SQL ---------------------------------------------     You configure connections to Snowflake at the table level. You can use  [secrets](../sql/language-manual/functions/secret.html)  to store and access text credentials without displaying them in plaintext. See the following example:        ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser secret('snowflake\_creds', 'my\_username'),  sfPassword secret('snowflake\_creds', 'my\_password'),  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' );  ```                                                       	Databricks and Snowflake are two different tools with different functionalities. Databricks is a unified analytics platform that provides a collaborative environment for data scientists, engineers, and business analysts to work together on big data and machine learning projects. On the other hand, Snowflake is a cloud-based data warehousing platform that provides a scalable and flexible solution for storing and analyzing large amounts of data. While both tools have their own unique features and benefits, the decision to use one over the other depends on the specific needs and requirements of the organization.	0
no_reaction	['https://docs.databricks.com/introduction/delta-comparison.html']	waht is delta	    What are all the  *Delta*   things in Databricks? ===================================================     This article is an introduction to the technologies collectively branded  *Delta*   on Databricks. Delta refers to technologies related to or in the  [Delta Lake open source project](https://delta.io/)   .       This article answers:     * What are the  *Delta*   technologies in Databricks? * What do they do? Or what are they used for? * How are they related to and distinct from one another?      What are the Delta things used for? -------------------------------------     Delta is a term introduced with Delta Lake, the foundation for storing data and tables in the Databricks Lakehouse Platform. Delta Lake was conceived of as a unified data management system for handling transactional real-time and batch big data, by extending Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling.           Delta Lake: OS data management for the lakehouse --------------------------------------------------    [Delta Lake](https://delta.io/)   is an open-source storage layer that brings reliability to data lakes by adding a transactional storage layer on top of data stored in cloud storage (on AWS S3, Azure Storage, and GCS). It allows for ACID transactions, data versioning, and rollback capabilities. It allows you to handle both batch and streaming data in a unified way.       Delta tables are built on top of this storage layer and provide a table abstraction, making it easy to work with large-scale structured data using SQL and the DataFrame API.         Delta tables: Default data table architecture -----------------------------------------------     Delta table is the default data table format in Databricks and is a feature of the Delta Lake open source data framework. Delta tables are typically used for data lakes, where data is ingested via streaming or in large batches.       See:     * [Delta Lake quickstart: Create a table](https://docs.delta.io/latest/quick-start.html#create-a-table) * [Updating and modifying Delta Lake tables](../delta/index.html#updates)  . * [DeltaTable class](https://docs.delta.io/latest/api/python/index.html#deltatable)   : Main class for interacting programmatically with Delta tables.         Delta Live Tables: Data pipelines -----------------------------------     Delta Live Tables manage the flow of data between many Delta tables, thus simplifying the work of data engineers on ETL development and management. The pipeline is the main unit of execution for  [Delta Live Tables](../delta-live-tables/index.html)  . Delta Live Tables offers declarative pipeline development, improved data reliability, and cloud-scale production operations. Users can perform both batch and streaming operations on the same table and the data is immediately available for querying. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling. Delta Live Tables Enhanced Autoscaling can handle streaming workloads which are spiky and unpredictable.       See  [Delta Live Tables quickstart](../delta-live-tables/tutorial-sql.html)  .           Delta tables vs. Delta Live Tables ------------------------------------     Delta table is a way to store data in tables, whereas Delta Live Tables allows you to describe how data flows between these tables declaratively. Delta Live Tables is a declarative framework that manages many delta tables, by creating them and keeping them up to date. In short, Delta tables is a data table architecture while Delta Live Tables is a data pipeline framework.         Delta: Open source or proprietary? ------------------------------------     A strength of the Databricks platform is that it doesn’t lock customers into proprietary tools: Much of the technology is powered by open source projects, which Databricks contributes to.       The Delta OSS projects are examples:     * [Delta Lake project](https://delta.io/)   : Open source storage for the Lakehouse. * [Delta Sharing protocol](https://delta.io/sharing/)   : Open protocol for secure data sharing.     Delta Live Tables is a proprietary framework in Databricks.         What are the other  *Delta*   things on Databricks? -----------------------------------------------------     Below are descriptions of other features that include  *Delta*   in their name.      ###   Delta Sharing     An open standard for secure data sharing,  [Delta Sharing](../data-sharing/index.html)  enables data sharing between organizations regardless of their compute platform.       ###   Delta engine     A query optimizer for big data that uses Delta Lake open source technology included in Databricks. Delta engine optimizes the performance of Spark SQL, Databricks SQL, and DataFrame operations by pushing computation to the data.         ###   Delta Lake transaction log (AKA DeltaLogs)     A single source of truth tracking all changes that users make to the table and the mechanism through which Delta Lake guarantees  [atomicity](../lakehouse/acid.html#atomicity)  . See the  [Delta transaction log protocol](https://github.com/delta-io/delta/blob/master/PROTOCOL.md)   on GitHub.       The transaction log is key to understanding Delta Lake, because it is the common thread that runs through many of its most important features:     * ACID transactions * Scalable metadata handling * Time travel * And more.                                                       	Delta refers to technologies related to or in the Delta Lake open source project, which is a unified data management system for handling transactional real-time and batch big data, by extending Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta tables are built on top of this storage layer and provide a table abstraction, making it easy to work with large-scale structured data using SQL and the DataFrame API. Delta Live Tables manage the flow of data between many Delta tables, thus simplifying the work of data engineers on ETL development and management.	1
no_reaction	['https://docs.databricks.com/structured-streaming/scheduler-pools.html']	what are the various types of schedulers in Apache Spark?	"   p    [org](../../../index.html)   .  [apache](../../index.html)   .  [spark](../index.html)    scheduler   [**](../../../../org/apache/spark/scheduler/index.html ""Permalink"") ==================================================================================  ####    package     scheduler   Spark's scheduling components. This includes the  `org.apache.spark.scheduler.DAGScheduler`   and lower level  `org.apache.spark.scheduler.TaskScheduler`   .     Source    [package.scala](https://github.com/apache/spark/tree/v3.4.0/core/src/main/scala/org/apache/spark/scheduler/package.scala)    Linear Supertypes    [AnyRef](../../../../scala/index.html#AnyRef=Object)   ,    Any    **   **    Ordering   1. Alphabetic 2. By Inheritance   Inherited      1. scheduler 2. AnyRef 3. Any  1. Hide All 2. Show All   Visibility   1. Public 2. All  ###   Type Members  1. [**](../../../../org/apache/spark/scheduler/AccumulableInfo.html ""Permalink"")   case class    [AccumulableInfo](AccumulableInfo.html "":: DeveloperApi :: Information about an org.apache.spark.util.AccumulatorV2 modified during a task or stage."")   extends    Product    with    Serializable     :: DeveloperApi :: Information about an  [org.apache.spark.util.AccumulatorV2](../util/AccumulatorV2.html)   modified during a task or stage.     :: DeveloperApi :: Information about an  [org.apache.spark.util.AccumulatorV2](../util/AccumulatorV2.html)   modified during a task or stage.     Annotations     @DeveloperApi     ()     Note     Once this is JSON serialized the types of  `update`   and  `value`   will be lost and be cast to strings. This is because the user can define an accumulator of any type and it will be difficult to preserve the type in consumers of the event log. This does not apply to internal accumulators that represent task level metrics. 2. [**](../../../../org/apache/spark/scheduler/InputFormatInfo.html ""Permalink"")   class    [InputFormatInfo](InputFormatInfo.html "":: DeveloperApi :: Parses and holds information about inputFormat (and files) specified as a parameter."")   extends    Logging     :: DeveloperApi :: Parses and holds information about inputFormat (and files) specified as a parameter.     :: DeveloperApi :: Parses and holds information about inputFormat (and files) specified as a parameter.     Annotations     @DeveloperApi     () 3. [**](../../../../org/apache/spark/scheduler/JobResult.html ""Permalink"")   sealed     trait    [JobResult](JobResult.html "":: DeveloperApi :: A result of a job in the DAGScheduler."")   extends  [AnyRef](../../../../scala/index.html#AnyRef=Object)    :: DeveloperApi :: A result of a job in the DAGScheduler.     :: DeveloperApi :: A result of a job in the DAGScheduler.     Annotations     @DeveloperApi     () 4. [**](../../../../org/apache/spark/scheduler/MiscellaneousProcessDetails.html ""Permalink"")   class    [MiscellaneousProcessDetails](MiscellaneousProcessDetails.html "":: DeveloperApi :: Stores information about an Miscellaneous Process to pass from the scheduler to SparkListeners."")   extends    Serializable     :: DeveloperApi :: Stores information about an Miscellaneous Process to pass from the scheduler to SparkListeners.     :: DeveloperApi :: Stores information about an Miscellaneous Process to pass from the scheduler to SparkListeners.     Annotations     @DeveloperApi     ()     @Since     (     ""3.2.0""     ) 5. [**](../../../../org/apache/spark/scheduler/SparkListener.html ""Permalink"")   abstract     class    [SparkListener](SparkListener.html "":: DeveloperApi :: A default implementation for SparkListenerInterface that has no-op implementations for all callbacks."")   extends    SparkListenerInterface     :: DeveloperApi :: A default implementation for  `SparkListenerInterface`   that has no-op implementations for all callbacks.     :: DeveloperApi :: A default implementation for  `SparkListenerInterface`   that has no-op implementations for all callbacks.     Note that this is an internal interface which might change in different Spark releases.     Annotations     @DeveloperApi     () 6. [**](../../../../org/apache/spark/scheduler/SparkListenerApplicationEnd.html ""Permalink"")   case class    [SparkListenerApplicationEnd](SparkListenerApplicationEnd.html)   (    time:    Long     )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 7. [**](../../../../org/apache/spark/scheduler/SparkListenerApplicationStart.html ""Permalink"")   case class    [SparkListenerApplicationStart](SparkListenerApplicationStart.html)   (    appName:    String     ,    appId:    Option    [    String    ]    ,    time:    Long     ,    sparkUser:    String     ,    appAttemptId:    Option    [    String    ]    ,    driverLogs:    Option    [    Map    [    String    ,    String    ]] =    [None](../../../../scala/index.html)    ,    driverAttributes:    Option    [    Map    [    String    ,    String    ]] =    [None](../../../../scala/index.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 8. [**](../../../../org/apache/spark/scheduler/SparkListenerBlockManagerAdded.html ""Permalink"")   case class    [SparkListenerBlockManagerAdded](SparkListenerBlockManagerAdded.html)   (    time:    Long     ,    blockManagerId:  [BlockManagerId](../storage/BlockManagerId.html)    ,    maxMem:    Long     ,    maxOnHeapMem:    Option    [    Long    ] =    [None](../../../../scala/index.html)    ,    maxOffHeapMem:    Option    [    Long    ] =    [None](../../../../scala/index.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 9. [**](../../../../org/apache/spark/scheduler/SparkListenerBlockManagerRemoved.html ""Permalink"")   case class    [SparkListenerBlockManagerRemoved](SparkListenerBlockManagerRemoved.html)   (    time:    Long     ,    blockManagerId:  [BlockManagerId](../storage/BlockManagerId.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 10. [**](../../../../org/apache/spark/scheduler/SparkListenerBlockUpdated.html ""Permalink"")   case class    [SparkListenerBlockUpdated](SparkListenerBlockUpdated.html)   (    blockUpdatedInfo:  [BlockUpdatedInfo](../storage/BlockUpdatedInfo.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 11. [**](../../../../org/apache/spark/scheduler/SparkListenerEnvironmentUpdate.html ""Permalink"")   case class    [SparkListenerEnvironmentUpdate](SparkListenerEnvironmentUpdate.html)   (    environmentDetails:    Map    [    String    ,    Seq    [(    String    ,    String    )]]    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 12. [**](../../../../org/apache/spark/scheduler/SparkListenerEvent.html ""Permalink"")   trait    [SparkListenerEvent](SparkListenerEvent.html)   extends  [AnyRef](../../../../scala/index.html#AnyRef=Object)    Annotations     @DeveloperApi     ()     @JsonTypeInfo     () 13. [**](../../../../org/apache/spark/scheduler/SparkListenerExecutorAdded.html ""Permalink"")   case class    [SparkListenerExecutorAdded](SparkListenerExecutorAdded.html)   (    time:    Long     ,    executorId:    String     ,    executorInfo:  [ExecutorInfo](cluster/ExecutorInfo.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 14. [**](../../../../org/apache/spark/scheduler/SparkListenerExecutorExcluded.html ""Permalink"")   case class    [SparkListenerExecutorExcluded](SparkListenerExecutorExcluded.html)   (    time:    Long     ,    executorId:    String     ,    taskFailures:    Int     )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     ()     @Since     (     ""3.1.0""     ) 15. [**](../../../../org/apache/spark/scheduler/SparkListenerExecutorExcludedForStage.html ""Permalink"")   case class    [SparkListenerExecutorExcludedForStage](SparkListenerExecutorExcludedForStage.html)   (    time:    Long     ,    executorId:    String     ,    taskFailures:    Int     ,    stageId:   "	There are three types of schedulers in Apache Spark:   1. FIFO Scheduler 2. Fair Scheduler 3. Deadline Scheduler	1
no_reaction	['https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html']	What is the recommended way of creating an MLOps pipeline?	    MLOps workflow on Databricks ==============================     This article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.        What is MLOps? ----------------     MLOps is a set of processes and automated steps to manage code, data, and models. It combines DevOps, DataOps, and ModelOps.      ![MLOps lakehouse](../../_images/mlops-lakehouse.png)    ML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.         General recommendations for MLOps -----------------------------------     This section includes some general recommendations for MLOps on Databricks with links for more information.      ###   Create a separate environment for each stage     An execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.       Databricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:     * [Development](#development-stage) * [Staging](#staging-stage) * [Production](#production-stage)     Other configurations can also be used to meet the specific needs of your organization.       ###   Access control and versioning     Access control and versioning are key components of any software operations process. Databricks recommends the following:     * **Use Git for version control.**   Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use  [Databricks Repos](../../repos/index.html)  to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see  [Developer tools and guidance](../../dev-tools/index.html)  . * **Store data in a Lakehouse architecture using Delta tables.**   Data should be stored in a  [Lakehouse architecture](../../lakehouse/index.html)  in your cloud account. Both raw data and feature tables should be stored as  [Delta tables](../../delta/index.html)  with access controls to determine who can read and modify them. * **Manage models and model development with MLflow.**   You can use  [MLflow](../../mlflow/index.html)  to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the  [Model Registry](../../mlflow/model-registry.html)  to manage model versioning and deployment status. The Model Registry provides  [webhooks](../../mlflow/model-registry-webhooks.html)  and an API so you can integrate with CD systems, and also handles  [access control for models](../../security/auth-authz/access-control/workspace-acl.html#configure-mlflow-model-permissions)  .     ###   Deploy code, not models     In most situations, Databricks recommends that during the ML development process, you promote  *code*   , rather than  *models*   , from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see  [Model deployment patterns](deployment-patterns.html)  .                Recommended MLOps workflow ----------------------------     The following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.       This section uses the terms “data scientist” and “ML engineer” as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.           Development stage -------------------     The focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.      ![MLOps development stage diagram](../../_images/mlops-dev-diagram.png)    The numbered steps correspond to the numbers shown in the diagram.      ###   1. Data sources     Data scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.       ###   2. Exploratory data analysis (EDA)     Data scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and  [Databricks SQL](../../sql/index.html)  .       This ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.       ###   3. Code     All of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of Databricks and synced with the Databricks workspace using  [Databricks Repos](../../repos/index.html)  .       ###   4. Update feature tables     The model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the  [Feature Store](../feature-store/index.html)  . This pipeline includes 2 tasks:     * **Data preparation.**   Check for data quality issues. * **Create or update feature tables.**   Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.       Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.     ###   5. Train model     Data scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.       This pipeline includes 2 tasks:     * **Training and tuning.**   The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.       When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.       When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing. * **Evaluation.**   Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.       If your organization’s governance requirements include additional information about the model, you can save it using  [MLflow tracking](../../mlflow/tracking.html)  . Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.     ###   6. Commit code     After developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.            Staging stage ---------------     The focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.       ML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.      ![MLOps staging stage diagram](../../_images/mlops-staging-diagram.png)    The numbered steps correspond to the numbers shown in the diagram.       The staging environment can have its own storage area for testing feature tables and ML pipelines. This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.      ###   1. Merge request     The deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.       ###   2. Unit tests     The CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.       ###   3. Integration tests (CI)     The CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.       To reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.       After the integration tests pass on the staging branch, you can promote the code to production.       ###   4. Merge to staging branch     If the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.       You can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.       ###   5. Create a release branch     When the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.            Production stage ------------------     ML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.       Data scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.      ![MLOps production stage diagram](../../_images/mlops-prod-diagram.png)    The numbered steps correspond to the numbers shown in the diagram.      ###   1. Update feature tables     As new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.       ###   2. Train model     Train the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.       This pipeline includes 2 tasks:     * **Training and tuning.**   As in the development stage, autologging saves a record of the training process to the MLflow tracking server. This includes model metrics, parameters, tags, and the model itself.       During development, data scientists may test many algorithms and hyperparameters. In the production training code, it’s common to consider only the top-performing options. Limiting tuning in this way saves time and can reduce the variance from tuning in automated retraining. * **Evaluation.**   Model quality is evaluated by testing on held-out production data. The results of these tests are logged to the MLflow tracking server. This step uses the evaluation metrics specified by data scientists in the development stage. These metrics may include custom code.     When model training is complete, register the model artifact in the  [MLflow Model Registry](../../mlflow/model-	The recommended way of creating an MLOps pipeline is to create separate environments for each stage of ML code and model development with clearly defined transitions between stages. Databricks recommends using Git for version control, storing data in a Lakehouse architecture using Delta tables, and managing models and model development with MLflow. In most situations, Databricks recommends promoting code, rather than models, from one environment to the next. The MLOps workflow on Databricks includes three stages: development, staging, and production. Data scientists develop features and models and run experiments to optimize model performance in the development stage. The staging stage is for testing the ML pipeline code to ensure it is ready for production. The production stage is where ML pipelines are deployed, and ML engineers own the production environment.	1
no_reaction	['https://spark.apache.org/docs/latest/api/R/reference/createExternalTable-deprecated.html']	create external table with other db not s3	"  ![]()   (Deprecated) Create an external table =======================================  `createExternalTable-deprecated.Rd`    Creates an external table based on the dataset in a data source, Returns a SparkDataFrame associated with the external table.     Usage -------  ``` createExternalTable(tableName, path = NULL, source = NULL, schema = NULL, ...) ```   Arguments -----------   tableName     a name of the table.     path     the path of files to load.     source     the name of external data source.     schema     the schema of the data required for some data sources.     ...     additional argument(s) passed to the method.     Value -------   A SparkDataFrame.     Details ---------   The data source is specified by the  `source`   and a set of options(...). If  `source`   is not specified, the default data source configured by ""spark.sql.sources.default"" will be used.     Note ------   createExternalTable since 1.4.0     See also ----------  [createTable](createTable.html)    Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() df <- createExternalTable(""myjson"", path=""path/to/json"", source=""json"", schema) }  ```  "	"The `createExternalTable` function in SparkR can be used to create an external table based on the dataset in a data source other than S3. The function takes arguments such as `tableName`, `path`, `source`, and `schema` to specify the details of the external table. The `source` argument specifies the name of the external data source, and if it is not specified, the default data source configured by ""spark.sql.sources.default"" will be used."	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.localCheckpoint.html']	detailed info in checkpointing?	"   pyspark.sql.DataFrame.checkpoint  [¶](#pyspark-sql-dataframe-checkpoint ""Permalink to this headline"") =======================================================================================================  `DataFrame.`  `checkpoint`    (   *eager     :     bool     =     True*   )    → pyspark.sql.dataframe.DataFrame  [[source]](../../../_modules/pyspark/sql/dataframe.html#DataFrame.checkpoint) [¶](#pyspark.sql.DataFrame.checkpoint ""Permalink to this definition"")    Returns a checkpointed version of this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  . Checkpointing can be used to truncate the logical plan of this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  , which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with  `SparkContext.setCheckpointDir()`  .     New in version 2.1.0.     Parameters    **eager**    bool, optional, default True     Whether to checkpoint this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  immediately.     Returns    [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")   Checkpointed DataFrame.     Notes     This API is experimental.     Examples    ``` >>> import tempfile >>> df = spark.createDataFrame([ ...     (14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""]) >>> with tempfile.TemporaryDirectory() as d: ...     spark.sparkContext.setCheckpointDir(""/tmp/bb"") ...     df.checkpoint(False) DataFrame[age: bigint, name: string]  ```  [pyspark.sql.DataFrame.cache](pyspark.sql.DataFrame.cache.html ""previous page"")  [pyspark.sql.DataFrame.coalesce](pyspark.sql.DataFrame.coalesce.html ""next page"")   "	The `DataFrame.checkpoint()` method returns a checkpointed version of a DataFrame. Checkpointing can be used to truncate the logical plan of the DataFrame, which is especially useful in iterative algorithms where the plan may grow exponentially. The checkpointed DataFrame will be saved to files inside the checkpoint directory set with `SparkContext.setCheckpointDir()`. The `eager` parameter is an optional boolean that determines whether to checkpoint the DataFrame immediately.	1
no_reaction	['https://docs.databricks.com/sql/user/dashboards/index.html']	can you embed databricks dashboards	      Databricks SQL dashboards ===========================     You can use  [SQL editor](../sql-editor/index.html)  to build a Databricks SQL dashboard that combines  [visualizations](../../../visualizations/index.html)  and text boxes that provide context with your data.      * [View and organize dashboards](#view-and-organize-dashboards) * [Create a dashboard](#create-a-dashboard) * [Clone a dashboard](#clone-a-dashboard) * [Using query parameters in dashboards](#using-query-parameters-in-dashboards) * [Parameter properties](#parameter-properties) * [Remove content from a dashboard](#remove-content-from-a-dashboard) * [Edit a dashboard](#edit-a-dashboard) * [Edit a dashboard visualization](#edit-a-dashboard-visualization) * [Refresh a dashboard](#refresh-a-dashboard) * [Download as PDF](#download-as-pdf) * [Move a dashboard to Trash](#move-a-dashboard-to-trash) * [Restore a dashboard from Trash](#restore-a-dashboard-from-trash) * [Permanently delete a dashboard](#permanently-delete-a-dashboard) * [Open a query](#open-a-query) * [Configure dashboard permissions](#configure-dashboard-permissions) * [Access admin view](#access-admin-view) * [Transfer ownership of a dashboard](#transfer-ownership-of-a-dashboard)      [View and organize dashboards](#id2) -------------------------------------     Two ways to view dashboards:     * View new dashboards in the workspace browser by clicking  ![Workspace Icon](../../../_images/workspace-icon.png) **Workspace**   in the sidebar. These dashboards are viewable, by default, in the  **Home**   folder. Users can organize dashboards into folders in the workspace browser along with other Databricks objects. To view and organize currently existing dashboards, users (or admins) must migrate them into the workspace browser. For information on the Databricks SQL workspace browser, including migration, see  [Workspace browser in Databricks SQL](../workspace-browser/index.html)  . * All dashboards, regardless of creation time or migration status, can also be viewed by clicking the  ![Dashboards Icon](../../../_images/dashboards-icon.png) **Dashboards**   in the sidebar.     By default, dashboards are sorted in reverse chronological order. Reorder the list by clicking the column headings.      ###   Organize dashboards into folders in the workspace browser     Organize new and existing dashboards into folders in the workspace browser along with other Databricks objects. For more information, see  [Workspace browser in Databricks SQL](../workspace-browser/index.html)  .       ###   Filter the list of saved dashboards     Filter the list of all dashboards by dashboards that you created (  **My Dashboards**   ), by  [favorites](../sql-editor/favorites-tags.html#favorites)  , and by  [tags](../sql-editor/favorites-tags.html#tags)  .         [Create a dashboard](#id3) ---------------------------     Follow these steps to create a new dashboard. To clone an existing dashboard, see  [Clone a dashboard](#clone)  .     1. Do one of the following:      * Click   ![New Icon](../../../_images/create-icon.png)  **New**    in the sidebar and select   **Dashboard**    .  * Click   ![Dashboards Icon](../../../_images/dashboards-icon.png)  **Dashboards**    in the sidebar and click   **+ Create Dashboard**    .  * Click   ![Workspace Icon](../../../_images/workspace-icon.png)  **Workspace**    in the sidebar and click   **+ Create Dashboard**    .  * If you don’t want to start from an empty dashboard, you can create a quick dashboard:          1. Click    ![Data Icon](../../../_images/data-icon.png)   **Data**     in the sidebar.   2. Browse to the table, then in the    **Actions**     menu, click    **Create a quick dashboard**     .   3. In the configuration page, select columns of interest and create a dashboard and supporting queries that provide some basic information using those columns and showcase dashboard-level parameters and other capabilities.   4. Continue with the following steps to customize the dashboard. 2. Enter a name for the dashboard. 3. When you create a dashboard, you have the option to specify a warehouse to be used for manual refresh. If you do not select and save a warehouse to the dashboard, it will fall back to using the warehouse saved to individual queries.     ..note:: If you select and save a warehouse to the dashboard and then it is deleted or otherwise becomes unavailable, manual refresh will fail until a new one is assigned.     1. Add content to the dashboard by clicking  **Add**   and selecting the type of content to add:      * Click   **Textbox**    to add commentary.            Enter text. You can style the text boxes using   [Markdown](https://daringfireball.net/projects/markdown/syntax)    . To include static images within text boxes on your dashboards, use Markdown image syntax:   `![alt-text](<image-url>)`   .  * Click   **Visualization**    to add a query visualization.          1. Select a query. Search existing queries or pick a recent one from the pre-populated list.   2. In the    **Choose Visualization**     drop-down, select the visualization type.                ![Add to dashboard](../../../_images/add-chart-widget.png) 2. Click  **Add to Dashboard**   . 3. Drag and drop content blocks on the dashboard. 4. Click  **Done Editing**   .      ![Complete dashboard](../../../_images/dashboard.png)     You can also create a dashboard with the  [Databricks Terraform provider](../../../dev-tools/terraform/index.html)  and  [databricks\_sql\_dashboard](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_dashboard)   . You can create a  [widget](#add-content-to-a-dashboard)  for a dashboard with  [databricks\_sql\_widget](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_widget)   . You can create a sample dashboard with  [dbsql-nyc-taxi-trip-analysis](https://github.com/databricks/terraform-databricks-examples/tree/main/modules/dbsql-nyc-taxi-trip-analysis)   .          [Clone a dashboard](#id4) --------------------------     If you have the  **Can Run**   ,  **Can Edit**   , and  **Can Manage**   permissions on a dashboard and each of its upstream queries, you can clone the dashboard and all upstream queries. You become the owner of the new dashboard and queries.        Important       Sharing settings, alerts, and subscriptions are not copied to the new dashboard.        To clone a dashboard:     1. Open the vertical ellipsis  ![Vertical Ellipsis](../../../_images/vertical-ellipsis.png)  at the top-right of the dashboard and select  **Clone**   . 2. Enter a name for the new dashboard, then click  **Confirm**   .      [Using query parameters in dashboards](#id5) ---------------------------------------------     Queries can optionally leverage parameters or static values. When a visualization based on a parameterized query is added to a dashboard, the visualization can either be configured to use a:     * Widget parameter       Widget parameters are specific to a single visualization within a dashboard, appear within the visualization panel, and the parameter values specified apply only to the query underlying the visualization. * Dashboard parameter       Dashboard parameters can apply to multiple visualizations. When you add a visualization based on a parameterized query to a dashboard, by default the parameter will be added as a dashboard parameter. Dashboard parameters are configured for one or more visualizations within a dashboard and appear at the top of the dashboard. The parameter values specified for a dashboard parameter apply to visualizations reusing that particular dashboard parameter. A dashboard can have multiple dashboard parameters, each of which may apply to some visualizations and not to other visualizations. * Static value       Static values are used in lieu of a parameter that responds to changes. Static values allow you to hard code a value in place of a parameter and will make the parameter “disappear” from the dashboard or widget where it previously appeared.     When you add a visualization containing a parameterized query, you can choose the the title and the source for the parameter in the visualization query by clicking the appropriate pencil icon  ![Pencil Icon](../../../_images/pencil-icon.png)  . You can also select the keyword and a default value. See  [Parameter properties](#parameter-properties)  .      ![Parameter mapping](../../../_images/dashboard_parameter_mapping.png)    After adding a visualization to a dashboard, you can access the parameter mapping interface by clicking the vertical ellipsis  ![Vertical Ellipsis](../../../_images/vertical-ellipsis.png)  on the top right of a dashboard widget then clicking  **Change widget settings**   .      ![Open dashboard parameter mapping](../../../_images/dashboard_parameter_mapping_settings.png)   ![Change parameter mapping view](../../../_images/dashboard_parameter_mapping_view.png)     [Parameter properties](#id6) -----------------------------     The dashboard widget parameter properties are:     * **Title**   : The display name that appears beside the value selector on your dashboard. It defaults to the title set in the query editor. To edit how it appears within the dashboard, click the pencil icon  ![Pencil Icon](../../../_images/pencil-icon.png)  . Titles are not displayed for static dashboard parameters because the value selector is hidden. If you select  **Static value**   as your Value Source then the Title field is grayed out. * **Keyword**   : The string literal for this parameter in the underlying query. This is useful for debugging if your dashboard does not return expected results. * **Default Value**   : The value set for that parameter on dashboard load until another is selected and changes applied. To change this default, open the underlying query in the SQL editor, change the parameter to your desired value and click the  **Save**   button. * **Value Source**   : The source of the parameter value. Click the pencil icon  ![Pencil Icon](../../../_images/pencil-icon.png)  to choose a source.      + **New dashboard parameter**    : Create a new dashboard-level parameter. This lets you set a parameter value in one place on your dashboard and map it to one or more visualizations. Note that parameters must have unique names within the dashboard.  + **Existing dashboard parameter**    : Map this visualization’s parameter to an existing dashboard parameter. You must specify which pre-existing dashboard parameter.  + **Widget parameter**    : Displays a value selector inside your dashboard widget. This is useful for one-off parameters that are not shared between widgets.  + **Static value**    : Choose a static value for the widget, regardless of the values used on other widgets. Statically mapped parameter values do not display a value selector anywhere on the dashboard which is more compact. This lets you take advantage of the flexibility of query parameters without cluttering the user interface on a dashboard when certain parameters are not expected to change frequently. ![Change parameter mapping](../../../_images/dashboard_parameter_mapping_change.png)      [	Yes, you can embed Databricks SQL dashboards that combine visualizations and text boxes that provide context with your data. You can create a dashboard by following the steps outlined in the context document. Once you have created a dashboard, you can access the parameter mapping interface by clicking the vertical ellipsis on the top right of a dashboard widget then clicking **Change widget settings**. From there, you can choose the source of the parameter value, including a widget parameter, dashboard parameter, or static value.	0
no_reaction	['https://docs.databricks.com/ingestion/auto-loader/file-notification-mode.html']	for autoloader's file notification mode in aws, what are required iam permissions?	"      What is Auto Loader file notification mode? =============================================     In file notification mode, Auto Loader automatically sets up a notification service and queue service that subscribes to file events from the input directory. You can use file notifications to scale Auto Loader to ingest millions of files an hour. When compared to directory listing mode, file notification mode is more performant and scalable for large input directories or a high volume of files but requires additional cloud permissions.       You can switch between file notifications and directory listing at any time and still maintain exactly-once data processing guarantees.            Cloud resources used in Auto Loader file notification mode ------------------------------------------------------------      Important       You need elevated permissions to automatically configure cloud infrastructure for file notification mode. Contact your cloud administrator or workspace admin. See:     * [Required permissions for configuring file notification for ADLS Gen2 and Azure Blob Storage](#permissions-azure) * [Required permissions for configuring file notification for AWS S3](#permissions-s3) * [Securely ingest data in a different AWS account](#permissions-gcs)      Auto Loader can set up file notifications for you automatically when you set the option  `cloudFiles.useNotifications`  to  `true`  and provide the necessary permissions to create cloud resources. In addition, you might need to provide  [additional options](options.html#file-notification-options)  to grant Auto Loader authorization to create these resources.       The following table summarizes which resources are created by Auto Loader.             |   Cloud Storage   |   Subscription Service   |   Queue Service   |   Prefix \*   |   Limit \*\*   | | --- | --- | --- | --- | --- | |   AWS S3   |   AWS SNS   |   AWS SQS   |   databricks-auto-ingest   |   100 per S3 bucket   | |   ADLS Gen2   |   Azure Event Grid   |   Azure Queue Storage   |   databricks   |   500 per storage account   | |   GCS   |   Google Pub/Sub   |   Google Pub/Sub   |   databricks-auto-ingest   |   100 per GCS bucket   | |   Azure Blob Storage   |   Azure Event Grid   |   Azure Queue Storage   |   databricks   |   500 per storage account   |     \* Auto Loader names the resources with this prefix.       \*\* How many concurrent file notification pipelines can be launched       If you require running more than the limited number of file notification pipelines for a given storage account, you can:     * Consider rearchitecting how files are uploaded to leverage directory listing mode with  [incremental listing](directory-listing-mode.html)  instead of file notifications. * Leverage a service such as AWS Lambda, Azure Functions, or Google Cloud Functions to fan out notifications from a single queue that listens to an entire container or bucket into directory specific queues.    ###   File notification events     AWS S3 provides an  `ObjectCreated`  event when a file is uploaded to an S3 bucket regardless of whether it was uploaded by a put or multi-part upload.       ADLS Gen2 provides different event notifications for files appearing in your Gen2 container.     * Auto Loader listens for the  `FlushWithClose`  event for processing a file. * Auto Loader streams created with Databricks Runtime 8.3 and after support the  `RenameFile`  action for discovering files.  `RenameFile`  actions require an API request to the storage system to get the size of the renamed file. * Auto Loader streams created with Databricks Runtime 9.0 and after support the  `RenameDirectory`  action for discovering files.  `RenameDirectory`  actions require API requests to the storage system to list the contents of the renamed directory.     Google Cloud Storage provides an  `OBJECT\_FINALIZE`  event when a file is uploaded, which includes overwrites and file copies. Failed uploads do not generate this event.        Note       Cloud providers do not guarantee 100% delivery of all file events under very rare conditions and do not provide strict SLAs on the latency of the file events. Databricks recommends that you trigger regular backfills with Auto Loader by using the  `cloudFiles.backfillInterval`  option to guarantee that all files are discovered within a given SLA if data completeness is a requirement. Triggering regular backfills does not cause duplicates.             Required permissions for configuring file notification for ADLS Gen2 and Azure Blob Storage ---------------------------------------------------------------------------------------------     You must have read permissions for the input directory. See  [Azure Blob Storage](../../storage/azure-storage.html)  .       To use file notification mode, you must provide authentication credentials for setting up and accessing the event notification services. In Databricks Runtime 8.1 and above, you only need a service principal for authentication. For Databricks Runtime 8.0 and below, you must provide both a service principal and a connection string.     * Service principal - using Azure built-in roles       Create  [an Azure Active Directory app and service principal](https://learn.microsoft.com/azure/active-directory/develop/howto-create-service-principal-portal)   in the form of client ID and client secret.       Assign this app the following roles to the storage account in which the input path resides:      + **[Contributor](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#storage-account-contributor)**   : This role is for setting up resources in your storage account, such as queues and event subscriptions.  + **[Storage Queue Data Contributor](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#storage-queue-data-contributor)**   : This role is for performing queue operations such as retrieving and deleting messages from the queues. This role is required in Databricks Runtime 8.1 and above only when you provide a service principal without a connection string.  Assign this app the following role to the related resource group:      + **[EventGrid EventSubscription Contributor](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#eventgrid-eventsubscription-contributor)**   : This role is for performing event grid subscription operations such as creating or listing event subscriptions.  For more information, see  [Assign Azure roles using the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal)   . * Service principal - using custom role       If you are concerned with the execessive permissions required for the preceding roles, you can create a  **[Custom Role](https://learn.microsoft.com/azure/role-based-access-control/custom-roles-portal)**  with at least the following permissions, listed below in Azure role JSON format:        ``` ""permissions"": [  {  ""actions"": [  ""Microsoft.EventGrid/eventSubscriptions/write"",  ""Microsoft.EventGrid/eventSubscriptions/read"",  ""Microsoft.EventGrid/eventSubscriptions/delete"",  ""Microsoft.EventGrid/locations/eventSubscriptions/read"",  ""Microsoft.Storage/storageAccounts/read"",  ""Microsoft.Storage/storageAccounts/write"",  ""Microsoft.Storage/storageAccounts/queueServices/read"",  ""Microsoft.Storage/storageAccounts/queueServices/write"",  ""Microsoft.Storage/storageAccounts/queueServices/queues/write"",  ""Microsoft.Storage/storageAccounts/queueServices/queues/read"",  ""Microsoft.Storage/storageAccounts/queueServices/queues/delete""  ],  ""notActions"": [],  ""dataActions"": [  ""Microsoft.Storage/storageAccounts/queueServices/queues/messages/delete"",  ""Microsoft.Storage/storageAccounts/queueServices/queues/messages/read"",  ""Microsoft.Storage/storageAccounts/queueServices/queues/messages/write"",  ""Microsoft.Storage/storageAccounts/queueServices/queues/messages/process/action""  ],  ""notDataActions"": []  } ]  ```      Then, you can assign this custom role to your app.       For more information, see  [Assign Azure roles using the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal)   . * Connection string       Auto Loader requires a  [connection string](https://learn.microsoft.com/azure/storage/common/storage-configure-connection-string)   to authenticate for Azure Queue Storage operations, such as creating a queue and reading and deleting messages from the queue. The queue is created in the same storage account where the input directory path is located. You can find your connection string in your  [account key](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage)   or  [shared access signature (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview)   .       If you are using Databricks Runtime 8.1 or above, you do not need a connection string.       If you are using Databricks Runtime 8.0 or below, you must provide a  [connection string](https://learn.microsoft.com/azure/storage/common/storage-configure-connection-string)   to authenticate for Azure Queue Storage operations, such as creating a queue and retrieving and deleting messages from the queue. The queue is created in the same storage account in which the input path resides. You can find your connection string in your  [account key](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage)   or  [shared access signature (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview)   . When configuring an SAS token, you must provide the following permissions:    ![Auto loader permissions](../../_images/auto-loader-permissions.png)   ###   Troubleshooting common errors    **Error:**        ``` java.lang.RuntimeException: Failed to create event grid subscription.  ```      If you see this error message when you run Auto Loader for the first time, the Event Grid is not registered as a Resource Provider in your Azure subscription. To register this on Azure portal:     1. Go to your subscription. 2. Click  **Resource Providers**   under the Settings section. 3. Register the provider  `Microsoft.EventGrid`  .    **Error:**        ``` 403 Forbidden ... does not have authorization to perform action 'Microsoft.EventGrid/eventSubscriptions/[read|write]' over scope ...  ```      If you see this error message when you run Auto Loader for the first time, ensure you have given the  **Contributor**   role to your service principal for Event Grid as well as your storage account.            Required permissions for configuring file notification for AWS S3 -------------------------------------------------------------------     You must have read permissions for the input directory. See  [S3 connection details](../../storage/amazon-s3.html)  for more details.       To use file notification mode, attach the following JSON policy document to your  [IAM user or role](../../aws/iam/instance-profile-tutorial.html)  .        ``` {  ""Version"": ""2012-10-17"",  ""Statement"": [  {  ""Sid"": ""DatabricksAutoLoaderSetup"",  ""Effect"": ""Allow"",  ""Action"": [  ""s3:GetBucketNotification"",  ""s3:PutBucketNotification"",  ""sns:ListSubscriptionsByTopic"",  ""sns:GetTopicAttributes"",  ""sns:SetTopicAttributes"",  ""sns:CreateTopic"",  ""sns:TagResource"",  ""sns:Publish"",  ""sns:Subscribe"",  ""sqs:CreateQueue"",  ""sqs:DeleteMessage"",  ""sqs:DeleteMessageBatch"",  ""sqs:ReceiveMessage"",  ""sqs:SendMessage"",  ""sqs:GetQueueUrl"",  ""sqs:GetQueueAttributes"",  ""sqs:SetQueueAttributes"",  ""sqs:TagQueue"",  """	"To configure Auto Loader file notification mode in AWS S3, you need to attach the following JSON policy document to your IAM user or role:  ``` {  ""Version"": ""2012-10-17"",  ""Statement"": [  {  ""Sid"": ""DatabricksAutoLoaderSetup"",  ""Effect"": ""Allow"",  ""Action"": [  ""s3:GetBucketNotification"",  ""s3:PutBucketNotification"",  ""sns:ListSubscriptionsByTopic"",  ""sns:GetTopicAttributes"",  ""sns:SetTopicAttributes"",  ""sns:CreateTopic"",  ""sns:TagResource"",  ""sns:Publish"",  ""sns:Subscribe"",  ""sqs:CreateQueue"",  ""sqs:DeleteMessage"",  ""sqs:DeleteMessageBatch"",  ""sqs:ReceiveMessage"",  ""sqs:SendMessage"",  ""sqs:GetQueueUrl"",  ""sqs:GetQueueAttributes"",  ""sqs:SetQueueAttributes"",  ""sqs:TagQueue"", ```"	1
no_reaction	['https://kb.databricks.com/python/dbfs-file-size-limit']	How do I read a file larger than 2gb from dbfs?	Title: Reading large DBFS-mounted files using Python APIs  Description: Learn how to resolve errors when reading large DBFS-mounted files using Python APIs.  Answer: This article explains how to resolve an error that occurs when you read large DBFS-mounted files using local Python APIs. Problem If you mount a folder onto dbfs:// and read a file larger than 2GB in a Python API like pandas, you will see following error: /databricks/python/local/lib/python2.7/site-packages/pandas/parser.so in pandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)() /databricks/python/local/lib/python2.7/site-packages/pandas/parser.so in pandas.parser.TextReader._setup_parser_source (pandas/parser.c:6883)() IOError: Initializing from file failed Cause The error occurs because one argument in the Python method to read a file is a signed int, the length of the file is an int, and if the object is a file larger than 2GB, the length can be larger than maximum signed int. Solution Move the file from dbfs:// to local file system (file://). Then read using the Python API. For example: Copy the file from dbfs:// to file://:%fs cp dbfs:/mnt/large_file.csv file:/tmp/large_file.csv Read the file in the pandasAPI:%python import pandas as pd pd.read_csv('file:/tmp/large_file.csv',).head()	To read a file larger than 2GB from DBFS (Databricks File Store), you can move the file from dbfs:// to local file system (file://) and then read using the Python API. Here is an example: Copy the file from dbfs:// to file://:%fs cp dbfs:/mnt/large_file.csv file:/tmp/large_file.csv Read the file in the pandasAPI:%python import pandas as pd pd.read_csv('file:/tmp/large_file.csv',).head().	1
no_reaction	['https://docs.databricks.com/notebooks/testing.html']	"Q82- Incorporating unit tests into a PySpark application requires upfront attention to the design of your jobs, or a potentially significant refactoring of existing code.  Which statement describes a main benefit that offset this additional effort? a-	Troubleshooting is easier since all steps are isolated and tested individually b-	Yields faster deployment and execution times c-	Improves the quality of your data d-	Ensures that all steps interact correctly to achieve the desired end result e-	Validates a complete use case of your application"	"    Unit testing for notebooks ============================     You can use  *unit testing*   to help improve the quality and consistency of your notebooks’ code. Unit testing is an approach to testing self-contained units of code, such as functions, early and often. This helps you find problems with your code faster, uncover mistaken assumptions about your code sooner, and streamline your overall coding efforts.       This article is an introduction to basic  [unit testing](https://en.wikipedia.org/wiki/Unit_testing)   with functions. Advanced concepts such as unit testing classes and interfaces, as well as the use of  [stubs](https://en.wikipedia.org/wiki/Method_stub)   ,  [mocks](https://en.wikipedia.org/wiki/Mock_object)   , and  [test harnesses](https://en.wikipedia.org/wiki/Test_harness)   , while also supported when unit testing for notebooks, are outside the scope of this article. This article also does not cover other kinds of testing methods, such as  [integration testing](https://en.wikipedia.org/wiki/Integration_testing)   ,  [system testing](https://en.wikipedia.org/wiki/System_testing)   ,  [acceptance testing](https://en.wikipedia.org/wiki/Acceptance_testing)   , or  [non-functional testing](https://en.wikipedia.org/wiki/Non-functional_testing)   methods such as  [performance testing](https://en.wikipedia.org/wiki/Software_performance_testing)   or  [usability testing](https://en.wikipedia.org/wiki/Usability_testing)   .       This article demonstrates the following:     * How to organize functions and their unit tests. * How to write functions in Python, R, Scala, as well as user-defined functions in SQL, that are well-designed to be unit tested. * How to call these functions from Python, R, Scala, and SQL notebooks. * How to write unit tests in Python, R, and Scala by using the popular test frameworks  [pytest](https://docs.pytest.org)   for Python,  [testthat](https://testthat.r-lib.org)   for R, and  [ScalaTest](https://docs.pytest.org)   for Scala. Also how to write SQL that unit tests SQL user-defined functions (SQL UDFs). * How to run these unit tests from Python, R, Scala, and SQL notebooks.      Organize functions and unit tests -----------------------------------       There are a few common approaches for organizing your functions and their unit tests with notebooks. Each approach has its benefits and challenges.       For Python, R, and Scala notebooks, common approaches include the following:     * [Store functions and their unit tests outside of notebooks.](test-notebooks.html#separate-test-code-from-the-notebook)  .      + Benefits: You can call these functions with and outside of notebooks. Test frameworks are better designed to run tests outside of notebooks.  + Challenges: This approach is not supported for Scala notebooks. This approach requires   [Databricks Repos](../repos/index.html)   . This approach also increases the number of files to track and maintain. * [Store functions in one notebook and their unit tests in a separate notebook.](test-notebooks.html#separate-test-code-from-the-notebook)  .      + Benefits: These functions are easier to reuse across notebooks.  + Challenges: The number of notebooks to track and maintain increases. These functions cannot be used outside of notebooks. These functions can also be more difficult to test outside of notebooks. * [Store functions and their unit tests within the same notebook.](test-notebooks.html)  .      + Benefits: Functions and their unit tests are stored within a single notebook for easier tracking and maintenance.  + Challenges: These functions can be more difficult to reuse across notebooks. These functions cannot be used outside of notebooks. These functions can also be more difficult to test outside of notebooks.     For Python and R notebooks, Databricks recommends storing functions and their unit tests outside of notebooks. For Scala notebooks, Databricks recommends including functions in one notebook and their unit tests in a separate notebook.       For SQL notebooks, Databricks recommends that you store functions as SQL user-defined functions (SQL UDFs) in your schemas (also known as databases). You can then call these SQL UDFs and their unit tests from SQL notebooks.         Write functions -----------------     This section describes a simple set of example functions that determine the following:     * Whether a table exists in a database. * Whether a column exists in a table. * How many rows exist in a column for a value within that column.     These functions are intended to be simple, so that you can focus on the unit testing details in this article rather than focus on the functions themselves.       To get the best unit testing results, a function should return a single predictable outcome and be of a single data type. For example, to check whether something exists, the function should return a boolean value of true or false. To return the number of rows that exist, the function should return a non-negative, whole number. It should not, in the first example, return either false if something does not exist or the thing itself if it does exist. Likewise, for the second example, it should not return either the number of rows that exist or false if no rows exist.       You can add these functions to an existing Databricks workspace as follows, in Python, R, Scala, or SQL.         The following code assumes you have  [Set up Databricks Repos](../repos/repos-setup.html)  ,  [added a repo](../repos/git-operations-with-repos.html)  , and have the repo open in your Databricks workspace.      [Create a file](../files/workspace-basics.html#create-a-new-file)  named  `myfunctions.py`  within the repo, and add the following contents to the file. Other examples in this article expect this file to be named  `myfunctions.py`  . You can use different names for your own files.        ``` import pyspark from pyspark.sql import SparkSession from pyspark.sql.functions import col  # Because this file is not a Databricks notebook, you # must create a Spark session. Databricks notebooks # create a Spark session for you by default. spark = SparkSession.builder \                     .appName('integrity-tests') \                     .getOrCreate()  # Does the specified table exist in the specified database? def tableExists(tableName, dbName):   return spark.catalog.tableExists(f""{dbName}.{tableName}"")  # Does the specified column exist in the given DataFrame? def columnExists(dataFrame, columnName):   if columnName in dataFrame.columns:     return True   else:     return False  # How many rows are there for the specified value in the specified column # in the given DataFrame? def numRowsInColumnForValue(dataFrame, columnName, columnValue):   df = dataFrame.filter(col(columnName) == columnValue)    return df.count()  ```        The following code assumes you have  [Set up Databricks Repos](../repos/repos-setup.html)  ,  [added a repo](../repos/git-operations-with-repos.html)  , and have the repo open in your Databricks workspace.      [Create a file](../files/workspace-basics.html#create-a-new-file)  named  `myfunctions.r`  within the repo, and add the following contents to the file. Other examples in this article expect this file to be named  `myfunctions.r`  . You can use different names for your own files.        ``` library(SparkR)  # Does the specified table exist in the specified database? table\_exists <- function(table\_name, db\_name) {   tableExists(paste(db\_name, ""."", table\_name, sep = """")) }  # Does the specified column exist in the given DataFrame? column\_exists <- function(dataframe, column\_name) {   column\_name %in% colnames(dataframe) }  # How many rows are there for the specified value in the specified column # in the given DataFrame? num\_rows\_in\_column\_for\_value <- function(dataframe, column\_name, column\_value) {   df = filter(dataframe, dataframe[[column\_name]] == column\_value)    count(df) }  ```        Create a  [Scala notebook](notebooks-manage.html#create-a-notebook)  named  `myfunctions`  with the following contents. Other examples in this article expect this notebook to be named  `myfunctions`  . You can use different names for your own notebooks.        ``` import org.apache.spark.sql.DataFrame import org.apache.spark.sql.functions.col  // Does the specified table exist in the specified database? def tableExists(tableName: String, dbName: String) : Boolean = {  return spark.catalog.tableExists(dbName + ""."" + tableName) }  // Does the specified column exist in the given DataFrame? def columnExists(dataFrame: DataFrame, columnName: String) : Boolean = {  val nameOfColumn = null   for(nameOfColumn <- dataFrame.columns) {  if (nameOfColumn == columnName) {  return true  }  }   return false }  // How many rows are there for the specified value in the specified column // in the given DataFrame? def numRowsInColumnForValue(dataFrame: DataFrame, columnName: String, columnValue: String) : Long = {  val df = dataFrame.filter(col(columnName) === columnValue)   return df.count() }  ```        The following code assumes you have the third-party sample dataset  [diamonds](../dbfs/databricks-datasets.html)  within a schema named  `default`  within a catalog named  `main`  that is accessible from your Databricks workspace. If the catalog or schema that you want to use has a different name, then change one or both of the following  `USE`  statements to match.       Create a  [SQL notebook](notebooks-manage.html#create-a-notebook)  and add the following contents to this new notebook. Then  [attach](notebook-ui.html#attach)  the notebook to a cluster and  [run](run-notebook.html)  the notebook to add the following SQL UDFs to the specified catalog and schema.        Note       The SQL UDFs  `table\_exists`  and  `column\_exists`  work only with Unity Catalog. SQL UDF support for Unity Catalog is in  [Public Preview](../release-notes/release-types.html)  .         ``` USE CATALOG main; USE SCHEMA default;  CREATE OR REPLACE FUNCTION table\_exists(catalog\_name STRING,  db\_name STRING,  table\_name STRING)  RETURNS BOOLEAN  RETURN if(  (SELECT count(\*) FROM system.information\_schema.tables  WHERE table\_catalog = table\_exists.catalog\_name  AND table\_schema = table\_exists.db\_name  AND table\_name = table\_exists.table\_name) > 0,  true,  false  );  CREATE OR REPLACE FUNCTION column\_exists(catalog\_name STRING,  db\_name STRING,  table\_name STRING,  column\_name STRING)  RETURNS BOOLEAN  RETURN if(  (SELECT count(\*) FROM system.information\_schema.columns  WHERE table\_catalog = column\_exists.catalog\_name  AND table\_schema = column\_exists.db\_name  AND table\_name = column\_exists.table\_name  AND column\_name = column\_exists.column\_name) >"	The main benefit of incorporating unit tests into a PySpark application is that troubleshooting is easier since all steps are isolated and tested individually. Therefore, option a is the correct answer.	0
no_reaction	['https://docs.databricks.com/delta-live-tables/tutorial-pipelines.html']	How to start using databricks delta live tables?	      Tutorial: Run your first Delta Live Tables pipeline =====================================================     This tutorial shows you how to configure a Delta Live Tables data pipeline from code in a Databricks notebook and to trigger an update. The instructions provided are general enough to cover most notebooks with properly-defined Delta Live Tables syntax.       You can use these instructions to schedule notebooks you created by following the  [Python](tutorial-python.html)  or  [SQL](tutorial-sql.html)  Delta Live Tables tutorials, or import and use one of the  [notebooks](#notebooks)  provided on this page.       You can configure Delta Live Tables pipelines and trigger updates using the Databricks workspace UI or automated tooling options such as the API and CLI. Databricks recommends familiarizing yourself with the UI first, which can be used to generate JSON configuration files for programmatic execution.        Important       To start a pipeline, you must have  [cluster creation permission](../clusters/clusters-manage.html#cluster-permissions)  or access to a cluster policy defining a Delta Live Tables cluster. The Delta Live Tables runtime creates a cluster before it runs your pipeline and fails if you don’t have the correct permission.           Create a pipeline -------------------     Delta Live Tables creates pipelines by resolving dependencies defined in notebooks or files (called  *source code*   or  *libraries*   ) using Delta Live Tables syntax. Each source code file can only contain one language, but you can mix libraries of different languages within your pipeline.     1. Click  ![Jobs Icon](../_images/jobs-icon.png) **Workflows**   in the sidebar, click the  **Delta Live Tables**   tab, and click  **Create Pipeline**   . 2. Give the pipeline a name and click  ![File Picker Icon](../_images/file-picker.png)  to select a notebook. 3. Select  **Triggered**   for  **Pipeline Mode**   . 4. (Optional) Enter a  **Storage location**   for output data from the pipeline. The system uses a default location if you leave  **Storage location**   empty. 5. (Optional) Specify a  **Target schema**   to publish your dataset to the metastore. See  [Publish datasets to the metastore](#publish)  . 6. Click  **Create**   .     The system displays the  **Pipeline Details**   page after you click  **Create**   . You can also access your pipeline by clicking the pipeline name in the  **Delta Live Tables**   tab.           Start a pipeline update -------------------------     To start an update for a pipeline, click the  ![Delta Live Tables Start Icon](../_images/dlt-start-button.png)  button in the top panel. The system returns a message confirming that your pipeline is starting.       After successfully starting the update, the Delta Live Tables system:     1. Starts a cluster using a cluster configuration created by the Delta Live Tables system. You can also specify a custom  [cluster configuration](settings.html#cluster-config)  . 2. Creates any tables that don’t exist and ensures that the schema is correct for any existing tables. 3. Updates tables with the latest data available. 4. Shuts down the cluster when the update is complete.      Note       Execution mode is set to  **Production**   by default, which deploys ephemeral compute resources for each update. You can use  **Development**   mode to change this behavior, allowing the same compute resources to be used for multiple pipeline updates during development and testing. See  [Development and production modes](updates.html#optimize-execution)  .            Publish datasets to the metastore -----------------------------------     You can make Delta Live Tables datasets available for querying by publishing tables to a schema in the metastore. If a target schema is not specified, tables created in Delta Live Tables pipelines can only be accessed by other operations within that same pipeline. See  [Publish data from Delta Live Tables pipelines](publish.html)  and  [What is a metastore?](../lakehouse/data-objects.html#metastore)  .           Example source code notebooks -------------------------------     You can import these notebooks into the Databricks workspace and use them to deploy a Delta Live Tables pipeline. To learn more about the logic and syntax used in these notebooks, see  [Tutorial: Declare a data pipeline with Python in Delta Live Tables](tutorial-python.html)  or  [Tutorial: Declare a data pipeline with SQL in Delta Live Tables](tutorial-sql.html)  .        ###   Get started with Delta Live Tables Python notebook     [Open notebook in new tab](/_extras/notebooks/source/dlt-wikipedia-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ###   Get started with Delta Live Tables SQL notebook     [Open notebook in new tab](/_extras/notebooks/source/dlt-wikipedia-sql.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 	To start using Databricks Delta Live Tables, you can configure a pipeline from code in a Databricks notebook and trigger an update. You can use the instructions provided in the tutorial to schedule notebooks you created by following the Python or SQL Delta Live Tables tutorials, or import and use one of the notebooks provided on the page. You can configure Delta Live Tables pipelines and trigger updates using the Databricks workspace UI or automated tooling options such as the API and CLI. To start a pipeline, you must have cluster creation permission or access to a cluster policy defining a Delta Live Tables cluster. The Delta Live Tables runtime creates a cluster before it runs your pipeline and fails if you don’t have the correct permission.	1
no_reaction	['https://docs.databricks.com/data/manage-permissions.html']	in unity catalog, how do I grant permissions on a view?	"    Create views ==============     This article shows how to create views in Unity Catalog.       A view is a read-only object composed from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs.      [Dynamic views](#dynamic-view)  can be used to provide row- and column-level access control, in addition to data masking.       The owner of a view must have the ability to read the tables and views referenced in the view. A reader of a view does not need the ability to read the tables and views referenced in the view, unless they are using a cluster with single user access mode.        Note       To read from a view from a cluster with  **Single User**  [access mode](index.html#cluster-security-mode)  , you must have  `SELECT`  on all referenced tables and views. Dynamic views are not supported with  **Single User**   access mode.        Example syntax for creating a view:        ``` CREATE VIEW main.default.experienced\_employee  (id COMMENT 'Unique identification number', Name)  COMMENT 'View for experienced employees' AS SELECT id, name  FROM all\_employee  WHERE working\_years > 5;  ```       Requirements --------------   * You must have the  `USE     CATALOG`  permission on the parent catalog and the  `USE     SCHEMA`  and  `CREATE     TABLE`  permissions on the parent schema. A metastore admin or the catalog owner can grant you all of these privileges. A schema owner can grant you  `USE     SCHEMA`  and  `CREATE     TABLE`  privileges on the schema. * You must have the ability to read the tables and views referenced in the view (  `SELECT`  on the table or view, as well as  `USE     CATALOG`  on the catalog and  `USE     SCHEMA`  on the schema).     If a view references tables in the workspace-local Hive metastore, the view can be accessed only from the workspace that contains the workspace-local tables. For this reason, Databricks recommends creating views only from tables or views that are in the Unity Catalog metastore.         Create a view ---------------     To create a view, run the following SQL command. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. * `<schema\_name>`  : The name of the schema. * `<view\_name>`  : A name for the view. * `<query>`  : The query, columns, and tables and views used to compose the view.       ``` CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS SELECT <query>;  ```       ``` spark.sql(""CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS ""   ""SELECT <query>"")  ```       ``` library(SparkR)  sql(paste(""CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS "",  ""SELECT <query>"",  sep = """"))  ```       ``` spark.sql(""CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS "" +  ""SELECT <query>"")  ```       For example, to create a view named  `sales\_redacted`  from columns in the  `sales\_raw`  table:         ``` CREATE VIEW sales\_metastore.sales.sales\_redacted AS SELECT  user\_id,  email,  country,  product,  total FROM sales\_metastore.sales.sales\_raw;  ```       ``` spark.sql(""CREATE VIEW sales\_metastore.sales.sales\_redacted AS ""   ""SELECT ""   "" user\_id, ""   "" email, ""   "" country, ""   "" product, ""   "" total ""   ""FROM sales\_metastore.sales.sales\_raw"")  ```       ``` library(SparkR)  sql(paste(""CREATE VIEW sales\_metastore.sales.sales\_redacted AS "",  ""SELECT "",  "" user\_id, "",  "" email, "",  "" country, "",  "" product, "",  "" total "",  ""FROM sales\_metastore.sales.sales\_raw"",  sep = """"))  ```       ``` spark.sql(""CREATE VIEW sales\_metastore.sales.sales\_redacted AS "" +  ""SELECT "" +  "" user\_id, "" +  "" email, "" +  "" country, "" +  "" product, "" +  "" total "" +  ""FROM sales\_metastore.sales.sales\_raw"")  ```       You can also create a view by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of view full names by using  [databricks\_views](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/views)   .           Create a dynamic view -----------------------     In Unity Catalog, you can use dynamic views to configure fine-grained access control, including:     * Security at the level of columns or rows. * Data masking.      Note       Fine-grained access control using dynamic views is not available on clusters with  **Single User**  [access mode](compute.html#access-mode)  .        Unity Catalog introduces the following functions, which allow you to dynamically limit which users can access a row, column, or record in a view:     * `current\_user()`  : Returns the current user’s email address. * `is\_account\_group\_member()`  : Returns  `TRUE`  if the current user is a member of a specific account-level group. Recommended for use in dynamic views against Unity Catalog data. * `is\_member()`  : Returns  `TRUE`  if the current user is a member of a specific workspace-level group. This function is provided for compatibility with the existing Hive metastore. Avoid using it with views against Unity Catalog data, because it does not evaluate account-level group membership.     Databricks recommends that you do not grant users the ability to read the tables and views referenced in the view.       The following examples illustrate how to create dynamic views in Unity Catalog.      ###   Column-level permissions     With a dynamic view, you can limit the columns a specific user or group can access. In the following example, only members of the  `auditors`  group can access email addresses from the  `sales\_raw`  table. During query analysis, Apache Spark replaces the  `CASE`  statement with either the literal string  `REDACTED`  or the actual contents of the email address column. Other columns are returned as normal. This strategy has no negative impact on the query performance.         ``` -- Alias the field 'email' to itself (as 'email') to prevent the -- permission logic from showing up directly in the column name results. CREATE VIEW sales\_redacted AS SELECT  user\_id,  CASE WHEN  is\_account\_group\_member('auditors') THEN email  ELSE 'REDACTED'  END AS email,  country,  product,  total FROM sales\_raw  ```       ``` # Alias the field 'email' to itself (as 'email') to prevent the # permission logic from showing up directly in the column name results. spark.sql(""CREATE VIEW sales\_redacted AS ""   ""SELECT ""   "" user\_id, ""   "" CASE WHEN ""   "" is\_account\_group\_member('auditors') THEN email ""   "" ELSE 'REDACTED' ""   "" END AS email, ""   "" country, ""   "" product, ""   "" total ""   ""FROM sales\_raw"")  ```       ``` library(SparkR)  # Alias the field 'email' to itself (as 'email') to prevent the # permission logic from showing up directly in the column name results. sql(paste(""CREATE VIEW sales\_redacted AS "",  ""SELECT "",  "" user\_id, "",  "" CASE WHEN "",  "" is\_account\_group\_member('auditors') THEN email "",  "" ELSE 'REDACTED' "",  "" END AS email, "",  "" country, "",  "" product, "",  "" total "",  ""FROM sales\_raw"",  sep = """"))  ```       ``` // Alias the field 'email' to itself (as 'email') to prevent the // permission logic from showing up directly in the column name results. spark.sql(""CREATE VIEW sales\_redacted AS "" +  ""SELECT "" +  "" user\_id, "" +  "" CASE WHEN "" +  "" is\_account\_group\_member('auditors') THEN email "" +  "" ELSE 'REDACTED' "" +  "" END AS email, "" +  "" country, "" +  "" product, "" +  "" total "" +  ""FROM sales\_raw"")  ```       ###   Row-level permissions     With a dynamic view, you can specify permissions down to the row or field level. In the following example, only members of the  `managers`  group can view transaction amounts when they exceed $1,000,000. Matching results are filtered out for other users.         ```  CREATE VIEW sales\_redacted AS  SELECT  user\_id,  country,  product,  total  FROM sales\_raw  WHERE  CASE  WHEN is\_account\_group\_member('managers') THEN TRUE  ELSE total <= 1000000  END;  ```       ```  spark.sql(""CREATE VIEW sales\_redacted AS ""    ""SELECT ""    "" user\_id, ""    "" country, ""    "" product, ""    "" total ""    ""FROM sales\_raw ""    ""WHERE ""    ""CASE ""    "" WHEN is\_account\_group\_member('managers') THEN TRUE ""    "" ELSE total <= 1000000 ""    ""END"")  ```       ```  library(SparkR)   sql(paste(""CREATE VIEW sales\_redacted AS "",  ""SELECT "",  "" user\_id, "",  "" country, "",  "" product, "",  "" total "",  ""FROM sales\_raw "",  ""WHERE "",  ""CASE "",  "" WHEN is\_account\_group\_member('managers') THEN TRUE "",  "" ELSE total <= 1000000 "",  ""END"",  sep = """"))  ```       ```  spark.sql(""CREATE VIEW sales\_redacted AS "" +  ""SELECT "" +  "" user\_id, "" +  "" country, "" +  "" product, "" +  "" total "" +  ""FROM sales\_raw "" +  ""WHERE "" +  ""CASE "" +  "" WHEN is\_account\_group\_member('managers') THEN TRUE "" +  "" ELSE total <= 1000000 "" +  ""END"")  ```       ###   Data masking     Because views in Unity Catalog use Spark SQL, you can implement advanced data masking by using more complex SQL expressions and regular expressions. In the following example, all users can analyze email domains, but only members of the  `auditors`  group can view a user’s entire email address.         ``` -- The regexp\_extract function takes an email address such as -- user.x.lastname@example.com and"	To grant permissions on a view in Unity Catalog, you must have the `USE CATALOG` permission on the parent catalog and the `USE SCHEMA` and `CREATE TABLE` permissions on the parent schema. The owner of a view must have the ability to read the tables and views referenced in the view. A reader of a view does not need the ability to read the tables and views referenced in the view, unless they are using a cluster with single-user access mode. To create a view, you can use the `CREATE VIEW` command followed by the name of the view and the query that defines the view.	0
no_reaction	['https://kb.databricks.com/data-sources/access-adls1-from-sparklyr']	how do I read files from ADLS in Databricks?	Title: Access files written by Apache Spark on ADLS Gen1  Description: Configure permissions to allow access to files that Apache Spark writes to ADLS Gen1 storage.  Answer: Problem You are using Azure Databricks and have a Spark job that is writing to ADLS Gen1 storage. When you try to manually read, write, or delete data in the folders you get an error message. Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation Cause When writing data to ADLS Gen1 storage, Apache Spark uses the service principal as the owner of the files it creates. The service principal is defined in dfs.adls.oauth2.client.id. When files are created, they inherit the default permissions from the Hadoop filesystem. The Hadoop filesystem has a default permission of 666 (-rw-rw-rw-) and a default umask of 022, which results in the 644 permission setting as the default for files. When folders are created, they inherit the parent folder permissions, which are 770 by default. Because the owner is the service principal and not the user, you don’t have permission to access the folder due to the 0 bit in the folder permissions. Solution Option 1 Make the service principal user part of the same group as the default user. This will allow access when accessing storage through the portal. Please reach out to Microsoft support for assistance. Option 2 Create a base folder in ADLS Gen1 and set the permissions to 777. Write Spark output under this folder. Because folders created by Spark inherit the parent folder permissions, all folders created by Spark will have 777 permissions. This allows any user to access the folders. Option 3 Change the default umask from 022 to 000 on your Azure Databricks clusters. Set spark.hadoop.fs.permissions.umask-mode 000 in the Spark config for your cluster. With a umask of 000, the default Hadoop filesystem permission of 666 becomes the default permission used when Azure Databricks creates objects.	To read files from ADLS in Databricks, you need to configure permissions to allow access to the files that Apache Spark writes to ADLS Gen1 storage. When writing data to ADLS Gen1 storage, Apache Spark uses the service principal as the owner of the files it creates. The service principal is defined in dfs.adls.oauth2.client.id. When files are created, they inherit the default permissions from the Hadoop filesystem. The Hadoop filesystem has a default permission of 666 (-rw-rw-rw-) and a default umask of 022, which results in the 644 permission setting as the default for files. You can use Option 1, Option 2, or Option 3 to solve the problem. Option 1 is to make the service principal user part of the same group as the default user. Option 2 is to create a base folder in ADLS Gen1 and set the permissions to 777. Option 3 is to change the default umask from 022 to 000 on your Azure Databricks clusters. Set spark.hadoop.fs.permissions.umask-mode 000 in the Spark config for your cluster.	0
no_reaction	['https://docs.databricks.com/sql/language-manual/data-types/struct-type.html']	With Databricks SQL how do I check if a struct has a field?	"  ![]()   structField =============  `structField.Rd`    Create a structField object that contains the metadata for a single field in a schema.     Usage -------  ``` structField(x, ...)  # S3 method for jobj structField(x, ...)  # S3 method for character structField(x, type, nullable = TRUE, ...) ```   Arguments -----------   x     the name of the field.     ...     additional argument(s) passed to the method.     type     The data type of the field     nullable     A logical vector indicating whether or not the field is nullable     Value -------   A structField object.     Note ------   structField since 1.4.0     Examples ----------  ``` if (FALSE) { field1 <- structField(""a"", ""integer"") field2 <- structField(""c"", ""string"") field3 <- structField(""avg"", ""double"") schema <- [structType](structType.html)(field1, field2, field3) df1 <- [gapply](gapply.html)(df, [list](https://rdrr.io/r/base/list.html)(""a"", ""c""),  function(key, x) { y <- [data.frame](https://rdrr.io/r/base/data.frame.html)(key, [mean](column_aggregate_functions.html)(x$b), stringsAsFactors = FALSE) },  schema) }  ```  "	To check if a struct has a field using Databricks SQL, you can use the `hasField` function. The `hasField` function takes two arguments: the first argument is the struct to check, and the second argument is the name of the field to check for. If the field exists in the struct, the function returns true, otherwise, it returns false.	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html']	Identify why it is important to add indicator variables for missing values that have been imputed or replaced.	"   Imputer  [¶](#imputer ""Permalink to this headline"") =====================================================  *class*  `pyspark.ml.feature.`  `Imputer`    (   *\**  ,  *strategy     :     str     =     'mean'*  ,  *missingValue     :     float     =     nan*  ,  *inputCols     :     Optional    [    List    [    str    ]     ]     =     None*  ,  *outputCols     :     Optional    [    List    [    str    ]     ]     =     None*  ,  *inputCol     :     Optional    [    str    ]     =     None*  ,  *outputCol     :     Optional    [    str    ]     =     None*  ,  *relativeError     :     float     =     0.001*   )   [[source]](../../_modules/pyspark/ml/feature.html#Imputer) [¶](#pyspark.ml.feature.Imputer ""Permalink to this definition"")    Imputation estimator for completing missing values, using the mean, median or mode of the columns in which the missing values are located. The input columns should be of numeric type. Currently Imputer does not support categorical features and possibly creates incorrect values for a categorical feature.     Note that the mean/median/mode value is computed after filtering out missing values. All Null values in the input columns are treated as missing, and so are also imputed. For computing median,  [`pyspark.sql.DataFrame.approxQuantile()`](../pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html#pyspark.sql.DataFrame.approxQuantile ""pyspark.sql.DataFrame.approxQuantile"")  is used with a relative error of    0.001    .     New in version 2.2.0.     Examples    ``` >>> df = spark.createDataFrame([(1.0, float(""nan"")), (2.0, float(""nan"")), (float(""nan""), 3.0), ...                             (4.0, 4.0), (5.0, 5.0)], [""a"", ""b""]) >>> imputer = Imputer() >>> imputer.setInputCols([""a"", ""b""]) Imputer... >>> imputer.setOutputCols([""out\_a"", ""out\_b""]) Imputer... >>> imputer.getRelativeError() 0.001 >>> model = imputer.fit(df) >>> model.setInputCols([""a"", ""b""]) ImputerModel... >>> model.getStrategy() 'mean' >>> model.surrogateDF.show() +---+---+ | a| b| +---+---+ |3.0|4.0| +---+---+ ... >>> model.transform(df).show() +---+---+-----+-----+ | a| b|out\_a|out\_b| +---+---+-----+-----+ |1.0|NaN| 1.0| 4.0| |2.0|NaN| 2.0| 4.0| |NaN|3.0| 3.0| 3.0| ... >>> imputer.setStrategy(""median"").setMissingValue(1.0).fit(df).transform(df).show() +---+---+-----+-----+ | a| b|out\_a|out\_b| +---+---+-----+-----+ |1.0|NaN| 4.0| NaN| ... >>> df1 = spark.createDataFrame([(1.0,), (2.0,), (float(""nan""),), (4.0,), (5.0,)], [""a""]) >>> imputer1 = Imputer(inputCol=""a"", outputCol=""out\_a"") >>> model1 = imputer1.fit(df1) >>> model1.surrogateDF.show() +---+ | a| +---+ |3.0| +---+ ... >>> model1.transform(df1).show() +---+-----+ | a|out\_a| +---+-----+ |1.0| 1.0| |2.0| 2.0| |NaN| 3.0| ... >>> imputer1.setStrategy(""median"").setMissingValue(1.0).fit(df1).transform(df1).show() +---+-----+ | a|out\_a| +---+-----+ |1.0| 4.0| ... >>> df2 = spark.createDataFrame([(float(""nan""),), (float(""nan""),), (3.0,), (4.0,), (5.0,)], ...                             [""b""]) >>> imputer2 = Imputer(inputCol=""b"", outputCol=""out\_b"") >>> model2 = imputer2.fit(df2) >>> model2.surrogateDF.show() +---+ | b| +---+ |4.0| +---+ ... >>> model2.transform(df2).show() +---+-----+ | b|out\_b| +---+-----+ |NaN| 4.0| |NaN| 4.0| |3.0| 3.0| ... >>> imputer2.setStrategy(""median"").setMissingValue(1.0).fit(df2).transform(df2).show() +---+-----+ | b|out\_b| +---+-----+ |NaN| NaN| ... >>> imputerPath = temp\_path + ""/imputer"" >>> imputer.save(imputerPath) >>> loadedImputer = Imputer.load(imputerPath) >>> loadedImputer.getStrategy() == imputer.getStrategy() True >>> loadedImputer.getMissingValue() 1.0 >>> modelPath = temp\_path + ""/imputer-model"" >>> model.save(modelPath) >>> loadedModel = ImputerModel.load(modelPath) >>> loadedModel.transform(df).head().out\_a == model.transform(df).head().out\_a True  ```   Methods    |  [`clear`](#pyspark.ml.feature.Imputer.clear ""pyspark.ml.feature.Imputer.clear"")  (param)   |   Clears a param from the param map if it has been explicitly set.   | |  [`copy`](#pyspark.ml.feature.Imputer.copy ""pyspark.ml.feature.Imputer.copy"")  ([extra])   |   Creates a copy of this instance with the same uid and some extra params.   | |  [`explainParam`](#pyspark.ml.feature.Imputer.explainParam ""pyspark.ml.feature.Imputer.explainParam"")  (param)   |   Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string.   | |  [`explainParams`](#pyspark.ml.feature.Imputer.explainParams ""pyspark.ml.feature.Imputer.explainParams"")  ()   |   Returns the documentation of all params with their optionally default values and user-supplied values.   | |  [`extractParamMap`](#pyspark.ml.feature.Imputer.extractParamMap ""pyspark.ml.feature.Imputer.extractParamMap"")  ([extra])   |   Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra.   | |  [`fit`](#pyspark.ml.feature.Imputer.fit ""pyspark.ml.feature.Imputer.fit"")  (dataset[, params])   |   Fits a model to the input dataset with optional parameters.   | |  [`fitMultiple`](#pyspark.ml.feature.Imputer.fitMultiple ""pyspark.ml.feature.Imputer.fitMultiple"")  (dataset, paramMaps)   |   Fits a model to the input dataset for each param map in    paramMaps    .   | |  [`getInputCol`](#pyspark.ml.feature.Imputer.getInputCol ""pyspark.ml.feature.Imputer.getInputCol"")  ()   |   Gets the value of inputCol or its default value.   | |  [`getInputCols`](#pyspark.ml.feature.Imputer.getInputCols ""pyspark.ml.feature.Imputer.getInputCols"")  ()   |   Gets the value of inputCols or its default value.   | |  [`getMissingValue`](#pyspark.ml.feature.Imputer.getMissingValue ""pyspark.ml.feature.Imputer.getMissingValue"")  ()   |   Gets the value of  [`missingValue`](#pyspark.ml.feature.Imputer.missingValue ""pyspark.ml.feature.Imputer.missingValue"")  or its default value.   | |  [`getOrDefault`](#pyspark.ml.feature.Imputer.getOrDefault ""pyspark.ml.feature.Imputer.getOrDefault"")  (param)   |   Gets the value of a param in the user-supplied param map or its default value.   | |  [`getOutputCol`](#pyspark.ml.feature.Imputer.getOutputCol ""pyspark.ml.feature.Imputer.getOutputCol"")  ()   |   Gets the value of outputCol or its default value.   | |  [`getOutputCols`](#pyspark.ml.feature.Imputer.getOutputCols ""pyspark.ml.feature.Imputer.getOutputCols"")  ()   |   Gets the value of outputCols or its default value.   | |  [`getParam`](#pyspark.ml.feature.Imputer.getParam ""pyspark.ml.feature.Imputer.getParam"")  (paramName)   |   Gets a param by its name.   | |  [`getRelativeError`](#pyspark.ml.feature.Imputer.getRelativeError ""pyspark.ml.feature.Imputer.getRelativeError"")  ()   |   Gets the value of relativeError or its default value.   | |  [`getStrategy`](#pyspark.ml.feature.Imputer.getStrategy ""pyspark.ml.feature.Imputer.getStrategy"")  ()   |   Gets the value of  [`strategy`](#pyspark.ml.feature.Imputer.strategy ""pyspark.ml.feature.Imputer.strategy"")  or its default value.   | |  [`hasDefault`](#pyspark.ml.feature.Imputer.hasDefault ""pyspark.ml.feature.Imputer.hasDefault"")  (param)   |   Checks whether a param has a default value.   | |  [`hasParam`](#pyspark.ml.feature.Imputer.hasParam ""pyspark.ml.feature.Imputer.hasParam"")  (paramName)   |   Tests whether this instance contains a param with a given (string) name. "	Adding indicator variables for missing values that have been imputed or replaced is important because it allows the model to distinguish between imputed values and original values. This can help prevent bias in the model and improve the accuracy of predictions. The Imputer class in PySpark can be used to impute missing values using mean, median, or mode.	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html']	how often are parquet checkpoint files written	"   pyspark.sql.DataFrame.checkpoint  [¶](#pyspark-sql-dataframe-checkpoint ""Permalink to this headline"") =======================================================================================================  `DataFrame.`  `checkpoint`    (   *eager     :     bool     =     True*   )    → pyspark.sql.dataframe.DataFrame  [[source]](../../../_modules/pyspark/sql/dataframe.html#DataFrame.checkpoint) [¶](#pyspark.sql.DataFrame.checkpoint ""Permalink to this definition"")    Returns a checkpointed version of this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  . Checkpointing can be used to truncate the logical plan of this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  , which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with  `SparkContext.setCheckpointDir()`  .     New in version 2.1.0.     Parameters    **eager**    bool, optional, default True     Whether to checkpoint this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  immediately.     Returns    [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")   Checkpointed DataFrame.     Notes     This API is experimental.     Examples    ``` >>> import tempfile >>> df = spark.createDataFrame([ ...     (14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""]) >>> with tempfile.TemporaryDirectory() as d: ...     spark.sparkContext.setCheckpointDir(""/tmp/bb"") ...     df.checkpoint(False) DataFrame[age: bigint, name: string]  ```  [pyspark.sql.DataFrame.cache](pyspark.sql.DataFrame.cache.html ""previous page"")  [pyspark.sql.DataFrame.coalesce](pyspark.sql.DataFrame.coalesce.html ""next page"")   "	Based on the context, the frequency of writing Parquet checkpoint files is not specified.	0
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/hive-metastore.html']	can i access an external hive metastore from within unity catalog?	"      Work with Unity Catalog and the legacy Hive metastore =======================================================     This article explains how to use a Unity Catalog metastore with the per-workspace Hive metastore in Databricks.       If your workspace was in service before it was enabled for Unity Catalog, it likely has a Hive metastore that contains data that you want to continue to use. Databricks recommends that you  [migrate the tables managed by the Hive metastore to the Unity Catalog metastore](migrate.html)  , but if you choose not to, this article explains how to work with data managed by both metastores.          Query the Hive metastore in Unity Catalog -------------------------------------------     The Unity Catalog metastore is additive, meaning it can be used with the per-workspace Hive metastore in Databricks. The Hive metastore appears as a top-level catalog called  `hive\_metastore`  in the three-level namespace.       For example, you can refer to a table called  `sales\_raw`  in the  `sales`  schema in the legacy Hive metastore by using the following notation:         ``` SELECT \* from hive\_metastore.sales.sales\_raw;  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       ``` library(SparkR)  display(tableToDF(""hive\_metastore.sales.sales\_raw""))  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       You can also specify the catalog and schema with a  `USE`  statement:         ``` USE hive\_metastore.sales; SELECT \* from sales\_raw;  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```       ``` library(SparkR)  sql(""USE hive\_metastore.sales"") display(tableToDF(""sales\_raw""))  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```           Access control in Unity Catalog and the Hive metastore --------------------------------------------------------     If you configured  [table access control](../table-acls/index.html)  on the Hive metastore, Databricks continues to enforce those access controls for data in the  `hive\_metastore`  catalog for clusters running in the shared access mode. The Unity Catalog access model differs slightly from legacy access controls, like no  `DENY`  statements. The Hive metastore is a workspace-level object. Permissions defined within the  `hive\_metastore`  catalog always refer to the local users and groups in the workspace. See  [Differences from table access control](#differences-from-table-access-control)  .         Differences from table access control ---------------------------------------     Unity Catalog has the following key differences from using  [table access controls](../table-acls/index.html)  in the legacy Hive metastore in each workspace.       The access control model in Unity Catalog has the following differences from table access control:     * **Account groups**   : Access control policies in Unity Catalog are applied to account groups, while access control policies for the Hive metastore are applied to workspace-local groups. See  [Special considerations for groups](../../administration-guide/users-groups/index.html#special-groups)  . * **`USE     CATALOG`  and  `USE     SCHEMA`  permissions are required on the catalog and schema for all operations on objects inside the catalog or schema**   : Regardless of a principal’s privileges on a table, the principal must also have the  `USE     CATALOG`  privilege on its parent catalog to access the schema and the  `USE     SCHEMA`  privilege to access objects within the schema. With workspace-level table access controls, on the other hand, granting  `USAGE`  on the root catalog automatically grants  `USAGE`  on all databases, but  `USAGE`  on the root catalog is not required. * **Views**   : In Unity Catalog, the owner of a view does not need to be an owner of the view’s referenced tables and views. Having the  `SELECT`  privilege is sufficient, along with  `USE     SCHEMA`  on the views’ parent schema and  `USE     CATALOG`  on the parent catalog. With workspace-level table access controls, a view’s owner needs to be an owner of all referenced tables and views. * **No support for  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  s**   : In Unity Catalog, there is no concept of an  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  permission. These permissions could be used to circumvent access control restrictions by allowing an unprivileged user to run privileged code.         Joins between Unity Catalog and Hive metastore objects --------------------------------------------------------     By using three-level namespace notation, you can join data in a Unity Catalog metastore with data in the legacy Hive metastore.        Note       A join with data in the legacy Hive metastore will only work on the workspace where that data resides. Trying to run such a join in another workspace results in an error. Databricks recommends that you  [upgrade](migrate.html)  legacy tables and views to Unity Catalog.        The following example joins results from the  `sales\_current`  table in the legacy Hive metastore with the  `sales\_historical`  table in the Unity Catalog metastore when the  `order\_id`  fields are equal.         ``` SELECT \* FROM hive\_metastore.sales.sales\_current JOIN main.shared\_sales.sales\_historical ON hive\_metastore.sales.sales\_current.order\_id = main.shared\_sales.sales\_historical.order\_id;  ```       ``` dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(   other = dfHistorical,   on = dfCurrent.order\_id == dfHistorical.order\_id ))  ```       ``` library(SparkR)  dfCurrent = tableToDF(""hive\_metastore.sales.sales\_current"") dfHistorical = tableToDF(""main.shared\_sales.sales\_historical"")  display(join(  x = dfCurrent,  y = dfHistorical,  joinExpr = dfCurrent$order\_id == dfHistorical$order\_id))  ```       ``` val dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") val dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(  right = dfHistorical,  joinExprs = dfCurrent(""order\_id"") === dfHistorical(""order\_id"") ))  ```         Default catalog -----------------     If you omit the top-level catalog name when you perform data operations, and there is no  `USE     CATALOG`  statement, the default catalog is assumed. Of course, if you are supplying the catalog name in another way, such as setting it on a JDBC string, then that catalog is assumed instead of the default catalog when you omit the catalog name in data operations.       By default, the default catalog is  `hive\_metastore`  . To configure a different default catalog for a workspace, you can set a Spark configuration or run a  [Databricks CLI](../../dev-tools/cli/unity-catalog-cli.html)  command:         Set the  `spark.databricks.sql.initial.catalog.name`  value.         This CLI command updates the default catalog only for SQL warehouses. To specify the default catalog for clusters, use the Spark config.        ``` databricks unity-catalog metastores assign --workspace-id 1234567890123456 \  --metastore-id 12a345b6-7890-1cd2-3456-e789f0a12b34 \  --default-catalog-name my_catalog  ```        If your Databricks workspace used the per-workspace Hive metastore before the workspace was enabled for Unity Catalog, Databricks recommends that you keep the default catalog value as  `hive\_metastore`  so that your existing code can operate on current Hive metastore data without any change.        Warning       Changing the default catalog can break existing data operations that depend on it.          Cluster instance profile --------------------------     When using the Hive metastore alongside Unity Catalog, the instance profile on the cluster is used to access Hive metastore data but not data registered in Unity Catalog.       If users access paths that are outside Unity Catalog (such as a path not registered as a table or external location) then the instance profile on the cluster is used.           Upgrade legacy tables to Unity Catalog ----------------------------------------     Tables in the Hive metastore do not benefit from the full set of security and governance features that Unity Catalog introduces, such as built-in auditing and access control. Databricks recommends that you  [upgrade your legacy tables](migrate.html)  by adding them to Unity Catalog.                                                        "	Yes, you can access an external Hive metastore from within Unity Catalog in Databricks. The Hive metastore appears as a top-level catalog called `hive_metastore` in the three-level namespace. You can refer to a table in the legacy Hive metastore by using the following notation: `SELECT * from hive_metastore.sales.sales_raw;`. You can also specify the catalog and schema with a `USE` statement.	1
no_reaction	['https://docs.databricks.com/machine-learning/automl-hyperparam-tuning/hyperopt-model-selection.html']	How do I evaluate models for accuracy?	"   c    [org](../../../index.html)   .  [apache](../../index.html)   .  [spark](../index.html)   .  [ml](index.html)    Model   [**](../../../../org/apache/spark/ml/Model.html ""Permalink"") =======================================================================  ###   ####    abstract     class     Model     [    M <:  Model  [    M    ]    ]     extends  [Transformer](Transformer.html)   A fitted model, i.e., a  [Transformer](Transformer.html)   produced by an  [Estimator](Estimator.html)   .     M     model type     Source    [Model.scala](https://github.com/apache/spark/tree/v3.4.0/mllib/src/main/scala/org/apache/spark/ml/Model.scala)    Linear Supertypes    [Transformer](Transformer.html)   ,  [PipelineStage](PipelineStage.html)   ,    Logging    ,  [Params](param/Params.html)   ,    Serializable    ,    Serializable    ,  [Identifiable](util/Identifiable.html)   ,  [AnyRef](../../../../scala/index.html#AnyRef=Object)   ,    Any     Known Subclasses    [PipelineModel](PipelineModel.html)   ,  [PredictionModel](PredictionModel.html)   ,  [ClassificationModel](classification/ClassificationModel.html)   ,  [DecisionTreeClassificationModel](classification/DecisionTreeClassificationModel.html)   ,  [FMClassificationModel](classification/FMClassificationModel.html)   ,  [GBTClassificationModel](classification/GBTClassificationModel.html)   ,  [LinearSVCModel](classification/LinearSVCModel.html)   ,  [LogisticRegressionModel](classification/LogisticRegressionModel.html)   ,  [MultilayerPerceptronClassificationModel](classification/MultilayerPerceptronClassificationModel.html)   ,  [NaiveBayesModel](classification/NaiveBayesModel.html)   ,  [OneVsRestModel](classification/OneVsRestModel.html)   ,  [ProbabilisticClassificationModel](classification/ProbabilisticClassificationModel.html)   ,  [RandomForestClassificationModel](classification/RandomForestClassificationModel.html)   ,  [BisectingKMeansModel](clustering/BisectingKMeansModel.html)   ,  [DistributedLDAModel](clustering/DistributedLDAModel.html)   ,  [GaussianMixtureModel](clustering/GaussianMixtureModel.html)   ,  [KMeansModel](clustering/KMeansModel.html)   ,  [LDAModel](clustering/LDAModel.html)   ,  [LocalLDAModel](clustering/LocalLDAModel.html)   ,  [Bucketizer](feature/Bucketizer.html)   ,  [CountVectorizerModel](feature/CountVectorizerModel.html)   ,  [IDFModel](feature/IDFModel.html)   ,  [ImputerModel](feature/ImputerModel.html)   ,  [MaxAbsScalerModel](feature/MaxAbsScalerModel.html)   ,  [MinMaxScalerModel](feature/MinMaxScalerModel.html)   ,  [OneHotEncoderModel](feature/OneHotEncoderModel.html)   ,  [PCAModel](feature/PCAModel.html)   ,  [RFormulaModel](feature/RFormulaModel.html)   ,  [RobustScalerModel](feature/RobustScalerModel.html)   ,  [StandardScalerModel](feature/StandardScalerModel.html)   ,  [StringIndexerModel](feature/StringIndexerModel.html)   ,  [UnivariateFeatureSelectorModel](feature/UnivariateFeatureSelectorModel.html)   ,  [VarianceThresholdSelectorModel](feature/VarianceThresholdSelectorModel.html)   ,  [VectorIndexerModel](feature/VectorIndexerModel.html)   ,  [Word2VecModel](feature/Word2VecModel.html)   ,  [FPGrowthModel](fpm/FPGrowthModel.html)   ,  [ALSModel](recommendation/ALSModel.html)   ,  [AFTSurvivalRegressionModel](regression/AFTSurvivalRegressionModel.html)   ,  [DecisionTreeRegressionModel](regression/DecisionTreeRegressionModel.html)   ,  [FMRegressionModel](regression/FMRegressionModel.html)   ,  [GBTRegressionModel](regression/GBTRegressionModel.html)   ,  [GeneralizedLinearRegressionModel](regression/GeneralizedLinearRegressionModel.html)   ,  [IsotonicRegressionModel](regression/IsotonicRegressionModel.html)   ,  [LinearRegressionModel](regression/LinearRegressionModel.html)   ,  [RandomForestRegressionModel](regression/RandomForestRegressionModel.html)   ,  [RegressionModel](regression/RegressionModel.html)   ,  [CrossValidatorModel](tuning/CrossValidatorModel.html)   ,  [TrainValidationSplitModel](tuning/TrainValidationSplitModel.html)   **   **    Ordering   1. Alphabetic 2. By Inheritance   Inherited      1. Model 2. Transformer 3. PipelineStage 4. Logging 5. Params 6. Serializable 7. Serializable 8. Identifiable 9. AnyRef 10. Any  1. Hide All 2. Show All   Visibility   1. Public 2. All  ###   Instance Constructors  1. [**](../../../../org/apache/spark/ml/Model.html#<init>():org.apache.spark.ml.Model[M] ""Permalink"")   new     Model     ()  ###   Abstract Value Members  1. [**](../../../../org/apache/spark/ml/Model.html#copy(extra:org.apache.spark.ml.param.ParamMap):M ""Permalink"")   abstract     def     copy     (    extra:  [ParamMap](param/ParamMap.html)    )     :    M     Creates a copy of this instance with the same UID and some extra params.     Creates a copy of this instance with the same UID and some extra params. Subclasses should implement this method and set the return type properly. See  `defaultCopy()`   .     Definition Classes    Model  →  [Transformer](Transformer.html)   →  [PipelineStage](PipelineStage.html)   →  [Params](param/Params.html) 2. [**](../../../../org/apache/spark/ml/Model.html#transform(dataset:org.apache.spark.sql.Dataset[_]):org.apache.spark.sql.DataFrame ""Permalink"")   abstract     def     transform     (    dataset:  [Dataset](../sql/Dataset.html)   [\_]    )     :  [DataFrame](../sql/index.html#DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])    Transforms the input dataset.     Transforms the input dataset.     Definition Classes    [Transformer](Transformer.html)    Annotations     @Since     (     ""2.0.0""     ) 3. [**](../../../../org/apache/spark/ml/Model.html#transformSchema(schema:org.apache.spark.sql.types.StructType):org.apache.spark.sql.types.StructType ""Permalink"")   abstract     def     transformSchema     (    schema:  [StructType](../sql/types/StructType.html)    )     :  [StructType](../sql/types/StructType.html)    Check transform validity and derive the output schema from the input schema.     Check transform validity and derive the output schema from the input schema.     We check validity for interactions between parameters during  `transformSchema`   and raise an exception if any parameter value is invalid. Parameter value checks which do not depend on other parameters are handled by  `Param.validate()`   .     Typical implementation should first conduct verification on schema change and parameter validity, including complex parameter interaction checks.     Definition Classes    [PipelineStage](PipelineStage.html) 4. [**](../../../../org/apache/spark/ml/Model.html#uid:String ""Permalink"")   abstract     val     uid     :    String     An immutable unique ID for the object and its derivatives.     An immutable unique ID for the object and its derivatives.     Definition Classes    [Identifiable](util/Identifiable.html)  ###   Concrete Value Members  1. [**](../../../../org/apache/spark/ml/Model.html#!=(x$1:Any):Boolean ""Permalink"")   final     def     !=     (    arg0:    Any     )     :    Boolean     Definition Classes     AnyRef → Any 2. [**](../../../../org/apache/spark/ml/Model.html###():Int ""Permalink"")   final     def     ##     ()     :    Int     Definition Classes     AnyRef → Any 3. [**](../../../../org/apache/spark/ml/Model.html#$[T](param:org.apache.spark.ml.param.Param[T]):T ""Permalink"")   final     def     $     [    T    ]     (    param:  [Param](param/Param.html)   [    T    ]    )     :    T     An alias for  `getOrDefault()`   .     An alias for  `getOrDefault()`   .     Attributes     protected     Definition Classes    [Params](param/Params.html) 4. [**](../../../../org/apache/spark/ml/Model.html#==(x$1:Any):Boolean ""Permalink"")   final     def     ==     (    arg0:    Any     )     :    Boolean     Definition Classes     AnyRef → Any 5. [**](../../../../org/apache/spark/ml/Model.html#asInstanceOf[T0]:T0 ""Permalink"")   final     def     asInstanceOf     [    T0    ]     :    T0     Definition Classes     Any 6. [**](../../../../org/apache/spark/ml/Model.html#clear(param:org.apache.spark.ml.param.Param[_]):Params.this.type ""Permalink"")   final     def     clear     (    param:  [Param](param/Param.html)   [\_]    )     :  Model  .this.type     Clears the user-supplied value for the input param"	To evaluate models for accuracy, you can use metrics such as accuracy, precision, recall, and F1 score. These metrics can be calculated using the `MulticlassClassificationEvaluator` or `BinaryClassificationEvaluator` classes in Apache Spark's MLlib library. You can also use cross-validation to evaluate the performance of a model on different subsets of the data.	0
no_reaction	['https://docs.databricks.com/partners/bi/qlik-sense.html']	databricks qlick sense connection	        Connect to Qlik Sense =======================     Qlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.       This article describes how to use Qlik Sense with a Databricks cluster or a Databricks SQL warehouse (formerly Databricks SQL endpoint) to analyze data in Delta Lake.        Note       For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into Delta Lake, see  [Connect to Qlik Replicate](../ingestion/qlik.html)  .             Connect to Qlik Sense using Partner Connect ---------------------------------------------      Note       Partner Connect only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.        To connect to Qlik Sense using Partner Connect, do the following:     1. [Connect to BI partners using Partner Connect](../../partner-connect/bi.html)  . 2. On the  **Qlik Sense Databricks Connect**   page, click the  **Sign up here**   link. 3. Follow the on-screen instructions to create a Qlik account and start your free Qlik Sense trial, then return to the  **Qlik Sense Databricks Connect**   page. 4. Enter your Qlik Sense tenant URL. 5. Click the  **Click here**   link.       A new tab opens in your browser that displays the Qlik Sense API Access Key Help page. 6. Follow the instructions to generate an API key for your Qlik Sense tenant, then return to the  **Qlik Sense Databricks Connect**   . 7. Enter your Qlik Sense Tenant API key, then click  **Submit**   .       The Qlik Management Console displays.         Connect to Qlik Sense manually --------------------------------     This section describes how to connect to Qlik Sense manually.        ###   Requirements     Before you connect to Qlik Sense manually, you must have the following:     * A cluster or SQL warehouse in your Databricks workspace.      + [Create a cluster](../../clusters/configure.html)   .  + [Configure SQL warehouses](../../sql/admin/create-sql-warehouse.html)   . * The connection details for your cluster or SQL warehouse, specifically the  **Server Hostname**   ,  **Port**   , and  **HTTP Path**   values.      + [Get connection details for a cluster](../../integrations/jdbc-odbc-bi.html#connection-details-cluster)   .  + [Get connection details for a SQL warehouse](../../integrations/jdbc-odbc-bi.html#sql)   . * A Databricks  [personal access token](../../dev-tools/auth.html#pat)  .        Note       As a security best practice, when authenticating with automated tools, systems, scripts, and apps, Databricks recommends you use access tokens belonging to  [service principals](../../administration-guide/users-groups/service-principals.html)  instead of workspace users. To create access tokens for service principals, see  [Manage access tokens for a service principal](../../administration-guide/users-groups/service-principals.html#personal-access-tokens)  .       ###   Steps to connect     To connect to Qlik Sense manually, do the following:     1. Sign in to the Qlik Sense app or website for your organization. 2. Do one of the following:      * If you have an existing app that you want to use, click the app’s tile on the home page to open it.  * If you do not have an existing app, click   **Add new > New analytics app**    , and follow the on-screen directions to finish creating the app and to open it. 3. With the app open, click  **Prepare > Data manager**   . 4. Click  **Add data > Files and other sources**   . 5. For  **Connect to a new data source**   , click  **Databricks**   . 6. In the  **Create new connection (Databricks)**   dialog, enter the following information:      1. For   **Host name**    , enter the   **Server Hostname**    value.  2. For   **Port**    , enter the   **Port**    value.  3. For   **Database name**    , enter the name of the database that you want to use.  4. For   **HTTP Path**    , enter the   **HTTP Path**    value.  5. In   **Credentials**    , for   **User name**    , enter the word   `token`   .  6. For   **Password**    , enter the token.  7. For   **SSL Options**    , select the boxes   **Enable SSL**    ,   **Allow Self-signed Server Certificate**    ,   **Allow Common Name Host Name Mismatch**    , and   **Use System Trust Store**    .  8. For   **Name**    , enter a name for this connection, or leave the default name.  9. You can leave the rest of the settings in this dialog with their default settings. 7. Click  **Test connection**   . 8. After the connection succeeds, click  **Create**   . 9. Follow the on-screen directions to add tables to your connection and to filter the tables’ data. 10. Click  **Next**   . 11. Follow the on-screen directions to analyze your data with  [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm)   ,  [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm)   , and other  [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm)   .          Next steps ------------     To continue using Qlik Sense, see the following resources:     * [Qlik Sense](https://www.qlik.com/products/qlik-sense) * [Get started with Qlik Sense](https://help.qlik.com/onboarding) * [Qlik Sense demos](https://demos.qlik.com/qliksense) * [Qlik help videos](https://www.youtube.com/channel/UCFxZPr8pHfZS0n3jxx74rpA) * [Qlik Sense on Windows help](https://help.qlik.com/sense) * [Qlik Sense help for administrators](https://help.qlik.com/sense-admin) * [Qlik Sense for developers help](https://help.qlik.com/sense-developer) * [Qlik support services and resources](https://www.qlik.com/services/support) * [Contact Qlik](https://www.qlik.com/contact) * [Support](https://support.qlik.com/)                                                      	The context is about how to use Qlik Sense with a Databricks cluster or a Databricks SQL warehouse to analyze data in Delta Lake. It provides instructions on how to connect to Qlik Sense using Partner Connect and manually. It also lists the requirements and steps to connect to Qlik Sense manually.	1
no_reaction	['https://docs.databricks.com/delta-live-tables/python-ref.html']	how to write create or refresh streaming live table in python? 	"   pyspark.sql.streaming.DataStreamWriter.toTable  [¶](#pyspark-sql-streaming-datastreamwriter-totable ""Permalink to this headline"") ===================================================================================================================================  `DataStreamWriter.`  `toTable`    (   *tableName     :     str*  ,  *format     :     Optional    [    str    ]     =     None*  ,  *outputMode     :     Optional    [    str    ]     =     None*  ,  *partitionBy     :     Union[str, List[str], None]     =     None*  ,  *queryName     :     Optional    [    str    ]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → pyspark.sql.streaming.query.StreamingQuery  [[source]](../../../_modules/pyspark/sql/streaming/readwriter.html#DataStreamWriter.toTable) [¶](#pyspark.sql.streaming.DataStreamWriter.toTable ""Permalink to this definition"")    Starts the execution of the streaming query, which will continually output results to the given table as new data arrives.     The returned  [`StreamingQuery`](pyspark.sql.streaming.StreamingQuery.html#pyspark.sql.streaming.StreamingQuery ""pyspark.sql.streaming.StreamingQuery"")  object can be used to interact with the stream.     New in version 3.1.0.     Parameters    **tableName**    str     string, for the name of the table.    **format**    str, optional     the format used to save.    **outputMode**    str, optional     specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.    * append    : Only the new rows in the streaming DataFrame/Dataset will be written to the sink * complete    : All the rows in the streaming DataFrame/Dataset will be written to the sink every time these are some updates * update    : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to    append    mode.  **partitionBy**    str or list, optional     names of partitioning columns    **queryName**    str, optional     unique name for the query    **\*\*options**    dict     All other string options. You may want to provide a    checkpointLocation    .     Notes     This API is evolving.     For v1 table, partitioning columns provided by    partitionBy    will be respected no matter the table exists or not. A new table will be created if the table not exists.     For v2 table,    partitionBy    will be ignored if the table already exists.    partitionBy    will be respected only if the v2 table does not exist. Besides, the v2 table created by this API lacks some functionalities (e.g., customized properties, options, and serde info). If you need them, please create the v2 table manually before the execution to avoid creating a table with incomplete information.     Examples     Save a data stream to a table.    ``` >>> import tempfile >>> import time >>> \_ = spark.sql(""DROP TABLE IF EXISTS my\_table2"") >>> with tempfile.TemporaryDirectory() as d: ...     # Create a table with Rate source. ...     q = spark.readStream.format(""rate"").option( ...         ""rowsPerSecond"", 10).load().writeStream.toTable( ...             ""my\_table2"", ...             queryName='that\_query', ...             outputMode=""append"", ...             format='parquet', ...             checkpointLocation=d) ...     time.sleep(3) ...     q.stop() ...     spark.read.table(""my\_table2"").show() ...     \_ = spark.sql(""DROP TABLE my\_table2"") +...---------+-----+ |...timestamp|value| +...---------+-----+ ...  ```  [pyspark.sql.streaming.DataStreamWriter.start](pyspark.sql.streaming.DataStreamWriter.start.html ""previous page"")  [pyspark.sql.streaming.DataStreamWriter.trigger](pyspark.sql.streaming.DataStreamWriter.trigger.html ""next page"")   "	"To create or refresh a streaming live table in Python, you can use the `toTable` method of the `DataStreamWriter` class in PySpark. This method starts the execution of the streaming query, which will continually output results to the given table as new data arrives. The returned `StreamingQuery` object can be used to interact with the stream. You can specify the table name, format, output mode, partitioning columns, query name, and other options using the method parameters. Here is an example code snippet:  ``` from pyspark.sql.streaming import DataStreamWriter  streamingDF.writeStream \     .format(""memory"") \     .queryName(""my_table"") \     .outputMode(""append"") \     .start() \     .awaitTermination() ```  Note that this API is evolving and the behavior may differ for v1 and v2 tables."	1
no_reaction	['https://docs.databricks.com/external-data/json.html']	How do i import a json file	"      JSON file ===========     You can read JSON files in  [single-line](#single-line-mode)  or  [multi-line](#multi-line-mode)  mode. In single-line mode, a file can be split into many parts and read in parallel. In multi-line mode, a file is loaded as a whole entity and  *cannot*   be split.       For further information, see  [JSON Files](https://spark.apache.org/docs/latest/sql-data-sources-json.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html?highlight=json#pyspark.sql.DataFrameReader.json)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#json(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html?highlight=json#pyspark.sql.DataFrameWriter.json)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#json(path:String):Unit)       Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.2 (Unsupported)](../release-notes/runtime/8.2.html)  and above.        The rescued data column ensures that you never lose or miss out on data during ETL. The rescued data column contains any data that wasn’t parsed, either because it was missing from the given schema, or because there was a type mismatch, or because the casing of the column in the record or file didn’t match with that in the schema. The rescued data column is returned as a JSON blob containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set(""spark.databricks.sql.rescuedDataColumn.filePath.enabled"",     ""false"")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name, such as  `\_rescued\_data`  with  `spark.read.option(""rescuedDataColumn"",     ""\_rescued\_data"").format(""json"").load(<path>)`  .       The JSON parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed JSON—are dropped or throw errors. If you use the option  `badRecordsPath`  when parsing JSON, data type mismatches are not considered as bad records when using the  `rescuedDataColumn`  . Only incomplete and malformed JSON records are stored in  `badRecordsPath`  .         Examples ----------    ###   Single-line mode     In this example, there is one JSON object per line:        ``` {""string"":""string1"",""int"":1,""array"":[1,2,3],""dict"": {""key"": ""value1""}} {""string"":""string2"",""int"":2,""array"":[2,4,6],""dict"": {""key"": ""value2""}} {""string"":""string3"",""int"":3,""array"":[3,6,9],""dict"": {""key"": ""value3"", ""extra\_key"": ""extra\_value3""}}  ```      To read the JSON data, use:        ``` val df = spark.read.format(""json"").load(""example.json"")  ```      Spark infers the schema automatically.        ``` df.printSchema  ```       ``` root  |-- array: array (nullable = true)  |    |-- element: long (containsNull = true)  |-- dict: struct (nullable = true)  |    |-- extra\_key: string (nullable = true)  |    |-- key: string (nullable = true)  |-- int: long (nullable = true)  |-- string: string (nullable = true)  ```      ###   Multi-line mode     This JSON object occupies multiple lines:        ``` [  {""string"":""string1"",""int"":1,""array"":[1,2,3],""dict"": {""key"": ""value1""}},  {""string"":""string2"",""int"":2,""array"":[2,4,6],""dict"": {""key"": ""value2""}},  {  ""string"": ""string3"",  ""int"": 3,  ""array"": [  3,  6,  9  ],  ""dict"": {  ""key"": ""value3"",  ""extra\_key"": ""extra\_value3""  }  } ]  ```      To read this object, enable multi-line mode:         ``` CREATE TEMPORARY VIEW multiLineJsonTable USING json OPTIONS (path=""/tmp/multi-line.json"",multiline=true)  ```       ``` val mdf = spark.read.option(""multiline"", ""true"").format(""json"").load(""/tmp/multi-line.json"") mdf.show(false)  ```         ###   Charset auto-detection     By default, the charset of input files is detected automatically. You can specify the charset explicitly using the  `charset`  option:        ``` spark.read.option(""charset"", ""UTF-16BE"").format(""json"").load(""fileInUTF16.json"")  ```      Some supported charsets include:  `UTF-8`  ,  `UTF-16BE`  ,  `UTF-16LE`  ,  `UTF-16`  ,  `UTF-32BE`  ,  `UTF-32LE`  ,  `UTF-32`  . For the full list of charsets supported by Oracle Java SE, see  [Supported Encodings](https://docs.oracle.com/javase/8/docs/technotes/guides/intl/encoding.doc.html)   .          Notebook ----------     The following notebook demonstrates single line and multi-line mode.        ###   Read JSON files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-json-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 "	"To import a JSON file, you can use the `spark.read.format(""json"").load(<path>)` command in Python or `spark.read.json(<path>)` command in Scala. The `<path>` parameter should be replaced with the path to the JSON file you want to import."	1
no_reaction	['https://docs.databricks.com/workspace-index.html']	How can you do ELT with databricks	   `elt`  function ================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime       Returns the nth expression.        Syntax --------      ``` elt(index, expr1 [, ...])  ```        Arguments -----------   * `index`  : An INTEGER expression greater than 0. * `exprN`  : Any expression that shares a  [least common type](../sql-ref-datatype-rules.html#least-common-type-resolution)  with all  `exprN`  .       Returns ---------     The result has the type of the  [least common type](../sql-ref-datatype-rules.html#least-common-type-resolution)  of the  `exprN`  .       Index must be between 1 and the number of  `expr`  . If index is out of bounds, an  [INVALID\_ARRAY\_INDEX](../../../error-messages/invalid-array-index-error-class.html)  error is raised.        Note       In Databricks Runtime, if  [spark.sql.ansi.enabled](../sql-ref-ansi-compliance.html)  is  `false`  , the function returns  `NULL`  instead of an error if the index is out of bounds.          Examples ----------      ``` > SELECT elt(1, 'scala', 'java');  scala  ```        Related functions -------------------   * [element\_at function](element_at.html)                                                      	The `elt` function in Databricks SQL and Databricks Runtime returns the nth expression. The `index` must be between 1 and the number of expressions. If the index is out of bounds, an `INVALID_ARRAY_INDEX` error is raised. The result has the type of the least common type of the expressions.	0
no_reaction	['https://kb.databricks.com/dbfs/list-all-mount-points-on-a-cluster-with-configuration-info']	can I define workspace-level mount points?	Title: List all mount points on a cluster with configuration info  Description: Display all cluster mount points and the associated configuration information in a notebook.  Answer: What are mount points? The mount point is a directory that is linked to an external storage blob. Once mounted, the external storage is browsable as part of the local file system. Any user of a cluster that has the required permissions can access the external storage via the mount point. List all mount points on a cluster When you are troubleshooting an issue, you may need to know who performed a specific operation and what configuration was used to mount a storage blob.  You can review the cluster audit logs (AWS | Azure | GCP) to identify the owner of the mount operation by looking for the dbfs/mount audit event. The audit logs also contain some basic mount configuration information. However, to obtain detailed mount configuration information, you must call the information in a notebook. Example code This sample code returns the storage sourceString (the external storage source), the mountPointString (the local directory containing the mount point), and configurations (configuration data required to access the external storage source). %scala import com.databricks.backend.daemon.data.common.DataMessages._ import scala.concurrent.duration._ import com.databricks.backend.daemon.data.server.util._ import com.databricks.common.util._ val dbfs = dbutils.fs.dbfs.asInstanceOf[com.databricks.backend.daemon.data.client.DBFS] display(dbfs.client.sendIdempotent(GetMountsV2(), 1.minutes, Some(5.minutes)).toSeq.toDF) Copy and paste the sample code into a notebook cell. Run the sample code. The notebook displays mount point information for the current cluster.	Yes, you can define workspace-level mount points. Mount points are directories that are linked to external storage blobs, and once mounted, the external storage is browsable as part of the local file system. Any user of a cluster that has the required permissions can access the external storage via the mount point. To obtain detailed mount configuration information, you must call the information in a notebook.	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/migration_guide/koalas_to_pyspark.html']	What's the difference between the pandas api and koalas?	"   Migrating from Koalas to pandas API on Spark  [¶](#migrating-from-koalas-to-pandas-api-on-spark ""Permalink to this headline"") ===============================================================================================================================  * The package name to import should be changed to  `pyspark.pandas`  from  `databricks.koalas`  .    >  >  >  >  >  > ``` > # import databricks.koalas as ks > import pyspark.pandas as ps >  > ``` >  >  >  >  > * `DataFrame.koalas`  in Koalas DataFrame was renamed to  `DataFrame.pandas\_on\_spark`  in pandas-on-Spark DataFrame.  `DataFrame.koalas`  was kept for compatibility reasons but deprecated as of Spark 3.2.  `DataFrame.koalas`  will be removed in the future releases. * Monkey-patched  `DataFrame.to\_koalas`  in PySpark DataFrame was renamed to  `DataFrame.pandas\_api`  in PySpark DataFrame.  `DataFrame.to\_koalas`  was kept for compatibility reasons.  `DataFrame.to\_koalas`  will be removed in the future releases. * Monkey-patched  `DataFrame.to\_pandas\_on\_spark`  in PySpark DataFrame was renamed to  `DataFrame.pandas\_api`  in PySpark DataFrame.  `DataFrame.to\_pandas\_on\_spark`  was kept for compatibility reasons but deprecated as of Spark 3.3.  `DataFrame.to\_pandas\_on\_spark`  will be removed in the future releases. * `databricks.koalas.\_\_version\_\_`  was removed.  `pyspark.\_\_version\_\_`  should be used instead.  [Upgrading PySpark](pyspark_upgrade.html ""previous page"")   "	The pandas API and Koalas are both libraries for working with data frames in Python, but Koalas is built on top of Apache Spark and provides a pandas-like API for working with big data. The pandas API is designed for working with smaller data sets that can fit into memory on a single machine.	1
no_reaction	['https://docs.databricks.com/data-sharing/index.html']	Can i partition data in delta sharing?	"      Create and manage shares for Delta Sharing ============================================     This article explains how to create and manage shares for Delta Sharing.       A share is a named object that contains a collection of tables (or parts of tables) in a Unity Catalog metastore that you want to share with one or more recipients. A share can contain tables from only one metastore. You can add or remove tables and table partitions from a share at any time.       For more information, see  [Shares and recipients](index.html#shares-recipients)  .        Requirements --------------     To create a share:     * You must be a metastore admin or have the  `CREATE     SHARE`  privilege for the Unity Catalog metastore where the data you want to share is registered. * You must create the share using a Databricks workspace that has that Unity Catalog metastore attached.     To add tables to a share, you must be one of these:     * A metastore admin. * The share owner, who must also have the  `SELECT`  privilege on the table. You must keep that privilege in order for the table to continue to be shared. If you lose it, the recipient cannot access the table through the share. Databricks therefore recommends that you use a group as the share owner.     Compute requirements:     * If you use a Databricks notebook to create the share, your cluster must use Databricks Runtime 11.2 or above and a Unity-Catalog-capable cluster access mode. * If you use Databricks SQL, your SQL warehouse must use compute version 2022.35 and above.       Create a share object -----------------------     To create a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `CREATE     SHARE`  SQL command in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or user with the  `CREATE     SHARE`  privilege for the metastore.       1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. Click  **Share data**   . 4. Enter the share  **Name**   and an optional comment.       Run the following command in a notebook or the Databricks SQL query editor:        ``` CREATE SHARE [IF NOT EXISTS] <share\_name> [COMMENT ""<comment>""];  ```        Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog shares create --name <share_name>  ```            Add tables to a share -----------------------     To add tables to a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or the owner of the share object. Owners must also have the  `SELECT`  privilege on the table and must maintain that privilege for as long as you want to share the table. For more information, see  [Requirements](#requirements)  .       1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find the share you want to add a table to and click its name. 4. Click  **Add tables**   . 5. On the  **Add tables**   page, select the catalog and database that contain the table, then select the table.       If you aren’t sure which catalog and database contain the table, you can search for it by name, column name, or comment using workspace search. See  [Search for workspace assets](../search/index.html)  . 6. (Optional) Click  **Advanced table options**   to specify:      * An alternate table name, or   **Alias**    to make the table name more readable. The alias is the table name that the recipient sees and must use in queries. Recipients cannot use the actual table name if an alias is specified.  * A   **Partition**    (for example,   `(column        =        'value')`   ) if you want to share only part of the table. See   [Specify table partitions to share](#partitions)   and   [Use recipient properties to do partition filtering](#properties)   .  * That you want to share the table history to allow recipients to perform time travel queries or read the table with Spark Structured Streaming, starting from version zero. Select   **History Sharing**    . Requires Databricks Runtime 12.1 or above.            The change data feed will also be shared if it is enabled on the source table and   **History Sharing**    is selected here, allowing recipients to track row-level changes between versions of the table. 7. Click  **Save**   .       Run the following command in a notebook or the Databricks SQL query editor:        ``` ALTER SHARE <share\_name> ADD TABLE <catalog\_name>.<schema\_name>.<table\_name> [COMMENT ""<comment>""]  [PARTITION(<clause>)] [AS <alias>]  [WITH HISTORY | WITHOUT HISTORY];  ```      Options include:     * `PARTITION(<clause>)`  : If you want to share only part of the table, you can specify a partition. For example,  `(column     =     'value')`  See  [Specify table partitions to share](#partitions)  and  [Use recipient properties to do partition filtering](#properties)  . * `AS     <alias>`  : An alternate table name, or  **Alias**   to make the table name more readable. The alias is the table name that the recipient sees and must use in queries. Recipients cannot use the actual table name if an alias is specified. * `WITH     HISTORY`  or  `WITHOUT     HISTORY`  : When WITH HISTORY is specified, share the table with full history, allowing recipients to perform time travel queries and streaming reads. The default behavior is WITHOUT HISTORY. Requires Databricks Runtime 12.1 or above.       You can allow recipients to track row-level changes between versions of the table by enabling change data feed on the source table and sharing it  `WITH     HISTORY`  .     For more information about  `ALTER     SHARE`  options, see  [ALTER SHARE](../sql/language-manual/sql-ref-syntax-ddl-alter-share.html)  or  [ALTER SHARE](../sql/language-manual/sql-ref-syntax-ddl-alter-share.html)  .         Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog shares update --name <share_name> \  --add-table <catalog>.<schema>.<table>  ```      To specify advanced options, you create a JSON file with those options and reference it in the CLI command. To learn about these options, view the instructions on the SQL tab.        ``` {  ""objects"": [{  ""shared\_as"": ""default.mytable"",  ""history\_data\_sharing\_status"": ""ENABLED"",  ""partitions"": [{  ""values"": [{  ""name"": ""a"",  ""op"": ""EQUAL"",  ""value"": ""1""  }]  }]  }] }  ```      Reference the file when you add the table. Replace  `update-share.json`  with the filename of the JSON file.        ``` databricks unity-catalog shares update --name <share_name> \  --add-table <catalog>.<schema>.<table>  --json-file update-share.json  ```      To learn about additional parameters, run  `databricks     unity-catalog     shares     update     --help`  .         For information about removing tables from a share, see  [Update shares](#update)  .        ###   Specify table partitions to share     To share only part of a table when you add the table to a share, you can provide a partition specification. You can specify partitions when you add a table to a share or update a share, using Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor. See  [Add tables to a share](#add-tables)  and  [Update shares](#update)  .      ####   Basic example     The following SQL example shares part of the data in the  `inventory`  table, partitioned by the  `year`  ,  `month`  , and  `date`  columns:     * Data for the year 2021. * Data for December 2020. * Data for December 25, 2019.      ``` ALTER SHARE share\_name ADD TABLE inventory PARTITION (year = ""2021""),  (year = ""2020"", month = ""Dec""),  (year = ""2019"", month = ""Dec"", date = ""2019-12-25"");  ```        ####   Use recipient properties to do partition filtering     You can share a table partition that matches  [data recipient properties](create-recipient.html#properties)  , also known as parameterized partition sharing.       Default properties include:     * `databricks.accountId`  : The Databricks account that a data recipient belongs to (Databricks-to-Databricks sharing only). * `databricks.metastoreId`  : The Unity Catalog metastore that a data recipient belongs to (Databricks-to-Databricks sharing only). * `databricks.name`  : The name of the data recipient.     You can create any custom property you like when you create or update a recipient.       Filtering by recipient property enables you to share the same tables, using the same share, across multiple Databricks accounts, workspaces, and users while maintaining data boundaries between them.       For example, if your tables include a Databricks account ID column, you can create a single share with table partitions defined by Databricks account ID. When you share, Delta Sharing dynamically delivers to each recipient only the data associated with their Databricks account.      ![Diagram of parameter-based dynamic partition sharing in Delta Sharing](../_images/parameterized-partitions.png)    Without the ability to dynamically partition by property, you would have to create a separate share for each recipient.           To specify a partition that filters by recipient properties when you create or update a share, you can use Data Explorer or the  `CURRENT\_RECPIENT`  SQL function in a Databricks notebook or the Databricks SQL query editor:            Note       Recipient properties are available on Databricks Runtime 12.2 and above.        1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find the share you want to update and click its name. 4. Click  **Add tables**   . 5. On the  **Add tables**   page, select the catalog and database that contain the table, then select the table.       If you aren’t sure which catalog and database contain the table, you can search for it by name, column name, or comment using workspace search. See  [Search for workspace assets](../search/index.html)  . 6. (Optional) Click  **Advanced table options**   to add  **Partition**   specifications.       On the  **Add partition to a table**   dialog,"	"Yes, you can partition data in Delta Sharing. When adding tables to a share, you can specify a partition (for example, `(column = 'value')`) if you want to share only part of the table. See the ""Add tables to a share"" section in the context for more information."	1
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/index.html']	how do I set up unity catalog?	"    Create and manage catalogs ============================     This article shows how to create and manage catalogs in Unity Catalog. A catalog contains  [schemas (databases)](create-schemas.html)  , and a schema contains tables and views.        Requirements --------------   * You must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore * Your Databricks account must be on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * You must have a Unity Catalog metastore  [linked to the workspace](create-metastore.html)  where you perform the catalog creation. * The  [compute resource](compute.html)  that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode.         Create a catalog ------------------     To create a catalog, you can use Data Explorer or a SQL command.       1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the  **Create Catalog**   button. 4. (Optional) Specify the location where data for  [managed tables](create-tables.html#managed-table)  in the catalog will be stored.       Specify a location here only if you do not want managed tables in this catalog to be stored in the default root storage location that was configured for the metastore. See  [Create a Unity Catalog metastore](create-metastore.html)  .       The path that you specify must be defined in an external location configuration, and you must have the  `CREATE     MANAGED     STORAGE`  privilege on that external location. You can also use a subpath of that path. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  . 5. Assign permissions for your catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  . 6. Click  **Save**   .     1. Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : Optional description or other comment.   ``` CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name>  [ MANAGED LOCATION '<location\_path>' ]  [ COMMENT <comment> ];  ```      For example, to create a catalog named  `example`  :        ``` CREATE CATALOG IF NOT EXISTS example;  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ```      For parameter descriptions, see  [CREATE CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-create-catalog.html)  . 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` library(SparkR)  sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` library(SparkR)  sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .       When you create a catalog, two schemas (databases) are automatically created:  `default`  and  `information\_schema`  .       You can also create a catalog by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog)   . You can retrieve information about catalogs by using  [databricks\_catalogs](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/catalogs)   .      ###   Next steps     Now you can  [add schemas (databases) to your catalog](create-schemas.html)  .          Delete a catalog ------------------     To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.         You must delete all schemas in the catalog except  `information\_schema`  before you can delete a catalog. This includes the auto-created  `default`  schema.     1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. In the  **Data**   pane, on the left, click the catalog you want to delete. 4. In the detail pane, click the three-dot menu to the left of the  **Create database**   button and select  **Delete**   . 5. On the  **Delete catalog**   dialog, click  **Delete**   .       Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` DROP CATALOG [ IF EXISTS ] <catalog\_name> [ RESTRICT | CASCADE ]  ```      For example, to delete a catalog named  `vaccine`  and its schemas:        ``` DROP CATALOG vaccine CASCADE  ```        Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` spark.sql(""DROP CATALOG [ IF EXISTS ] <catalog\_name>"	To set up Unity Catalog, you need to be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore. Your Databricks account must be on the Premium plan and above. You must have a Unity Catalog metastore linked to the workspace where you perform the catalog creation. The compute resource that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode. To create a catalog, you can use Data Explorer or a SQL command. There are different SQL commands to create a catalog in a notebook or Databricks SQL editor. Once you create a catalog, two schemas (databases) are automatically created: default and information_schema. To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.	1
no_reaction	['https://spark.apache.org/docs/latest/api/R/reference/dropDuplicates.html']	how to duplicate rows in a spark dataframe?	"   pyspark.pandas.Series.duplicated  [¶](#pyspark-pandas-series-duplicated ""Permalink to this headline"") =======================================================================================================  `Series.`  `duplicated`    (   *keep     :     Union    [    bool    ,    str    ]     =     'first'*   )    → pyspark.pandas.series.Series  [[source]](../../../_modules/pyspark/pandas/series.html#Series.duplicated) [¶](#pyspark.pandas.Series.duplicated ""Permalink to this definition"")    Indicate duplicate Series values.     Duplicated values are indicated as  `True`  values in the resulting Series. Either all duplicates, all except the first or all except the last occurrence of duplicates can be indicated.     New in version 3.4.0.     Parameters    **keep**    {‘first’, ‘last’, False}, default ‘first’     Method to handle marking duplicates: - ‘first’ : Mark duplicates as  `True`  except for the first occurrence. - ‘last’ : Mark duplicates as  `True`  except for the last occurrence. -  `False`  : Mark all duplicates as  `True`  .     Returns     Series     Series indicating whether each value has occurred in the preceding values     See also    [`Index.drop\_duplicates`](pyspark.pandas.Index.drop_duplicates.html#pyspark.pandas.Index.drop_duplicates ""pyspark.pandas.Index.drop_duplicates"")   Remove duplicate values from Index.    [`DataFrame.duplicated`](pyspark.pandas.DataFrame.duplicated.html#pyspark.pandas.DataFrame.duplicated ""pyspark.pandas.DataFrame.duplicated"")   Equivalent method on DataFrame.    [`Series.drop\_duplicates`](pyspark.pandas.Series.drop_duplicates.html#pyspark.pandas.Series.drop_duplicates ""pyspark.pandas.Series.drop_duplicates"")   Remove duplicate values from Series.     Examples     By default, for each set of duplicated values, the first occurrence is set on False and all others on True:    ``` >>> animals = ps.Series(['lama', 'cow', 'lama', 'beetle', 'lama']) >>> animals.duplicated().sort\_index() 0 False 1 False 2 True 3 False 4 True dtype: bool  ```   which is equivalent to    ``` >>> animals.duplicated(keep='first').sort\_index() 0 False 1 False 2 True 3 False 4 True dtype: bool  ```   By using ‘last’, the last occurrence of each set of duplicated values is set on False and all others on True:    ``` >>> animals.duplicated(keep='last').sort\_index() 0 True 1 False 2 True 3 False 4 False dtype: bool  ```   By setting keep on  `False`  , all duplicates are True:    ``` >>> animals.duplicated(keep=False).sort\_index() 0 True 1 False 2 True 3 False 4 True dtype: bool  ```  [pyspark.pandas.Series.drop\_duplicates](pyspark.pandas.Series.drop_duplicates.html ""previous page"")  [pyspark.pandas.Series.equals](pyspark.pandas.Series.equals.html ""next page"")   "	The `duplicated` method in PySpark's Pandas API can be used to indicate duplicate values in a Series. Duplicated values are indicated as `True` values in the resulting Series. Either all duplicates, all except the first or all except the last occurrence of duplicates can be indicated. To duplicate rows in a Spark DataFrame, you can use the `union` method to concatenate the DataFrame with itself.	0
no_reaction	['https://docs.databricks.com/query-federation/snowflake.html']	How does Databricks compare with Snowflake	"    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format(""snowflake"")   .option(""dbtable"", table\_name)   .option(""sfUrl"", database\_host\_url)   .option(""sfUser"", username)   .option(""sfPassword"", password)   .option(""sfDatabase"", database\_name)   .option(""sfSchema"", schema\_name)   .option(""sfWarehouse"", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format(""snowflake"")  .option(""dbtable"", table\_name)  .option(""sfUrl"", database\_host\_url)  .option(""sfUser"", username)  .option(""sfPassword"", password)  .option(""sfDatabase"", database\_name)  .option(""sfSchema"", schema\_name)  .option(""sfWarehouse"", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         "	Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake. You can configure a connection to Snowflake and then query data. Databricks also provides simple examples of how to write data to and read data from Snowflake.	0
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/create-schemas.html']	how to create schema	"    Create and manage schemas (databases) =======================================     This article shows how to create and manage schemas (databases) in Unity Catalog. A schema contains tables, views, and functions. You create schemas inside  [catalogs](create-catalogs.html)  .        Requirements --------------   * You must have the  `USE     CATALOG`  and  `CREATE     SCHEMA` [data permissions](manage-privileges/privileges.html)  on the schema’s parent catalog. Either a metastore admin or the owner of the catalog can grant you these privileges. If you are a metastore admin, you can grant these privileges to yourself. * Your Databricks account must be on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * You must have a Unity Catalog metastore  [linked to the workspace](create-metastore.html)  where you perform the schema creation. * The  [compute resource](compute.html)  that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode.         Create a schema -----------------     To create a schema (database), you can use Data Explorer or SQL commands.       1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. In the  **Data**   pane on the left, click the catalog you want to create the schema in. 4. In the detail pane, click  **Create database**   . 5. Give the schema a name and add any comment that would help users understand the purpose of the schema. 6. (Optional) Specify the location where data for  [managed tables](create-tables.html#managed-table)  in the schema will be stored.       ..note:: This option appears only if you are using Databricks SQL or a cluster running Databricks Runtime 11.3 or above.       Specify a location here only if you do not want managed tables in this schema to be stored in the default root storage location that was configured for the metastore or the managed storage location specified for the catalog (if any). See  [Create a Unity Catalog metastore](create-metastore.html)  .       The path that you specify must be defined in an external location configuration, and you must have the  `CREATE     MANAGED     STORAGE`  privilege on that external location. You can also use a subpath of that path. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  . 7. Click  **Create**   . 8. Assign permissions for your catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  . 9. Click  **Save**   .     1. Run the following SQL commands in a notebook or Databricks SQL editor. Items in brackets are optional. You can use either  `SCHEMA`  or  `DATABASE`  . Replace the placeholder values:      * `<catalog\_name>`   : The name of the parent catalog for the schema.  * `<schema\_name>`   : A name for the schema.  * `<location\_path>`   : Optional. Available only if you are using Databricks SQL or a cluster running Databricks Runtime 11.3 or above. Provide a storage location path if you want managed tables in this schema to be stored in a location that is different than the catalog’s or metastore’s root storage location. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ).  * `<comment>`   : Optional description or other comment.  * `<property\_key>        =        <property\_value>        [        ,        ...        ]`   : Optional. Spark SQL properties and values to set for the schema.  For parameter descriptions, see  [CREATE SCHEMA](../../sql/language-manual/sql-ref-syntax-ddl-create-schema.html)  .        ``` USE CATALOG <catalog>; CREATE { DATABASE | SCHEMA } [ IF NOT EXISTS ] <schema_name>     [ MANAGED LOCATION '<location_path>' ]     [ COMMENT <comment> ]     [ WITH DBPROPERTIES ( <property_key = property_value [ , ... ]> ) ];  ```      You can optionally omit the  `USE     CATALOG`  statement and replace  `<schema\_name>`  with  `<catalog\_name>.<schema\_name>`  . 2. Assign privileges to the schema. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL commands in a notebook. Items in brackets are optional. You can use either  `SCHEMA`  or  `DATABASE`  . Replace the placeholder values:      * `<catalog\_name>`   : The name of the parent catalog for the schema.  * `<schema\_name>`   : A name for the schema.  * `<location\_path>`   : Optional. Available only if you are using Databricks SQL or a cluster running Databricks Runtime 11.3 or above. Provide a storage location path if you want managed tables in this schema to be stored in a location that is different than the catalog’s or metastore’s root storage location. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ).  * `<comment>`   : An optional comment.  * `<property\_key>        =        <property\_value>        [        ,        ...        ]`   : The Spark SQL properties and values to set for the schema.  For parameter descriptions and more options, see  [CREATE SCHEMA](../../sql/language-manual/sql-ref-syntax-ddl-create-schema.html)  .        ``` spark.sql(""USE CATALOG <catalog>"")  spark.sql(""CREATE { DATABASE | SCHEMA } [ IF NOT EXISTS ] <schema\_name> "" \   ""[ MANAGED LOCATION '<location\_path>' ] "" \   ""[ COMMENT <comment> ] "" \   ""[ WITH DBPROPERTIES ( <property\_key = property\_value [ , ... ]> ) ]"")  ```      You can optionally omit the  `USE     CATALOG`  statement and replace  `<schema\_name>`  with  `<catalog\_name>.<schema\_name>`  . 2. Assign privileges to the schema. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL commands in a notebook. Items in brackets are optional. You can use either  `SCHEMA`  or  `DATABASE`  . Replace the placeholder values:      * `<catalog\_name>`   : The name of the parent catalog for the schema.  * `<schema\_name>`   : A name for the schema.  * `<location\_path>`   : Optional. Available only if you are using Databricks SQL or a cluster running Databricks Runtime 11.3 or above. Provide a storage location path if you want managed tables in this schema to be stored in a location that is different than the catalog’s or metastore’s root storage location. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ).  * `<comment>`   : An optional comment.  * `<property\_key>        =        <property\_value>        [        ,        ...        ]`   : The Spark SQL properties and values to set for the schema.  For parameter descriptions, see  [CREATE SCHEMA](../../sql/language-manual/sql-ref-syntax-ddl-create-schema.html)  .        ``` library(SparkR)  sql(""USE CATALOG <catalog>"")  sql(paste(""CREATE { DATABASE | SCHEMA } [ IF NOT EXISTS ] <schema\_name> "",  ""[ MANAGED LOCATION '<location\_path>' ] "" ,  ""[ COMMENT <comment> ] "",  ""[ WITH DBPROPERTIES ( <property\_key = property\_value [ , ... ]> ) ]"",  sep = """"))  ```      You can optionally omit the  `USE     CATALOG`  statement and replace  `<schema\_name>`  with  `<catalog\_name>.<schema\_name>`  . 2. Assign privileges to the schema. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL commands in a notebook. Items in brackets are optional. You can use either  `SCHEMA`  or  `DATABASE`  . Replace the placeholder values:      * `<catalog\_name>`   : The name of the parent catalog for the schema.  * `<schema\_name>`   : A name for the schema.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this schema to be stored in a managed location that is different than the catalog’s or metastore’s root storage location. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.  * `<property\_key>        =        <property\_value>        [        , "	To create a schema in Unity Catalog, you can use Data Explorer or SQL commands. To create a schema using Data Explorer, you need to log in to a workspace that is linked to the metastore, click on the Data icon, select the catalog you want to create the schema in, click on Create database, give the schema a name, add any comment that would help users understand the purpose of the schema, and specify the location where data for managed tables in the schema will be stored. To create a schema using SQL commands, you need to run the following SQL commands in a notebook or Databricks SQL editor:   ``` USE CATALOG <catalog>; CREATE { DATABASE | SCHEMA } [ IF NOT EXISTS ] <schema_name>     [ MANAGED LOCATION '<location_path>' ]     [ COMMENT <comment> ]     [ WITH DBPROPERTIES ( <property_key = property_value [ , ... ]> ) ]; ```  You can optionally omit the `USE CATALOG` statement and replace `<schema_name>` with `<catalog_name>.<schema_name>`. After creating the schema, you need to assign privileges to it.	1
no_reaction	['https://docs.databricks.com/optimizations/index.html']	How do I optimize my cluster for highly sparse data?	"   SparseMatrix  [¶](#sparsematrix ""Permalink to this headline"") ===============================================================  *class*  `pyspark.ml.linalg.`  `SparseMatrix`    (   *numRows     :     int*  ,  *numCols     :     int*  ,  *colPtrs     :     Union    [    bytes    ,    Iterable    [    int    ]     ]*  ,  *rowIndices     :     Union    [    bytes    ,    Iterable    [    int    ]     ]*  ,  *values     :     Union    [    bytes    ,    Iterable    [    float    ]     ]*  ,  *isTransposed     :     bool     =     False*   )   [[source]](../../_modules/pyspark/ml/linalg.html#SparseMatrix) [¶](#pyspark.ml.linalg.SparseMatrix ""Permalink to this definition"")    Sparse Matrix stored in CSC format.     Methods    |  [`toArray`](#pyspark.ml.linalg.SparseMatrix.toArray ""pyspark.ml.linalg.SparseMatrix.toArray"")  ()   |   Return a numpy.ndarray   | |  [`toDense`](#pyspark.ml.linalg.SparseMatrix.toDense ""pyspark.ml.linalg.SparseMatrix.toDense"")  ()   |   |   Methods Documentation    `toArray`    (     )    → numpy.ndarray  [[source]](../../_modules/pyspark/ml/linalg.html#SparseMatrix.toArray) [¶](#pyspark.ml.linalg.SparseMatrix.toArray ""Permalink to this definition"")    Return a numpy.ndarray    `toDense`    (     )    →  [pyspark.ml.linalg.DenseMatrix](pyspark.ml.linalg.DenseMatrix.html#pyspark.ml.linalg.DenseMatrix ""pyspark.ml.linalg.DenseMatrix"")  [[source]](../../_modules/pyspark/ml/linalg.html#SparseMatrix.toDense) [¶](#pyspark.ml.linalg.SparseMatrix.toDense ""Permalink to this definition"")   [DenseMatrix](pyspark.ml.linalg.DenseMatrix.html ""previous page"")  [Matrices](pyspark.ml.linalg.Matrices.html ""next page"")   "	To optimize a cluster for highly sparse data, you can use the `SparseMatrix` class in `pyspark.ml.linalg`. This class represents a sparse matrix stored in Compressed Sparse Column (CSC) format. You can use the `toArray` method to convert the sparse matrix to a numpy array and the `toDense` method to convert it to a dense matrix.	0
no_reaction	['https://docs.databricks.com/delta/update-schema.html']	Does create or replace table allow for schema drift?	"      Update Delta Lake table schema ================================     Delta Lake lets you update the schema of a table. The following types of changes are supported:     * Adding new columns (at arbitrary positions) * Reordering existing columns * Renaming existing columns     You can make these changes explicitly using DDL or implicitly using DML.        Important       When you update a Delta table schema, streams that read from that table terminate. If you want the stream to continue you must restart it.       For recommended methods, see  [Production considerations for Structured Streaming](../structured-streaming/production.html)  .         Explicitly update schema to add columns -----------------------------------------      ``` ALTER TABLE table\_name ADD COLUMNS (col\_name data\_type [COMMENT col\_comment] [FIRST|AFTER colA\_name], ...)  ```      By default, nullability is  `true`  .       To add a column to a nested field, use:        ``` ALTER TABLE table\_name ADD COLUMNS (col\_name.nested\_col\_name data\_type [COMMENT col\_comment] [FIRST|AFTER colA\_name], ...)  ```      For example, if the schema before running  `ALTER     TABLE     boxes     ADD     COLUMNS     (colB.nested     STRING     AFTER     field1)`  is:        ``` - root | - colA | - colB | +-field1 | +-field2  ```      the schema after is:        ``` - root | - colA | - colB | +-field1 | +-nested | +-field2  ```       Note       Adding nested columns is supported only for structs. Arrays and maps are not supported.          Explicitly update schema to change column comment or ordering ---------------------------------------------------------------      ``` ALTER TABLE table\_name ALTER [COLUMN] col\_name (COMMENT col\_comment | FIRST | AFTER colA\_name)  ```      To change a column in a nested field, use:        ``` ALTER TABLE table\_name ALTER [COLUMN] col\_name.nested\_col\_name (COMMENT col\_comment | FIRST | AFTER colA\_name)  ```      For example, if the schema before running  `ALTER     TABLE     boxes     ALTER     COLUMN     colB.field2     FIRST`  is:        ``` - root | - colA | - colB | +-field1 | +-field2  ```      the schema after is:        ``` - root | - colA | - colB | +-field2 | +-field1  ```        Explicitly update schema to replace columns ---------------------------------------------      ``` ALTER TABLE table\_name REPLACE COLUMNS (col\_name1 col\_type1 [COMMENT col\_comment1], ...)  ```      For example, when running the following DDL:        ``` ALTER TABLE boxes REPLACE COLUMNS (colC STRING, colB STRUCT<field2:STRING, nested:STRING, field1:STRING>, colA STRING)  ```      if the schema before is:        ``` - root | - colA | - colB | +-field1 | +-field2  ```      the schema after is:        ``` - root | - colC | - colB | +-field2 | +-nested | +-field1 | - colA  ```          Explicitly update schema to rename columns --------------------------------------------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .         Note       This feature is available in Databricks Runtime 10.2 and above.        To rename columns without rewriting any of the columns’ existing data, you must enable column mapping for the table. See  [Rename and drop columns with Delta Lake column mapping](delta-column-mapping.html)  .       To rename a column:        ``` ALTER TABLE table\_name RENAME COLUMN old\_col\_name TO new\_col\_name  ```      To rename a nested field:        ``` ALTER TABLE table\_name RENAME COLUMN col\_name.old\_nested\_field TO new\_nested\_field  ```      For example, when you run the following command:        ``` ALTER TABLE boxes RENAME COLUMN colB.field1 TO field001  ```      If the schema before is:        ``` - root | - colA | - colB | +-field1 | +-field2  ```      Then the schema after is:        ``` - root | - colA | - colB | +-field001 | +-field2  ```      See  [Rename and drop columns with Delta Lake column mapping](delta-column-mapping.html)  .           Explicitly update schema to drop columns ------------------------------------------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .         Note       This feature is available in Databricks Runtime 11.0 and above.        To drop columns as a metadata-only operation without rewriting any data files, you must enable column mapping for the table. See  [Rename and drop columns with Delta Lake column mapping](delta-column-mapping.html)  .        Important       Dropping a column from metadata does not delete the underlying data for the column in files. To purge the dropped column data, you can use  [REORG TABLE](../sql/language-manual/delta-reorg-table.html)  to rewrite files. You can then use  [VACUUM](../sql/language-manual/delta-vacuum.html)  to physically delete the files that contain the dropped column data.        To drop a column:        ``` ALTER TABLE table\_name DROP COLUMN col\_name  ```      To drop multiple columns:        ``` ALTER TABLE table\_name DROP COLUMNS (col\_name\_1, col\_name\_2)  ```          Explicitly update schema to change column type or name --------------------------------------------------------     You can change a column’s type or name or drop a column by rewriting the table. To do this, use the  `overwriteSchema`  option.       The following example shows changing a column type:        ``` (spark.read.table(...)   .withColumn(""birthDate"", col(""birthDate"").cast(""date""))   .write   .mode(""overwrite"")   .option(""overwriteSchema"", ""true"")   .saveAsTable(...) )  ```      The following example shows changing a column name:        ``` (spark.read.table(...)   .withColumnRenamed(""dateOfBirth"", ""birthDate"")   .write   .mode(""overwrite"")   .option(""overwriteSchema"", ""true"")   .saveAsTable(...) )  ```        Add columns with automatic schema update ------------------------------------------     Columns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when:     * `write`  or  `writeStream`  have  `.option(""mergeSchema"",     ""true"")` * `spark.databricks.delta.schema.autoMerge.enabled`  is  `true`     When both options are specified, the option from the  `DataFrameWriter`  takes precedence. The added columns are appended to the end of the struct they are present in. Case is preserved when appending a new column.        Note     * `mergeSchema`  cannot be used with  `INSERT     INTO`  or  `.write.insertInto()`  .            Automatic schema evolution for Delta Lake merge -------------------------------------------------     Schema evolution allows users to resolve schema mismatches between the target and source table in merge. It handles the following two cases:     1. A column in the source table is not present in the target table. The new column is added to the target schema, and its values are inserted or updated using the source values. 2. A column in the target table is not present in the source table. The target schema is left unchanged; the values in the additional target column are either left unchanged (for  `UPDATE`  ) or set to  `NULL`  (for  `INSERT`  ).     ..important:: To use schema evolution, you must set the Spark session configuration`spark.databricks.delta.schema.autoMerge.enabled` to  `true`  before you run the  `merge`  command.        Note     * In Databricks Runtime 7.3 LTS,  `merge`  supports schema evolution of only top-level columns, and not of nested columns. * In Databricks Runtime 12.2 and above, columns present in the source table can be specified by name in insert or update actions. In Databricks Runtime 12.1 and below, only  `INSERT     \*`  or  `UPDATE     SET     \*`  actions can be used for schema evolution with merge.      Here are a few examples of the effects of  `merge`  operation with and without schema evolution.            |   Columns   |   Query (in SQL)   |   Behavior without schema evolution (default)   |   Behavior with schema evolution   | | --- | --- | --- | --- | |   Target columns:  `key,     value`    Source columns:  `key,     value,     new\_value`  |    ``` MERGE INTO target\_table t USING source\_table s ON t.key = s.key WHEN MATCHED  THEN UPDATE SET \* WHEN NOT MATCHED  THEN INSERT \*  ```    |   The table schema remains unchanged; only columns  `key`  ,  `value`  are updated/inserted.   |   The table schema is changed to  `(key,     value,     new\_value)`  . Existing records with matches are updated with the  `value`  and  `new\_value`  in the source. New rows are inserted with the schema  `(key,     value,     new\_value)`  .   | |   Target columns:  `key,     old\_value`    Source columns:  `key,     new\_value`  |    ``` MERGE INTO target\_table t USING source\_table s ON t.key = s.key WHEN MATCHED  THEN UPDATE SET \* WHEN NOT MATCHED  THEN INSERT \*  ```    |  `UPDATE`  and  `INSERT`  actions throw an error because the target column  `old\_value`  is not in the source.   |   The table schema is changed to  `(key,     old\_value,     new\_value)`  . Existing records with matches are updated with the  `new\_value`  in the source leaving  `old\_value`  unchanged. New records are inserted with the specified  `key`  ,  `new\_value`  , and  `NULL`  for the  `old\_value`  .   | |   Target columns:  `key,"	Yes, `create or replace table` allows for schema drift.	0
no_reaction	['https://kb.databricks.com/streaming/ss-read-from-last-offset']	How do you use offset in streaming query	"Title: How to restart a structured streaming query from last written offset  Description: Learn how to restart a structured streaming query from the last written offset.  Answer: Scenario You have a stream, running a windowed aggregation query, that reads from Apache Kafka and writes files in Append mode. You want to upgrade the application and restart the query with the offset equal to the last written offset. You want to discard all state information that hasn’t been written to the sink, start processing from the earliest offsets that contributed to the discarded state, and modify the checkpoint directory accordingly. However, if you use existing checkpoints after upgrading the application code, old states and objects from the previous application version are re-used, which results in unexpected output such as reading from old sources or processing with old application code. Solution Apache Spark maintains state across the execution and binary objects on checkpoints. Therefore you cannot modify the checkpoint directory. As an alternative, copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. Make sure to delete the checkpoint directory. You can get the current offsets by using asynchronous APIs: %scala spark.streams.addListener(new StreamingQueryListener() {     override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {         println(""Query started:"" + queryStarted.id)     }     override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {         println(""Query terminated"" + queryTerminated.id)     }     override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {      println(""Query made progress"")         println(""Starting offset:"" + queryProgress.progress.sources(0).startOffset)         println(""Ending offset:"" + queryProgress.progress.sources(0).endOffset)         //Logic to save these offsets     } }) You can use readStream with the latest offset written by the process shown above: %scala option(""startingOffsets"",  """""" {""articleA"":{""0"":23,""1"":-1},""articleB"":{""0"":-2}} """""") The input schema for streaming records is: root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- article: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) Also, you can implement logic to save and update the offset to a database and read it at the next restart."	To use offset in streaming query, you can copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. You can get the current offsets by using asynchronous APIs and use readStream with the latest offset written by the process shown above.	1
no_reaction	['https://docs.databricks.com/delta/data-skipping.html']	what is z order	"Title: Z-Ordering will be ineffective, not collecting stats  Description: Z-Ordering is ineffective, error about not collecting stats. Reorder table so the columns you want to optimize on are within the first 32 columns.  Answer: Problem You are trying to optimize a Delta table by Z-Ordering and receive an error about not collecting stats for the columns. AnalysisException: Z-Ordering on [col1, col2] will be ineffective, because we currently do not collect stats for these columns. Info Please review Z-Ordering (multi-dimensional clustering) (AWS | Azure | GCP) for more information on data skipping and z-ordering. Cause Delta Lake collects statistics on the first 32 columns defined in your table schema. If the columns you are attempting to Z-Order are not in the first 32 columns, no statistics are collected for those columns. Solution Reorder the columns in your table, so the columns you are attempting to Z-Order are in the first 32 columns in your table. You can use an ALTER TABLE statement to reorder the columns. %sql ALTER TABLE table_name CHANGE [COLUMN] col_name col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name] For example, this statement brings the column with &lt;column-name&gt; to the first column in the table. %sql ALTER TABLE &lt;delta-table-name&gt; CHANGE COLUMN &lt;column-name&gt; &lt;column-name&gt; &lt;data-type&gt; FIRST Recompute the statistics after you have reordered the columns in the table. %scala import com.databricks.sql.transaction.tahoe._ import org.apache.spark.sql.catalyst.TableIdentifier import com.databricks.sql.transaction.tahoe.stats.StatisticsCollection val tableName = ""&lt;name of table&gt;"" val deltaLog = DeltaLog.forTable(spark, TableIdentifier(tableName)) StatisticsCollection.recompute(spark, deltaLog) Rerun the Z-Order on the table and it should complete successfully."	Z-Ordering is a technique used to optimize Delta tables by clustering data based on certain columns. However, Delta Lake collects statistics on the first 32 columns defined in the table schema. If the columns you are attempting to Z-Order are not in the first 32 columns, no statistics are collected for those columns, and Z-Ordering will be ineffective. To solve this problem, you need to reorder the columns in your table so that the columns you want to optimize on are within the first 32 columns. You can use an ALTER TABLE statement to reorder the columns, and then recompute the statistics after you have reordered the columns in the table.	0
no_reaction	['https://docs.databricks.com/sql/language-manual/functions/timestampdiff.html']	what is the correct unit name for nanoseconds for use with timestampdiff sql function?	   `timestampdiff`  function ==========================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime 10.4 and above       Returns the difference between two timestamps measured in  `unit`  s.        Syntax --------      ``` timestampdiff(unit, start, end)  unit  { MICROSECOND |    MILLISECOND |    SECOND |    MINUTE |    HOUR |    DAY |    WEEK |    MONTH |    QUARTER |    YEAR }  ```        Arguments -----------   * `unit`  : A unit of measure. * `start`  : A starting TIMESTAMP expression. * `end`  : A ending TIMESTAMP expression.       Returns ---------     A BIGINT.       If  `start`  is greater than  `end`  the result is negative.       The function counts whole elapsed units based on  `UTC`  with a  `DAY`  being 86400 seconds.       One month is considered elapsed when the calendar month has increased and the calendar day and time is equal or greater to the start. Weeks, quarters, and years follow from that.         Examples ----------      ``` -- One second shy of a month elapsed > SELECT timestampdiff(MONTH, TIMESTAMP'2021-02-28 12:00:00', TIMESTAMP'2021-03-28 11:59:59');  0  -- One month has passed even though its' not end of the month yet because day and time line up. > SELECT timestampdiff(MONTH, TIMESTAMP'2021-02-28 12:00:00', TIMESTAMP'2021-03-28 12:00:00');  1  -- Start is greater than the end > SELECT timestampdiff(YEAR, DATE'2021-01-01', DATE'1900-03-28');  -120  ```        Related functions -------------------   * [add\_months function](add_months.html) * [date\_add function](date_add.html) * [date\_sub function](date_sub.html) * [datediff function](datediff.html) * [datediff (timestamp) function](datediff3.html) * [timestamp function](timestamp.html) * [timestampadd function](timestampadd.html)                                                      	The correct unit name for nanoseconds for use with the `timestampdiff` SQL function is not listed in the syntax. Therefore, it is not possible to use nanoseconds as a unit of measure with the `timestampdiff` function.	0
no_reaction	['https://docs.databricks.com/security/secrets/index.html']	how to manage secrets	    Secret management ===================     Sometimes accessing data requires that you authenticate to external data sources through JDBC. Instead of directly entering your credentials into a notebook, use Databricks secrets to store your credentials and reference them in notebooks and jobs. To manage secrets, you can use the  [Databricks CLI](../../dev-tools/cli/index.html)  to access the  [Secrets API 2.0](../../dev-tools/api/latest/secrets.html)  .        Warning       Administrators, secret creators, and users granted  [permission](../auth-authz/access-control/secret-acl.html)  can read Databricks secrets. While Databricks makes an effort to redact secret values that might be displayed in notebooks, it is not possible to prevent such users from reading secrets. For more information, see  [Secret redaction](redaction.html)  .        To set up secrets you:     1. [Create a secret scope](secret-scopes.html)  . Secret scope names are case insensitive. 2. [Add secrets to the scope](secrets.html)  . Secret names are case insensitive. 3. If you have the  [Premium plan and above](https://databricks.com/product/aws-pricing)   ,  [assign access control](../auth-authz/access-control/secret-acl.html)  to the secret scope.     This guide shows you how to perform these setup tasks and manage secrets. For more information, see:     * An end-to-end  [example](example-secret-workflow.html)  of how to use secrets in your workflows. * Reference for the  [Secrets CLI](../../dev-tools/cli/secrets-cli.html)  . * Reference for the  [Secrets API 2.0](../../dev-tools/api/latest/secrets.html)  . * How to use  [Secrets utility (dbutils.secrets)](../../dev-tools/databricks-utils.html#dbutils-secrets)  to reference secrets in notebooks and jobs.                                                       	To manage secrets in Databricks, you can use Databricks secrets to store your credentials and reference them in notebooks and jobs. To set up secrets, you need to create a secret scope, add secrets to the scope, and assign access control to the secret scope if you have the Premium plan and above. You can use the Databricks CLI to access the Secrets API 2.0.	1
no_reaction	['https://docs.databricks.com/archive/spark-sql-2.x-language-manual/clear-cache.html']	how do I clear the cache	    CLEAR CACHE =============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Removes the entries and associated data from the in-memory and/or on-disk cache for all cached tables and views in Apache Spark cache.        Syntax --------      ``` > CLEAR CACHE  ```      See  [Automatic and manual caching](../../optimizations/disk-cache.html#manual-caching)  for the differences between disk caching and the Apache Spark cache.         Examples ----------      ``` > CLEAR CACHE;  ```        Related statements --------------------   * [CACHE TABLE](sql-ref-syntax-aux-cache-cache-table.html) * [UNCACHE TABLE](sql-ref-syntax-aux-cache-uncache-table.html) * [REFRESH TABLE](sql-ref-syntax-aux-cache-refresh-table.html) * [REFRESH](sql-ref-syntax-aux-cache-refresh.html) * [REFRESH FUNCTION](sql-ref-syntax-aux-cache-refresh-function.html)                                                      	To clear the cache for all cached tables and views in Apache Spark cache, you can use the `CLEAR CACHE` command. The command removes the entries and associated data from the in-memory and/or on-disk cache.	1
no_reaction	['https://kb.databricks.com/notebooks/service-principal-cannot-create-access-token']	"""On-behalf-of token creation for service principals is not enabled for this workspace"""	"Title: Service principal cannot create access token  Description: You cannot create a token on behalf of a service principal with the API when token usage is disabled.  Answer: Problem You are trying to create a token on behalf of a service principal , using /2.0/token-management/on-behalf-of/tokens in the REST API but are getting a PERMISSION_DENIED error. { ""error_code"": ""PERMISSION_DENIED"", ""message"": ""User xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx does not have permission to use tokens."" } Cause This happens when the service principals are assigned to a user group that has token usage disabled. Solution Your workspace admin should enable token usage for the user group that contains the service principals. Info You should create separate user groups for service principals and users who need token access and those who don't. This limits access only to those who need it and doesn't provide token access to all users in your workspace. Enable token usage via the UI Click Settings in the left hand menu. Click Admin Console. Click the Workspace settings tab. Click Permission Settings in the Personal Access Tokens field. Add the groups that need token access in the Token Usage window. Remove any groups that should not have token access. Click Save to apply the changes and close the window. Enable token usage via the REST API Review the token permissions API settings. Use this sample code to update the token permissions. Replace the following values in the sample code before running it on your local machine: &lt;admin-access-token&gt; - Admin personal access token. &lt;user-group-name&gt; - The name of the user group to grant token access permission. You can add multiple group entries if needed. &lt;workspace-url&gt; - Replace this value with your Workspace URL . curl --location --request PATCH 'https://&lt;workspace-url&gt;/api/2.0/preview/permissions/authorization/tokens'; \ --header 'Authorization: Bearer &lt;admin-access-token&gt;' \ --header 'Content-Type: application/json' \ --data-raw '{   ""access_control_list"": [     {       ""group_name"": ""&lt;user-group-name&gt;"",       ""permission_level"": ""CAN_USE""     },     {       ""group_name"": ""&lt;user-group-name&gt;"",       ""permission_level"": ""CAN_USE""     }   ] }' Info This sample code only allows you to add token permissions via the API. It does not allow you to delete them. To delete token permissions via the API you must replace the token permissions for the entire workspace API using PUT instead of PATCH. Review the token permissions API settings for more details."	"The error message ""On-behalf-of token creation for service principals is not enabled for this workspace"" occurs when trying to create a token on behalf of a service principal using the REST API, but token usage is disabled for the user group that contains the service principals. To resolve this issue, the workspace admin should enable token usage for the user group that contains the service principals. This can be done via the UI or the REST API. It is recommended to create separate user groups for service principals and users who need token access and those who don't to limit access only to those who need it and not provide token access to all users in the workspace."	1
no_reaction	['https://docs.databricks.com/delta/table-details.html']	How do I describe a delta table?	    Describe Table ================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [DESCRIBE TABLE](../../sql/language-manual/sql-ref-syntax-aux-describe-table.html)  .           Describe Table (Delta Lake on Databricks) -------------------------------------------      ``` DESCRIBE [EXTENDED] [db\_name.]table\_name  DESCRIBE [EXTENDED] delta.`<path-to-table>`  ```      Return the metadata of an existing table (column names, data types, and comments). If the table does not exist, an exception is thrown.      **`EXTENDED`**      Display detailed information about the table, including parent database, table type, storage information, and properties.           Describe Partition (Delta Lake on Databricks) -----------------------------------------------      ``` DESCRIBE [EXTENDED] [db\_name.]table\_name PARTITION partition\_spec  DESCRIBE [EXTENDED] delta.`<path-to-table>` PARTITION partition\_spec  ```      Return the metadata of a specified partition. The  `partition\_spec`  must provide the values for all the partition columns.      **`EXTENDED`**      Display basic information about the table and the partition-specific storage information.           Describe Columns (Delta Lake on Databricks) ---------------------------------------------      ``` DESCRIBE [EXTENDED] [db\_name.]table\_name column\_name  DESCRIBE [EXTENDED] delta.`<path-to-table>`  ```      Return the metadata of a specified column.      **`EXTENDED`**      Display detailed information about the specified columns, including the column statistics collected by the command  `ANALYZE     TABLE     table\_name     COMPUTE     STATISTICS     FOR     COLUMNS     column\_name     [column\_name,     ...]`  .           Describe Formatted (Delta Lake on Databricks) -----------------------------------------------      ``` DESCRIBE FORMATTED [db\_name.]table\_name  DESCRIBE FORMATTED delta.`<path-to-table>`  ```      Return the table format.             Describe Detail (Delta Lake on Databricks) --------------------------------------------      ``` DESCRIBE DETAIL [db\_name.]table\_name  DESCRIBE DETAIL delta.`<path-to-table>`  ```      Return information about schema, partitioning, table size, and so on. For example, you can see the current  [reader and writer versions](../../delta/feature-compatibility.html)  of a table.                                                        	To describe a Delta table, you can use the `DESCRIBE` command in Delta Lake on Databricks. The command can be used in the following formats:  - `DESCRIBE [EXTENDED] [db_name.]table_name` - `DESCRIBE [EXTENDED] delta.<path-to-table>`  The command returns the metadata of an existing table, including column names, data types, and comments. The `EXTENDED` option can be used to display detailed information about the table, including parent database, table type, storage information, and properties.	1
no_reaction	['https://docs.databricks.com/delta-live-tables/cdc.html']	delta live tables apply changes into from full snapshot	"      Change data capture with Delta Live Tables ============================================          Preview       Delta Live Tables support for SCD type 2 is in  [Public Preview](../release-notes/release-types.html)  .        You can use change data capture (CDC) in Delta Live Tables to update tables based on changes in source data. CDC is supported in the Delta Live Tables SQL and Python interfaces. Delta Live Tables supports updating tables with slowly changing dimensions (SCD) type 1 and type 2:     * Use SCD type 1 to update records directly. History is not retained for records that are updated. * Use SCD type 2 to retain a history of records, either on all updates or on updates to a specified set of columns. See  [Track history for only specified columns with SCD type 2](#type2-track-history)     For syntax and other references, see:     * [Change data capture with Python in Delta Live Tables](python-ref.html#cdc) * [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) * [CDC table properties](properties.html#cdc)      Note       This article describes how to update tables in your Delta Live Tables pipeline based on changes in source data. To learn how to record and query row-level change information for Delta tables, see  [Use Delta Lake change data feed on Databricks](../delta/delta-change-data-feed.html)  .           How is CDC implemented with Delta Live Tables? ------------------------------------------------     You must specify a column in the source data on which to sequence records, which Delta Live Tables interprets as a monotonically increasing representation of the proper ordering of the source data. Delta Live Tables automatically handles data that arrives out of order. For SCD Type 2 changes, Delta Live Tables propagates the appropriate sequencing values to the  `\_\_START\_AT`  and  `\_\_END\_AT`  columns of the target table. There should be at most one distinct update per key at each sequencing value, and NULL sequencing values are unsupported.       To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an  `APPLY     CHANGES     INTO`  statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the  `CREATE     OR     REFRESH     STREAMING     TABLE`  statement in SQL or the  `create\_streaming\_live\_table()`  function in Python. To create the statement defining the CDC processing, use the  `APPLY     CHANGES`  statement in SQL or the  `apply\_changes()`  function in Python. For syntax details, see  [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc)  or  [Change data capture with Python in Delta Live Tables](python-ref.html#cdc)  .             What data objects are used for Delta Live Tables CDC processing? ------------------------------------------------------------------     When you declare the target table, two data structures are created in the Hive metastore:     * A view using the name assigned to the target table. * An internal backing table used by Delta Live Tables table to manage CDC processing. This table is named by prepending  `\_\_apply\_changes\_storage\_`  to the target table name.     For example, if you declare a target table named  `dlt\_cdc\_target`  , you will see a view named  `dlt\_cdc\_target`  and a table named  `\_\_apply\_changes\_storage\_dlt\_cdc\_target`  in the metastore. Creating a view allows Delta Live Tables to filter out the extra information (for example, tombstones and versions) that is required to handle out-of-order data. To view the processed data, query the target view. You can also query the raw data in the  `\_\_apply\_changes\_storage\_`  table to see deleted records and extra version columns. If you add data manually to the table, the records are assumed to come before other changes because the version columns are missing.         Limitations -------------   * Metrics for the target table, such as number of output rows, are not available. * SCD type 2 updates will add a history row for every input row, even if no columns have changed. * The target of the  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function cannot be used as a source for a streaming table. A table that reads from the target of an  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function must be a live table. * Expectations are not supported in an  `APPLY     CHANGES     INTO`  query or  `apply\_changes()`  function. To use expectations for the source or target dataset:      + Add expectations on source data by defining an intermediate table with the required expectations and use this dataset as the source for the target table.  + Add expectations on target data with a downstream table that reads input data from the target table.         SCD type 1 and SCD type 2 on Databricks -----------------------------------------     The following sections provide examples that demonstrate Delta Live Tables SCD type 1 and type 2 queries that update target tables based on source events that:     1. Create new user records. 2. Delete a user record. 3. Update user records. In the SCD type 1 example, the last  `UPDATE`  operations arrive late and are dropped from the target table, demonstrating the handling of out of order events.     All of the following examples assume familiarity with configuring and updating Delta Live Tables pipelines. See  [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html)  .       In order to run these examples, you must begin by creating a sample dataset. See  [Generate test data](#generate-data)  .       The following are the input records for these examples:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   124   |   Raul   |   Oaxaca   |   INSERT   |   1   | |   123   |   Isabel   |   Monterrey   |   INSERT   |   1   | |   125   |   Mercedes   |   Tijuana   |   INSERT   |   2   | |   126   |   Lily   |   Cancun   |   INSERT   |   2   | |   123   |   null   |   null   |   DELETE   |   6   | |   125   |   Mercedes   |   Guadalajara   |   UPDATE   |   6   | |   125   |   Mercedes   |   Mexicali   |   UPDATE   |   5   | |   123   |   Isabel   |   Chihuahua   |   UPDATE   |   5   |     If you uncomment the final row in the example data, it will insert the following record that specifies where records should be truncated:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   null   |   null   |   null   |   TRUNCATE   |   3   |      Note       All the following examples include options to specify both  `DELETE`  and  `TRUNCATE`  operations, but each of these are optional.            Process SCD type 1 updates ----------------------------     The following code example demonstrates processing SCD type 1 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format(""delta"").table(""cdc\_data.users"")  dlt.create\_streaming\_live\_table(""target"")  dlt.apply\_changes(   target = ""target"",   source = ""users"",   keys = [""userId""],   sequence\_by = col(""sequenceNum""),   apply\_as\_deletes = expr(""operation = 'DELETE'""),   apply\_as\_truncates = expr(""operation = 'TRUNCATE'""),   except\_column\_list = [""operation"", ""sequenceNum""],   stored\_as\_scd\_type = 1 )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = ""DELETE"" APPLY AS TRUNCATE WHEN  operation = ""TRUNCATE"" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 1;  ```       After running the SCD type 1 example, the target table contains the following records:           |   userId   |   name   |   city   | | --- | --- | --- | |   124   |   Raul   |   Oaxaca   | |   125   |   Mercedes   |   Guadalajara   | |   126   |   Lily   |   Cancun   |     After running the SCD type 1 example with the additional  `TRUNCATE`  record, records  `124`  and  `126`  are truncated because of the  `TRUNCATE`  operation at  `sequenceNum=3`  , and the target table contains the following record:           |   userId   |   name   |   city   | | --- | --- | --- | |   125   |   Mercedes   |   Guadalajara   |         Process SCD type 2 updates ----------------------------     The following code example demonstrates processing SCD type 2 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format(""delta"").table(""cdc\_data.users"")  dlt.create\_streaming\_live\_table(""target"")  dlt.apply\_changes(   target = ""target"",   source = ""users"",   keys = [""userId""],   sequence\_by = col(""sequenceNum""),   apply\_as\_deletes = expr(""operation = 'DELETE'""),   except\_column\_list = [""operation"", ""sequenceNum""],   stored\_as\_scd\_type = ""2"" )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = ""DELETE"" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 2;  ```       After running the SCD type 2 example, the target table contains the following records:             |   userId   |   name   |   city   |   \_\_START\_AT   |   \_\_END\_AT   | | --- | --- | --- | --- | --- | |   123   |   Isabel   |   Monterrey   |   1   |   5   | |   123   |   Isabel   |  "	Delta Live Tables support updating tables with slowly changing dimensions (SCD) type 1 and type 2. To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an `APPLY CHANGES INTO` statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the `CREATE OR REFRESH STREAMING TABLE` statement in SQL or the `create_streaming_live_table()` function in Python. To create the statement defining the CDC processing, use the `APPLY CHANGES` statement in SQL or the `apply_changes()` function in Python. For syntax details, see [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) or [Change data capture with Python in Delta Live Tables](python-ref.html#cdc).	0
no_reaction	['https://docs.databricks.com/error-messages/user-defined-functions-error-class.html']	how can i trigger a user defined function	    Functions ===========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Spark SQL provides two function features to meet a wide range of needs: built-in functions and user-defined functions (UDFs).        Built-in functions --------------------     This article presents the usages and descriptions of categories of frequently used built-in functions for aggregation, arrays and maps, dates and timestamps, and JSON data.      * [Built-in functions](sql-ref-functions-builtin.html)        SQL user-defined functions ----------------------------     SQL user-defined functions (UDFs) are functions you can define yourself which can return scalar values or result sets.       See  [CREATE FUNCTION (SQL)](sql-ref-syntax-ddl-create-sql-function.html)  for more information.      ###   User-defined functions     UDFs allow you to define your own functions when the system’s built-in functions are not enough to perform the desired task. To use UDFs, you first define the function, then register the function with Spark, and finally call the registered function. A UDF can act on a single row or act on multiple rows at once. Spark SQL also supports integration of existing Hive implementations of UDFs, user defined aggregate functions (UDAF), and user defined table functions (UDTF).      * [User-defined aggregate functions (UDAFs)](sql-ref-functions-udf-aggregate.html) * [Integration with Hive UDFs, UDAFs, and UDTFs](sql-ref-functions-udf-hive.html) * [User-defined scalar functions (UDFs)](sql-ref-functions-udf-scalar.html)                                                        	To trigger a user-defined function in Spark SQL, you first need to define the function, then register the function with Spark, and finally call the registered function.	1
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/create-metastore.html']	Is the Unity Catalog metastore in the data plane?	      Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .                                                      	Based on the context, it is not explicitly stated whether the Unity Catalog metastore is in the data plane or not.	1
no_reaction	['https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html']	How to query object ID in Databricks SQL warehouse using only SQL? I can see on Databricks SQL warehouse Data tab that clusters, catalogs and schemas have a unique ID. User created tables, views and functions must have and unique ID too, but it is not exposed to the user as far as I can tell.  I need to retrieve the IDs for all these objects for a data governance solution I'm working on. information_schema does not show any object ID.  Is there a way to do this (like in Oracle for example, query the ALL_OBJECTS system view). Thanks!	"    Get identifiers for workspace assets ======================================     This article explains how to get workspace, cluster, directory, model, notebook, and job identifiers and URLs in Databricks.          Workspace instance names, URLs, and IDs -----------------------------------------     An  *instance name*   is assigned to each Databricks deployment. To segregate the workload and grant access to relevant users only, usually Databricks customers create separate instances for development, staging, and production. The instance name is the first part of the URL when you log into your Databricks deployment:      ![Workspace](../_images/workspace-aws.png)    If you log into  `https://cust-success.cloud.databricks.com/`  , the instance name is  `cust-success.cloud.databricks.com`  .       A Databricks  *[workspace](index.html)*  is where the Databricks platform runs and where you can create Spark clusters and schedule workloads. Some types of workspaces have a unique workspace ID. If there is  `o=`  in the deployment URL, for example,  `https://<databricks-instance>/?o=6280049833385130`  , the random number after  `o=`  is the Databricks workspace ID. Here the workspace ID is  `6280049833385130`  . If there is no  `o=`  in the deployment URL, the workspace ID is  `0`  .         Cluster URL and ID --------------------     A Databricks  *[cluster](../clusters/index.html)*  provides a unified platform for various use cases such as running production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning. Each cluster has a unique ID called the cluster ID. This applies to both all-purpose and job clusters. To get the details of a cluster using the REST API, the cluster ID is essential.       To get the cluster ID, click the  **Clusters**   tab in sidebar and then select a cluster name. The cluster ID is the number after the  `/clusters/`  component in the URL of this page        ``` https://<databricks-instance>/#/setting/clusters/<cluster-id>  ```      In the following screenshot, the cluster ID is  `1115-164516-often242`  :      ![Cluster URL](../_images/aws-cluster.png)        Notebook URL and ID ---------------------     A  *[notebook](../notebooks/index.html)*  is a web-based interface to a document that contains runnable code, visualizations, and narrative text. Notebooks are one interface for interacting with Databricks. Each notebook has a unique ID. The notebook URL has the notebook ID, hence the notebook URL is unique to a notebook. It can be shared with anyone on Databricks platform with permission to view and edit the notebook. In addition, each notebook command (cell) has a different URL.       To find a notebook URL or ID, open a notebook. To find a cell URL, click the contents of the command.     * Example notebook URL:        ``` https://cust-success.cloud.databricks.com/#notebook/333096  ``` * Example notebook ID:  `333096`  . * Example command (cell) URL:        ``` https://cust-success.cloud.databricks.com/#notebook/333096/command/333099  ```       Folder ID -----------     A  *[folder](workspace-objects.html#folders)*  is a directory used to store files that can used in the Databricks workspace. These files can be notebooks, libraries or subfolders. There is a specific id associated with each folder and each individual sub-folder. The Permissions API refers to this id as a directory\_id and is used in setting and updating permissions for a folder.       To retrieve the directory\_id , use the Workspace API:        ``` curl -n -X GET -H 'Content-Type: application/json' -d '{""path"": ""/Users/me@example.com/MyFolder""}' \ https://<databricks-instance>/api/2.0/workspace/get-status  ```      This is an example of the API call response:        ``` {  ""object\_type"": ""DIRECTORY"",  ""path"": ""/Users/me@example.com/MyFolder"",  ""object\_id"": 123456789012345 }  ```        Model ID ----------     A model refers to an MLflow  *[registered model](../mlflow/model-registry.html)*  , which lets you manage MLflow Models in production through stage transitions and versioning. The registered model ID is required for changing the permissions on the model programmatically through the  [Permissions API 2.0](../dev-tools/api/latest/permissions.html)  .       To get the ID of a registered model, you can use the  [REST API (latest)](../dev-tools/api/latest/index.html)  endpoint  `mlflow/databricks/registered-models/get`  . For example, the following code returns the registered model object with its properties, including its ID:        ``` curl -n -X GET -H 'Content-Type: application/json' -d '{""name"": ""model\_name""}' \ https://<databricks-instance>/api/2.0/mlflow/databricks/registered-models/get  ```      The returned value has the format:        ``` {  ""registered\_model\_databricks"": {  ""name"":""model\_name"",  ""id"":""ceb0477eba94418e973f170e626f4471""  } }  ```        Job URL and ID ----------------     A  *[job](../workflows/jobs/jobs.html)*  is a way of running a notebook or JAR either immediately or on a scheduled basis.       To get a job URL, click  ![Jobs Icon](../_images/jobs-icon.png) **Workflows**   in the sidebar and click a job name. The job ID is after the text  `#job/`  in the URL. The job URL is required to troubleshoot the root cause of failed job runs.       In the following screenshot, the job URL is:        ``` https://cust-success.cloud.databricks.com/#job/25612  ```      In this example, the job ID  `25612`  .      ![Job URL](../_images/aws-jobs.png)                                                     "	Unfortunately, it seems that there is no direct way to query object IDs for user-created tables, views, and functions in Databricks SQL warehouse using only SQL. The information_schema does not show any object ID.	0
no_reaction	['https://docs.databricks.com/reference/mlflow-api.html']	how can I use a generated token to authenticate my requests to the MLflow REST API ?	"Title: MLflow 'invalid access token' error  Description: Long running ML tasks require an access token with an extended lifetime to ensure the tasks complete before the token expires.  Answer: Problem You have long-running MLflow tasks in your notebook or job and the tasks are not completed. Instead, they return a (403) Invalid access token error message. Error stack trace: MlflowException: API request to endpoint /api/2.0/mlflow/runs/create failed with error code  403 != 200. Response body: '&lt;html&gt; &lt;head&gt; &lt;meta data-fr-http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/&gt; &lt;title&gt;Error 403 Invalid access token.&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;h2&gt;HTTP ERROR 403&lt;/h2&gt; &lt;p&gt;Problem accessing /api/2.0/mlflow/runs/create. Reason: &lt;pre&gt; Invalid access token.&lt;/pre&gt;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Cause The Databricks access token that the MLflow Python client uses to communicate with the tracking server expires after several hours. If your ML tasks run for an extended period of time, the access token may expire before the task completes. This results in MLflow calls failing with a (403) Invalid access token error message in both notebooks and jobs. Solution You can work around this issue by manually creating an access token with an extended lifetime and then configuring that access token in your notebook prior to running MLflow tasks. Generate a personal access token (AWS | Azure ) and configure it with an extended lifetime. Set up the Databricks CLI (AWS | Azure).  Use the Databricks CLI to create a new secret with the personal access token you just created.databricks secrets put --scope {&lt;secret-name&gt;} --key mlflow-access-token --string-value {&lt;personal-access-token&gt;} Insert this sample code at the beginning of your notebook. Include your secret name and your Workspace URL (AWS | Azure).%python access_token = dbutils.secrets.get(scope=""{&lt;secret-name&gt;}"", key=""mlflow-access-token"") import os os.environ[""DATABRICKS_TOKEN""] = access_token os.environ[""DATABRICKS_HOST""] = ""https://&lt;workspace-url&gt;"" from databricks_cli.configure import provider config_provider = provider.EnvironmentVariableConfigProvider() provider.set_config_provider(config_provider) Run your notebook or job as normal."	To use a generated token to authenticate requests to the MLflow REST API, you can manually create an access token with an extended lifetime and configure it in your notebook prior to running MLflow tasks. You can generate a personal access token and configure it with an extended lifetime, set up the Databricks CLI, and use the Databricks CLI to create a new secret with the personal access token you just created. Then, you can insert the sample code provided in the context at the beginning of your notebook, including your secret name and your Workspace URL, and run your notebook or job as normal.	1
no_reaction	['https://docs.databricks.com/dev-tools/api/latest/clusters.html']	what script should i use to access the clusters api	"              Clusters API 2.0 ==================     The Clusters API allows you to create, start, edit, list, terminate, and delete clusters. The maximum allowed size of a request to the Clusters API is 10MB.       Cluster lifecycle methods require a cluster ID, which is returned from  [Create](#clusterclusterservicecreatecluster)  . To obtain a list of clusters, invoke  [List](#clusterclusterservicelistclusters)  .       Databricks maps cluster node instance types to compute units known as DBUs. See the  [instance type pricing page](https://databricks.com/product/aws-pricing/instance-types)   for a list of the supported instance types and their corresponding DBUs. For instance provider information, see  [AWS instance type specifications](https://aws.amazon.com/ec2/instance-types/)   and  [pricing](https://aws.amazon.com/ec2/pricing/)   .       Databricks always provides one year’s deprecation notice before ceasing support for an instance type.        Warning       You should never hard code secrets or store them in plain text. Use the  [Secrets API 2.0](secrets.html)  to manage secrets in the  [Databricks CLI setup & documentation](../../cli/index.html)  . Use the  [Secrets utility (dbutils.secrets)](../../databricks-utils.html#dbutils-secrets)  to reference secrets in notebooks and jobs.         Important       To access Databricks REST APIs, you must  [authenticate](authentication.html)  .           Create --------        |   Endpoint   |   HTTP Method   | | --- | --- | |  `2.0/clusters/create`  |  `POST`  |     Create a new Apache Spark cluster. This method acquires new instances from the cloud provider if necessary. This method is asynchronous; the returned  `cluster\_id`  can be used to poll the cluster state. When this method returns, the cluster is in a  `PENDING`  state. The cluster is usable once it enters a  `RUNNING`  state. See  [ClusterState](#clusterclusterstate)  .        Note       Databricks may not be able to acquire some of the requested nodes, due to cloud provider limitations or transient network issues. If Databricks acquires at least 85% of the requested on-demand nodes, cluster creation will succeed. Otherwise the cluster will terminate with an informative error message.           ###   Examples      ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""my-cluster"",  ""spark\_version"": ""7.3.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""spark\_conf"": {  ""spark.speculation"": true  },  ""aws\_attributes"": {  ""availability"": ""SPOT"",  ""zone\_id"": ""us-west-2a""  },  ""num\_workers"": 25 }  ```       ``` { ""cluster\_id"": ""1234-567890-cited123"" }  ```      Here is an example for an autoscaling cluster. This cluster will start with two nodes, the minimum.        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""autoscaling-cluster"",  ""spark\_version"": ""7.3.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""autoscale"" : {  ""min\_workers"": 2,  ""max\_workers"": 50  } }  ```       ``` { ""cluster\_id"": ""1234-567890-batch123"" }  ```          This example creates a  [Single Node cluster](../../../clusters/single-node.html)  . To create a Single Node cluster:     * Set  `spark\_conf`  and  `custom\_tags`  to the exact values in the example. * Set  `num\_workers`  to  `0`  .      ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""single-node-cluster"",  ""spark\_version"": ""7.6.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""num\_workers"": 0,  ""spark\_conf"": {  ""spark.databricks.cluster.profile"": ""singleNode"",  ""spark.master"": ""[\*, 4]""  },  ""custom\_tags"": {  ""ResourceClass"": ""SingleNode""  } }  ```       ``` { ""cluster\_id"": ""1234-567890-ruins123"" }  ```          This example creates a cluster and mounts an Amazon EFS file system.        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""efs-cluster"",  ""spark\_version"": ""7.6.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""instance\_type"": ""i3.xlarge"",  ""cluster\_mount\_infos"":[  {  ""network\_filesystem\_info"":{  ""server\_address"":""hostname.efs.us-east-1.amazonaws.com"",  ""mount\_options"": ""rsize=1048576,wsize=1048576,hard,timeo=600""  },  ""remote\_mount\_dir\_path"": ""/"",  ""local\_mount\_dir\_path"": ""/mnt/efs-mount""  }  ],  ""aws\_attributes"":{  ""availability"": ""SPOT"",  ""zone\_id"": ""us-east-2""  },  ""num\_workers"": 25 }  ```       ``` { ""cluster\_id"": ""1234-567890-pouch123"" }  ```      To create a job or submit a run with a new cluster using a policy and the policy’s default values, set  `policy\_id`  to the policy ID and  `apply\_policy\_default\_values`  to  `true`  :        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""num\_workers"": null,  ""autoscale"": {  ""min\_workers"": 2,  ""max\_workers"": 8  },  ""cluster\_name"": ""my-cluster"",  ""spark\_version"": ""7.3.x-scala2.12"",  ""spark\_conf"": {},  ""aws\_attributes"": {  ""first\_on\_demand"": 1,  ""availability"": ""SPOT\_WITH\_FALLBACK"",  ""zone\_id"": ""us-west-2a"",  ""instance\_profile\_arn"": null,  ""spot\_bid\_price\_percent"": 100,  ""ebs\_volume\_count"": 0  },  ""node\_type\_id"": ""i3.xlarge"",  ""ssh\_public\_keys"": [],  ""custom\_tags"": {},  ""spark\_env\_vars"": {  ""PYSPARK\_PYTHON"": ""/databricks/python3/bin/python3""  },  ""autotermination\_minutes"": 120,  ""init\_scripts"": [],  ""policy\_id"": ""C65B864F02000008"",  ""apply\_policy\_default\_values"": true }  ```       ``` { ""cluster\_id"": ""1234-567890-buyer123"" }  ```      To create a new cluster, define the cluster’s properties in  `new\_cluster`  :        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/jobs/create \ --data @create-job.json  ```     `create-job.json`  :        ``` {  ""run\_name"": ""my spark task"",  ""new\_cluster"": {  ""spark\_version"": ""7.3.x-scala2.12"",  ""node\_type\_id"": ""r3.xlarge"",  ""aws\_attributes"": {  ""availability"": ""ON\_DEMAND""  },  ""num\_workers"": 10,  ""policy\_id"": ""ABCD000000000000""  },  ""libraries"": [  {  ""jar"": ""dbfs:/my-jar.jar""  },  {  ""maven"": {  ""coordinates"": ""org.jsoup:jsoup:1.7.2""  }  }  ],  ""spark\_jar\_task"": {  ""main\_class\_name"": ""com.databricks.ComputeModels""  } }  ```       ``` { ""job\_id"": 244 }  ```            ###   Request structure of the cluster definition         |   Field Name   |   Type   |   Description   | | --- | --- | --- | |   num\_workers OR autoscale   |  `INT32`  OR  [AutoScale](#clusterautoscale)  |   If num\_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num\_workers executors for a total of num\_workers + 1 Spark nodes.    **Note:**   When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in  `executors`  will gradually increase from 5 to 10 as the new nodes are provisioned.     If autoscale, parameters needed in order to automatically scale clusters up and down based on load.   | |   cluster\_name   |  `STRING`  |   Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.   | |   spark\_version   |  `STRING"	To access the Clusters API, you can use the `2.0/clusters/create` endpoint with the `POST` HTTP method to create a new Apache Spark cluster. The `cluster_id` returned from the `Create` method is required to use other cluster lifecycle methods. To obtain a list of clusters, you can invoke the `List` method.	1
no_reaction	['https://docs.databricks.com/ingestion/auto-loader/unity-catalog.html']	How do I use autoloader to ingest data from 3 separate S3 buckets into one delta tables. Can I use union?	"      Using Auto Loader in Delta Live Tables ========================================     You can use Auto Loader in your  [Delta Live Tables](../../delta-live-tables/index.html)  pipelines. Delta Live Tables extends functionality in Apache Spark Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:     * Autoscaling compute infrastructure for cost savings * Data quality checks with  [expectations](../../delta-live-tables/expectations.html) * Automatic  [schema evolution](schema.html)  handling * Monitoring via metrics in the  [event log](../../delta-live-tables/observability.html#event-log)     You do not need to provide a schema or checkpoint location because Delta Live Tables automatically manages these settings for your pipelines. See  [Load data with Delta Live Tables](../../delta-live-tables/load.html)  .        Auto Loader syntax for DLT ----------------------------     Delta Live Tables provides slightly modified Python syntax for Auto Loader, and adds SQL support for Auto Loader.       The following examples use Auto Loader to create datasets from CSV and JSON files:         ``` @dlt.table def customers():   return (     spark.readStream.format(""cloudFiles"")       .option(""cloudFiles.format"", ""csv"")       .load(""/databricks-datasets/retail-org/customers/"")   )  @dlt.table def sales\_orders\_raw():   return (     spark.readStream.format(""cloudFiles"")       .option(""cloudFiles.format"", ""json"")       .load(""/databricks-datasets/retail-org/sales\_orders/"")   )  ```       ``` CREATE OR REFRESH STREAMING LIVE TABLE customers AS SELECT \* FROM cloud\_files(""/databricks-datasets/retail-org/customers/"", ""csv"")  CREATE OR REFRESH STREAMING LIVE TABLE sales\_orders\_raw AS SELECT \* FROM cloud\_files(""/databricks-datasets/retail-org/sales\_orders/"", ""json"")  ```       You can use supported  [format options](options.html#format-options)  with Auto Loader. Using the  `map()`  function, you can pass any number of options to the  `cloud\_files()`  method. Options are key-value pairs, where the keys and values are strings. The following describes the syntax for working with Auto Loader in SQL:        ``` CREATE OR REFRESH STREAMING LIVE TABLE <table\_name> AS SELECT \*  FROM cloud\_files(  ""<file\_path>"",  ""<file\_format>"",  map(  ""<option\_key>"", ""<option\_value"",  ""<option\_key>"", ""<option\_value"",  ...  )  )  ```      The following example reads data from tab-delimited CSV files with a header:        ``` CREATE OR REFRESH STREAMING LIVE TABLE customers AS SELECT \* FROM cloud\_files(""/databricks-datasets/retail-org/customers/"", ""csv"", map(""delimiter"", ""\t"", ""header"", ""true""))  ```      You can use the  `schema`  to specify the format manually; you must specify the  `schema`  for formats that do not support  [schema inference](schema.html)  :         ``` @dlt.table def wiki\_raw():   return (     spark.readStream.format(""cloudFiles"")       .schema(""title STRING, id INT, revisionId INT, revisionTimestamp TIMESTAMP, revisionUsername STRING, revisionUsernameId INT, text STRING"")       .option(""cloudFiles.format"", ""parquet"")       .load(""/databricks-datasets/wikipedia-datasets/data-001/en\_wikipedia/articles-only-parquet"")   )  ```       ``` CREATE OR REFRESH STREAMING LIVE TABLE wiki\_raw AS SELECT \*  FROM cloud\_files(  ""/databricks-datasets/wikipedia-datasets/data-001/en\_wikipedia/articles-only-parquet"",  ""parquet"",  map(""schema"", ""title STRING, id INT, revisionId INT, revisionTimestamp TIMESTAMP, revisionUsername STRING, revisionUsernameId INT, text STRING"")  )  ```        Note       Delta Live Tables automatically configures and manages the schema and checkpoint directories when using Auto Loader to read files. However, if you manually configure either of these directories, performing a full refresh does not affect the contents of the configured directories. Databricks recommends using the automatically configured directories to avoid unexpected side effects during processing.                                                         "	To use autoloader to ingest data from 3 separate S3 buckets into one delta table, you can create three separate Auto Loader tables for each S3 bucket and then use a union to combine the data from the three tables into one Delta table.	0
no_reaction	['https://docs.databricks.com/security/index.html']	What are security best practices of Databricks?	   Spark Security ================  * [Spark Security: Things You Need To Know](#spark-security-things-you-need-to-know) * [Spark RPC (Communication protocol between Spark processes)](#spark-rpc-communication-protocol-between-spark-processes)   + [Authentication](#authentication)    - [YARN](#yarn)   - [Kubernetes](#kubernetes)  + [Encryption](#encryption) * [Local Storage Encryption](#local-storage-encryption) * [Web UI](#web-ui)   + [Authentication and Authorization](#authentication-and-authorization)  + [Spark History Server ACLs](#spark-history-server-acls)  + [SSL Configuration](#ssl-configuration)  + [Preparing the key stores](#preparing-the-key-stores)    - [YARN mode](#yarn-mode)   - [Standalone mode](#standalone-mode)   - [Mesos mode](#mesos-mode)  + [HTTP Security Headers](#http-security-headers) * [Configuring Ports for Network Security](#configuring-ports-for-network-security)   + [Standalone mode only](#standalone-mode-only)  + [All cluster managers](#all-cluster-managers) * [Kerberos](#kerberos)   + [Long-Running Applications](#long-running-applications)    - [Using a Keytab](#using-a-keytab)   - [Using a ticket cache](#using-a-ticket-cache)  + [Secure Interaction with Kubernetes](#secure-interaction-with-kubernetes) * [Event Logging](#event-logging) * [Persisting driver logs in client mode](#persisting-driver-logs-in-client-mode)   Spark Security: Things You Need To Know =========================================   Security features like authentication are not enabled by default. When deploying a cluster that is open to the internet or an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications from running on the cluster.     Spark supports multiple deployments types and each one supports different levels of security. Not all deployment types will be secure in all environments and none are secure by default. Be sure to evaluate your environment, what Spark supports, and take the appropriate measure to secure your Spark deployment.     There are many different types of security concerns. Spark does not necessarily protect against all things. Listed below are some of the things Spark supports. Also check the deployment documentation for the type of deployment you are using for deployment specific settings. Anything not documented, Spark does not support.     Spark RPC (Communication protocol between Spark processes) ============================================================   Authentication ----------------   Spark currently supports authentication for RPC channels using a shared secret. Authentication can be turned on by setting the  `spark.authenticate`   configuration parameter.     The exact mechanism used to generate and distribute the shared secret is deployment-specific. Unless specified below, the secret must be defined by setting the  `spark.authenticate.secret`   config option. The same secret is shared by all Spark applications and daemons in that case, which limits the security of these deployments, especially on multi-tenant clusters.     The REST Submission Server and the MesosClusterDispatcher do not support authentication. You should ensure that all network access to the REST API & MesosClusterDispatcher (port 6066 and 7077 respectively by default) are restricted to hosts that are trusted to submit jobs.    ###   YARN   For Spark on  [YARN](running-on-yarn.html)   , Spark will automatically handle generating and distributing the shared secret. Each application will use a unique shared secret. In the case of YARN, this feature relies on YARN RPC encryption being enabled for the distribution of secrets to be secure.    ###   Kubernetes   On Kubernetes, Spark will also automatically generate an authentication secret unique to each application. The secret is propagated to executor pods using environment variables. This means that any user that can list pods in the namespace where the Spark application is running can also see their authentication secret. Access control rules should be properly set up by the Kubernetes admin to ensure that Spark authentication is secure.    |   Property Name   |   Default   |   Meaning   |   Since Version   | | --- | --- | --- | --- | | `spark.authenticate`  |   false   |   Whether Spark authenticates its internal connections.   |   1.0.0   | | `spark.authenticate.secret`  |   None   |   The secret key used authentication. See above for when this configuration should be set.   |   1.0.0   |   Alternatively, one can mount authentication secrets using files and Kubernetes secrets that the user mounts into their pods.    |   Property Name   |   Default   |   Meaning   |   Since Version   | | --- | --- | --- | --- | | `spark.authenticate.secret.file`  |   None   |   Path pointing to the secret key to use for securing connections. Ensure that the  contents of the file have been securely generated. This file is loaded on both the driver  and the executors unless other settings override this (see below).   |   3.0.0   | | `spark.authenticate.secret.driver.file`  |   The value of  `spark.authenticate.secret.file`  |   When specified, overrides the location that the Spark driver reads to load the secret.  Useful when in client mode, when the location of the secret file may differ in the pod versus  the node the driver is running in. When this is specified,  `spark.authenticate.secret.executor.file`   must be specified so that the driver  and the executors can both use files to load the secret key. Ensure that the contents of the file  on the driver is identical to the contents of the file on the executors.   |   3.0.0   | | `spark.authenticate.secret.executor.file`  |   The value of  `spark.authenticate.secret.file`  |   When specified, overrides the location that the Spark executors read to load the secret.  Useful in client mode, when the location of the secret file may differ in the pod versus  the node the driver is running in. When this is specified,  `spark.authenticate.secret.driver.file`   must be specified so that the driver  and the executors can both use files to load the secret key. Ensure that the contents of the file  on the driver is identical to the contents of the file on the executors.   |   3.0.0   |   Note that when using files, Spark will not mount these files into the containers for you. It is up you to ensure that the secret files are deployed securely into your containers and that the driver’s secret file agrees with the executors’ secret file.     Encryption ------------   Spark supports AES-based encryption for RPC connections. For encryption to be enabled, RPC authentication must also be enabled and properly configured. AES encryption uses the  [Apache Commons Crypto](https://commons.apache.org/proper/commons-crypto/)   library, and Spark’s configuration system allows access to that library’s configuration for advanced users.     There is also support for SASL-based encryption, although it should be considered deprecated. It is still required when talking to shuffle services from Spark versions older than 2.2.0.     The following table describes the different options available for configuring this feature.    |   Property Name   |   Default   |   Meaning   |   Since Version   | | --- | --- | --- | --- | | `spark.network.crypto.enabled`  |   false   |   Enable AES-based RPC encryption, including the new authentication protocol added in 2.2.0.   |   2.2.0   | | `spark.network.crypto.config.*`  |   None   |   Configuration values for the commons-crypto library, such as which cipher implementations to  use. The config name should be the name of commons-crypto configuration without the  `commons.crypto`   prefix.   |   2.2.0   | | `spark.network.crypto.saslFallback`  |   true   |   Whether to fall back to SASL authentication if authentication fails using Spark's internal  mechanism. This is useful when the application is connecting to old shuffle services that  do not support the internal Spark authentication protocol. On the shuffle service side,  disabling this feature will block older clients from authenticating.   |   2.2.0   | | `spark.authenticate.enableSaslEncryption`  |   false   |   Enable SASL-based encrypted communication.   |   2.2.0   | | `spark.network.sasl.serverAlwaysEncrypt`  |   false   |   Disable unencrypted connections for ports using SASL authentication. This will deny connections  from clients that have authentication enabled, but do not request SASL-based encryption.   |   1.4.0   |   Local Storage Encryption ==========================   Spark supports encrypting temporary data written to local disks. This covers shuffle files, shuffle spills and data blocks stored on disk (for both caching and broadcast variables). It does not cover encrypting output data generated by applications with APIs such as  `saveAsHadoopFile`   or  `saveAsTable`   . It also may not cover temporary files created explicitly by the user.     The following settings cover enabling encryption for data written to disk:    |   Property Name   |   Default   |   Meaning   |   Since Version   | | --- | --- | --- | --- | | `spark.io.encryption.enabled`  |   false   |   Enable local disk I/O encryption. Currently supported by all modes except Mesos. It's strongly  recommended that RPC encryption be enabled when using this feature.   |   2.1.0   | | `spark.io.encryption.keySizeBits`  |   128   |   IO encryption key size in bits. Supported values are 128, 192 and 256.   |   2.1.0   | | `spark.io.encryption.keygen.algorithm`  |   HmacSHA1   |   The algorithm to use when generating the IO encryption key. The supported algorithms are  described in the KeyGenerator section of the Java Cryptography Architecture Standard Algorithm  Name Documentation.   |   2.1.0   | | `spark.io.encryption.commons.config.*`  |   None   |   Configuration values for the commons-crypto library, such as which cipher implementations to  use. The config name should be the name of commons-crypto configuration without the  `commons.crypto`   prefix.   |   2.1.0   |   Web UI ========   Authentication and Authorization ----------------------------------   Enabling authentication for the Web UIs is done using  [javax servlet filters](https://docs.oracle.com/javaee/6/api/javax/servlet/Filter.html)   . You will need a filter that implements the authentication method you want to deploy. Spark does not provide any built-in authentication filters.     Spark also supports access control to the UI when an authentication filter is present. Each application can be configured with its own separate access control lists (ACLs). Spark differentiates between “view” permissions (who is allowed to see the application’s UI), and “modify” permissions (who can do things like kill jobs in a running application).     ACLs can be configured for either users or groups. Configuration entries accept comma-separated lists as input, meaning multiple users or groups can be given the desired privileges. This can be used if you run on a shared cluster and have a set of administrators or developers who need to monitor applications they may not have started themselves. A wildcard (  `*`   ) added to specific ACL means that all users will have the respective privilege. By default, only the user submitting the application is added to the ACLs. 	Databricks provides several security best practices for Spark deployments. Some of the security features include authentication, encryption, local storage encryption, and access control to the Web UI. It is important to secure access to the cluster to prevent unauthorized applications from running on the cluster. Spark supports multiple deployment types, and each one supports different levels of security. Not all deployment types will be secure in all environments, and none are secure by default. It is recommended to evaluate your environment, what Spark supports, and take the appropriate measures to secure your Spark deployment.	0
no_reaction	['https://spark.apache.org/docs/latest/sql-data-sources-text.html']	sql files	  ###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) + [Generic Load/Save Functions](sql-data-sources-load-save-functions.html) + [Generic File Source Options](sql-data-sources-generic-options.html) + [Parquet Files](sql-data-sources-parquet.html) + [ORC Files](sql-data-sources-orc.html) + [JSON Files](sql-data-sources-json.html) + [CSV Files](sql-data-sources-csv.html) + [Text Files](sql-data-sources-text.html) + [Hive Tables](sql-data-sources-hive-tables.html) + [JDBC To Other Databases](sql-data-sources-jdbc.html) + [Avro Files](sql-data-sources-avro.html) + [Protobuf data](sql-data-sources-protobuf.html) + [Whole Binary Files](sql-data-sources-binaryFile.html) + [Troubleshooting](sql-data-sources-troubleshooting.html)  * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) * [Error Conditions](sql-error-conditions.html)   Data Sources ==============   Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data. This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.    * [Generic Load/Save Functions](sql-data-sources-load-save-functions.html)   + [Manually Specifying Options](sql-data-sources-load-save-functions.html#manually-specifying-options)  + [Run SQL on files directly](sql-data-sources-load-save-functions.html#run-sql-on-files-directly)  + [Save Modes](sql-data-sources-load-save-functions.html#save-modes)  + [Saving to Persistent Tables](sql-data-sources-load-save-functions.html#saving-to-persistent-tables)  + [Bucketing, Sorting and Partitioning](sql-data-sources-load-save-functions.html#bucketing-sorting-and-partitioning) * [Generic File Source Options](sql-data-sources-generic-options.html)   + [Ignore Corrupt Files](sql-data-sources-generic-options.html#ignore-corrupt-files)  + [Ignore Missing Files](sql-data-sources-generic-options.html#ignore-missing-files)  + [Path Global Filter](sql-data-sources-generic-options.html#path-global-filter)  + [Recursive File Lookup](sql-data-sources-generic-options.html#recursive-file-lookup) * [Parquet Files](sql-data-sources-parquet.html)   + [Loading Data Programmatically](sql-data-sources-parquet.html#loading-data-programmatically)  + [Partition Discovery](sql-data-sources-parquet.html#partition-discovery)  + [Schema Merging](sql-data-sources-parquet.html#schema-merging)  + [Hive metastore Parquet table conversion](sql-data-sources-parquet.html#hive-metastore-parquet-table-conversion)  + [Configuration](sql-data-sources-parquet.html#configuration) * [ORC Files](sql-data-sources-orc.html) * [JSON Files](sql-data-sources-json.html) * [CSV Files](sql-data-sources-csv.html) * [Text Files](sql-data-sources-text.html) * [Hive Tables](sql-data-sources-hive-tables.html)   + [Specifying storage format for Hive tables](sql-data-sources-hive-tables.html#specifying-storage-format-for-hive-tables)  + [Interacting with Different Versions of Hive Metastore](sql-data-sources-hive-tables.html#interacting-with-different-versions-of-hive-metastore) * [JDBC To Other Databases](sql-data-sources-jdbc.html) * [Avro Files](sql-data-sources-avro.html)   + [Deploying](sql-data-sources-avro.html#deploying)  + [Load and Save Functions](sql-data-sources-avro.html#load-and-save-functions)  + [to\_avro() and from\_avro()](sql-data-sources-avro.html#to_avro-and-from_avro)  + [Data Source Option](sql-data-sources-avro.html#data-source-option)  + [Configuration](sql-data-sources-avro.html#configuration)  + [Compatibility with Databricks spark-avro](sql-data-sources-avro.html#compatibility-with-databricks-spark-avro)  + [Supported types for Avro -> Spark SQL conversion](sql-data-sources-avro.html#supported-types-for-avro---spark-sql-conversion)  + [Supported types for Spark SQL -> Avro conversion](sql-data-sources-avro.html#supported-types-for-spark-sql---avro-conversion) * [Protobuf data](sql-data-sources-protobuf.html)   + [Deploying](sql-data-sources-protobuf.html#deploying)  + [to\_protobuf() and from\_protobuf()](sql-data-sources-protobuf.html#to_protobuf-and-from_protobuf)  + [Supported types for Protobuf -> Spark SQL conversion](sql-data-sources-protobuf.html#supported-types-for-protobuf---spark-sql-conversion)  + [Supported types for Spark SQL -> Protobuf conversion](sql-data-sources-protobuf.html#supported-types-for-spark-sql---protobuf-conversion)  + [Handling circular references protobuf fields](sql-data-sources-protobuf.html#handling-circular-references-protobuf-fields) * [Whole Binary Files](sql-data-sources-binaryFile.html) * [Troubleshooting](sql-data-sources-troubleshooting.html)  	The context is about the Spark SQL Guide, specifically the section on Data Sources. It describes how Spark SQL supports operating on a variety of data sources through the DataFrame interface, and provides information on how to load and save data using the Spark Data Sources. It also goes into specific options that are available for the built-in data sources, such as Parquet Files, ORC Files, JSON Files, CSV Files, Text Files, Hive Tables, JDBC to Other Databases, Avro Files, Protobuf data, and Whole Binary Files. The section also provides troubleshooting tips.	0
no_reaction	['https://docs.databricks.com/dev-tools/cli/index.html']	configure datatbricks cli	    Use the command line ======================     Databricks provides the following command-line tools.          |   Name   |   Use this tool when you want to…   | | --- | --- | |  [Databricks CLI setup & documentation](cli/index.html)  |   Use the command line to work with Databricks workspace assets such as cluster policies, clusters, file systems, groups, pools, jobs, libraries, runs, secrets, and tokens.   | |  [Databricks SQL CLI](databricks-sql-cli.html)  |   Use the command line to run SQL commands and scripts on a Databricks SQL warehouse.   |                                                       	To configure Databricks CLI, you can follow the setup and documentation provided in the Databricks CLI setup & documentation tool. This tool allows you to work with Databricks workspace assets such as cluster policies, clusters, file systems, groups, pools, jobs, libraries, runs, secrets, and tokens through the command line.	1
no_reaction	['https://docs.databricks.com/delta-live-tables/python-ref.html']	can I use live table in apply change into function?	"      Change data capture with Delta Live Tables ============================================          Preview       Delta Live Tables support for SCD type 2 is in  [Public Preview](../release-notes/release-types.html)  .        You can use change data capture (CDC) in Delta Live Tables to update tables based on changes in source data. CDC is supported in the Delta Live Tables SQL and Python interfaces. Delta Live Tables supports updating tables with slowly changing dimensions (SCD) type 1 and type 2:     * Use SCD type 1 to update records directly. History is not retained for records that are updated. * Use SCD type 2 to retain a history of records, either on all updates or on updates to a specified set of columns. See  [Track history for only specified columns with SCD type 2](#type2-track-history)     For syntax and other references, see:     * [Change data capture with Python in Delta Live Tables](python-ref.html#cdc) * [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) * [CDC table properties](properties.html#cdc)      Note       This article describes how to update tables in your Delta Live Tables pipeline based on changes in source data. To learn how to record and query row-level change information for Delta tables, see  [Use Delta Lake change data feed on Databricks](../delta/delta-change-data-feed.html)  .           How is CDC implemented with Delta Live Tables? ------------------------------------------------     You must specify a column in the source data on which to sequence records, which Delta Live Tables interprets as a monotonically increasing representation of the proper ordering of the source data. Delta Live Tables automatically handles data that arrives out of order. For SCD Type 2 changes, Delta Live Tables propagates the appropriate sequencing values to the  `\_\_START\_AT`  and  `\_\_END\_AT`  columns of the target table. There should be at most one distinct update per key at each sequencing value, and NULL sequencing values are unsupported.       To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an  `APPLY     CHANGES     INTO`  statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the  `CREATE     OR     REFRESH     STREAMING     TABLE`  statement in SQL or the  `create\_streaming\_live\_table()`  function in Python. To create the statement defining the CDC processing, use the  `APPLY     CHANGES`  statement in SQL or the  `apply\_changes()`  function in Python. For syntax details, see  [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc)  or  [Change data capture with Python in Delta Live Tables](python-ref.html#cdc)  .             What data objects are used for Delta Live Tables CDC processing? ------------------------------------------------------------------     When you declare the target table, two data structures are created in the Hive metastore:     * A view using the name assigned to the target table. * An internal backing table used by Delta Live Tables table to manage CDC processing. This table is named by prepending  `\_\_apply\_changes\_storage\_`  to the target table name.     For example, if you declare a target table named  `dlt\_cdc\_target`  , you will see a view named  `dlt\_cdc\_target`  and a table named  `\_\_apply\_changes\_storage\_dlt\_cdc\_target`  in the metastore. Creating a view allows Delta Live Tables to filter out the extra information (for example, tombstones and versions) that is required to handle out-of-order data. To view the processed data, query the target view. You can also query the raw data in the  `\_\_apply\_changes\_storage\_`  table to see deleted records and extra version columns. If you add data manually to the table, the records are assumed to come before other changes because the version columns are missing.         Limitations -------------   * Metrics for the target table, such as number of output rows, are not available. * SCD type 2 updates will add a history row for every input row, even if no columns have changed. * The target of the  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function cannot be used as a source for a streaming table. A table that reads from the target of an  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function must be a live table. * Expectations are not supported in an  `APPLY     CHANGES     INTO`  query or  `apply\_changes()`  function. To use expectations for the source or target dataset:      + Add expectations on source data by defining an intermediate table with the required expectations and use this dataset as the source for the target table.  + Add expectations on target data with a downstream table that reads input data from the target table.         SCD type 1 and SCD type 2 on Databricks -----------------------------------------     The following sections provide examples that demonstrate Delta Live Tables SCD type 1 and type 2 queries that update target tables based on source events that:     1. Create new user records. 2. Delete a user record. 3. Update user records. In the SCD type 1 example, the last  `UPDATE`  operations arrive late and are dropped from the target table, demonstrating the handling of out of order events.     All of the following examples assume familiarity with configuring and updating Delta Live Tables pipelines. See  [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html)  .       In order to run these examples, you must begin by creating a sample dataset. See  [Generate test data](#generate-data)  .       The following are the input records for these examples:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   124   |   Raul   |   Oaxaca   |   INSERT   |   1   | |   123   |   Isabel   |   Monterrey   |   INSERT   |   1   | |   125   |   Mercedes   |   Tijuana   |   INSERT   |   2   | |   126   |   Lily   |   Cancun   |   INSERT   |   2   | |   123   |   null   |   null   |   DELETE   |   6   | |   125   |   Mercedes   |   Guadalajara   |   UPDATE   |   6   | |   125   |   Mercedes   |   Mexicali   |   UPDATE   |   5   | |   123   |   Isabel   |   Chihuahua   |   UPDATE   |   5   |     If you uncomment the final row in the example data, it will insert the following record that specifies where records should be truncated:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   null   |   null   |   null   |   TRUNCATE   |   3   |      Note       All the following examples include options to specify both  `DELETE`  and  `TRUNCATE`  operations, but each of these are optional.            Process SCD type 1 updates ----------------------------     The following code example demonstrates processing SCD type 1 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format(""delta"").table(""cdc\_data.users"")  dlt.create\_streaming\_live\_table(""target"")  dlt.apply\_changes(   target = ""target"",   source = ""users"",   keys = [""userId""],   sequence\_by = col(""sequenceNum""),   apply\_as\_deletes = expr(""operation = 'DELETE'""),   apply\_as\_truncates = expr(""operation = 'TRUNCATE'""),   except\_column\_list = [""operation"", ""sequenceNum""],   stored\_as\_scd\_type = 1 )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = ""DELETE"" APPLY AS TRUNCATE WHEN  operation = ""TRUNCATE"" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 1;  ```       After running the SCD type 1 example, the target table contains the following records:           |   userId   |   name   |   city   | | --- | --- | --- | |   124   |   Raul   |   Oaxaca   | |   125   |   Mercedes   |   Guadalajara   | |   126   |   Lily   |   Cancun   |     After running the SCD type 1 example with the additional  `TRUNCATE`  record, records  `124`  and  `126`  are truncated because of the  `TRUNCATE`  operation at  `sequenceNum=3`  , and the target table contains the following record:           |   userId   |   name   |   city   | | --- | --- | --- | |   125   |   Mercedes   |   Guadalajara   |         Process SCD type 2 updates ----------------------------     The following code example demonstrates processing SCD type 2 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format(""delta"").table(""cdc\_data.users"")  dlt.create\_streaming\_live\_table(""target"")  dlt.apply\_changes(   target = ""target"",   source = ""users"",   keys = [""userId""],   sequence\_by = col(""sequenceNum""),   apply\_as\_deletes = expr(""operation = 'DELETE'""),   except\_column\_list = [""operation"", ""sequenceNum""],   stored\_as\_scd\_type = ""2"" )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = ""DELETE"" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 2;  ```       After running the SCD type 2 example, the target table contains the following records:             |   userId   |   name   |   city   |   \_\_START\_AT   |   \_\_END\_AT   | | --- | --- | --- | --- | --- | |   123   |   Isabel   |   Monterrey   |   1   |   5   | |   123   |   Isabel   |  "	Yes, you can use live table in apply change into function. Delta Live Tables support for SCD type 2 is in Public Preview. You can use change data capture (CDC) in Delta Live Tables to update tables based on changes in source data. CDC is supported in the Delta Live Tables SQL and Python interfaces. Delta Live Tables supports updating tables with slowly changing dimensions (SCD) type 1 and type 2. To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an `APPLY CHANGES INTO` statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the `CREATE OR REFRESH STREAMING TABLE` statement in SQL or the `create_streaming_live_table()` function in Python. To create the statement defining the CDC processing, use the `APPLY CHANGES` statement in SQL or the `apply_changes()` function in Python.	1
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Normalizer.html']	"Explain this error: TypeError: 'Column' object is not callable --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <command-4134732480957838> in <module> 1 AccessJson = spark.read.format(""json"").load(""file:/Workspace/Repos/Ivan.Santos@ADP.com/shared-workspace-security-config-dbs/942420684378/hro-data-governance/prod.json"",multiLine=True) ----> 2 pd.json_normalize(AccessJson) /databricks/python/lib/python3.8/site-packages/pandas/io/json/_normalize.py in _json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level) 268 269 if record_path is None: --> 270 if any([isinstance(x, dict) for x in y.values()] for y in data): 271 # naive normalization, this is idempotent for flat records 272 # and potentially will inflate the data considerably for /databricks/python/lib/python3.8/site-packages/pandas/io/json/_normalize.py in <genexpr>(.0) 268 269 if record_path is None: --> 270 if any([isinstance(x, dict) for x in y.values()] for y in data): 271 # naive normalization, this is idempotent for flat records 272 # and potentially will inflate the data considerably for TypeError: 'Column' object is not callable"	"   Normalizer  [¶](#normalizer ""Permalink to this headline"") ===========================================================  *class*  `pyspark.ml.feature.`  `Normalizer`    (   *\**  ,  *p     :     float     =     2.0*  ,  *inputCol     :     Optional    [    str    ]     =     None*  ,  *outputCol     :     Optional    [    str    ]     =     None*   )   [[source]](../../_modules/pyspark/ml/feature.html#Normalizer) [¶](#pyspark.ml.feature.Normalizer ""Permalink to this definition"")   >  >  >  >  Normalize a vector to have unit norm using the given p-norm. >   >  >  >  >    New in version 1.4.0.     Examples    ``` >>> from pyspark.ml.linalg import Vectors >>> svec = Vectors.sparse(4, {1: 4.0, 3: 3.0}) >>> df = spark.createDataFrame([(Vectors.dense([3.0, -4.0]), svec)], [""dense"", ""sparse""]) >>> normalizer = Normalizer(p=2.0) >>> normalizer.setInputCol(""dense"") Normalizer... >>> normalizer.setOutputCol(""features"") Normalizer... >>> normalizer.transform(df).head().features DenseVector([0.6, -0.8]) >>> normalizer.setParams(inputCol=""sparse"", outputCol=""freqs"").transform(df).head().freqs SparseVector(4, {1: 0.8, 3: 0.6}) >>> params = {normalizer.p: 1.0, normalizer.inputCol: ""dense"", normalizer.outputCol: ""vector""} >>> normalizer.transform(df, params).head().vector DenseVector([0.4286, -0.5714]) >>> normalizerPath = temp\_path + ""/normalizer"" >>> normalizer.save(normalizerPath) >>> loadedNormalizer = Normalizer.load(normalizerPath) >>> loadedNormalizer.getP() == normalizer.getP() True >>> loadedNormalizer.transform(df).take(1) == normalizer.transform(df).take(1) True  ```   Methods    |  [`clear`](#pyspark.ml.feature.Normalizer.clear ""pyspark.ml.feature.Normalizer.clear"")  (param)   |   Clears a param from the param map if it has been explicitly set.   | |  [`copy`](#pyspark.ml.feature.Normalizer.copy ""pyspark.ml.feature.Normalizer.copy"")  ([extra])   |   Creates a copy of this instance with the same uid and some extra params.   | |  [`explainParam`](#pyspark.ml.feature.Normalizer.explainParam ""pyspark.ml.feature.Normalizer.explainParam"")  (param)   |   Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string.   | |  [`explainParams`](#pyspark.ml.feature.Normalizer.explainParams ""pyspark.ml.feature.Normalizer.explainParams"")  ()   |   Returns the documentation of all params with their optionally default values and user-supplied values.   | |  [`extractParamMap`](#pyspark.ml.feature.Normalizer.extractParamMap ""pyspark.ml.feature.Normalizer.extractParamMap"")  ([extra])   |   Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra.   | |  [`getInputCol`](#pyspark.ml.feature.Normalizer.getInputCol ""pyspark.ml.feature.Normalizer.getInputCol"")  ()   |   Gets the value of inputCol or its default value.   | |  [`getOrDefault`](#pyspark.ml.feature.Normalizer.getOrDefault ""pyspark.ml.feature.Normalizer.getOrDefault"")  (param)   |   Gets the value of a param in the user-supplied param map or its default value.   | |  [`getOutputCol`](#pyspark.ml.feature.Normalizer.getOutputCol ""pyspark.ml.feature.Normalizer.getOutputCol"")  ()   |   Gets the value of outputCol or its default value.   | |  [`getP`](#pyspark.ml.feature.Normalizer.getP ""pyspark.ml.feature.Normalizer.getP"")  ()   |   Gets the value of p or its default value.   | |  [`getParam`](#pyspark.ml.feature.Normalizer.getParam ""pyspark.ml.feature.Normalizer.getParam"")  (paramName)   |   Gets a param by its name.   | |  [`hasDefault`](#pyspark.ml.feature.Normalizer.hasDefault ""pyspark.ml.feature.Normalizer.hasDefault"")  (param)   |   Checks whether a param has a default value.   | |  [`hasParam`](#pyspark.ml.feature.Normalizer.hasParam ""pyspark.ml.feature.Normalizer.hasParam"")  (paramName)   |   Tests whether this instance contains a param with a given (string) name.   | |  [`isDefined`](#pyspark.ml.feature.Normalizer.isDefined ""pyspark.ml.feature.Normalizer.isDefined"")  (param)   |   Checks whether a param is explicitly set by user or has a default value.   | |  [`isSet`](#pyspark.ml.feature.Normalizer.isSet ""pyspark.ml.feature.Normalizer.isSet"")  (param)   |   Checks whether a param is explicitly set by user.   | |  [`load`](#pyspark.ml.feature.Normalizer.load ""pyspark.ml.feature.Normalizer.load"")  (path)   |   Reads an ML instance from the input path, a shortcut of    read().load(path)    .   | |  [`read`](#pyspark.ml.feature.Normalizer.read ""pyspark.ml.feature.Normalizer.read"")  ()   |   Returns an MLReader instance for this class.   | |  [`save`](#pyspark.ml.feature.Normalizer.save ""pyspark.ml.feature.Normalizer.save"")  (path)   |   Save this ML instance to the given path, a shortcut of ‘write().save(path)’.   | |  [`set`](#pyspark.ml.feature.Normalizer.set ""pyspark.ml.feature.Normalizer.set"")  (param, value)   |   Sets a parameter in the embedded param map.   | |  [`setInputCol`](#pyspark.ml.feature.Normalizer.setInputCol ""pyspark.ml.feature.Normalizer.setInputCol"")  (value)   |   Sets the value of  [`inputCol`](#pyspark.ml.feature.Normalizer.inputCol ""pyspark.ml.feature.Normalizer.inputCol"")  .   | |  [`setOutputCol`](#pyspark.ml.feature.Normalizer.setOutputCol ""pyspark.ml.feature.Normalizer.setOutputCol"")  (value)   |   Sets the value of  [`outputCol`](#pyspark.ml.feature.Normalizer.outputCol ""pyspark.ml.feature.Normalizer.outputCol"")  .   | |  [`setP`](#pyspark.ml.feature.Normalizer.setP ""pyspark.ml.feature.Normalizer.setP"")  (value)   |   Sets the value of  [`p`](#pyspark.ml.feature.Normalizer.p ""pyspark.ml.feature.Normalizer.p"")  .   | |  [`setParams`](#pyspark.ml.feature.Normalizer.setParams ""pyspark.ml.feature.Normalizer.setParams"")  (self, \\*[, p, inputCol, outputCol])   |   Sets params for this Normalizer.   | |  [`transform`](#pyspark.ml.feature.Normalizer.transform ""pyspark.ml.feature.Normalizer.transform"")  (dataset[, params])   |   Transforms the input dataset with optional parameters.   | |  [`write`](#pyspark.ml.feature.Normalizer.write ""pyspark.ml.feature.Normalizer.write"")  ()   |   Returns an MLWriter instance for this ML instance.   |   Attributes    |  [`inputCol`](#pyspark.ml.feature.Normalizer.inputCol ""pyspark.ml.feature.Normalizer.inputCol"")  |   | |  [`outputCol`](#pyspark.ml.feature.Normalizer.outputCol ""pyspark.ml.feature.Normalizer.outputCol"")  |   | |  [`p`](#pyspark.ml.feature.Normalizer.p ""pyspark.ml.feature.Normalizer.p"")  |   | |  [`params`](#pyspark.ml.feature.Normalizer.params ""pyspark.ml.feature.Normalizer.params"")  |   Returns all params ordered by name.   |   Methods Documentation    `clear`    (   *param     :    [pyspark.ml.param.Param](pyspark.ml.param.Param.html#pyspark.ml.param.Param ""pyspark.ml.param.Param"")*   )    → None  [¶](#pyspark.ml.feature.Normalizer.clear ""Permalink to this definition"")    Clears a param from the param map if it has been explicitly set.    `copy`    (   *extra     :     Optional    [    ParamMap    ]     =     None*   )    → JP  [¶](#pyspark.ml.feature.Normalizer.copy ""Permalink to this definition"")    Creates a copy of this instance with the same uid and some extra params. This implementation first calls Params.copy and then make a copy of the companion Java pipeline component with extra params. So both the Python wrapper and the Java pipeline component get copied.     Parameters    **extra**    dict, optional     Extra parameters to copy to the new instance     Returns    `JavaParams`   Copy of this instance    `explainParam`    (   *param     :     Union    [    str    ,   [pyspark.ml.param.Param](pyspark.ml.param.Param.html#pyspark.ml.param.Param ""pyspark.ml.param.Param"")    ]*   )    → str  [¶](#pyspark.ml.feature.Normalizer.explainParam ""Permalink to this definition"")    Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string.    `explainParams`    (     )    → str  [¶](#pyspark.ml.feature."	"The error message ""TypeError: 'Column' object is not callable"" is not related to the `Normalizer` class. It is likely caused by the line of code `pd.json_normalize(AccessJson)` which is trying to call a `Column` object as a function. This error can occur when a `Column` object is mistakenly used as a function call, for example, if parentheses are used instead of square brackets to access a column in a DataFrame."	0
no_reaction	['https://docs.databricks.com/administration-guide/account-settings/no-isolation-shared.html']	Give me a cluster policy for no isolation shared clusters	    Enable admin protection for “No isolation shared” clusters on your account ============================================================================         Account admins can prevent internal credentials from being automatically generated for Databricks workspace admins on No Isolation Shared clusters. No Isolation Shared clusters are clusters that have the  **Access mode**   dropdown set to  **No isolation shared**   .        Important       The clusters UI recently changed. The No Isolation Shared access mode setting for a cluster previously appeared as the Standard cluster mode. If you used the High Concurrency cluster mode  *without additional security settings such as  [table access control (Table ACLs)](../../data-governance/table-acls/table-acl.html)  or credential passthrough*   , the same settings are used as with Standard cluster mode. The account-level admin setting that this article discusses applies to both the No Isolation Shared access mode and its equivalent legacy cluster modes. For a comparison of the old UI and new UI cluster types, see  [Clusters UI changes and cluster access modes](../../archive/compute/cluster-ui-preview.html)  .        The admin protection for No Isolation Shared clusters on your account helps protect admin accounts from sharing internal credentials in an environment that is shared with other users. Enabling this setting may impact workloads that are run by admins. See  [Limitations](#limitations)  .       No Isolation Shared clusters run arbitrary code from multiple users in the same shared environment, similar to what happens on a cloud Virtual Machine that is shared across multiple users. Data or internal credentials provisioned to that environment might be accessible to any code running within that environment. To call Databricks APIs for normal operations, access tokens are provisioned on behalf of users to these clusters. When a higher-privileged user, such as a workspace administrator, runs commands on a cluster, their higher-privileged token is visible in the same environment.       You can determine which clusters in a workspace have cluster types that are affected by this setting. See  [Find all your No Isolation Shared clusters (including equivalent legacy cluster modes)](#notebook)  .       In addition to this account-level setting, there is a  [workspace-level setting called Enforce User Isolation](../workspace/settings/enforce-user-isolation.html)  . Account admins can enable it to prevent creating or starting a “No isolation shared” cluster access type or its  [equivalent legacy cluster types](../../archive/compute/cluster-ui-preview.html)  .       The account console includes this setting only for accounts on the  [E2 version of the platform](../../getting-started/overview.html#e2-architecture)  . For other account types, you can contact your Databricks representative to enable this feature. If you are unsure which account type you have, contact your Databricks representative.        Enable the account-level admin protection setting ---------------------------------------------------   1. As an account admin, log in to the  [Account Console](https://accounts.cloud.databricks.com/login)   . 2. Click  **Settings**  ![Settings icon](../../_images/settings-icon.png)  . 3. Click the  **Feature enablement**   tab. 4. Under  **Enable Admin Protection for “No Isolation Shared” Clusters**   , click the setting to enable or disable this feature.      * If the feature is enabled, Databricks prevents automatic generation of Databricks API internal credentials for Databricks workspace admins on No Isolation Shared clusters.  * Changes may take up to two minutes to take effect on all workspaces.         Limitations -------------     When used with No Isolation Shared clusters or the equivalent legacy cluster modes, the following Databricks features do not work if you enable admin protection for No Isolation Shared clusters on your account:     * [Machine Learning Runtime](../../runtime/mlruntime.html)  workloads. * [Files in Repos](../../files/workspace.html)  . * [dbutils Secrets utility](../../dev-tools/databricks-utils.html#dbutils-secrets)  . * [dbutils Notebook utility](../../dev-tools/databricks-utils.html#dbutils-workflow)  . * [Delta Lake](../../delta/index.html)  operations by admins that create, modify, or update data.     Other features might not work for admin users on this cluster type because these features rely on automatically generated internal credentials.       In those cases, Databricks recommends that admins do one of the following:     * Use a different cluster type than “No isolation shared” or its  [equivalent legacy cluster types](../../archive/compute/cluster-ui-preview.html)  . * Create a non-admin user when using No Isolation Shared clusters.     If you have questions about this setting, contact your Databricks representative.           Find all your No Isolation Shared clusters (including equivalent legacy cluster modes) ----------------------------------------------------------------------------------------     You can determine which clusters in a workspace are affected by this account-level setting.       Import the following notebook into all your workspaces and run the notebook.        ###   Get a list of all No Isolation Shared clusters     [Open notebook in new tab](/_extras/notebooks/source/no-isolation-shared.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 	"I don't see any specific cluster policy mentioned in the context for ""No isolation shared"" clusters. However, the context provides instructions on how to enable admin protection for ""No isolation shared"" clusters on your account, which helps protect admin accounts from sharing internal credentials in an environment that is shared with other users."	1
no_reaction	['https://docs.databricks.com/mlflow/tracking-ex-pyspark.html']	What if I want to put this into a SparkML model?	"   Source code for pyspark.ml.recommendation ===========================================  ```  # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the ""License""); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. #  import sys from typing import Any, Dict, Optional, TYPE\_CHECKING  from pyspark import since, keyword\_only from pyspark.ml.param.shared import (     HasPredictionCol,     HasBlockSize,     HasMaxIter,     HasRegParam,     HasCheckpointInterval,     HasSeed, ) from pyspark.ml.wrapper import JavaEstimator, JavaModel from pyspark.ml.common import inherit\_doc from pyspark.ml.param import Params, TypeConverters, Param from pyspark.ml.util import JavaMLWritable, JavaMLReadable from pyspark.sql import DataFrame  if TYPE\_CHECKING:     from py4j.java\_gateway import JavaObject  \_\_all\_\_ = [""ALS"", ""ALSModel""]  @inherit\_doc class \_ALSModelParams(HasPredictionCol, HasBlockSize):  """"""  Params for :py:class:`ALS` and :py:class:`ALSModel`.   .. versionadded:: 3.0.0  """"""      userCol: Param[str] = Param(         Params.\_dummy(),         ""userCol"",         ""column name for user ids. Ids must be within "" + ""the integer value range."",         typeConverter=TypeConverters.toString,     )     itemCol: Param[str] = Param(         Params.\_dummy(),         ""itemCol"",         ""column name for item ids. Ids must be within "" + ""the integer value range."",         typeConverter=TypeConverters.toString,     )     coldStartStrategy: Param[str] = Param(         Params.\_dummy(),         ""coldStartStrategy"",         ""strategy for dealing with ""         + ""unknown or new users/items at prediction time. This may be useful ""         + ""in cross-validation or production scenarios, for handling ""         + ""user/item ids the model has not seen in the training data. ""         + ""Supported values: 'nan', 'drop'."",         typeConverter=TypeConverters.toString,     )      def \_\_init\_\_(self, \*args: Any):         super(\_ALSModelParams, self).\_\_init\_\_(\*args)         self.\_setDefault(blockSize=4096)      @since(""1.4.0"")     def getUserCol(self) -> str:  """"""  Gets the value of userCol or its default value.  """"""         return self.getOrDefault(self.userCol)      @since(""1.4.0"")     def getItemCol(self) -> str:  """"""  Gets the value of itemCol or its default value.  """"""         return self.getOrDefault(self.itemCol)      @since(""2.2.0"")     def getColdStartStrategy(self) -> str:  """"""  Gets the value of coldStartStrategy or its default value.  """"""         return self.getOrDefault(self.coldStartStrategy)  @inherit\_doc class \_ALSParams(\_ALSModelParams, HasMaxIter, HasRegParam, HasCheckpointInterval, HasSeed):  """"""  Params for :py:class:`ALS`.   .. versionadded:: 3.0.0  """"""      rank: Param[int] = Param(         Params.\_dummy(), ""rank"", ""rank of the factorization"", typeConverter=TypeConverters.toInt     )     numUserBlocks: Param[int] = Param(         Params.\_dummy(),         ""numUserBlocks"",         ""number of user blocks"",         typeConverter=TypeConverters.toInt,     )     numItemBlocks: Param[int] = Param(         Params.\_dummy(),         ""numItemBlocks"",         ""number of item blocks"",         typeConverter=TypeConverters.toInt,     )     implicitPrefs: Param[bool] = Param(         Params.\_dummy(),         ""implicitPrefs"",         ""whether to use implicit preference"",         typeConverter=TypeConverters.toBoolean,     )     alpha: Param[float] = Param(         Params.\_dummy(),         ""alpha"",         ""alpha for implicit preference"",         typeConverter=TypeConverters.toFloat,     )      ratingCol: Param[str] = Param(         Params.\_dummy(),         ""ratingCol"",         ""column name for ratings"",         typeConverter=TypeConverters.toString,     )     nonnegative: Param[bool] = Param(         Params.\_dummy(),         ""nonnegative"",         ""whether to use nonnegative constraint for least squares"",         typeConverter=TypeConverters.toBoolean,     )     intermediateStorageLevel: Param[str] = Param(         Params.\_dummy(),         ""intermediateStorageLevel"",         ""StorageLevel for intermediate datasets. Cannot be 'NONE'."",         typeConverter=TypeConverters.toString,     )     finalStorageLevel: Param[str] = Param(         Params.\_dummy(),         ""finalStorageLevel"",         ""StorageLevel for ALS model factors."",         typeConverter=TypeConverters.toString,     )      def \_\_init\_\_(self, \*args: Any):         super(\_ALSParams, self).\_\_init\_\_(\*args)         self.\_setDefault(             rank=10,             maxIter=10,             regParam=0.1,             numUserBlocks=10,             numItemBlocks=10,             implicitPrefs=False,             alpha=1.0,             userCol=""user"",             itemCol=""item"",             ratingCol=""rating"",             nonnegative=False,             checkpointInterval=10,             intermediateStorageLevel=""MEMORY\_AND\_DISK"",             finalStorageLevel=""MEMORY\_AND\_DISK"",             coldStartStrategy=""nan"",         )      @since(""1.4.0"")     def getRank(self) -> int:  """"""  Gets the value of rank or its default value.  """"""         return self.getOrDefault(self.rank)      @since(""1.4.0"")     def getNumUserBlocks(self) -> int:  """"""  Gets the value of numUserBlocks or its default value.  """"""         return self.getOrDefault(self.numUserBlocks)      @since(""1.4.0"")     def getNumItemBlocks(self) -> int:  """"""  Gets the value of numItemBlocks or its default value.  """"""         return self.getOrDefault(self.numItemBlocks)      @since(""1.4.0"")     def getImplicitPrefs(self) -> bool:  """"""  Gets the value of implicitPrefs or its default value.  """"""         return self.getOrDefault(self.implicitPrefs)      @since(""1.4.0"")     def getAlpha(self) -> float:  """"""  Gets the value of alpha or its default value.  """"""         return self.getOrDefault(self.alpha)      @since(""1.4.0"")     def getRatingCol(self) -> str:  """"""  Gets the value of ratingCol or its default value.  """"""         return self.getOrDefault(self.ratingCol)      @since(""1.4.0"")     def getNonnegative(self) -> bool:  """"""  Gets the value of nonnegative or its default value.  """"""         return self.getOrDefault(self.nonnegative)      @since(""2.0.0"")     def getIntermediateStorageLevel(self) -> str:  """"""  Gets the value of intermediateStorageLevel or its default value.  """"""         return self.getOrDefault(self.intermediateStorageLevel)      @since(""2.0.0"")     def getFinalStorageLevel(self) -> str:  """"""  Gets the value of finalStorageLevel or its default value.  """"""         return self.getOrDefault(self.finalStorageLevel)  [[docs]](../../../reference/api/pyspark.ml.recommendation.ALS.html#pyspark.ml.recommendation.ALS)@inherit\_doc class ALS(JavaEstimator[""ALSModel""], \_ALSParams, JavaMLWritable, JavaMLReadable[""ALS""]):  """"""  Alternating Least Squares (ALS) matrix factorization.   ALS attempts to estimate the ratings matrix `R` as the product of  two lower-rank mat"	Based on the context, it seems like you want to use the ALS algorithm in SparkML to create a recommendation model. You can use the `ALS` class in `pyspark.ml.recommendation` to create an instance of the ALS algorithm and then fit it to your data to create a model.	0
no_reaction	['https://docs.databricks.com/sql/language-manual/functions/current_metastore.html']	how do i get the metastore id in databricks sql	   `current\_metastore`  function ===============================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime 11.1 and above       Returns the current metastore id.        Syntax --------      ``` current\_metastore()  ```        Arguments -----------     This function takes no arguments.         Returns ---------     A STRING.       The metastore id is of the form:  `<cloud>:<region>:<uuid>`        Examples ----------      ``` > SELECT current\_metastore();  azure:westus:f64dcb39-5046-4d4c-9588-c6ac84f9b38a  ```        Related functions -------------------   * [CREATE RECIPIENT](../sql-ref-syntax-ddl-create-recipient.html)                                                      	To get the metastore id in Databricks SQL, you can use the `current_metastore()` function, which returns the current metastore id in the form of `<cloud>:<region>:<uuid>`.	1
no_reaction	['https://docs.databricks.com/dev-tools/ide-how-to.html']	Show me sample code for connecting dbx in python	    Use dbx with Visual Studio Code =================================      Tip       The  [Databricks extension for Visual Studio Code](vscode-ext.html)  provides an alternative to using  `dbx`  with Visual Studio Code. However, the Databricks extension for Visual Studio Code is in Public Preview, and it does not yet provide some  `dbx`  features such as defining multiple deployment environments and multiple deployment workflows, as well as providing CI/CD project templates.        This article describes a Python-based  [code sample](https://github.com/databricks/ide-best-practices)   that you can work with in any Python-compatible IDE. Specifically, this article describes how to work with this code sample in  [Visual Studio Code](https://code.visualstudio.com/)   , which provides the following developer productivity features:     * [Code completion](https://code.visualstudio.com/docs/languages/python#_autocomplete-and-intellisense) * [Linting](https://code.visualstudio.com/docs/languages/python#_linting) * [Testing](https://code.visualstudio.com/docs/languages/python#_testing) * [Debugging](https://code.visualstudio.com/docs/editor/debugging)   code objects that do not require a real-time connection to remote Databricks resources.     This article uses  [dbx by Databricks Labs](dbx.html)  along with Visual Studio Code to submit the code sample to a remote Databricks  [workspace](../workspace/index.html)  .  `dbx`  instructs Databricks to  [What is Databricks Workflows?](../workflows/index.html)  to run the submitted code on a Databricks  [jobs cluster](../clusters/index.html)  in that workspace.       You can use popular third-party Git providers for version control and continuous integration and continuous delivery or continuous deployment (CI/CD) of your code. For version control, these Git providers include the following:     * [GitHub](http://github.com) * [Bitbucket](https://bitbucket.org) * [GitLab](https://about.gitlab.com/) * [Azure DevOps](https://azure.microsoft.com/services/devops)   (not available in Azure China regions) * [AWS CodeCommit](https://aws.amazon.com/codecommit) * [GitHub AE](https://docs.github.com/en/github-ae@latest/admin/overview/about-github-ae)     For CI/CD,  `dbx`  supports the following CI/CD platforms:     * [GitHub Actions](https://github.com/features/actions) * [Azure Pipelines](https://azure.microsoft.com/services/devops/pipelines/) * [GitLab CI/CD](https://docs.gitlab.com/ee/ci)     To demonstrate how version control and CI/CD can work, this article describes how to use Visual Studio Code,  `dbx`  , and this code sample, along with GitHub and GitHub Actions.        Code sample requirements --------------------------     To use this code sample, you must have the following:     * A Databricks workspace in your Databricks account.  [Create a workspace](https://docs.databricks.com/administration-guide/account-settings-e2/workspaces.html)   if you do not already have one. * A GitHub account.  [Create a GitHub account](https://docs.github.com/en/get-started/signing-up-for-github/signing-up-for-a-new-github-account)   , if you do not already have one.     Additionally, on your local development machine, you must have the following:     * [Python](https://www.python.org/downloads/)   version 3.8 or above.       You should use a version of Python that matches the one that is installed on your target clusters. To get the version of Python that is installed on an existing cluster, you can use the cluster’s  [web terminal](../clusters/web-terminal.html)  to run the  `python     --version`  command. See also the “System environment” section in the  [Databricks runtime releases](../release-notes/runtime/releases.html)  for the Databricks Runtime version for your target clusters. In any case, the version of Python must be 3.8 or above.       To get the version of Python that is currently referenced on your local machine, run  `python     --version`  from your local terminal. (Depending on how you set up Python on your local machine, you may need to run  `python3`  instead of  `python`  throughout this article.) See also  [Select a Python interpreter](https://code.visualstudio.com/docs/python/python-tutorial#_select-a-python-interpreter)   . * [pip](https://pip.pypa.io/en/stable/installation/)   .  `pip`  is automatically installed with newer versions of Python. To check whether  `pip`  is already installed, run  `pip     --version`  from your local terminal. (Depending on how you set up Python or  `pip`  on your local machine, you may need to run  `pip3`  instead of  `pip`  throughout this article.) * [dbx](https://pypi.org/project/dbx/)   version 0.8.0 or above. You can install the  `dbx`  package from the Python Package Index (PyPI) by running  `pip     install     dbx`  .        Note       You do not need to install  `dbx`  now. You can install it later in the code sample  [setup](#set-up-the-code-sample)  section. * A method to create Python  [virtual environments](https://docs.python.org/3/library/venv.html)   to ensure you are using the correct versions of Python and package dependencies in your  `dbx`  projects. This article covers  [pipenv](https://pipenv.pypa.io/)   . * The  [Databricks CLI setup & documentation](cli/index.html)  , set up with  [authentication](cli/index.html#set-up-authentication)  .        Note       You do not need to install the Databricks CLI now. You can install it later in the code sample  [setup](#set-up-the-code-sample)  section. If you want to install it later, you must remember to set up authentication at that time instead. * [Visual Studio Code](https://code.visualstudio.com/)   . * The  [Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python)   extension for Visual Studio Code. * The  [GitHub Pull Requests and Issues](https://marketplace.visualstudio.com/items?itemName=GitHub.vscode-pull-request-github)   extension for Visual Studio Code. * [Git](https://git-scm.com/downloads)   .       About the code sample -----------------------     The Python code sample for this article, available in the  [databricks/ide-best-practices](https://github.com/databricks/ide-best-practices)   repo in GitHub, does the following:     1. Gets data from the  [owid/covid-19-data](https://github.com/owid/covid-19-data)   repo in GitHub. 2. Filters the data for a specific ISO country code. 3. Creates a pivot table from the data. 4. Performs data cleansing on the data. 5. Modularizes the code logic into reusable functions. 6. Unit tests the functions. 7. Provides  `dbx`  project configurations and settings to enable the code to write the data to a Delta table in a remote Databricks workspace.       Set up the code sample ------------------------     After you have the  [requirements](#code-sample-requirements)  in place for this code sample, complete the following steps to begin using the code sample.        Note       These steps do not include setting up this code sample for CI/CD. You do not need to set up CI/CD to run this code sample. If you want to set up CI/CD later, see  [Run with GitHub Actions](#run-with-github-actions)  .       ###   Step 1: Create a Python virtual environment   1. From your terminal, create a blank folder to contain a virtual environment for this code sample. These instructions use a parent folder named  `ide-demo`  . You can give this folder any name you want. If you use a different name, replace the name throughout this article. After you create the folder, switch to it, and then start Visual Studio Code from that folder. Be sure to include the dot (  `.`  ) after the  `code`  command.       For Linux and macOS:        ``` mkdir ide-demo cd ide-demo code .  ```       Tip       If you get the error  `command     not     found:     code`  , see  [Launching from the command line](https://code.visualstudio.com/docs/setup/mac#_launching-from-the-command-line)   on the Microsoft website.        For Windows:        ``` md ide-demo cd ide-demo code .  ``` 2. In Visual Studio Code, on the menu bar, click  **View > Terminal**   . 3. From the root of the  `ide-demo`  folder, run the  `pipenv`  command with the following option, where  `<version>`  is the target version of Python that you already have installed locally (and, ideally, a version that matches your target clusters’ version of Python), for example  `3.8.14`  .        ``` pipenv --python <version>  ```      Make a note of the  `Virtualenv     location`  value in the output of the  `pipenv`  command, as you will need it in the next step. 4. Select the target Python interpreter, and then activate the Python virtual environment:      1. On the menu bar, click   **View > Command Palette**    , type   `Python:        Select`   , and then click   **Python: Select Interpreter**    .  2. Select the Python interpreter within the path to the Python virtual environment that you just created. (This path is listed as the   `Virtualenv        location`   value in the output of the   `pipenv`   command.)  3. On the menu bar, click   **View > Command Palette**    , type   `Terminal:        Create`   , and then click   **Terminal: Create New Terminal**    .  4. Make sure that the command prompt indicates that you are in the   `pipenv`   shell. To confirm, you should see something like   `(<your-username>)`   before your command prompt. If you do not see it, run the following command:               ```  pipenv shell    ```           To exit the   `pipenv`   shell, run the command   `exit`   , and the parentheses disappear.  For more information, see  [Using Python environments in VS Code](https://code.visualstudio.com/docs/python/environments)   in the Visual Studio Code documentation.     ###   Step 2: Clone the code sample from GitHub   1. In Visual Studio Code, open the  `ide-demo`  folder (  **File > Open Folder**   ), if it is not already open. 2. Click  **View > Command Palette**   , type  `Git:     Clone`  , and then click  **Git: Clone**   . 3. For  **Provide repository URL or pick a repository source**  	The following Python-based code sample can be used to connect to a remote Databricks workspace using dbx: https://github.com/databricks/ide-best-practices. The article provides detailed instructions on how to set up the code sample in Visual Studio Code and how to use dbx to submit the code to a remote Databricks workspace.	1
no_reaction	['https://kb.databricks.com/administration/find-your-workspace-id']	How do I find my Workspace ID?	"Title: Find your workspace ID  Description: Learn how to find your Databricks workspace ID in the web UI as well as via a notebook command.  Answer: Everything you do in Databricks occurs within a workspace. When you use the web UI you are interacting with clusters and notebooks in the workspace. When you run automated jobs or connect to your workspace outside of the web UI you may need to know your workspace ID. This article covers two different ways to easily find your workspace ID. Instructions From the browser When viewing a Databricks workspace, the workspace ID is contained within the web URL. After you have logged into your Databricks workspace, look at the URL displayed in your browser's address bar. You should see a series of numbers displayed in the URL after o=. The numbers following the o= make up the workspace ID. Given https://&lt;databricks-instance&gt;.com/o=XXXXX, the workspace ID is the numeric value represented by XXXXX. In this example URL, the workspace ID is 2281745829657864 https://abcd-teste2-test-spcse2.cloud.databricks.com/?o=2281745829657864# From a notebook You can get the workspace ID from within a notebook by running spark.conf.get(""spark.databricks.clusterUsageTags.clusterOwnerOrgId"") in a Python or Scala cell. It returns the workspace ID when run."	"There are two ways to find your Databricks workspace ID. The first way is to look at the URL displayed in your browser's address bar after logging into your Databricks workspace. The workspace ID is the numeric value represented by the series of numbers following o=. The second way is to run spark.conf.get(""spark.databricks.clusterUsageTags.clusterOwnerOrgId"") in a Python or Scala cell within a notebook, which returns the workspace ID."	1
thumb_down	['https://spark.apache.org/docs/latest/sql-ref.html']	Where is the documentation on Databricks SQL?	  ###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) + [ANSI Compliance](sql-ref-ansi-compliance.html) + [Data Types](sql-ref-datatypes.html) + [Datetime Pattern](sql-ref-datetime-pattern.html) + [Number Pattern](sql-ref-number-pattern.html) + [Functions](sql-ref-functions.html) + [Identifiers](sql-ref-identifier.html) + [Literals](sql-ref-literals.html) + [Null Semantics](sql-ref-null-semantics.html) + [SQL Syntax](sql-ref-syntax.html)  * [Error Conditions](sql-error-conditions.html)   SQL Reference ===============   Spark SQL is Apache Spark’s module for working with structured data. This guide is a reference for Structured Query Language (SQL) and includes syntax, semantics, keywords, and examples for common SQL usage. It contains information for the following topics:    * [ANSI Compliance](sql-ref-ansi-compliance.html) * [Data Types](sql-ref-datatypes.html) * [Datetime Pattern](sql-ref-datetime-pattern.html) * [Number Pattern](sql-ref-number-pattern.html) * [Functions](sql-ref-functions.html)   + [Built-in Functions](sql-ref-functions-builtin.html)  + [Scalar User-Defined Functions (UDFs)](sql-ref-functions-udf-scalar.html)  + [User-Defined Aggregate Functions (UDAFs)](sql-ref-functions-udf-aggregate.html)  + [Integration with Hive UDFs/UDAFs/UDTFs](sql-ref-functions-udf-hive.html) * [Identifiers](sql-ref-identifier.html) * [Literals](sql-ref-literals.html) * [Null Semantics](sql-ref-null-semantics.html) * [SQL Syntax](sql-ref-syntax.html)   + [DDL Statements](sql-ref-syntax.html#ddl-statements)  + [DML Statements](sql-ref-syntax.html#dml-statements)  + [Data Retrieval Statements](sql-ref-syntax.html#data-retrieval-statements)  + [Auxiliary Statements](sql-ref-syntax.html#auxiliary-statements)  	The documentation on Databricks SQL can be found in the Spark SQL Guide, specifically in the SQL Reference section which includes syntax, semantics, keywords, and examples for common SQL usage.	1
no_reaction	['https://docs.databricks.com/data-sharing/grant-access.html']	How do I grant access to my Delta share	"      Grant and manage access to Delta Sharing data shares ======================================================     This article explains how to grant a data recipient access to a Delta Sharing share. It also explains how to view, update, and revoke access.        Requirements --------------     To share data with recipients:     * You must be a metastore admin, have the  `CREATE\_SHARE`  privilege for the Unity Catalog metastore where the data you want to share is registered, or be the owner of both the share and recipient objects. * You must use a Databricks workspace that has a Unity Catalog metastore attached. * You must use a SQL warehouse or cluster that uses a Unity-Catalog-capable cluster access mode. * Shares and recipients must already be defined.       Grant recipient access to share ---------------------------------     To grant share access to recipients, you can use Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or share object owner. The share object owner must also be the owner of any recipient objects being added.         To add recipients to a share (starting at the share):     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. Click  **Add recipient**   . 5. On the  **Add recipient**   dialog, start typing the recipient name or click the drop-down menu to select the recipients you want to add to the share. 6. Click  **Add**   .     To grant share access to a recipient (starting at the recipient):     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. Click  **Grant share**   . 5. On the  **Grant share**   dialog, start typing the share name or click the drop-down menu to select the shares you want to grant. 6. Click  **Grant**   .       Run the following command in a notebook or the Databricks SQL query editor.        ``` GRANT SELECT ON SHARE <share\_name> TO RECIPIENT <recipient\_name>;  ```     `SELECT`  is the only privilege that you can grant a recipient on a share.         Create a JSON file that includes the following, replacing  `recipient-name`  .  `SELECT`  is the only privilege that you can grant on a share.        ``` {  ""privilege\_assignments"": [{  ""principal"": ""recipient-name"",  ""privileges"": [  ""SELECT""  ]  }] }  ```      Then run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  . Replace  `<share\_name>`  with the name of the share you want to grant to the recipient, and replace  `update-share-perms.json`  with the filename of the JSON file.        ``` databricks unity-catalog shares update-permissions --name <share_name> \  --json-file update-share-perms.json  ```            Revoke recipient access to a share ------------------------------------     To revoke a recipient’s access to a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `REVOKE     ON     SHARE`  SQL command in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or share object owner.         To revoke a recipient’s access to a share, starting at the share:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. On the  **Recipients**   tab, find the recipient. 5. Click the  ![Kebab menu](../_images/kebab-menu.png)  kebab menu (also known as the three-dot menu) and select  **Revoke**   . 6. On the confirmation dialog, click  **Revoke**   .     To revoke a recipient’s access to a share, starting at the recipient:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. On the  **Shares**   tab, find the share. 5. Click the  ![Kebab menu](../_images/kebab-menu.png)  kebab menu (also known as the three-dot menu) on the share row and select  **Revoke**   . 6. On the confirmation dialog, click  **Revoke**   .       Run the following command in a notebook or the Databricks SQL query editor.        ``` REVOKE SELECT ON SHARE <share\_name> FROM RECIPIENT <recipient\_name>;  ```        Create a JSON file that includes the following, replacing  `recipient-name`  :        ``` {  ""changes"": [{  ""principal"": ""recipient-name"",  ""remove"": [""SELECT""]  }] }  ```      Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  . Replace  `<share\_name>`  with the name of the share you want to grant to the recipient, and replace  `update-share-perms.json`  with the filename of the JSON file.        ``` databricks unity-catalog shares update-permissions --name <share_name> --json-file update-share-perms.json  ```          View grants on a share or grants possessed by a recipient -----------------------------------------------------------     To view the current grants on a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `SHOW     GRANTS     ON     SHARE`  SQL command in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : If you are viewing recipients granted access to a share, you must be a metastore admin or the share object owner. If you are viewing shares granted to a recipient, you must be a metastore admin or the recipient object owner.         To view recipients with access to a share:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. Go to the  **Recipients**   tab to view all recipients who have access to the share.       Run the following command in a notebook or the Databricks SQL query editor.        ``` SHOW GRANT ON SHARE <share\_name>;  ```        Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog shares list-permissions --name <share_name>  ```        To view the current share grants possessed by a recipient, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `SHOW     GRANTS     TO     RECIPIENT`  SQL command in a Databricks notebook or the Databricks SQL query editor.         To view shares granted to a recipient:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. Go to the  **Shares**   tab to view all shares that the recipient has access to.       Run the following command in a notebook or the Databricks SQL query editor.        ``` SHOW GRANTS TO RECIPIENT <recipient\_name>;  ```        Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog recipients list-permissions --name <recipient_name>  ```                                                         "	To grant access to a Delta share, you can use Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor. To add recipients to a share, you need to be a metastore admin, have the CREATE_SHARE privilege for the Unity Catalog metastore where the data you want to share is registered, or be the owner of both the share and recipient objects. You also need to use a Databricks workspace that has a Unity Catalog metastore attached and use a SQL warehouse or cluster that uses a Unity-Catalog-capable cluster access mode. Once you have met these requirements, you can follow the steps outlined in the context to grant access to your Delta share.	1
no_reaction	['https://docs.databricks.com/machine-learning/automl-hyperparam-tuning/index.html']	Perform random search as a method for tuning hyperparameters	      Hyperparameter tuning =======================     Databricks Runtime for Machine Learning incorporates Hyperopt, an open source tool that automates the process of model selection and hyperparameter tuning.          Hyperparameter tuning with Hyperopt -------------------------------------     Databricks Runtime ML includes  [Hyperopt](https://github.com/hyperopt/hyperopt)   , a Python library that facilitates distributed hyperparameter tuning and model selection. With Hyperopt, you can scan a set of Python models while varying algorithms and hyperparameters across spaces that you define. Hyperopt works with both distributed ML algorithms such as Apache Spark MLlib and Horovod, as well as with single-machine ML models such as scikit-learn and TensorFlow.       The basic steps when using Hyperopt are:     1. Define an objective function to minimize. Typically this is the training or validation loss. 2. Define the hyperparameter search space. Hyperopt provides a conditional search space, which lets you compare different ML algorithms in the same run. 3. Specify the search algorithm. Hyperopt uses stochastic tuning algorithms that perform a more efficient search of hyperparameter space than a deterministic grid search. 4. Run the Hyperopt function  `fmin()`  .  `fmin()`  takes the items you defined in the previous steps and identifies the set of hyperparameters that minimizes the objective function.     To get started quickly using Hyperopt with scikit-learn algorithms, see:      * [Parallelize hyperparameter tuning with scikit-learn and MLflow](hyperopt-spark-mlflow-integration.html) * [Compare model types with Hyperopt and MLflow](hyperopt-model-selection.html)      For more details about how Hyperopt works, and for additional examples, see:      * [Hyperopt concepts](hyperopt-concepts.html) * [Use distributed training algorithms with Hyperopt](hyperopt-distributed-ml.html) * [Best practices: Hyperparameter tuning with Hyperopt](hyperopt-best-practices.html)        Automated MLflow tracking ---------------------------      Note       MLlib automated MLflow tracking is deprecated on clusters that run Databricks Runtime 10.1 ML and above, and it is disabled by default on clusters running Databricks Runtime 10.2 ML and above. Instead, use  [MLflow PySpark ML autologging](https://www.mlflow.org/docs/latest/python_api/mlflow.pyspark.ml.html#mlflow.pyspark.ml.autolog)   by calling  `mlflow.pyspark.ml.autolog()`  , which is enabled by default with  [Databricks Autologging](../../mlflow/databricks-autologging.html)  .       To use the old MLlib automated MLflow tracking in Databricks Runtime 10.2 ML and above, enable it by setting the  [Spark configurations](../../clusters/configure.html#spark-configuration) `spark.databricks.mlflow.trackMLlib.enabled     true`  and  `spark.databricks.mlflow.autologging.enabled     false`  .       * [Apache Spark MLlib and automated MLflow tracking](mllib-mlflow-integration.html)                                                       	Yes, random search can be used as a method for tuning hyperparameters. Hyperopt, an open source tool that automates the process of model selection and hyperparameter tuning, uses stochastic tuning algorithms that perform a more efficient search of hyperparameter space than a deterministic grid search. The basic steps when using Hyperopt are to define an objective function to minimize, define the hyperparameter search space, specify the search algorithm, and run the Hyperopt function `fmin()`.	1
no_reaction	['https://docs.databricks.com/files/unzip-files.html']	How to read zip files that have more than 2 GB in spark?	"    Expand and read Zip compressed files ======================================     You can use the  `unzip`  Bash command to expand files or directories of files that have been Zip compressed. If you download or encounter a file or directory ending with  `.zip`  , expand the data before trying to continue.        Note       Apache Spark provides native codecs for interacting with compressed Parquet files. By default, Parquet files written by Databricks end with  `.snappy.parquet`  , indicating they use snappy compression.         How to unzip data -------------------     The Databricks  `%sh` [magic command](../notebooks/notebooks-code.html#language-magic)  enables execution of arbitrary Bash code, including the  `unzip`  command.       The following example uses a zipped CSV file downloaded from the internet. You can also use the Databricks Utilities to move files to the driver volume before expanding them. See  [Download data from the internet](download-internet-files.html)  and  [Databricks Utilities](../dev-tools/databricks-utils.html)  .       The following code uses  `curl`  to download and then  `unzip`  to expand the data:        ``` %sh curl https://resources.lendingclub.com/LoanStats3a.csv.zip --output /tmp/LoanStats3a.csv.zip unzip /tmp/LoanStats3a.csv.zip  ```      Use dbutils to move the expanded file back to cloud object storage to allow for parallel reading, as in the following:        ``` dbutils.fs.mv(""file:/LoanStats3a.csv"", ""dbfs:/tmp/LoanStats3a.csv"")  ```      In this example, the downloaded data has a comment in the first row and a header in the second. Now that the data has been expanded and moved, use standard options for reading CSV files, as in the following example:        ``` df = spark.read.format(""csv"").option(""skipRows"", 1).option(""header"", True).load(""/tmp/LoanStats3a.csv"") display(df)  ```                                                       "	To read zip files that have more than 2 GB in Spark, you can use the `unzip` Bash command to expand the files or directories of files that have been Zip compressed. After expanding the data, you can use dbutils to move the expanded file back to cloud object storage to allow for parallel reading. Finally, you can use standard options for reading CSV files.	1
no_reaction	['https://docs.databricks.com/getting-started/index.html']	how much does databricks cost	    What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .                                                          	Based on the context, the cost of Databricks is not mentioned.	0
no_reaction	['https://kb.databricks.com/cloud/security-vulns-db-ec2-instances']	How frequently does databricks patch vulnerabilities	Title: Vulnerability scan shows vulnerabilities in Databricks EC2 instances  Description: Learn how to resolve vulnerabilities in long-running Databricks EC2 instances.  Answer: Problem The Corporate Information Security (CIS) Vulnerability Management team identifies vulnerabilities in AWS instances that are traced to EC2 instances created by Databricks (worker AMI). Cause The Databricks security team addresses all critical vulnerabilities and updates the core and worker AMIs on a regular basis. However, if there are long-running clusters that have not been restarted, the newest AMIs don’t get picked up by the cluster EC2 instances. Therefore, a scan might find vulnerabilities. Solution Restart long-running clusters or start a brand new cluster when you do vulnerability scans against specific Databricks worker EC2 instances.	According to the context, the Databricks security team addresses all critical vulnerabilities and updates the core and worker AMIs on a regular basis. However, if there are long-running clusters that have not been restarted, the newest AMIs don’t get picked up by the cluster EC2 instances. Therefore, a scan might find vulnerabilities. It is recommended to restart long-running clusters or start a brand new cluster when doing vulnerability scans against specific Databricks worker EC2 instances. The frequency of patching vulnerabilities is not explicitly mentioned in the context.	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.UDFRegistration.register.html']	create a pandas UDF to add 2 numbers with code	"   pyspark.pandas.Series.add  [¶](#pyspark-pandas-series-add ""Permalink to this headline"") =========================================================================================  `Series.`  `add`    (   *other     :     Any*   )    → pyspark.pandas.series.Series  [[source]](../../../_modules/pyspark/pandas/series.html#Series.add) [¶](#pyspark.pandas.Series.add ""Permalink to this definition"")    Return Addition of series and other, element-wise (binary operator    +    ).     Equivalent to  `series     +     other`   Parameters    **other**    Series or scalar value     Returns     Series     The result of the operation.     See also    [`Series.radd`](pyspark.pandas.Series.radd.html#pyspark.pandas.Series.radd ""pyspark.pandas.Series.radd"")   Examples    ``` >>> df = ps.DataFrame({'a': [2, 2, 4, np.nan], ...                    'b': [2, np.nan, 2, np.nan]}, ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b']) >>> df  a b a 2.0 2.0 b 2.0 NaN c 4.0 2.0 d NaN NaN  ```  ``` >>> df.a.add(df.b) a 4.0 b NaN c 6.0 d NaN dtype: float64  ```  ``` >>> df.a.radd(df.b) a 4.0 b NaN c 6.0 d NaN dtype: float64  ```  [pyspark.pandas.Series.get](pyspark.pandas.Series.get.html ""previous page"")  [pyspark.pandas.Series.div](pyspark.pandas.Series.div.html ""next page"")   "	"This context is about the `add` method of the `pyspark.pandas.Series` class in PySpark. It returns the addition of two series or a series and a scalar value, element-wise. Here is an example of how to use it:  ``` import pyspark.sql.functions as F import pyspark.sql.types as T import pandas as pd  @F.pandas_udf(T.DoubleType()) def add_udf(a: pd.Series, b: pd.Series) -> pd.Series:     return a.add(b)  df = spark.createDataFrame([(1, 2), (3, 4)], [""a"", ""b""]) df.select(add_udf(F.col(""a""), F.col(""b"")).alias(""sum"")).show() ```  This code creates a PySpark DataFrame with two columns `a` and `b`, and then applies the `add_udf` function to add the values in `a` and `b` element-wise, and returns a new column `sum`. The result is shown using the `show` method."	0
no_reaction	['https://docs.databricks.com/sql/admin/notification-destinations.html']	Notification destination subject template	    Manage notification destinations ==================================      Preview       This feature is in  [Public Preview](../../../release-notes/release-types.html)  .        In this article, you learn how to create and configure notification destinations for your workspace.       System notifications are messages that tell you when your workflow experiences a run event (start, success, and failure). By default, notifications are sent to user email addresses, but admins can configure alternate notification destinations using webhooks. This allows you to build event-driven integrations with Databricks.       You must be a Databricks workspace admin to manage notification destinations. Once configured, the destinations are available to all users.        Supported destinations for jobs ---------------------------------     As part of the public preview, Databricks supports the following destinations for job notifications:     * HTTP: Get an HTTP request on a configured URL with information about your job runs. * Slack: Instantly receive a message on a configured channel and take action on job run statuses.       Create a new notification destination ---------------------------------------     To configure a new notification destination     1. Click your username in the top bar of the workspace and select  **Admin Console**   from the dropdown. 2. Click the  **Notification destinations**   tab. 3. Click  **+New destination**   . 4. Select a destination type. The following destinations are currently supported:      * Email  * [Slack](#slack-destination)  * Webhook  * MS Teams  * PagerDuty 5. Configure the destination based on the type. 6. Click  **Create**   .         Use different credentials for each destination ------------------------------------------------     The configuration of a destination is securely stored encrypted in your Databricks workspace. To improve security in the event the third-party endpoint is compromised, Databricks recommends that you use different credentials for each configured destination. These include:     * Slack: the URL to which the notification is sent. * MS Teams: the URL to which the notification is sent. * PagerDuty: integration key that is used to uniquely route notifications to a PagerDuty service. * Webhook: username and password that are used to authenticate to a 3rd party endpoint using HTTP Basic authentication when delivering notifications.     Using different secrets for each configured destinations allows you to individually revoke access of an individual notification destinations without impacting the operation of all other destinations in your Databricks workspace.           Slack destination -------------------     To set up a Slack destination, follow the instructions in  [Incoming webhooks for Slack](https://slack.com/help/articles/115005265063-Incoming-webhooks-for-Slack)   . Paste the generated URL into your Databricks notification destination.         Add a webhook to a job ------------------------     After you’ve configured a destination, you can add the notification destination to a job. For more information, see  [Add email and system notifications for job events](../../../workflows/jobs/job-notifications.html)  .       Users can configure up to three system destinations for each event type per job.         Limitations -------------     Notification destinations currently has the following limitations:     * You can only configure notifications for Databricks SQL and Jobs. For more on notifications in Databricks SQL, see  [Configure notification destinations](../../../sql/admin/notification-destinations.html)  . * For jobs, only Slack and Webhook are currently supported as destinations.                                                      	The document explains how to create and configure notification destinations for a Databricks workspace. It also lists the supported destinations for job notifications, including HTTP and Slack. The document also provides instructions on how to create a new notification destination and how to add a notification destination to a job. Finally, it lists the limitations of notification destinations, including that only Databricks SQL and Jobs can be configured for notifications and that only Slack and Webhook are currently supported as destinations for jobs.	0
no_reaction	['https://kb.databricks.com/scala/job-fails-with-executorlostfailure-due-to-out-of-memory-error']	What is an out of memory exception in Spark?	Title: Job fails with ExecutorLostFailure due to “Out of memory” error  Description: Resolve executor failures where the root cause is due to the executor running out of memory..  Answer: Problem Job fails with an ExecutorLostFailure error message. ExecutorLostFailure (executor &lt;1&gt; exited caused by one of the running tasks) Reason: Executor heartbeat timed out after &lt;148564&gt; ms Cause The ExecutorLostFailure error message means one of the executors in the Apache Spark cluster has been lost. This is a generic error message which can have more than one root cause. In this article, we will look how to resolve issues when the root cause is due to the executor running out of memory  Let's say your executor has too much data to process and the amount of memory available in the executor is not sufficient to process the amount of data, then this issue could occur. For e.g. if the executor in your cluster has 24GB capacity and if the cumulative amount of the data size corresponding to all the tasks that are getting executed on that executor is greater than 24GB, then this issue could occur How do you determine if OOM is the reason for the executor getting lost?  Open the Spark UI. Click Stages. Click Failed stages. Click the description that corresponds to the failed stage. Review the bottom of the stage details page. Sort the list of tasks on the error column. The error messages describe why a specific task failed. If you see an error message that says out of memory, or a similar error like java.lang.OutOfMemoryError it means the task failed because the executor ran out of memory. Solution When an executor is failing due to running out of memory, you should review the following items. Is there a data skew?  Check whether the data is equally distributed across executors, or if there is any skew in the data. You can find this by checking the stage summary table on the stage details page of the Spark UI. If there is data skew and if this is the only executor that has more data in it, you need to resolve the skew to prevent the executor from running out of memory.  In most cases Adaptive Query Execution (AQE) automatically detects data skew and resolves the issue. However, there are some edge cases where AQE may not detect data skew correctly. Please review Why didn’t AQE detect my data skew? (AWS | Azure | GCP) for more information. If you are having trouble resolving data skew, you can try increasing the number of partitions or by explicitly mentioning the skew hints as explained in the How to specify skew hints in dataset and DataFrame-based join commands article. A partition is considered skewed when both (partition size &gt; skewedPartitionFactor * median partition size) and (partition size &gt; skewedPartitionThresholdInBytes) are true. For example, given a median partition size of 200 MB, if any partition exceeds 1 GB (200 MB * 5 (five is the default skewedPartitionFactor value)), it is considered skewed. Under this example, if you have a partition size of 900 MB it wouldn't be considered as skewed with the default settings. Now say your application code does a lot of transformations on the data (like explode, cartesian join, etc.). If you are performing a high number of transformations, you can overwhelm the executor, even if the partition isn't normally considered skewed. Using our example defaults, you may find that a 900 MB partition is too much to successfully process. If that is the case, you should reduce the skewedPartitionFactor value. By reducing this value to 4, the system then considers any partition over 800 MB as skewed and automatically assigns the appropriate skew hints. Please review the AQE documentation on dynamically handling skew join (AWS | Azure | GCP) for more information. Is the executor capable enough? If data is equally distributed across all executors and you still see out of memory errors, the executor does not have enough resources to handle the load you are trying to run. Increase horizontally by increasing the number of workers and/or increase vertically by selecting a Worker type with more memory when creating your clusters. Is it a properly configured streaming job? If there is no apparent data skew, but the executor is still getting too much data to process, you should use maxFilesPerTrigger and/or the trigger frequency settings to reduce the amount of data that is processed at any one time. Reducing the load on the executors also helps reduce the memory requirement, at the expense of slightly higher latency. In exchange for the increase in latency, the streaming job processed streaming events in a more controlled manner. A steady flow of events is reliably processed with every micro batch. Please review the Optimize streaming transactions with .trigger article for more information. You should also review the Spark Structured Streaming Programming Guide documentation on input sources and triggers.  If you want to increase the speed of the processing, you need to increase the number of executors in your cluster. You can also repartition the input streaming DataFrame, so the number of tasks is less than or equal to the number of cores in the cluster.	An out of memory exception in Spark occurs when an executor in the Apache Spark cluster runs out of memory. This can happen when the executor has too much data to process and the amount of memory available in the executor is not sufficient to process the amount of data. To determine if OOM is the reason for the executor getting lost, you can open the Spark UI, click on Stages, click on Failed stages, click the description that corresponds to the failed stage, review the bottom of the stage details page, and sort the list of tasks on the error column. If you see an error message that says out of memory, or a similar error like java.lang.OutOfMemoryError, it means the task failed because the executor ran out of memory. To resolve this issue, you should review whether there is a data skew, whether the executor is capable enough, and whether it is a properly configured streaming job.	1
no_reaction	['https://docs.databricks.com/machine-learning/train-model/distributed-training/horovod-spark.html']	What are the best practices for using Horovod on Databricks	     `horovod.spark`  : distributed deep learning with Horovod ==========================================================     Databricks supports the  `horovod.spark`  package, which provides an estimator API that you can use in ML pipelines with Keras and PyTorch. For details, see  [Horovod on Spark](https://github.com/horovod/horovod/blob/master/docs/spark.rst)   , which includes a section on  [Horovod on Databricks](https://github.com/horovod/horovod/blob/master/docs/spark.rst#horovod-on-databricks)   .        Note     * Databricks installs the  `horovod`  package with dependencies. If you upgrade or downgrade these dependencies, there might be compatibility issues. * When using  `horovod.spark`  with custom callbacks in Keras, you must save models in the TensorFlow SavedModel format.      + With TensorFlow 2.x, use the   `.tf`   suffix in the file name.  + With TensorFlow 1.x, set the option   `save\_weights\_only=True`   .       Requirements --------------     Databricks Runtime ML 7.4 or above.         Examples ----------     Here is a basic example to run a distributed training function using  `horovod.spark`  :        ``` def train():   import horovod.tensorflow as hvd   hvd.init()  import horovod.spark horovod.spark.run(train, num\_proc=2)  ```     ###   Example notebooks     These notebooks demonstrate how to use the Horovod Spark Estimator API with Keras and PyTorch.        ####   Horovod Spark Estimator Keras notebook     [Open notebook in new tab](/_extras/notebooks/source/deep-learning/horovod-spark-estimator-keras.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Horovod Spark Estimator PyTorch notebook     [Open notebook in new tab](/_extras/notebooks/source/deep-learning/horovod-spark-estimator-pytorch.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                  	The best practices for using Horovod on Databricks include using the `horovod.spark` package, which provides an estimator API that can be used in ML pipelines with Keras and PyTorch. It is important to note that Databricks installs the `horovod` package with dependencies, and if these dependencies are upgraded or downgraded, there might be compatibility issues. When using `horovod.spark` with custom callbacks in Keras, models must be saved in the TensorFlow SavedModel format. Databricks Runtime ML 7.4 or above is required. Example notebooks demonstrating how to use the Horovod Spark Estimator API with Keras and PyTorch are available.	0
no_reaction	['https://kb.databricks.com/scala/spark-job-fail-parquet-column-convert']	 com.databricks.sql.io.FileReadException: Error while reading file dbfs:/pipelines/140dfd6b-b388-4f15-91bb-1de1e17e2a70/tables/sfdc_opportunity_bronze/part-00000-0add7622-059e-4747-9fdb-c126e164568c-c000.snappy.parquet. Possible cause: Parquet column cannot be converted.	"Title: Apache Spark job fails with Parquet column cannot be converted error  Description: Parquet column cannot be converted error appears when you are reading decimal data in Parquet format and writing to a Delta table.  Answer: Problem You are reading data in Parquet format and writing to a Delta table when you get a Parquet column cannot be converted error message. The cluster is running Databricks Runtime 7.3 LTS or above. org.apache.spark.SparkException: Task failed while writing rows. Caused by: com.databricks.sql.io.FileReadException: Error while reading file s3://bucket-name/landing/edw/xxx/part-xxxx-tid-c00.snappy.parquet. Parquet column cannot be converted. Column: [Col1], Expected: DecimalType(10,0), Found: FIXED_LEN_BYTE_ARRAY Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException. Cause The vectorized Parquet reader is decoding the decimal type column to a binary format. The vectorized Parquet reader is enabled by default in Databricks Runtime 7.3 and above for reading datasets in Parquet files. The read schema uses atomic data types: binary, boolean, date, string, and timestamp. Info This error only occurs if you have decimal type columns in the source data. Solution If you have decimal type columns in your source data, you should disable the vectorized Parquet reader. Set spark.sql.parquet.enableVectorizedReader to false in the cluster’s Spark configuration to disable the vectorized Parquet reader at the cluster level. You can also disable the vectorized Parquet reader at the notebook level by running: %scala spark.conf.set(""spark.sql.parquet.enableVectorizedReader"",""false"") Info The vectorized Parquet reader enables native record-level filtering using push-down filters, improving memory locality, and cache utilization. If you disable the vectorized Parquet reader, there may be a minor performance impact. You should only disable it, if you have decimal type columns in your source data."	"The error message ""com.databricks.sql.io.FileReadException: Error while reading file DBFS (Databricks File Store):/pipelines/140dfd6b-b388-4f15-91bb-1de1e17e2a70/tables/sfdc_opportunity_bronze/part-00000-0add7622-059e-4747-9fdb-c126e164568c-c000.snappy.parquet. Possible cause: Parquet column cannot be converted."" indicates that there is an issue with reading a Parquet file. This error can occur when reading decimal data in Parquet format and writing to a Delta table. The solution is to disable the vectorized Parquet reader by setting spark.sql.parquet.enableVectorizedReader to false in the cluster's Spark configuration or at the notebook level."	1
no_reaction	['https://docs.databricks.com/workflows/jobs/jobs.html']	Can you give me an example of how to programtically create a new job with a new cluster in python? Just have it run a simple program	"    Create, run, and manage Databricks Jobs =========================================     This article details how to create, edit, run, and monitor Databricks Jobs using the Jobs UI. To learn about using the Databricks CLI to create and run jobs, see  [Jobs CLI](../../dev-tools/cli/jobs-cli.html)  . To learn about using the Jobs API, see  [Jobs API 2.1](../../dev-tools/api/latest/jobs.html)  .       To create your first workflow with a Databricks job, see the  [quickstart](jobs-quickstart.html)  .        Important     * You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. * A workspace is limited to 1000 concurrent task runs. A  `429     Too     Many     Requests`  response is returned when you request a run that cannot start immediately. * The number of jobs a workspace can create in an hour is limited to 10000 (includes “runs submit”). This limit also affects jobs created by the REST API and notebook workflows.         Create a job --------------   1. Do one of the following:      * Click   ![Jobs Icon](../../_images/jobs-icon.png)  **Workflows**    in the sidebar and click   ![Create Job Button](../../_images/create-job.png)   .  * In the sidebar, click   ![New Icon](../../_images/create-icon.png)  **New**    and select   **Job**    .  The  **Tasks**   tab appears with the create task dialog.      ![Create task screen](../../_images/create-task.png) 2. Replace  **Add a name for your job…**   with your job name. 3. Enter a name for the task in the  **Task name**   field. 4. In the  **Type**   dropdown menu, select the type of task to run. See  [Task type options](#task-types)  . 5. Configure the cluster where the task runs. In the  **Cluster**   dropdown menu, select either  **New job cluster**   or  **Existing All-Purpose Clusters**   .      * **New Job Cluster**    : Click   **Edit**    in the   **Cluster**    dropdown menu and complete the   [cluster configuration](../../clusters/configure.html)   .  * **Existing All-Purpose Cluster**    : Select an existing cluster in the   **Cluster**    dropdown menu. To open the cluster in a new page, click the   ![External Link](../../_images/external-link.png)   icon to the right of the cluster name and description.  To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  . 6. To add dependent libraries, click  **+ Add**   next to  **Dependent libraries**   . See  [Dependent libraries](#task-config-dependent-libraries)  . 7. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters.      * **Notebook**    : Click   **Add**    and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the   [Run a job with different parameters](#job-run-with-different-params)   option. Parameters set the value of the   [notebook widget](../../notebooks/widgets.html)   specified by the key of the parameter. Use   [task parameter variables](#parameter-variables)   to pass a limited set of dynamic values as part of a parameter value.  * **JAR**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See   [Configure JAR job parameters](#configure-jar-job-parameters)   .  * **Spark Submit**    task: Parameters are specified as a JSON-formatted array of strings. Conforming to the   [Apache Spark spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html)    convention, parameters after the JAR path are passed to the main method of the main class.  * **Python script**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the   [argparse](https://docs.python.org/3/library/argparse.html)    module in Python.  * **Python Wheel**    : In the   **Parameters**    dropdown menu, select   **Positional arguments**    to enter parameters as a JSON-formatted array of strings, or select   **Keyword arguments > Add**    to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. 8. To optionally receive notifications for task start, success, or failure, click  **+ Add**   next to  **Emails**   . Failure notifications are sent on initial task failure and any subsequent retries. 9. To optionally configure a retry policy for the task, click  **+ Add**   next to  **Retries**   . See  [Retries](#retry-policies)  . 10. To optionally configure a timeout for the task, click  **+ Add**   next to  **Timeout in seconds**   . See  [Timeout](#timeout-setting)  . 11. Click  **Create**   .     After creating the first task, you can configure job-level settings such as notifications, job triggers, and permissions. See  [Edit a job](#job-edit)  .       To add another task, click  ![Add Task Button](../../_images/add-task.png)  in the DAG view. A shared cluster option is provided if you have configured a  **New Job Cluster**   for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  .           Task type options -------------------     The following are the task types you can add to your Databricks job and available options for the different task types:     * **Notebook**   : In the  **Source**   dropdown menu, select a location for the notebook; either  **Workspace**   for a notebook located in a Databricks workspace folder or  **Git provider**   for a notebook located in a remote Git repository.      **Workspace**   : Use the file browser to find the notebook, click the notebook name, and click  **Confirm**   .      **Git provider**   : Click  **Edit**   and enter the Git repository information. See  [Use version controlled notebooks in a Databricks job](how-to/use-repos.html)  . * **JAR**   : Specify the  **Main class**   . Use the fully qualified name of the class containing the main method, for example,  `org.apache.spark.examples.SparkPi`  . Then click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. One of these libraries must contain the main class.       To learn more about JAR tasks, see  [JAR jobs](#jar-jobs)  . * **Spark Submit**   : In the  **Parameters**   text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the  `DFSReadWriteTest`  from the Apache Spark examples:        ``` [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark\_examples\_2\_12\_3\_1\_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]  ```       Important       There are several limitations for  **spark-submit**   tasks:       + You can run spark-submit tasks only on new clusters.  + Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see   [Cluster autoscaling](../../clusters/configure.html#autoscaling)   .  + Spark-submit does not support   [Databricks Utilities](../../dev-tools/databricks-utils.html)   . To use Databricks Utilities, use JAR tasks instead.  + If you are using a Unity Catalog-enabled cluster, spark-submit is supported only if the cluster uses Single User   [access mode](../../data-governance/unity-catalog/compute.html#access-mode)   . Shared access mode is not supported. * **Python script**   : In the  **Source**   drop-down, select a location for the Python script, either  **Workspace**   for a script in the local workspace, or  **DBFS / S3**   for a script located on DBFS or cloud storage. In the  **Path**   textbox, enter the path to the Python script:      **Workspace**   : In the  **Select Python File**   dialog, browse to the Python script and click  **Confirm**   . Your script must be in a  [Databricks repo](../../files/workspace.html)  .      **DBFS**   : Enter the URI of a Python script on DBFS or cloud storage; for example,  `dbfs:/FileStore/myscript.py`  . * **Delta Live Tables Pipeline**   : In the  **Pipeline**   dropdown menu, select an existing  [Delta Live Tables](../../delta-live-tables/index.html)  pipeline.        Important       You can use only triggered pipelines with the  **Pipeline**   task. Continuous pipelines are not supported as a job task. To learn more about triggered and continuous pipelines, see  [Continuous vs. triggered pipeline execution](../../delta-live-tables/updates.html#continuous-triggered)  . * **Python Wheel**   : In the  **Package name**   text box, enter the package to import, for example,  `myWheel-1.0-py2.py3-none-any.whl`  . In the  **Entry Point**   text box, enter the function to call when starting the wheel. Click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. * **SQL**   : In the  **SQL task**   dropdown menu, select  **Query**   ,  **Dashboard**   , or  **Alert**   .        Note       + The   **SQL**    task is in   [Public Preview](../../release-notes/release-types.html)   .  + The   **SQL**    task requires Databricks SQL and a   [serverless or pro SQL warehouse](../../sql/index.html#warehouse-types)   .   **Query**   : In the  **SQL query**   dropdown menu, select the query to execute when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Dashboard**   : In the  **SQL dashboard**   dropdown menu, select a dashboard to be updated when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Alert**   : In the  **SQL alert**   dropdown menu, select an alert to trigger for evaluation. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task. * **dbt**   : See  [Use dbt transformations in a Databricks job](how-to/use-dbt-in-workflows.html)  for a detailed example"	"To programatically create a new job with a new cluster in Python, you can use the Databricks Jobs API. Here is an example of how to create a new job with a new cluster in Python:  ``` import requests import json  # Set the API endpoint and token api_endpoint = ""https://<databricks-instance>/api/2.0/jobs/create"" token = ""<your-token>""  # Set the job configuration job_config = {     ""name"": ""My Job"",     ""new_cluster"": {         ""spark_version"": ""7.3.x-scala2.12"",         ""node_type_id"": ""Standard_DS3_v2"",         ""num_workers"": 1     },     ""spark_python_task"": {         ""python_file"": ""dbfs:/path/to/my_script.py""     } }  # Send the API request response = requests.post(api_endpoint, headers={""Authorization"": f""Bearer {token}""}, json=job_config)  # Print the response print(json.loads(response.content)) ```  This example creates a new job named ""My Job"" with a new cluster that has one worker node of type ""Standard_DS3_v2"" and Spark version ""7.3.x-scala2.12"". The job runs a Python script located at ""dbfs:/path/to/my_script.py""."	0
no_reaction	['https://kb.databricks.com/clusters/termination-reasons']	How to terminate a cluster/	Title: Unexpected cluster termination  Description: Learn how to troubleshoot a Databricks cluster that stopped unexpectedly.  Answer: Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination. A cluster can be terminated for many reasons. Some terminations are initiated by Databricks and others are initiated by the cloud provider. This article describes termination reasons and steps for remediation. Databricks initiated request limit exceeded To defend against API abuses, ensure quality of service, and prevent you from accidentally creating too many large clusters, Databricks throttles all cluster up-sizing requests, including cluster creation, starting, and resizing. The throttling uses the token bucket algorithm to limit the total number of nodes that anyone can launch over a defined interval across your Databricks deployment, while allowing burst requests of certain sizes. Requests coming from both the web UI and the APIs are subject to rate limiting. When cluster requests exceed rate limits, the limit-exceeding request fails with a REQUEST_LIMIT_EXCEEDED error. Solution If you hit the limit for your legitimate workflow, Databricks recommends that you do the following: Retry your request a few minutes later. Spread out your recurring workflow evenly in the planned time frame. For example, instead of scheduling all of your jobs to run at an hourly boundary, try distributing them at different intervals within the hour. Consider using clusters with a larger node type and smaller number of nodes. Use autoscaling clusters. If these options don’t work for you, contact Databricks Support to request a limit increase for the core instance. For other Databricks initiated termination reasons, see Termination Code. Cloud provider initiated terminations This article lists common cloud provider related termination reasons and remediation steps. AWS Provider limit Databricks launches a cluster by requesting resources on behalf of your cloud account. Sometimes, these requests fail because they would exceed your cloud account’s resource limits. In AWS, common error codes include: InstanceLimitExceeded AWS limits the number of running instances for each node type. Possible solutions include: Request a cluster with fewer nodes. Request a cluster with a different node type. Ask AWS support to increase instance limits. Client.VolumeLimitExceeded The cluster creation request exceeded the EBS volume limit. AWS has two types of volume limits: a limit on the total number of EBS volumes, and a limit on the total storage size of EBS volumes. Potential remediation steps: Request a cluster with fewer nodes. Check which of the two limits was exceeded. (AWS trusted advisor shows service limits for free). If the request exceeded the total number of EBS volumes, try reducing the requested number of volumes per node. If the request exceeded the total EBS storage size, try reducing the requested storage size and/or the number of EBS volumes. Ask AWS support to increase EBS volume limits. RequestLimitExceeded AWS limits the rate of API requests made for an AWS account. Wait a while before retrying the request. Provider shutdown The Spark driver is a single point of failure because it holds all cluster state. If the instance hosting the driver node is shut down, Databricks terminates the cluster. In AWS, common error codes include: Client.UserInitiatedShutdown Instance was terminated by a direct request to AWS which did not originate from Databricks. Contact your AWS administrator for more details. Server.InsufficientInstanceCapacity AWS could not satisfy the instance request. Wait a while and retry the request. Contact AWS support if the problem persists. Server.SpotInstanceTermination Instance was terminated by AWS because the current spot price has exceeded the maximum bid made for this instance. Use an on-demand instance for the driver, choose a different availability zone, or specify a higher spot bid price. For other shutdown-related error codes, refer to AWS docs. Launch failure AWS In AWS, common error codes include: UnauthorizedOperation Databricks was not authorized to launch the requested instances. Possible reasons include: Your AWS administrator invalidated the AWS access key or IAM role used to launch instances. You are trying to launch a cluster using an IAM role that Databricks does not have permission to use. Contact the AWS administrator who set up the IAM role. For more information, see Secure Access to S3 Buckets Using IAM Roles. Unsupported with message “EBS-optimized instances are not supported for your requested configuration” The selected instance type is not available in the selected availability zone (AZ). It does not actually have anything to do with EBS-optimization being enabled. To remediate, you can choose a different instance type or AZ. AuthFailure.ServiceLinkedRoleCreationNotPermitted The provided credentials do not have permission to create the service-linked role for EC2 spot instances. The Databricks administrator needs to update the credentials used to launch instances in your account. Instructions and the updated policy can be found AWS Account. See Error Codes for a complete list of AWS error codes. Azure This termination reason occurs when Azure Databricks fails to acquire virtual machines. The error code and message from the API are propagated to help you troubleshoot the issue. OperationNotAllowed You have reached a quota limit, usually number of cores, that your subscription can launch. Request a limit increase in Azure portal. See Azure subscription and service limits, quotas, and constraints. PublicIPCountLimitReached You have reached the limit of the public IPs that you can have running. Request a limit increase in Azure Portal. SkuNotAvailable The resource SKU you have selected (such as VM size) is not available for the location you have selected. To resolve, see Resolve errors for SKU not available. ReadOnlyDisabledSubscription Your subscription was disabled. Follow the steps in Why is my Azure subscription disabled and how do I reactivate it? to reactivate your subscription. ResourceGroupBeingDeleted Can occur if someone cancels your Azure Databricks workspace in the Azure portal and you try to create a cluster at the same time. The cluster fails because the resource group is being deleted. SubscriptionRequestsThrottled Your subscription is hitting the Azure Resource Manager request limit (see Throttling Resource Manager requests). Typical cause is that another system outside Azure Databricks) making a lot of API calls to Azure. Contact Azure support to identify this system and then reduce the number of API calls. Communication lost Databricks was able to launch the cluster, but lost the connection to the instance hosting the Spark driver. AWS Caused by an incorrect networking configuration (for example, changing security group settings for Databricks workers) or a transient AWS networking issue. Azure Caused by the driver virtual machine going down or a networking issue.	To terminate a Databricks cluster, you can go to the Clusters tab in the Databricks workspace, select the cluster you want to terminate, and click the Terminate button. However, if a cluster is terminated unexpectedly, it can be due to various reasons such as Databricks initiated request limit exceeded, cloud provider initiated terminations, launch failure, or communication lost. The article provides detailed steps for remediation for each termination reason.	0
no_reaction	['https://docs.databricks.com/delta-live-tables/tutorial-sql.html']	how can i create a delta table using sql	    What are all the  *Delta*   things in Databricks? ===================================================     This article is an introduction to the technologies collectively branded  *Delta*   on Databricks. Delta refers to technologies related to or in the  [Delta Lake open source project](https://delta.io/)   .       This article answers:     * What are the  *Delta*   technologies in Databricks? * What do they do? Or what are they used for? * How are they related to and distinct from one another?      What are the Delta things used for? -------------------------------------     Delta is a term introduced with Delta Lake, the foundation for storing data and tables in the Databricks Lakehouse Platform. Delta Lake was conceived of as a unified data management system for handling transactional real-time and batch big data, by extending Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling.           Delta Lake: OS data management for the lakehouse --------------------------------------------------    [Delta Lake](https://delta.io/)   is an open-source storage layer that brings reliability to data lakes by adding a transactional storage layer on top of data stored in cloud storage (on AWS S3, Azure Storage, and GCS). It allows for ACID transactions, data versioning, and rollback capabilities. It allows you to handle both batch and streaming data in a unified way.       Delta tables are built on top of this storage layer and provide a table abstraction, making it easy to work with large-scale structured data using SQL and the DataFrame API.         Delta tables: Default data table architecture -----------------------------------------------     Delta table is the default data table format in Databricks and is a feature of the Delta Lake open source data framework. Delta tables are typically used for data lakes, where data is ingested via streaming or in large batches.       See:     * [Delta Lake quickstart: Create a table](https://docs.delta.io/latest/quick-start.html#create-a-table) * [Updating and modifying Delta Lake tables](../delta/index.html#updates)  . * [DeltaTable class](https://docs.delta.io/latest/api/python/index.html#deltatable)   : Main class for interacting programmatically with Delta tables.         Delta Live Tables: Data pipelines -----------------------------------     Delta Live Tables manage the flow of data between many Delta tables, thus simplifying the work of data engineers on ETL development and management. The pipeline is the main unit of execution for  [Delta Live Tables](../delta-live-tables/index.html)  . Delta Live Tables offers declarative pipeline development, improved data reliability, and cloud-scale production operations. Users can perform both batch and streaming operations on the same table and the data is immediately available for querying. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling. Delta Live Tables Enhanced Autoscaling can handle streaming workloads which are spiky and unpredictable.       See  [Delta Live Tables quickstart](../delta-live-tables/tutorial-sql.html)  .           Delta tables vs. Delta Live Tables ------------------------------------     Delta table is a way to store data in tables, whereas Delta Live Tables allows you to describe how data flows between these tables declaratively. Delta Live Tables is a declarative framework that manages many delta tables, by creating them and keeping them up to date. In short, Delta tables is a data table architecture while Delta Live Tables is a data pipeline framework.         Delta: Open source or proprietary? ------------------------------------     A strength of the Databricks platform is that it doesn’t lock customers into proprietary tools: Much of the technology is powered by open source projects, which Databricks contributes to.       The Delta OSS projects are examples:     * [Delta Lake project](https://delta.io/)   : Open source storage for the Lakehouse. * [Delta Sharing protocol](https://delta.io/sharing/)   : Open protocol for secure data sharing.     Delta Live Tables is a proprietary framework in Databricks.         What are the other  *Delta*   things on Databricks? -----------------------------------------------------     Below are descriptions of other features that include  *Delta*   in their name.      ###   Delta Sharing     An open standard for secure data sharing,  [Delta Sharing](../data-sharing/index.html)  enables data sharing between organizations regardless of their compute platform.       ###   Delta engine     A query optimizer for big data that uses Delta Lake open source technology included in Databricks. Delta engine optimizes the performance of Spark SQL, Databricks SQL, and DataFrame operations by pushing computation to the data.         ###   Delta Lake transaction log (AKA DeltaLogs)     A single source of truth tracking all changes that users make to the table and the mechanism through which Delta Lake guarantees  [atomicity](../lakehouse/acid.html#atomicity)  . See the  [Delta transaction log protocol](https://github.com/delta-io/delta/blob/master/PROTOCOL.md)   on GitHub.       The transaction log is key to understanding Delta Lake, because it is the common thread that runs through many of its most important features:     * ACID transactions * Scalable metadata handling * Time travel * And more.                                                       	"To create a Delta table using SQL, you can use the CREATE TABLE AS SELECT (CTAS) statement in SQL. The CTAS statement creates a new table and populates it with the results of a SELECT statement. You can specify the format of the table as DELTA to create a Delta table. For example, the SQL statement to create a Delta table named ""my_delta_table"" from an existing table named ""my_table"" would be:  CREATE TABLE my_delta_table USING DELTA AS SELECT * FROM my_table"	1
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.merge.html']	pyspark merge	"   pyspark.pandas.merge  [¶](#pyspark-pandas-merge ""Permalink to this headline"") ===============================================================================  `pyspark.pandas.`  `merge`    (   *obj     :     pyspark.pandas.frame.DataFrame*  ,  *right     :     pyspark.pandas.frame.DataFrame*  ,  *how     :     str     =     'inner'*  ,  *on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *left\_on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *right\_on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *left\_index     :     bool     =     False*  ,  *right\_index     :     bool     =     False*  ,  *suffixes     :     Tuple    [    str    ,    str    ]     =     '\_x', '\_y'*   )    → pyspark.pandas.frame.DataFrame  [[source]](../../../_modules/pyspark/pandas/namespace.html#merge) [¶](#pyspark.pandas.merge ""Permalink to this definition"")    Merge DataFrame objects with a database-style join.     The index of the resulting DataFrame will be one of the following:    * 0…n if no index is used for merging * Index of the left DataFrame if merged only on the index of the right DataFrame * Index of the right DataFrame if merged only on the index of the left DataFrame * All involved indices if merged using the indices of both DataFrames     e.g. if    left    with indices (a, x) and    right    with indices (b, x), the result will be an index (x, a, b)   Parameters    **right: Object to merge with.**   **how: Type of merge to be performed.**    {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’     left: use only keys from left frame, like a SQL left outer join; preserve key     order.     right: use only keys from right frame, like a SQL right outer join; preserve key     order.     outer: use union of keys from both frames, like a SQL full outer join; sort keys     lexicographically.     inner: use intersection of keys from both frames, like a SQL inner join;     preserve the order of the left keys.    **on: Column or index level names to join on. These must be found in both DataFrames. If on**    is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.    **left\_on: Column or index level names to join on in the left DataFrame. Can also**    be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.    **right\_on: Column or index level names to join on in the right DataFrame. Can also**    be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.    **left\_index: Use the index from the left DataFrame as the join key(s). If it is a**    MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.    **right\_index: Use the index from the right DataFrame as the join key. Same caveats as**    left\_index.    **suffixes: Suffix to apply to overlapping column names in the left and right side,**    respectively.     Returns     DataFrame     A DataFrame of the two merged objects.     Notes     As described in #263, joining string columns currently returns None for missing values     instead of NaN.     Examples    ``` >>> df1 = ps.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'], ...                     'value': [1, 2, 3, 5]}, ...                    columns=['lkey', 'value']) >>> df2 = ps.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'], ...                     'value': [5, 6, 7, 8]}, ...                    columns=['rkey', 'value']) >>> df1  lkey value 0 foo 1 1 bar 2 2 baz 3 3 foo 5 >>> df2  rkey value 0 foo 5 1 bar 6 2 baz 7 3 foo 8  ```   Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, \_x and \_y, appended.    ``` >>> merged = ps.merge(df1, df2, left\_on='lkey', right\_on='rkey') >>> merged.sort\_values(by=['lkey', 'value\_x', 'rkey', 'value\_y'])    lkey value\_x rkey value\_y ...bar 2 bar 6 ...baz 3 baz 7 ...foo 1 foo 5 ...foo 1 foo 8 ...foo 5 foo 5 ...foo 5 foo 8  ```  ``` >>> left\_psdf = ps.DataFrame({'A': [1, 2]}) >>> right\_psdf = ps.DataFrame({'B': ['x', 'y']}, index=[1, 2])  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True).sort\_index()  A B 1 2 x  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='left').sort\_index()  A B 0 1 None 1 2 x  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='right').sort\_index()  A B 1 2.0 x 2 NaN y  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='outer').sort\_index()  A B 0 1.0 None 1 2.0 x 2 NaN y  ```  [pyspark.pandas.melt](pyspark.pandas.melt.html ""previous page"")  [pyspark.pandas.merge\_asof](pyspark.pandas.merge_asof.html ""next page"")   "	The `pyspark.pandas.merge` function is used to merge DataFrame objects with a database-style join. It takes in two DataFrames, `left` and `right`, and performs a merge based on the specified `how` parameter, which can be 'left', 'right', 'outer', or 'inner'. The resulting DataFrame will have an index that depends on the type of merge performed. The `on` parameter specifies the column or index level names to join on, and if it is None and not merging on indexes, it defaults to the intersection of the columns in both DataFrames. The `left_on` and `right_on` parameters specify the column or index level names to join on in the left and right DataFrames, respectively. The `left_index` and `right_index` parameters specify whether to use the index from the left or right DataFrame as the join key(s). The `suffixes` parameter specifies the suffix to apply to overlapping column names in the left and right side, respectively.	1
no_reaction	['https://docs.databricks.com/dev-tools/python-api.html']	Write my python code that uses the Databricks REST API to turn on a cluster called test	"    Call the Databricks REST API with Python ==========================================     You can call the  [Databricks REST API](api/index.html)  to automate Databricks with Python code, instead of using non-Python command-line tools such as  `curl`  or API clients such as Postman. To call the Databricks REST API with Python, you can use the  [Databricks CLI package](https://github.com/databricks/databricks-cli)   as a library. This library is written in Python and enables you to call the Databricks REST API through Python classes that closely model the Databricks REST API request and response payloads.        Note       Direct use of the Python  [requests](https://docs.python-requests.org/)   library is another approach. However, you would need to work at a lower level, manually providing the necessary headers, handling errors, and other related low-level coding tasks. For more information, see  [Use Python to invoke the Databricks REST API](api/index.html#use-python)  .         Experimental       This Databricks CLI is under active development and is released as an  [Experimental](../release-notes/release-types.html)  client. This means that interfaces are still subject to change.        The Databricks CLI supports calling the following Databricks REST APIs:     * Cluster Policies API 2.0 * Clusters API 2.0 * DBFS API 2.0 * Groups API 2.0 * Instance Pools API 2.0 * Jobs API 2.1, 2.0 * Libraries API 2.0 * Delta Live Tables API 2.0 * Repos API 2.0 * Secrets API 2.0 * Token API 2.0 * Unity Catalog API 2.1 * Workspace API 2.0     The Databricks CLI does not support calling the following Databricks REST APIs:     * Account API 2.0 * Databricks SQL Queries, Dashboards, and Alerts API 2.0 * Databricks SQL Query History API 2.0 * Databricks SQL Warehouses API 2.0 * Git Credentials API 2.0 * Global Init Scripts API 2.0 * Instance Profiles API 2.0 * IP Access List API 2.0 * MLflow API 2.0 * Permissions API 2.0 * SCIM API 2.0 * Token Management API 2.0 * API 1.2     For detailed information, see the  [Databricks REST API reference](api/index.html)  .        Requirements --------------   * Python version 3.6 or above. To check whether Python is installed, and if so to check the installed version, run  `python     --version`  from your terminal of PowerShell.  [Install Python](https://python-docs.readthedocs.io/en/latest/starting/installation.html)   , if it is not already installed.        ``` python --version  ```       Note       Some installations of Python require  `python3`  instead of  `python`  . If so, replace  `python`  with  `python3`  throughout this article. * The  [Databricks CLI](cli/index.html)  . To check whether the Databricks CLI is installed, and if so to check the installed version, run  `databricks     --version`  . To install the Databricks CLI, run  `pip     install     databricks-cli`  or  `python     -m     pip     install     databricks-cli`  .        ``` # Check whether the Databricks CLI is installed, and if so check the installed version. databricks --version  # Install the Databricks CLI. pip install databricks-cli  # Or...  python -m pip install databricks-cli  ```       Note       Some installations of  `pip`  require  `pip3`  instead of  `pip`  . If so, replace  `pip`  with  `pip3`  throughout this article. * Your  [workspace instance URL](../workspace/workspace-details.html#workspace-url)  , for example  `https://dbc-a1b2345c-d6e7.cloud.databricks.com` * A Databricks personal access token for your Databricks workspace. To create a Databricks personal access token, see  [Databricks personal access tokens](auth.html#pat)  ; see also  [Manage personal access tokens](../administration-guide/access-control/tokens.html)  .       Step 1: Set up authentication -------------------------------     To authenticate with the Databricks REST API through the Databricks CLI package library, your Python code requires two pieces of information at minimum:     * Your  [workspace instance URL](../workspace/workspace-details.html#workspace-url)  , for example  `https://dbc-a1b2345c-d6e7.cloud.databricks.com`  . * A Databricks personal access token for your Databricks workspace. To create a Databricks personal access token, see  [Databricks personal access tokens](auth.html#pat)  ; see also  [Manage personal access tokens](../administration-guide/access-control/tokens.html)  .     For code modularity, portability, and security, you should not hard-code this information into your Python code. Instead, you should retrieve this information from a secure location at run time. For example, the code in this article uses the following environment variables:     * `DATABRICKS\_HOST`  , which represents your  [workspace instance URL](../workspace/workspace-details.html#workspace-url)  , for example  `https://dbc-a1b2345c-d6e7.cloud.databricks.com`  . * `DATABRICKS\_TOKEN`  . A Databricks personal access token for your Databricks workspace. To create a Databricks personal access token, see  [Databricks personal access tokens](auth.html#pat)  ; see also  [Manage personal access tokens](../administration-guide/access-control/tokens.html)  .     You can set these environment variables as follows:         To set the environment variables for only the current terminal session, run the following commands. To set the environment variables for all terminal sessions, enter the following commands into your shell’s startup file and then restart your terminal. Replace the example values here with your own values.        ``` export DATABRICKS\_HOST=""https://dbc-a1b2345c-d6e78.cloud.databricks.com"" export DATABRICKS\_TOKEN=""dapi1234567890b2cd34ef5a67bc8de90fa12b""  ```        To set the environment variables for only the current PowerShell session, run the following commands. Replace the example values here with your own values.        ``` set DATABRICKS\_HOST=""https://dbc-a1b2345c-d6e78.cloud.databricks.com"" set DATABRICKS\_TOKEN=""dapi1234567890b2cd34ef5a67bc8de90fa12b""  ```      To set the environment variables for all Command Prompt sessions, run the following commands and then restart your Command Prompt. Replace the example values here with your own values.        ``` setx DATABRICKS\_HOST ""https://dbc-a1b2345c-d6e78.cloud.databricks.com"" setx DATABRICKS\_TOKEN ""dapi1234567890b2cd34ef5a67bc8de90fa12b""  ```          Step 2: Write your code -------------------------   1. In your Python code file, import the  [os](https://docs.python.org/3/library/os.html)   library to enable your code to get the environment variable values.        ``` import os  ``` 2. Import the  `ApiClient`  class from the  [databricks\_cli.sdk.api\_client](https://github.com/databricks/databricks-cli/blob/main/databricks_cli/sdk/api_client.py)   module to enable your code to authenticate with the Databricks REST API.        ``` from databricks\_cli.sdk.api\_client import ApiClient  ``` 3. Import additional classes as needed to enable your code to call the Databricks REST API after authenticating, as follows.         |   REST API   |  `import`  statements   | | --- | --- | |   Cluster Policies API 2.0   |  `from     databricks\_cli.cluster\_policies.api     import     ClusterPolicyApi`  | |   Clusters API 2.0   |  `from     databricks\_cli.clusters.api     import     ClusterApi`  | |   DBFS API 2.0   |  `from     databricks\_cli.dbfs.api     import     DbfsApi`   `from     databricks\_cli.dbfs.dbfs\_path     import     DbfsPath`  | |   Groups API 2.0   |  `from     databricks\_cli.groups.api     import     GroupsApi`  | |   Instance Pools API 2.0   |  `from     databricks\_cli.instance\_pools.api     import     InstancePoolsApi`  | |   Jobs API 2.1   |  `from     databricks\_cli.jobs.api     import     JobsApi`  ( 1 )    `from     databricks\_cli.runs.api     import     RunsApi`  ( 2 )   | |   Libraries API 2.0   |  `from     databricks\_cli.libraries.api     import     LibrariesApi`  | |   Delta Live Tables API 2.0   |  `from     databricks\_cli.pipelines.api     import     PipelinesApi,     LibraryObject`  | |   Repos API 2.0   |  `from     databricks\_cli.repos.api     import     ReposApi`  | |   Secrets API 2.0   |  `from     databricks\_cli.secrets.api     import     SecretApi`  | |   Token API 2.0   |  `from     databricks\_cli.tokens.api     import     TokensApi`  | |   Unity Catalog API 2.1   |  `from     databricks\_cli.unity\_catalog.api     import     UnityCatalogApi`  | |   Workspace API 2.0   |  `from     databricks\_cli.workspace.api     import     WorkspaceApi`  |    * ( 1 ) Required only for working with jobs.  * ( 2 ) Required only for working with job runs.  For example, to call the Clusters API 2.0, add the following code:        ``` from databricks\_cli.clusters.api import ClusterApi  ``` 4. Use the  `ApiClient`  class to authenticate with the Databricks REST API. Use the  `os`  library’s  `getenv`  function to get the  [workspace instance URL](../workspace/workspace-details.html#workspace-url)  , for example  `https://dbc-a1b2345c-d"	"To turn on a cluster called test using the Databricks REST API with Python, you can use the `ClusterApi` class from the `databricks_cli.clusters.api` module. After importing the necessary classes, you can use the `ApiClient` class to authenticate with the Databricks REST API by providing your workspace instance URL and personal access token as environment variables. Then, you can use the `ClusterApi` class to turn on the cluster by calling the `start` method with the cluster ID. Here is an example code snippet:  ``` import os from databricks_cli.sdk.api_client import ApiClient from databricks_cli.clusters.api import ClusterApi  # Set up authentication host = os.getenv(""DATABRICKS_HOST"") token = os.getenv(""DATABRICKS_TOKEN"") client = ApiClient(host=host, token=token)  # Turn on the cluster cluster_id = ""test"" cluster_api = ClusterApi(client) cluster_api.start(cluster_id) ```"	1
no_reaction	['https://docs.databricks.com/structured-streaming/triggers.html']	"Q87- A Structured Streaming job deployed to production has been resulting in higher than expected out during normal execution, each microbatch of data is processed  in less than 3s, at least 12 times per me processed that contains 0 records.  The streaming write was configured using the default trigger setting The currently scheduled alongside many other Databricks jobs in a  workspace with instance pois prosine time for jobs with batch execution. Holding all other variables constant and assuming records need to be processed in less than 10 min meet the requirement? A.	Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing allowable threshold should minimize this cost. B.	Increase the number of shuffle partitions to maximize parallelism, since the trigger interval cannot be m checkpoint directory. C.	Set the trigger interval to 3 seconds; the default trigger interval is consuming too many records per much can increase volume costs. D.	Set the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the frequently E.	Use the trigger once option and configure a Databricks job to execute the query every 10 minute both compute and storage. "	"Title: Optimize streaming transactions with .trigger  Description: Use .trigger to define the storage update interval. A higher value reduces the number of storage transactions.  Answer: When running a structured streaming application that uses cloud storage buckets (S3, ADLS Gen2, etc.) it is easy to incur excessive transactions as you access the storage bucket. Failing to specify a .trigger option in your streaming code is one common reason for a high number of storage transactions. When a .trigger option is not specified, the storage can be polled frequently. This happens immediately after the completion of each micro-batch by default. The default behavior is described in the official Apache Spark documentation on triggers as, ""If no trigger setting is explicitly specified, then by default, the query will be executed in micro-batch mode, where micro-batches will be generated as soon as the previous micro-batch has completed processing."" This sample code does not have a .trigger option defined. If run, it would result in excessive storage transactions. %python spark.readStream.format(""delta"").load(""&lt;delta_table_path&gt;"") .writeStream .format(""delta"") .outputMode(""append"") .option(""checkpointLocation"",""&lt;checkpoint_path&gt;"") .options(**writeConfig) .start() You can reduce the number of storage transactions by setting the .trigger option in the .writeStream. Setting .trigger processing time to a few seconds prevents short polling. Instructions The default behavior is to check the source for updates every 10 ms. For most users, a longer interval between source updates will have no noticeable effect on performance, but the transaction costs are greatly reduced. For example, let's use a processing time of 5 seconds. That is 500 times slower than 10 ms. The storage calls are reduced accordingly. Setting a processing time of 5 seconds requires adding .trigger(processingTime='5 seconds') to the .writeStream. For example, modifying our existing sample code to include a .trigger processing time of 5 seconds only requires the addition of one line. %python spark.readStream.format(""delta"").load(""&lt;delta_table_path&gt;"") .writeStream .format(""delta"") .trigger(processingTime='5 seconds') #Added line of code that defines .trigger processing time. .outputMode(""append"") .option(""checkpointLocation"",""&lt;checkpoint_path&gt;"") .options(**writeConfig) .start() You should experiment with the .trigger processing time to determine a value that is optimized for your application."	Based on the context, the answer to the question is to set the trigger interval to 500 milliseconds. This is because the current microbatch processing time is less than 3 seconds and at least 12 times per minute, which means that setting a small but non-zero trigger interval ensures that the frequently. This will help to meet the requirement of processing records in less than 10 minutes while minimizing volume costs. Option A is incorrect because setting the trigger interval to 10 minutes will not meet the requirement of processing records in less than 10 minutes. Option B is also incorrect because increasing the number of shuffle partitions will not necessarily improve performance in this case. Option C is incorrect because setting the trigger interval to 3 seconds will still consume too many records per minute and increase volume costs. Option E is also incorrect because using the trigger once option and configuring a Databricks job to execute the query every 10 minutes will not meet the requirement of processing records in less than 10 minutes.	0
no_reaction	['https://docs.databricks.com/delta-live-tables/cdc.html']	"Here is the code: dlt.apply_changes(   target = ""cat_reporting_clean_dlt"",   source = ""join_with_stream"",   keys = [""fileName, firmROEID, actionType, orderID"", ""eventTimestamp""],   sequence_by = ""EventTimestamp"" )"	"      Change data capture with Delta Live Tables ============================================          Preview       Delta Live Tables support for SCD type 2 is in  [Public Preview](../release-notes/release-types.html)  .        You can use change data capture (CDC) in Delta Live Tables to update tables based on changes in source data. CDC is supported in the Delta Live Tables SQL and Python interfaces. Delta Live Tables supports updating tables with slowly changing dimensions (SCD) type 1 and type 2:     * Use SCD type 1 to update records directly. History is not retained for records that are updated. * Use SCD type 2 to retain a history of records, either on all updates or on updates to a specified set of columns. See  [Track history for only specified columns with SCD type 2](#type2-track-history)     For syntax and other references, see:     * [Change data capture with Python in Delta Live Tables](python-ref.html#cdc) * [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) * [CDC table properties](properties.html#cdc)      Note       This article describes how to update tables in your Delta Live Tables pipeline based on changes in source data. To learn how to record and query row-level change information for Delta tables, see  [Use Delta Lake change data feed on Databricks](../delta/delta-change-data-feed.html)  .           How is CDC implemented with Delta Live Tables? ------------------------------------------------     You must specify a column in the source data on which to sequence records, which Delta Live Tables interprets as a monotonically increasing representation of the proper ordering of the source data. Delta Live Tables automatically handles data that arrives out of order. For SCD Type 2 changes, Delta Live Tables propagates the appropriate sequencing values to the  `\_\_START\_AT`  and  `\_\_END\_AT`  columns of the target table. There should be at most one distinct update per key at each sequencing value, and NULL sequencing values are unsupported.       To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an  `APPLY     CHANGES     INTO`  statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the  `CREATE     OR     REFRESH     STREAMING     TABLE`  statement in SQL or the  `create\_streaming\_live\_table()`  function in Python. To create the statement defining the CDC processing, use the  `APPLY     CHANGES`  statement in SQL or the  `apply\_changes()`  function in Python. For syntax details, see  [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc)  or  [Change data capture with Python in Delta Live Tables](python-ref.html#cdc)  .             What data objects are used for Delta Live Tables CDC processing? ------------------------------------------------------------------     When you declare the target table, two data structures are created in the Hive metastore:     * A view using the name assigned to the target table. * An internal backing table used by Delta Live Tables table to manage CDC processing. This table is named by prepending  `\_\_apply\_changes\_storage\_`  to the target table name.     For example, if you declare a target table named  `dlt\_cdc\_target`  , you will see a view named  `dlt\_cdc\_target`  and a table named  `\_\_apply\_changes\_storage\_dlt\_cdc\_target`  in the metastore. Creating a view allows Delta Live Tables to filter out the extra information (for example, tombstones and versions) that is required to handle out-of-order data. To view the processed data, query the target view. You can also query the raw data in the  `\_\_apply\_changes\_storage\_`  table to see deleted records and extra version columns. If you add data manually to the table, the records are assumed to come before other changes because the version columns are missing.         Limitations -------------   * Metrics for the target table, such as number of output rows, are not available. * SCD type 2 updates will add a history row for every input row, even if no columns have changed. * The target of the  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function cannot be used as a source for a streaming table. A table that reads from the target of an  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function must be a live table. * Expectations are not supported in an  `APPLY     CHANGES     INTO`  query or  `apply\_changes()`  function. To use expectations for the source or target dataset:      + Add expectations on source data by defining an intermediate table with the required expectations and use this dataset as the source for the target table.  + Add expectations on target data with a downstream table that reads input data from the target table.         SCD type 1 and SCD type 2 on Databricks -----------------------------------------     The following sections provide examples that demonstrate Delta Live Tables SCD type 1 and type 2 queries that update target tables based on source events that:     1. Create new user records. 2. Delete a user record. 3. Update user records. In the SCD type 1 example, the last  `UPDATE`  operations arrive late and are dropped from the target table, demonstrating the handling of out of order events.     All of the following examples assume familiarity with configuring and updating Delta Live Tables pipelines. See  [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html)  .       In order to run these examples, you must begin by creating a sample dataset. See  [Generate test data](#generate-data)  .       The following are the input records for these examples:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   124   |   Raul   |   Oaxaca   |   INSERT   |   1   | |   123   |   Isabel   |   Monterrey   |   INSERT   |   1   | |   125   |   Mercedes   |   Tijuana   |   INSERT   |   2   | |   126   |   Lily   |   Cancun   |   INSERT   |   2   | |   123   |   null   |   null   |   DELETE   |   6   | |   125   |   Mercedes   |   Guadalajara   |   UPDATE   |   6   | |   125   |   Mercedes   |   Mexicali   |   UPDATE   |   5   | |   123   |   Isabel   |   Chihuahua   |   UPDATE   |   5   |     If you uncomment the final row in the example data, it will insert the following record that specifies where records should be truncated:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   null   |   null   |   null   |   TRUNCATE   |   3   |      Note       All the following examples include options to specify both  `DELETE`  and  `TRUNCATE`  operations, but each of these are optional.            Process SCD type 1 updates ----------------------------     The following code example demonstrates processing SCD type 1 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format(""delta"").table(""cdc\_data.users"")  dlt.create\_streaming\_live\_table(""target"")  dlt.apply\_changes(   target = ""target"",   source = ""users"",   keys = [""userId""],   sequence\_by = col(""sequenceNum""),   apply\_as\_deletes = expr(""operation = 'DELETE'""),   apply\_as\_truncates = expr(""operation = 'TRUNCATE'""),   except\_column\_list = [""operation"", ""sequenceNum""],   stored\_as\_scd\_type = 1 )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = ""DELETE"" APPLY AS TRUNCATE WHEN  operation = ""TRUNCATE"" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 1;  ```       After running the SCD type 1 example, the target table contains the following records:           |   userId   |   name   |   city   | | --- | --- | --- | |   124   |   Raul   |   Oaxaca   | |   125   |   Mercedes   |   Guadalajara   | |   126   |   Lily   |   Cancun   |     After running the SCD type 1 example with the additional  `TRUNCATE`  record, records  `124`  and  `126`  are truncated because of the  `TRUNCATE`  operation at  `sequenceNum=3`  , and the target table contains the following record:           |   userId   |   name   |   city   | | --- | --- | --- | |   125   |   Mercedes   |   Guadalajara   |         Process SCD type 2 updates ----------------------------     The following code example demonstrates processing SCD type 2 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format(""delta"").table(""cdc\_data.users"")  dlt.create\_streaming\_live\_table(""target"")  dlt.apply\_changes(   target = ""target"",   source = ""users"",   keys = [""userId""],   sequence\_by = col(""sequenceNum""),   apply\_as\_deletes = expr(""operation = 'DELETE'""),   except\_column\_list = [""operation"", ""sequenceNum""],   stored\_as\_scd\_type = ""2"" )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = ""DELETE"" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 2;  ```       After running the SCD type 2 example, the target table contains the following records:             |   userId   |   name   |   city   |   \_\_START\_AT   |   \_\_END\_AT   | | --- | --- | --- | --- | --- | |   123   |   Isabel   |   Monterrey   |   1   |   5   | |   123   |   Isabel   |  "	"The given code is an example of how to use change data capture (CDC) in Delta Live Tables to update tables based on changes in source data. The `DLT.apply_changes()` function is used to specify the source, keys, and sequencing for the change feed. In this example, the target table is ""cat_reporting_clean_dlt"", the source table is ""join_with_stream"", the keys are [""fileName, firmROEID, actionType, orderID"", ""eventTimestamp""], and the sequence is based on ""EventTimestamp""."	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.util.Saveable.html']	what does save() do?	"   JavaSaveable  [¶](#javasaveable ""Permalink to this headline"") ===============================================================  *class*  `pyspark.mllib.util.`  `JavaSaveable`  [[source]](../../_modules/pyspark/mllib/util.html#JavaSaveable) [¶](#pyspark.mllib.util.JavaSaveable ""Permalink to this definition"")    Mixin for models that provide save() through their Scala implementation.     New in version 1.3.0.     Methods    |  [`save`](#pyspark.mllib.util.JavaSaveable.save ""pyspark.mllib.util.JavaSaveable.save"")  (sc, path)   |   Save this model to the given path.   |   Methods Documentation    `save`    (   *sc     :     pyspark.context.SparkContext*  ,  *path     :     str*   )    → None  [[source]](../../_modules/pyspark/mllib/util.html#JavaSaveable.save) [¶](#pyspark.mllib.util.JavaSaveable.save ""Permalink to this definition"")    Save this model to the given path.     New in version 1.3.0.    [JavaLoader](pyspark.mllib.util.JavaLoader.html ""previous page"")  [LinearDataGenerator](pyspark.mllib.util.LinearDataGenerator.html ""next page"")   "	The `save()` method is used to save a model to a specified path.	1
no_reaction	['https://docs.databricks.com/data/manage-ownership.html']	How i create unity catalog if i am an adminisrrator to account console ?	"    Create and manage catalogs ============================     This article shows how to create and manage catalogs in Unity Catalog. A catalog contains  [schemas (databases)](create-schemas.html)  , and a schema contains tables and views.        Requirements --------------   * You must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore * Your Databricks account must be on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * You must have a Unity Catalog metastore  [linked to the workspace](create-metastore.html)  where you perform the catalog creation. * The  [compute resource](compute.html)  that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode.         Create a catalog ------------------     To create a catalog, you can use Data Explorer or a SQL command.       1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the  **Create Catalog**   button. 4. (Optional) Specify the location where data for  [managed tables](create-tables.html#managed-table)  in the catalog will be stored.       Specify a location here only if you do not want managed tables in this catalog to be stored in the default root storage location that was configured for the metastore. See  [Create a Unity Catalog metastore](create-metastore.html)  .       The path that you specify must be defined in an external location configuration, and you must have the  `CREATE     MANAGED     STORAGE`  privilege on that external location. You can also use a subpath of that path. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  . 5. Assign permissions for your catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  . 6. Click  **Save**   .     1. Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : Optional description or other comment.   ``` CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name>  [ MANAGED LOCATION '<location\_path>' ]  [ COMMENT <comment> ];  ```      For example, to create a catalog named  `example`  :        ``` CREATE CATALOG IF NOT EXISTS example;  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ```      For parameter descriptions, see  [CREATE CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-create-catalog.html)  . 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` library(SparkR)  sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` library(SparkR)  sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .       When you create a catalog, two schemas (databases) are automatically created:  `default`  and  `information\_schema`  .       You can also create a catalog by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog)   . You can retrieve information about catalogs by using  [databricks\_catalogs](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/catalogs)   .      ###   Next steps     Now you can  [add schemas (databases) to your catalog](create-schemas.html)  .          Delete a catalog ------------------     To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.         You must delete all schemas in the catalog except  `information\_schema`  before you can delete a catalog. This includes the auto-created  `default`  schema.     1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. In the  **Data**   pane, on the left, click the catalog you want to delete. 4. In the detail pane, click the three-dot menu to the left of the  **Create database**   button and select  **Delete**   . 5. On the  **Delete catalog**   dialog, click  **Delete**   .       Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` DROP CATALOG [ IF EXISTS ] <catalog\_name> [ RESTRICT | CASCADE ]  ```      For example, to delete a catalog named  `vaccine`  and its schemas:        ``` DROP CATALOG vaccine CASCADE  ```        Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` spark.sql(""DROP CATALOG [ IF EXISTS ] <catalog\_name>"	To create a Unity Catalog, you must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore. Your Databricks account must be on the Premium plan and above. You must have a Unity Catalog metastore linked to the workspace where you perform the catalog creation. The compute resource that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode. To create a catalog, you can use Data Explorer or a SQL command. To create a catalog using SQL command, you can run the following SQL command in a notebook or Databricks SQL editor: CREATE CATALOG [ IF NOT EXISTS ] <catalog_name> [ MANAGED LOCATION '<location_path>' ] [ COMMENT <comment> ].	1
thumb_down	['https://docs.databricks.com/languages/python.html']	install a python library to be available to all notebooks for all users that run on a Databricks Runtime for Machine Learning cluster	    Databricks for Python developers ==================================     This section provides a guide to developing notebooks and jobs in Databricks using the Python language. The first subsection provides links to tutorials for common workflows and tasks. The second subsection provides links to APIs, libraries, and key tools.       A basic workflow for getting started is:     * [Import code](#manage-code-with-notebooks-and-databricks-repos)  : Either import your own code from files or Git repos or try a tutorial listed below. Databricks recommends learning using interactive Databricks Notebooks. * [Run your code on a cluster](#clusters-and-libraries)  : Either create a cluster of your own, or ensure you have permissions to use a shared cluster. Attach your notebook to the cluster, and run the notebook. * Beyond this, you can branch out into more specific topics:      + [Work with larger data sets](#python-apis)   using Apache Spark  + [Add visualizations](#visualizations)  + [Automate your workload](#jobs)   as a job  + [Use machine learning](#machine-learning)   to analyze your data  + [Develop in IDEs](#ides-developer-tools-and-apis)      Tutorials -----------     The below tutorials provide example code and notebooks to learn about common workflows. See  [Import a notebook](../notebooks/notebook-export-import.html#import-a-notebook)  for instructions on importing notebook examples into your workspace.      ###   Interactive data science and machine learning   * Getting started with Apache Spark DataFrames for data preparation and analytics:  [Tutorial: Work with PySpark DataFrames on Databricks](../getting-started/dataframes-python.html) * [Tutorial: End-to-end ML models on Databricks](../mlflow/end-to-end-example.html)  . For additional examples, see  [Tutorials: Get started with ML](../machine-learning/tutorial/index.html)  and the MLflow guide’s  [Quickstart Python](../mlflow/quick-start-python.html)  . * [Databricks AutoML](../machine-learning/automl/index.html)  lets you get started quickly with developing machine learning models on your own datasets. Its glass-box approach generates notebooks with the complete machine learning workflow, which you may clone, modify, and rerun.     ###   Data engineering   * [Tutorial: Work with PySpark DataFrames on Databricks](../getting-started/dataframes-python.html)  provides a walkthrough to help you learn about Apache Spark DataFrames for data preparation and analytics. * [Tutorial: Delta Lake](../delta/tutorial.html)  . * [Tutorial: Declare a data pipeline with Python in Delta Live Tables](../delta-live-tables/tutorial-python.html)  . * [Tutorial: Declare a data pipeline with SQL in Delta Live Tables](../delta-live-tables/tutorial-sql.html)  . * [Tutorial: Run your first Delta Live Tables pipeline](../delta-live-tables/tutorial-pipelines.html)  .     ###   Production machine learning and machine learning operations   * [MLflow Model Registry example](../mlflow/model-registry-example.html) * [Tutorial: End-to-end ML models on Databricks](../mlflow/end-to-end-example.html)       ###   Debug in Python notebooks     The example notebook illustrates how to use the Python debugger (pdb) in Databricks notebooks. To use the Python debugger, you must be running Databricks Runtime 11.2 or above.       With Databricks Runtime 12.1 and above, you can use  [variable explorer](../notebooks/notebooks-code.html#variable-explorer)  to track the current value of Python variables in the notebook UI. You can use variable explorer to observe the values of Python variables as you step through breakpoints.        ####   Python debugger example notebook     [Open notebook in new tab](/_extras/notebooks/source/python-debugger.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 Note      `breakpoint()`  is  [not supported in IPython](https://github.com/ipython/ipykernel/issues/897)   and thus does not work in Databricks notebooks. You can use  `import     pdb;     pdb.set\_trace()`  instead of  `breakpoint()`  .           Reference -----------     The below subsections list key features and tips to help you begin developing in Databricks with Python.      ###   Python APIs     Python code that runs outside of Databricks can generally run within Databricks, and vice versa. If you have existing code, just import it into Databricks to get started. See  [Manage code with notebooks and Databricks Repos](#manage-code-with-notebooks-and-databricks-repos)  below for details.       Databricks can run both single-machine and distributed Python workloads. For single-machine computing, you can use Python APIs and libraries as usual; for example, pandas and scikit-learn will “just work.” For distributed Python workloads, Databricks offers two popular APIs out of the box: the Pandas API on Spark and PySpark.      ####   Pandas API on Spark      Note       The  [Koalas open-source project](https://koalas.readthedocs.io/)   now recommends switching to the Pandas API on Spark. The Pandas API on Spark is available on clusters that run  [Databricks Runtime 10.0 (Unsupported)](../release-notes/runtime/10.0.html)  and above. For clusters that run  [Databricks Runtime 9.1 LTS](../release-notes/runtime/9.1.html)  and below, use  [Koalas](../archive/legacy/koalas.html)  instead.       [pandas](https://pandas.pydata.org)   is a Python package commonly used by data scientists for data analysis and manipulation. However, pandas does not scale out to big data.  [Pandas API on Spark](../pandas/pandas-on-spark.html)  fills this gap by providing pandas-equivalent APIs that work on Apache Spark. This  [open-source API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)   is an ideal choice for data scientists who are familiar with pandas but not Apache Spark.       ####   PySpark API     PySpark is the official Python API for Apache Spark. This API provides more flexibility than the Pandas API on Spark. These links provide an introduction to and reference for PySpark.     * [Introduction to DataFrames](../getting-started/dataframes-python.html) * [Introduction to Structured Streaming](../structured-streaming/examples.html) * [PySpark API reference](https://spark.apache.org/docs/latest/api/python/)      ###   Manage code with notebooks and Databricks Repos    [Databricks notebooks](../notebooks/index.html)  support Python. These notebooks provide functionality similar to that of Jupyter, but with additions such as built-in visualizations using big data, Apache Spark integrations for debugging and performance monitoring, and MLflow integrations for tracking machine learning experiments. Get started by  [importing a notebook](../notebooks/notebook-export-import.html#import-a-notebook)  . Once you have access to a cluster, you can  [attach a notebook](../notebooks/notebook-ui.html#attach)  to the cluster and  [run the notebook](../notebooks/run-notebook.html)  .        Tip       To completely reset the state of your notebook, it can be useful to restart the iPython kernel. For Jupyter users, the “restart kernel” option in Jupyter corresponds to detaching and re-attaching a notebook in Databricks. To restart the kernel in a Python notebook, click on the cluster dropdown in the upper-left and click  **Detach & Re-attach**   . This detaches the notebook from your cluster and reattaches it, which restarts the Python process.       [Databricks Repos](../repos/index.html)  allows users to synchronize notebooks and other files with Git repositories. Databricks Repos helps with code versioning and collaboration, and it can simplify importing a full repository of code into Databricks, viewing past notebook versions, and integrating with IDE development. Get started by  [cloning a remote Git repository](../repos/git-operations-with-repos.html)  . You can then open or create notebooks with the repository clone,  [attach the notebook](../notebooks/notebook-ui.html#attach)  to a cluster, and  [run the notebook](../notebooks/run-notebook.html)  .       ###   Clusters and libraries     Databricks  [Clusters](../clusters/index.html)  provide compute management for clusters of any size: from single node clusters up to large clusters. You can customize cluster hardware and libraries according to your needs. Data scientists will generally begin work either by  [creating a cluster](../clusters/configure.html)  or using an existing  [shared cluster](../clusters/clusters-manage.html#control-access-to-clusters)  . Once you have access to a cluster, you can  [attach a notebook](../notebooks/notebook-ui.html#attach)  to the cluster or  [run a job](../workflows/jobs/jobs.html#create-a-job)  on the cluster.     * For small workloads which only require single nodes, data scientists can use  [Single Node clusters](../clusters/single-node.html)  for cost savings. * For detailed tips, see  [Best practices: Cluster configuration](../clusters/cluster-config-best-practices.html) * Administrators can set up  [cluster policies](../administration-guide/clusters/policies.html)  to simplify and guide cluster creation.     Databricks clusters use a Databricks Runtime, which provides many popular libraries out-of-the-box, including Apache Spark, Delta Lake, pandas, and more. You can also install additional third-party or custom Python libraries to use with notebooks and jobs.     * Start with the default libraries in the  [Databricks Runtime](../runtime/dbr.html)  . Use the  [Introduction to Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  for machine learning workloads. For full lists of pre-installed libraries, see  [Databricks runtime releases](../release-notes/runtime/releases.html)  . * Customize your environment using  [Notebook-scoped Python libraries](../libraries/notebooks-python-libraries.html)  , which allow you to modify your notebook or job environment with libraries from PyPI or other repositories. The  `%pip     install     my\_library`  magic command installs  `my\_library`  to all nodes in your currently attached cluster, yet does not interfere with other workloads on shared clusters. * Install non-Python libraries as  [Cluster libraries](../libraries/cluster-libraries.html)  as needed. * For more details, see  [Libraries](../libraries/index.html)  .     ###   Visualizations     Databricks Python notebooks have built-in support for many types of  [visualizations](../visualizations/index.html)  . You can also use  [legacy visualizations](../visualizations/legacy-visualizations.html#visualizations-in-python)  .       You can also visualize data using third-party libraries; some are pre-installed in the Databricks Runtime, but you can install custom libraries as well. Popular options include:     * [Bokeh](../visualizations/bokeh.html) * [Matplotlib](../visualizations/matplotlib.html) * [Plotly](../visualizations/plotly.html)     ###   Jobs     You can automate Python workloads as scheduled or triggered  [Create, run, and manage Databricks Jobs](../workflows/jobs/jobs.html)  in Databricks	To install a Python library to be available to all notebooks for all users that run on a Databricks Runtime for Machine Learning cluster, you can customize your environment using Notebook-scoped Python libraries, which allow you to modify your notebook or job environment with libraries from PyPI or other repositories. The `%pip install my_library` magic command installs `my_library` to all nodes in your currently attached cluster, yet does not interfere with other workloads on shared clusters. You can also install non-Python libraries as Cluster libraries as needed. For more details, see Libraries.	1
no_reaction	['https://docs.databricks.com/sql/language-manual/parameters/max_partition_bytes.html']	Q88- Which configuration parameter directly affects the size of a spark-partition upon ingestion of delta  A. spark.sql.files.maxPartitionBytes B. spark.sql.files.openCostInBytes C. spark.sql.adaptive.advisoryPartitionSizeInBytes D. spark.sql.adaptive.coalescePartitions.minPartitionllum E. spark.sql.autoBroadcast.JoinThreshold 	    MAX\_FILE\_PARTITION\_BYTES =============================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL       The  `MAX\_FILE\_PARTITION\_BYTES`  configuration parameter controls the maximum size of partitions when reading from a file data source. This affects the degree of parallelism for processing of the data source.        Settings ----------     The setting can be any positive integral number and optionally include a measure such as  `b`  (bytes),  `k`  or  `kb`  (1024 bytes).       You can set this parameter at the session level using the  [SET statement](../sql-ref-syntax-aux-conf-mgmt-set.html)  .         System default ----------------     The system default is  `128m`  (128 mebibytes).         Examples ----------      ``` > SET max\_file\_partition\_bytes = 64mb;  ```        Related ---------   * [RESET](../sql-ref-syntax-aux-conf-mgmt-reset.html) * [SET statement](../sql-ref-syntax-aux-conf-mgmt-set.html)                                                      	The configuration parameter that directly affects the size of a spark-partition upon ingestion of delta is A. spark.sql.files.maxPartitionBytes.	1
no_reaction	['https://kb.databricks.com/data-sources/kafka-client-term-offsetoutofrange']	read from kafka topic. 	"    Stream processing with Apache Kafka and Databricks ====================================================     This article describes how you can use Apache Kafka as either a source or a sink when running Structured Streaming workloads on Databricks.       For more Kafka, see the  [Kafka documentation](https://kafka.apache.org/documentation/)   .        Read data from Kafka ----------------------     The following is an example for reading data from Kafka:        ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""subscribe"", ""<topic>"")   .option(""startingOffsets"", ""latest"")   .load() )  ```        Write data to Kafka ---------------------     The following is an example for writing data to Kafka:        ``` (df.writeStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""topic"", ""<topic>"")   .option(""checkpointLocation"", ""<checkpoint\_path>"")   .start() )  ```            Configure Kafka for Apache Spark on Databricks ------------------------------------------------     Databricks provides the  `kafka`  keyword as a data format to configure connections to Kafka 0.10+.       The following are the most common configurations for Kafka:       There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:           |   Option   |   Value   |   Description   | | --- | --- | --- | |   subscribe   |   A comma-separated list of topics.   |   The topic list to subscribe to.   | |   subscribePattern   |   Java regex string.   |   The pattern used to subscribe to topic(s).   | |   assign   |   JSON string  `{""topicA"":[0,1],""topic"":[2,4]}`  .   |   Specific topicPartitions to consume.   |     Other notable configurations:            |   Option   |   Value   |   Default Value   |   Description   | | --- | --- | --- | --- | |   kafka.bootstrap.servers   |   Comma-separated list of host:port.   |   empty   |   [Required] The Kafka  `bootstrap.servers`  configuration. If you find there is no data from Kafka, check the broker address list first. If the broker address list is incorrect, there might not be any errors. This is because Kafka client assumes the brokers will become available eventually and in the event of network errors retry forever.   | |   failOnDataLoss   |  `true`  or  `false`  .   |  `true`  |   [Optional] Whether to fail the query when it’s possible that data was lost. Queries can permanently fail to read data from Kafka due to many scenarios such as deleted topics, topic truncation before processing, and so on. We try to estimate conservatively whether data was possibly lost or not. Sometimes this can cause false alarms. Set this option to  `false`  if it does not work as expected, or you want the query to continue processing despite data loss.   | |   minPartitions   |   Integer >= 0, 0 = disabled.   |   0 (disabled)   |   [Optional] Minimum number of partitions to read from Kafka. You can configure Spark to use an arbitrary minimum of partitions to read from Kafka using the  `minPartitions`  option. Normally Spark has a 1-1 mapping of Kafka topicPartitions to Spark partitions consuming from Kafka. If you set the  `minPartitions`  option to a value greater than your Kafka topicPartitions, Spark will divvy up large Kafka partitions to smaller pieces. This option can be set at times of peak loads, data skew, and as your stream is falling behind to increase processing rate. It comes at a cost of initializing Kafka consumers at each trigger, which may impact performance if you use SSL when connecting to Kafka.   | |   kafka.group.id   |   A Kafka consumer group ID.   |   not set   |   [Optional] Group ID to use while reading from Kafka. Use this with caution. By default, each query generates a unique group ID for reading data. This ensures that each query has its own consumer group that does not face interference from any other consumer, and therefore can read all of the partitions of its subscribed topics. In some scenarios (for example, Kafka group-based authorization), you may want to use specific authorized group IDs to read data. You can optionally set the group ID. However, do this with extreme caution as it can cause unexpected behavior.  * Concurrently running queries (both, batch and streaming) with the same group ID are likely interfere with each other causing each query to read only part of the data. * This may also occur when queries are started/restarted in quick succession. To minimize such issues, set the Kafka consumer configuration  `session.timeout.ms`  to be very small.  | |   startingOffsets   |   earliest , latest   |   latest   |   [Optional] The start point when a query is started, either “earliest” which is from the earliest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.   |     See  [Structured Streaming Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)   for other optional configurations.         Schema for Kafka records --------------------------     The schema of Kafka records is:          |   Column   |   Type   | | --- | --- | |   key   |   binary   | |   value   |   binary   | |   topic   |   string   | |   partition   |   int   | |   offset   |   long   | |   timestamp   |   long   | |   timestampType   |   int   |     The  `key`  and the  `value`  are always deserialized as byte arrays with the  `ByteArrayDeserializer`  . Use DataFrame operations (such as  `cast(""string"")`  ) to explicitly deserialize the keys and values.           Retrieve Kafka metrics ------------------------      Note       Available in Databricks Runtime 8.1 and above.        You can get the average, min, and max of the number of offsets that the streaming query is behind the latest available offset among all the subscribed topics with the  `avgOffsetsBehindLatest`  ,  `maxOffsetsBehindLatest`  , and  `minOffsetsBehindLatest`  metrics. See  [Reading Metrics Interactively](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reading-metrics-interactively)   .        Note       Available in Databricks Runtime 9.1 and above.        Get the estimated total number of bytes that the query process has not consumed from the subscribed topics by examining the value of  `estimatedTotalBytesBehindLatest`  . This estimate is based on the batches that were processed in the last 300 seconds. The timeframe that the estimate is based on can be changed by setting the option  `bytesEstimateWindowLength`  to a different value. For example, to set it to 10 minutes:        ``` df = (spark.readStream   .format(""kafka"")   .option(""bytesEstimateWindowLength"", ""10m"") # m for minutes, you can also use ""600s"" for 600 seconds )  ```      If you are running the stream in a notebook, you can see these metrics under the  **Raw Data**   tab in the streaming query progress dashboard:        ``` {  ""sources"" : [ {  ""description"" : ""KafkaV2[Subscribe[topic]]"",  ""metrics"" : {  ""avgOffsetsBehindLatest"" : ""4.0"",  ""maxOffsetsBehindLatest"" : ""4"",  ""minOffsetsBehindLatest"" : ""4"",  ""estimatedTotalBytesBehindLatest"" : ""80.0""  },  } ] }  ```        Use SSL to connect Databricks to Kafka ----------------------------------------     To enable SSL connections to Kafka, follow the instructions in the Confluent documentation  [Encryption and Authentication with SSL](https://docs.confluent.io/current/kafka/authentication_ssl.html#clients)   . You can provide the configurations described there, prefixed with  `kafka.`  , as options. For example, you specify the trust store location in the property  `kafka.ssl.truststore.location`  .       Databricks recommends that you:     * Store your certificates in cloud object storage. You can restrict access to the certificates only to clusters that can access Kafka. See  [Data governance guide](../data-governance/index.html)  . * Store your certificate passwords as  [secrets](../security/secrets/secrets.html)  in a  [secret scope](../security/secrets/secret-scopes.html)  .     The following example uses object storage locations and Databricks secrets to enable an SSL connection:        ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ...)   .option(""kafka.security.protocol"", ""SASL\_SSL"")   .option(""kafka.ssl.truststore.location"", <truststore-location>)   .option(""kafka.ssl.keystore.location"", <keystore-location>)   .option(""kafka.ssl.keystore.password"", dbutils.secrets.get(scope=<certificate-scope-name>,key=<keystore-password-key-name>))   .option(""kafka.ssl.truststore.password"", dbutils.secrets.get(scope=<certificate-scope-name>,key=<truststore-password-key-name>)) )  ```                                                       "	This document describes how to use Apache Kafka as either a source or a sink when running Structured Streaming workloads on Databricks. It provides examples for reading and writing data to Kafka, as well as common configurations for Kafka. It also describes the schema of Kafka records and how to retrieve Kafka metrics. Additionally, it explains how to use SSL to connect Databricks to Kafka.	1
no_reaction	['https://docs.databricks.com/dev-tools/api/latest/scim/index.html']	workspace scim	    SCIM API 2.0 ==============     This article describes how to use the Databricks SCIM APIs to provision users, service principals, and groups to Databricks.      [SCIM](http://www.simplecloud.info/)   , or System for Cross-domain Identity Management, is an open standard that allows you to automate user provisioning. Databricks supports both UI-based SCIM provisioning and provisioning using REST APIs and JSON. The Databricks SCIM API follows version 2.0 of the SCIM protocol.       For UI-based SCIM provisioning setup, see  [Sync users and groups from your identity provider](../../../../administration-guide/users-groups/scim/index.html)  .              Account-level and workspace-level SCIM provisioning -----------------------------------------------------     You can either configure one SCIM provisioning connector from your identity provider to your Databricks account, using account-level SCIM provisioning, or configure separate SCIM provisioning connectors to each workspace, using workspace-level SCIM provisioning.     * **Account-level SCIM provisioning:**   Databricks recommends you use account-level SCIM provisioning to create, update, and delete all users from the account. You will manage the assignment of users and groups to workspaces within Databricks. Your workspaces must be  [enabled for identity federation](../../../../administration-guide/users-groups/index.html#enable-identity-federation)  , in order to manage the assignment of users to workspaces. Databricks recommends you assign users to workspaces using the  [workspace assignment API]( https://api-docs.databricks.com/rest/latest/permission-assignment-account-api.html)   .    ![Account-level SCIM diagram](../../../../_images/account-level-scim-diagram.png)  * **Workspace-level SCIM provisioning (public preview):**   If none of your workspaces are enabled for  [identity federation](../../../../administration-guide/users-groups/index.html#enable-identity-federation)  , or if you have a mix of workspaces, some enabled for identity federation and others not, you must manage account-level and workspace-level SCIM provisioning in parallel. In a mixed scenario, you don’t need workspace-level SCIM provisioning for any workspaces that are enabled for identity federation.       If you already have workspace-level SCIM provisioning set up for workspaces that you are enabling for identity federation, we recommend that you set up account-level SCIM provisioning and turn off the workspace-level SCIM provisioner. See  [Migrate workspace-level SCIM provisioning to the account level](../../../../administration-guide/users-groups/scim/index.html#migrate)  .     To manage account-level SCIM provisioning for users, service principals, and groups using the REST API, see:     * [SCIM API 2.0 (Accounts)](account-scim.html)     To manage workspace-level SCIM provisioning (public preview) using the REST API, see:     * [SCIM API 2.0 (Me) for workspaces](scim-me.html) * [SCIM API 2.0 (Users) for workspaces](scim-users.html) * [SCIM API 2.0 (Groups) for workspaces](scim-groups.html) * [SCIM API 2.0 (ServicePrincipals) for workspaces](scim-sp.html)     For more information about workspace-level vs account-level SCIM provisioning, see  [Sync users and groups from your identity provider](../../../../administration-guide/users-groups/scim/index.html)  .           Requirements --------------   * Your Databricks account must have the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * To provision and manage  [service principals](scim-sp.html)  to a Databricks workspace, the workspace must be on the  [E2 version of the Databricks platform](../../../../getting-started/overview.html#e2-architecture)  . For information about creating E2 workspaces, see  [Create a workspace using the account console](../../../../administration-guide/workspace/create-workspace.html)  . All new Databricks accounts and most existing accounts are now E2. If you are not sure which account type you have, contact your Databricks representative.       SCIM 2.0 APIs ---------------     A Databricks account admins can invoke the account-level SCIM API endpoints:     * [SCIM API 2.0 (Accounts)](account-scim.html)     A Databricks  [workspace admin](../../../../administration-guide/users-groups/users.html)  can invoke all workspace-level SCIM API endpoints:     * [SCIM API 2.0 (Me) for workspaces](scim-me.html) * [SCIM API 2.0 (Users) for workspaces](scim-users.html) * [SCIM API 2.0 (ServicePrincipals) for workspaces](scim-sp.html) * [SCIM API 2.0 (Groups) for workspaces](scim-groups.html)     Non-admin users and service principals can invoke the workspace-level  [Me](scim-me.html)  Get endpoint, the workspace-level  [Users](scim-users.html)  Get endpoint to display names and IDs, and the workspace-level  [Group](scim-groups.html)  Get endpoint to display group display names and IDs.       For error codes, see  [SCIM API 2.0 Error Codes](scim-errors.html)  .      ###   Call account-level SCIM APIs     You must be an account admin to call  [SCIM API 2.0 (Accounts)](account-scim.html)  .       You call the account-level SCIM API on the  `accounts.cloud.databricks.com`  domain, not your workspace domain. For example:        ``` https://accounts.cloud.databricks.com/api/2.0/accounts/{account\_id}/scim/v2/Users/{user\_id}  ```      ###   Call workspace-level SCIM APIs      Preview       This feature is in  [Public Preview](../../../../release-notes/release-types.html)  .        To call workspace-level SCIM APIs, replace  `<databricks-instance>`  with the  [workspace URL](../../../../workspace/workspace-details.html#workspace-url)  of your Databricks deployment.        ``` https://<databricks-instance>/api/2.0/preview/scim/v2/<api-endpoint>  ```      ###   Header parameters         |   Parameter   |   Type   |   Description   | | --- | --- | --- | |   Authorization (required)     Or:     The  `.netrc`  file (if using  `curl`  )   |  `STRING`  |   Set to  `Bearer     <access-token>`  .     See  [Authentication using Databricks personal access tokens](../authentication.html)  to learn how to generate tokens using the UI and  [Token API 2.0](../tokens.html)  to learn how to generate tokens using the API.    **Important!**   The Databricks admin user who generates this token should not be managed by your identity provider (IdP). A Databricks admin user who is managed by the IdP can be deprovisioned using the IdP, which would cause your SCIM provisioning integration to be disabled.     Instead of an  `Authorization`  header, you can use the  [.netrc](https://everything.curl.dev/usingcurl/netrc)   file along with the  `--netrc`  (or  `-n`  ) option. This file stores machine names and tokens separate from your code and reduces the need to type credential strings multiple times. The  `.netrc`  contains one entry for each combination of  `<databricks-instance>`  and token. For example:    `machine     <databricks-instance>     login     token     password     <access-token>`  | |   Content-Type (required for write operations)   |  `STRING`  |   Set to  `application/scim+json`  .   | |   Accept (required for read operations)   |  `STRING`  |   Set to  `application/scim+json`  .   |        Filter results ----------------     Use filters to return a subset of users or groups. For all users, the user  `userName`  and group  `displayName`  fields are supported. Admin users can filter users on the  `active`  attribute.           |   Operator   |   Description   |   Behavior   | | --- | --- | --- | |   eq   |   equals   |   Attribute and operator values must be identical.   | |   ne   |   not equal to   |   Attribute and operator values are not identical.   | |   co   |   contains   |   Operator value must be a substring of attribute value.   | |   sw   |   starts with   |   Attribute must start with and contain operator value.   | |   and   |   logical AND   |   Match when all expressions evaluate to true.   | |   or   |   logical OR   |   Match when any expression evaluates to true.   |       Sort results --------------     Sort results using the  `sortBy`  and  `sortOrder` [query parameters](https://tools.ietf.org/html/rfc7644#section-3.4.2.3)   . The default is to sort by ID.                                                        	The context is about using the Databricks SCIM APIs to provision users, service principals, and groups to Databricks. It describes the account-level and workspace-level SCIM provisioning, requirements, SCIM 2.0 APIs, and how to call account-level and workspace-level SCIM APIs.	1
no_reaction	['https://docs.databricks.com/data-sharing/index.html']	what's delta sharing?	      Delta Sharing ===============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data.       There are three components to Delta Sharing:     * **[Providers](#providers)**      A provider is an entity which has made data available for sharing. * **[Shares](#shares)**      A share defines a logical grouping for the tables you intend to share. * **[Recipients](#recipients)**      A recipient identifies an organization with which you want to share any number of shares.     For a detailed guide on how to use Delta Sharing see  [Share data securely using Delta Sharing](../../data-sharing/index.html)  .        Providers -----------    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.1 and above       A data provider is an object representing the organization in the real world who shares the data. A provider contains shares which further contain the shared data. Once a provider has added you as a  **[Recipients](#recipients)**  you can     * Customize the local name of the provider using  [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html)  . * List the sets of data shared with you with  [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html)  . * Create a catalog using shares of interest to you with  [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html)  .    ###   Examples      ``` -- Change the data provider name locally. > ALTER PROVIDER `Center for Disease Control` RENAME TO cdc;  -- List the shares the provider has granted you access too. > SHOW SHARES IN PROVIDER cdc;  vaccinedata  -- Make the share accessible locally as a catalog. > CREATE CATALOG cdcdata USING cdc.vaccinedata;  -- Use the data. > USE CATALOG cdcdata; > SELECT COUNT(\*) FROM information\_schema.tables;  10  ```      ###   Related articles   * [DESCRIBE PROVIDER](sql-ref-syntax-aux-describe-provider.html) * [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html) * [DROP PROVIDER](sql-ref-syntax-ddl-drop-provider.html) * [SHOW PROVIDERS](sql-ref-syntax-aux-show-providers.html) * [COMMENT ON PROVIDER](sql-ref-syntax-ddl-comment.html) * [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html) * [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Shares --------     A share is a container instantiated with the  [CREATE SHARE](sql-ref-syntax-ddl-create-share.html)  command. Once created you can iteratively register a collection of existing tables defined within the metastore using the  [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html)  command. You can register tables under their original name, qualified by their original schema, or provide alternate exposed names.       You must be a metastore admin or account admin to create, alter, and drop shares.        ###   Examples      ``` -- Create share `customer\_share` only if share with same name doesn't exist, with a comment. > CREATE SHARE IF NOT EXISTS customer\_share COMMENT 'This is customer share';  -- Add 2 tables to the share. -- Expose my\_schema.tab1 a different name. -- Expose only two partitions of other\_schema.tab2 > ALTER SHARE customer\_share ADD TABLE my\_schema.tab1 AS their\_schema.tab1; > ALTER SHARE customer\_share ADD TABLE other\_schema.tab2 PARTITION (c1 = 5), (c1 = 7);  -- List the content of the share > SHOW ALL IN SHARE customer\_share;  name type shared\_object added\_at added\_by comment partitions  ----------------- ---- ---------------------- ---------------------------- -------------------------- ------- -----------------  other\_schema.tab2 TABLE main.other\_schema.tab2 2022-01-01T00:00:01.000+0000 alwaysworks@databricks.com NULL  their\_schema.tab1 TABLE main.myschema.tab2 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com NULL (c1 = 5), (c1 = 7)  ```        ###   Related articles   * [Change data feed](../../delta/delta-change-data-feed.html) * [CREATE SHARE](sql-ref-syntax-ddl-create-share.html) * [DESCRIBE SHARE](sql-ref-syntax-aux-describe-share.html) * [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html) * [DROP SHARE](sql-ref-syntax-ddl-drop-share.html) * [SHOW SHARES](sql-ref-syntax-aux-show-shares.html) * [SHOW ALL IN SHARE](sql-ref-syntax-aux-show-all-in-share.html) * [COMMENT ON SHARE](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Recipients ------------     A recipient is an object you create using  [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html)  to represent an organization which you want to allow access shares. When you create a recipient Databricks SQL generates an activation link you can send to the organization. To retrieve the activation link after creation you use  [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html)  .       Once a recipient has been created you can give it  `SELECT`  privileges on shares of your choice using  [GRANT ON SHARE](security-grant-share.html)  .       You must be a metastore administrator to create recipients, drop recipients, and grant access to shares.        ###   Examples      ``` -- Create a recipient. > CREATE RECIPIENT IF NOT EXISTS other\_org COMMENT 'other.org';  -- Retrieve the activation link to send to other.org > DESCRIBE RECIPIENT other\_org;  name created\_at created\_by comment activation\_link active\_token\_id active\_token\_expiration\_time rotated\_token\_id rotated\_token\_expiration\_time  --------- ---------------------------- -------------------------- --------- --------------- ------------------------------------ ---------------------------- ---------------- -----------------------------  other\_org 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com other.org https://.... 0160c81f-5262-40bb-9b03-3ee12e6d98d7 9999-12-31T23:59:59.999+0000 NULL NULL  -- Choose shares that other.org has access to > GRANT SELECT ON SHARE customer\_share TO RECIPIENT other\_org;  ```        ###   Related articles   * [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html) * [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html) * [ALTER RECIPIENT](sql-ref-syntax-ddl-alter-recipient.html) * [DROP RECIPIENT](sql-ref-syntax-ddl-drop-recipient.html) * [SHOW RECIPIENTS](sql-ref-syntax-aux-show-recipients.html) * [COMMENT ON RECIPIENT](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)                                                       	Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data.	1
no_reaction	['https://docs.databricks.com/mlflow/index.html']	databrick의 mlflow만 사용가능할까요?	    April 2019 ============     These features and Databricks platform improvements were released in April 2019.        Note       Releases are staged. Your Databricks account may not be updated until up to a week after the initial release date.         MLflow on Databricks (GA) ---------------------------    **April 25, 2019**       Managed MLflow on Databricks is now generally available. MLflow on Databricks offers a hosted version of MLflow fully integrated with the Databricks security model and interactive workspace. See  [MLflow guide](../../../mlflow/index.html)  .           Delta Lake on Databricks --------------------------    **April 24, 2019**       Databricks has open sourced the  [Delta Lake](https://delta.io)   project. Delta Lake is a storage layer that brings reliability to data lakes built on HDFS and cloud storage by providing ACID transactions through optimistic concurrency control between writes and snapshot isolation for consistent reads during writes. Delta Lake also provides built-in data versioning for easy rollbacks and reproducing reports.        Note       What was previously called Databricks Delta is now the Delta Lake open source project plus optimizations available on Databricks. See  [What is Delta Lake?](../../../delta/index.html)  .          MLflow runs sidebar ---------------------    **April 9 - 16, 2019: Version 2.95**       You can now view the MLflow runs and the notebook revisions that produced these runs in a sidebar next to your notebook. In the notebook’s right sidebar, click the  **Experiment**   icon  ![Experiment icon](../../../_images/experiment.png)  .       See  [Create notebook experiment](../../../mlflow/tracking.html#mlflow-notebook-experiments)  .           C5d series Amazon EC2 instance types (Beta) ---------------------------------------------    **April 9 - 16, 2019: Version 2.95**       Databricks now provides Beta support for the  [Amazon EC2 C5d series](https://aws.amazon.com/about-aws/whats-new/2018/05/introducing-amazon-ec2-c5d-instances/)   .           Databricks Runtime 5.3 (GA) -----------------------------    **April 3, 2019**       Databricks Runtime 5.3 is now generally available. Databricks Runtime 5.3 includes new Delta Lake features and upgrades, and upgraded Python, R, Java, and Scala libraries.       Major upgrades include:     * Databricks Delta time travel GA * MySQL table replication to Delta, Public Preview * Notebook-scoped library improvements * New Databricks Advisor hints     For details, see  [Databricks Runtime 5.3 (Unsupported)](../../runtime/5.3.html)  .           Databricks Runtime 5.3 ML (GA) --------------------------------    **April 3, 2019**       With Databricks Runtime 5.3 for Machine Learning, we have achieved our first GA of Databricks Runtime ML! Databricks Runtime ML provides a ready-to-go environment for machine learning and data science. It builds on Databricks Runtime and adds many popular machine learning libraries, including TensorFlow, PyTorch, Keras, and XGBoost. It also supports distributed training using Horovod.       This version is built on Databricks Runtime 5.3, with additional libraries, some different library versions, and Conda package management for Python libraries. Major new features since Databricks Runtime 5.2 ML Beta include:     * MLlib integration with MLflow (Private Preview), which provides automatic logging of MLflow runs for models fit using the PySpark tuning algorithms  `CrossValidator`  and  `TrainValidationSplit`  .       If you want to participate in the preview, contact your Databricks account representative. * Upgrades to the PyArrow, Horovod, and TensorboardX libraries.       The PyArrow update adds the ability to use  `BinaryType`  when you perform Arrow-based conversion and makes it available in pandas UDF.     For more information, see  [Databricks Runtime 5.3 ML (Unsupported)](../../runtime/5.3ml.html)  . For instructions on creating a Databricks Runtime ML cluster, see  [Introduction to Databricks Runtime for Machine Learning](../../../runtime/mlruntime.html)  .                                                        	Yes, MLflow on Databricks is available and it is fully integrated with the Databricks security model and interactive workspace. It was released in April 2019 and is now generally available.	1
no_reaction	['https://docs.databricks.com/ingestion/auto-loader/index.html']	load files using autoloader parquet	    Auto Loader FAQ =================     Commonly asked questions about Databricks Auto Loader.        Does Auto Loader process the file again when the file gets appended or overwritten? -------------------------------------------------------------------------------------     Files are processed exactly once unless  `cloudFiles.allowOverwrites`  is enabled. If a file is appended to or overwritten, Databricks does not guarantee which version of the file is processed. Databricks recommends you use Auto Loader to ingest only immutable files. If this does not meet your requirements, contact your Databricks representative.         If my data files do not arrive continuously, but in regular intervals, for example, once a day, should I still use this source and are there any benefits? ------------------------------------------------------------------------------------------------------------------------------------------------------------     In this case, you can set up a  `Trigger.Once`  or  `Trigger.AvailableNow`  (available in Databricks Runtime 10.2 and later) Structured Streaming job and schedule to run after the anticipated file arrival time. Auto Loader works well with both infrequent or frequent updates. Even if the eventual updates are very large, Auto Loader scales well to the input size. Auto Loader’s efficient file discovery techniques and schema evolution capabilities make Auto Loader the recommended method for incremental data ingestion.         What happens if I change the checkpoint location when restarting the stream? ------------------------------------------------------------------------------     A checkpoint location maintains important identifying information of a stream. Changing the checkpoint location effectively means that you have abandoned the previous stream and started a new stream.         Do I need to create event notification services beforehand? -------------------------------------------------------------     No. If you choose file notification mode and provide the required permissions, Auto Loader can create file notification services for you. See  [What is Auto Loader file notification mode?](file-notification-mode.html)        How do I clean up the event notification resources created by Auto Loader? ----------------------------------------------------------------------------     You can use the  [cloud resource manager](file-notification-mode.html#cloud-resource-management)  to list and tear down resources. You can also delete these resources manually using the cloud provider’s UI or APIs.         Can I run multiple streaming queries from different input directories on the same bucket/container? -----------------------------------------------------------------------------------------------------     Yes, as long as they are not parent-child directories; for example,  `prod-logs/`  and  `prod-logs/usage/`  would not work because  `/usage`  is a child directory of  `/prod-logs`  .         Can I use this feature when there are existing file notifications on my bucket or container? ----------------------------------------------------------------------------------------------     Yes, as long as your input directory does not conflict with the existing notification prefix (for example, the above parent-child directories).         How does Auto Loader infer schema? ------------------------------------     When the DataFrame is first defined, Auto Loader lists your source directory and chooses the most recent (by file modification time) 50 GB of data or 1000 files, and uses those to infer your data schema.       Auto Loader also infers partition columns by examining the source directory structure and looks for file paths that contain the  `/key=value/`  structure. If the source directory has an inconsistent structure, for example:        ``` base/path/partition=1/date=2020-12-31/file1.json // inconsistent because date and partition directories are in different orders base/path/date=2020-12-31/partition=2/file2.json // inconsistent because the date directory is missing base/path/partition=3/file3.json  ```      Auto Loader infers the partition columns as empty. Use  `cloudFiles.partitionColumns`  to explicitly parse columns from the directory structure.         How does Auto Loader behave when the source folder is empty? --------------------------------------------------------------     If the source directory is empty, Auto Loader requires you to provide a schema as there is no data to perform inference.         When does Autoloader infer schema? Does it evolve automatically after every micro-batch? ------------------------------------------------------------------------------------------     The schema is inferred when the DataFrame is first defined in your code. During each micro-batch, schema changes are evaluated on the fly; therefore, you don’t need to worry about performance hits. When the stream restarts, it picks up the evolved schema from the schema location and starts executing without any overhead from inference.         What’s the performance impact on ingesting the data when using Auto Loader schema inference? ----------------------------------------------------------------------------------------------     You should expect schema inference to take a couple of minutes for very large source directories during initial schema inference. You shouldn’t observe significant performance hits otherwise during stream execution. If you run your code in a Databricks notebook, you can see status updates that specify when Auto Loader will be listing your directory for sampling and inferring your data schema.         Due to a bug, a bad file has changed my schema drastically. What should I do to roll back a schema change? ------------------------------------------------------------------------------------------------------------     Contact Databricks support for help.                                                        	To load files using autoloader parquet, you can use Databricks Auto Loader. Auto Loader efficiently discovers files and infers schema for incremental data ingestion. When the DataFrame is first defined, Auto Loader lists your source directory and chooses the most recent (by file modification time) 50 GB of data or 1000 files, and uses those to infer your data schema. During each micro-batch, schema changes are evaluated on the fly; therefore, you don’t need to worry about performance hits. If the source directory is empty, Auto Loader requires you to provide a schema as there is no data to perform inference.	0
no_reaction	['https://kb.databricks.com/security/set-an-unlimited-lifetime-for-service-principal-access-token']	"how to increase lifetime of the token. ""The access token will expire in one hour. You must request a new OAuth access token after the expiration."""	"Title: Set an unlimited lifetime for service principal access token  Description: Configure an extended or unlimited lifetime for a service principal access token.  Answer: Info This article applies to clusters using Databricks Runtime 11.2 and above on AWS. You are using Databricks service principals to delegate permissions to automated tools and systems. In order to secure the service principals, Databricks recommends creating an access token for the service principal. Please review the create a Databricks access token for a Databricks service principal documentation for more information. You should also review the manage service principals and personal access tokens documentation which covers using service principals with Partner Connect. By default, the access token has a limited lifespan, defined in seconds. This is defined in the JSON block when calling the Token Management API to create the access token for the service principal. { ""application_id"": ""&lt;application-id&gt;"", ""comment"": ""&lt;comment&gt;"", ""lifetime_seconds"": 1209600 } Instructions There are some use cases, such as setting up an automation pipeline, where you may want the service principal to have an access token with a long expiration. Warning Although it is possible to configure service principals with access tokens that do not expire, it is not recommended for standard use cases. Access tokens that do not expire do not follow security best practices. They may also cause issues during a compliance audit. Long life access token To configure a security principal access token for a long life, determine the number of days the token needs to be active and multiply it by 86400. 86400 is the number of seconds in one day. For example, if you want a security token to last for 30 days, set the lifetime to 2592000 (30 x 86400) seconds when creating the security token. { ""application_id"": ""&lt;application-id&gt;"", ""comment"": ""&lt;comment&gt;"", ""lifetime_seconds"": 2592000 } Unlimited life access token To configure a security principal access token that does not expire, set the lifetime to -1 seconds when creating the security token. { ""application_id"": ""&lt;application-id&gt;"", ""comment"": ""&lt;comment&gt;"", ""lifetime_seconds"": -1 }  "	To increase the lifetime of a token, you can configure a security principal access token with a longer lifespan or an unlimited lifespan. To configure a security principal access token for a long life, determine the number of days the token needs to be active and multiply it by 86400. 86400 is the number of seconds in one day. For example, if you want a security token to last for 30 days, set the lifetime to 2592000 (30 x 86400) seconds when creating the security token. To configure a security principal access token that does not expire, set the lifetime to -1 seconds when creating the security token. However, it is not recommended to configure access tokens that do not expire for standard use cases as they do not follow security best practices and may cause issues during a compliance audit.	1
no_reaction	['https://docs.databricks.com/security/privacy/hipaa.html']	how to check if the workspaces in a specific account is HIPAA enabled ?	    HIPAA compliance features ===========================      Important       If you are an existing HIPAA customer and your account is not on the E2 version of the Databricks platform, and you need to create and verify a cluster for legacy HIPAA support, see  [Create and verify a cluster for legacy HIPAA support](hipaa-legacy-cluster.html)  .        HIPAA compliance features requires enabling the  *compliance security profile*   , which adds monitoring agents, enforces instance types for inter-node encryption, provides a hardened compute image, and other features. For technical details, see  [Enable the compliance security profile](security-profile.html)  . It is your responsibility to  [confirm that each workspace has the compliance security profile enabled](security-profile.html#verify)  .       To use the compliance security profile, your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the  [pricing page](https://databricks.com/product/aws-pricing)   .       This feature requires your account to be on the Enterprise tier.       The data plane enhancements that are discussed in this document apply only to the  [Classic data plane in your AWS account](../../getting-started/overview.html)  . The additional security controls and monitoring do not apply to  [serverless compute](../../serverless-compute/index.html)  , which runs compute resources in the  [serverless data plane](../../serverless-compute/index.html#compare)  in your Databricks account. For example, these new controls apply to pro and classic SQL warehouses, but do not apply to serverless SQL warehouses.        HIPAA Overview ----------------     The Health Insurance Portability and Accountability Act of 1996 (HIPAA), and the Health Information Technology for Economic and Clinical Health (HITECH) and the regulations issued under HIPAA are a set of US healthcare laws. Among other provisions, these laws establish requirements for the use, disclosure, and safeguarding of protected health information (PHI).       HIPAA applies to  [covered entities and business associates](https://www.hhs.gov/hipaa/for-professionals/covered-entities/index.html)   that create, receive, maintain, transmit, or access PHI. When a covered entity or business associate engages the services of a cloud service provider (CSP), such as Databricks, the CSP becomes a business associate under HIPAA.       HIPAA regulations require that covered entities and their business associates enter into a contract called a Business Associate Agreement (BAA) to ensure the business associates will protect PHI adequately. Among other things, a BAA establishes the permitted and required uses and disclosures of PHI by the business associate, based on the relationship between the parties and the activities and services being performed by the business associate.         Does Databricks permit the processing of PHI data on Databricks? ------------------------------------------------------------------     Databricks permits the processing of PHI data under the condition of a signed agreement. Contact your Databricks representative for more information.                 Configure your account and workspace for HIPAA on E2 ------------------------------------------------------     If you are an existing HIPAA customer and your account is  **not**   on the E2 version of the Databricks platform:     * You must contact your Databricks representative to upgrade your account to the E2 version of the platform. * Note that the E2 platform is a multi-tenant platform and your choice to deploy HIPAA on E2 will be treated as a waiver of any provision in your contract that would be in conflict with our ability to provide you HIPAA on E2.         When ordering, you have the option to enable HIPAA compliance features across all workspaces on an account, or only on individual workspaces       To configure your account or workspace to support processing of data regulated by the HIPAA standard, the workspace must have the  [compliance security profile](security-profile.html)  enabled. One of the steps to enable it includes contacting your Databricks representative. You will receive additional information and agreements to sign. Note that enabling HIPAA compliance features for an account is permanent and cannot be removed later.       After your Databricks account is enabled for HIPAA on E2, workspaces in the account have HIPAA compliance features for all  [E2 regions](../../resources/supported-regions.html)  . To deploy a workspace without HIPAA compliance features, you must create a separate Databricks account.        Important     * You are wholly responsible for ensuring your own compliance with all applicable laws and regulations. Information provided in Databricks online documentation does not constitute legal advice, and you should consult your legal advisor for any questions regarding regulatory compliance. * Databricks does not support the use of preview features for the processing of PHI on the HIPAA on E2 platform, with the exception of the features listed in  [Preview features that are supported for processing of PHI data](#supported-preview-features)  .          Preview features that are supported for processing of PHI data ----------------------------------------------------------------     The following preview features are supported for processing of PHI:     * [Customer-managed keys for managed services](../keys/customer-managed-keys-managed-services-aws.html) * [Store interactive notebook results in customer account](../../administration-guide/workspace/settings/notebook-results.html)  . * [SCIM provisioning](../../dev-tools/api/latest/scim/index.html) * [Cluster policies](../../administration-guide/clusters/policies.html) * [Service Principals and OBO tokens](../../administration-guide/users-groups/service-principals.html) * [IAM passthrough](../../data-governance/credential-passthrough/iam-passthrough.html) * [Deliver and access billable usage logs](../../administration-guide/account-settings-e2/usage.html) * [Secret paths in environment variables](../secrets/index.html)         Shared responsibility of HIPAA compliance -------------------------------------------     Complying with HIPAA has three major areas, with different responsibilities. While each party has numerous responsibilities, below we enumerate key responsibilities of ours, along with your responsibilities.       This article use the Databricks terminology  *control plane*   and a  *data plane*   , which are two main parts of how Databricks works:     * The Databricks  [control plane](../../getting-started/overview.html)  includes the backend services that Databricks manages in its own AWS account. * The data plane is where your data lake is processed. The  [Classic data plane](../../getting-started/overview.html)  includes an AWS VPC in your AWS account, and clusters of compute resources to process your notebooks, jobs, and pro or classic SQL warehouses.        Important       For workspaces with HIPAA compliance features enabled,  *data plane*   refers to the Classic data plane in your own AWS account. As of this release,  [serverless compute](../../serverless-compute/index.html)  features are disabled on a workspace with HIPAA compliance features enabled.     Key responsibilities of AWS include:     * Perform its obligations as a business associate under your BAA with AWS. * Provide you the EC2 machines under your contract with AWS that support HIPAA compliance. * Provide hardware-accelerated encryption at rest and in-transit encryption within the AWS Nitro Instances that is adequate under HIPAA. * Delete encryption keys and data when Databricks releases the EC2 instances.     Key responsibilities of Databricks include:     * Encrypt in-transit PHI data that is transmitted to or from the control plane. * Encrypt PHI data at rest in the control plane * Limit the set of instance types to the AWS Nitro instance types that enforce in-transit encryption and encryption at rest. For the list of supported instance types, see AWS Nitro System and HIPAA compliance features. Databricks limits the instance types both in the account console and through the API. * Deprovision EC2 instances when you indicate in Databricks that they are to be deprovisioned, for example auto-termination or manual termination, so that AWS can wipe them.     Key responsibilities of yours:     * Configure your workspace to use either  [customer-managed keys for managed services](../keys/customer-managed-keys-managed-services-aws.html)  or the  [Store interactive notebook results in customer account](../../administration-guide/workspace/settings/notebook-results.html)  feature. * Do not use preview features within Databricks to process PHI. However, it is supported to use the preview features listed in  [Preview features that are supported for processing of PHI data](#supported-preview-features) * Follow  [security](../index.html)  best practices, such as disable unnecessary egress from the data plane and use the Databricks  [secrets](../../dev-tools/api/latest/secrets.html)  feature (or other similar functionality) to store access keys that provide access to PHI. * Enter into a business associate agreement with AWS to cover all data processed within the VPC where the EC2 instances are deployed. * Do not do something within a virtual machine that would be a violation of HIPAA. For example, direct Databricks to send unencrypted PHI to an endpoint. * Ensure that all data that may contain PHI is encrypted at rest when you store it in locations that the Databricks platform may interact with. This includes setting the encryption settings on each workspace’s root S3 bucket that you create as part of workspace creation. You are responsible for ensuring the encryption (as well as performing backups) for your root S3 bucket and all other data sources. * Ensure that all data that may contain PHI is encrypted in transit between Databricks and any of your data storage locations or external locations you access from a data plane machine. For example, any APIs that you use in a notebook that might connect to external data source must use appropriate encryption on any outgoing connections. * Ensure that all data that may contain PHI is encrypted at rest when you store it in locations that the Databricks platform may interact with. This includes setting the encryption settings on each workspace’s root S3 bucket that you create as part of workspace creation. * Ensure the encryption (as well as performing backups) for your root S3 bucket and all other data sources. * Ensure that all data that may contain PHI is encrypted in transit between Databricks and any of your data storage locations or external locations you access from a data plane machine. For example, any APIs that you use in a notebook that might connect to external data source must use appropriate encryption on any outgoing connections.     Note the following about customer-managed keys:     * You can add customer-managed keys for your workspace’s root S3 bucket using the  [customer-managed keys for workspace storage](../keys/customer-managed-keys-storage-aws.html)  feature, but Databricks does not require you to do so. * As an optional part of the  [customer-managed keys for workspace storage](../keys/customer-managed-keys-storage-aws.html)  feature, you can add customer-managed keys for EBS volumes, but this is not necessary for HIPAA compliance.                                                      	To check if the workspaces in a specific account is HIPAA enabled, you need to confirm that each workspace has the compliance security profile enabled. To enable the compliance security profile, you must contact your Databricks representative. After your Databricks account is enabled for HIPAA on E2, workspaces in the account have HIPAA compliance features for all E2 regions. To deploy a workspace without HIPAA compliance features, you must create a separate Databricks account.	1
no_reaction	['https://docs.databricks.com/data/table-insights.html']	Where can I find a list of my frequently used queries?	    View frequent queries and users of a table ============================================     You can use the Insights tab in Data Explorer to view the most frequent recent queries and users of any table registered in Unity Catalog. The Insights tab reports on frequent queries and user access for the past 30 days.       This information can help you answer questions like:     * Can I trust this data? * What are some good ways to use this data? * Which users can answer my questions about this data?      Note       The queries and users listed on the Insights tab are limited to queries performed using Databricks SQL.         Before you begin ------------------     You must have the following permissions to view frequent queries and user data on the Insights tab.       In Unity Catalog:     * `SELECT`  privilege on the table. * `USE     SCHEMA`  privilege on the table’s parent schema. * `USE     CATALOG`  privilege on the table’s parent catalog.     Metastore admins have these privileges by default. See  [Manage privileges in Unity Catalog](../data-governance/unity-catalog/manage-privileges/index.html)  .       In Databricks SQL:     * `Can     View`  permissions on the queries. You will not see queries that you do not have permission to view. See  [Query access control](../security/auth-authz/access-control/query-acl.html)  .       View the Insights tab -----------------------   1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   to open Data Explorer. 2. Search for or navigate to the table you want insights on.       See  [Search for workspace assets](../search/index.html)  and  [Explore tables](explore-tables.html)  . 3. On the table page, click the  **Insights**   tab.       Queries made on the table and users who accessed the table in the past 30 days are listed in order of frequency, with the most frequent on top.    ![Insights tab showing the most frequent recent queries and users on a table](../_images/insights-tab.png)                                                     	You can find a list of your frequently used queries by going to the Insights tab in Data Explorer and selecting the table you want insights on. The Insights tab reports on frequent queries and user access for the past 30 days. The queries and users listed on the Insights tab are limited to queries performed using Databricks SQL.	1
no_reaction	['https://docs.databricks.com/delta-live-tables/testing.html']	Can delta live tables adjust source code of a running pipeline?	"      How to develop and test Delta Live Tables pipelines =====================================================     This article describes patterns you can use to develop and test Delta Live Tables pipelines. Through the pipeline settings, Delta Live Tables allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.        Use development mode to run pipeline updates ----------------------------------------------     Delta Live Tables provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:     * Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start. * Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.     Databricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.       See  [Development and production modes](updates.html#optimize-execution)  .         Specify a target schema during all development lifecycle phases -----------------------------------------------------------------     All datasets in a Delta Live Tables pipeline reference the  `LIVE`  virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the  `LIVE`  virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.       You must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.       By creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.       See  [Publish data from Delta Live Tables pipelines](publish.html)  .           Use Databricks Repos to manage Delta Live Tables pipelines ------------------------------------------------------------     Databricks recommends using Repos during Delta Live Tables pipeline development, testing, and deployment to production. Repos enables the following:     * Keeping track of how code is changing over time. * Merging changes that are being made by multiple developers. * Software development practices such as code reviews.     Databricks recommends configuring a single Git repository for all code related to a pipeline.       Each developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.       The resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.       While Repos can be used to synchronize code across environments, pipeline settings need to be kept up to date either manually or using tools like Terraform.       This workflow is similar to using Repos for CI/CD in all Databricks jobs. See  [CI/CD workflows with Git integration and Databricks Repos](../repos/ci-cd-techniques-with-repos.html)  .         Segment libraries for ingestion and transformation steps ----------------------------------------------------------     Databricks recommends isolating queries that ingest data from transformation logic that enriches and validates data. You can then organize libraries used for ingesting data from development or testing data sources in a separate directory from production data ingestion logic, allowing you to easily configure pipelines for various environments. You can then use smaller datasets for testing, accelerating development. See  [Create sample datasets for development and testing](#sample-data)  .       You can also use parameters to control data sources for development, testing, and production. See  [Control data sources with parameters](#parameters)  .       Because Delta Live Tables pipelines use the  `LIVE`  virtual schema for managing all dataset relationships, by configuring development and testing pipelines with ingestion libraries that load sample data, you can substitute sample datasets using production table names to test code. The same transformation logic can be used in all environments.           Create sample datasets for development and testing ----------------------------------------------------     Databricks recommends creating development and test datasets to test pipeline logic with both expected data and potential malformed or corrupt records. There are multiple ways to create datasets that can be useful for development and testing, including the following:     * Select a subset of data from a production dataset. * Use anonymized or artificially generated data for sources containing PII. * Create test data with well-defined outcomes based on downstream transformation logic. * Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.     For example, if you have a notebook that defines a dataset using the following code:        ``` CREATE OR REFRESH STREAMING LIVE TABLE input\_data AS SELECT \* FROM cloud\_files(""/production/data"", ""json"")  ```      You could create a sample dataset containing specific records using a query like the following:        ``` CREATE OR REFRESH LIVE TABLE input\_data AS SELECT ""2021/09/04"" AS date, 22.4 as sensor\_reading UNION ALL SELECT ""2021/09/05"" AS date, 21.5 as sensor\_reading  ```      The following example demonstrates filtering published data to create a subset of the production data for development or testing:        ``` CREATE OR REFRESH LIVE TABLE input\_data AS SELECT \* FROM prod.input\_data WHERE date > current\_date() - INTERVAL 1 DAY  ```      To use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the  `LIVE.input\_data`  dataset but is configured to include the notebook that creates the dataset specific to the environment.           Control data sources with parameters --------------------------------------     You can reference parameters set during pipeline configuration from within your libraries. These parameters are set as key-value pairs in the  **Compute > Advanced > Configurations**   portion of the pipeline settings UI. This pattern allows you to specify different data sources in different configurations of the same pipeline.       For example, you can specify different paths in development, testing, and production configurations for a pipeline using the variable  `data\_source\_path`  and then reference it using the following code:         ``` CREATE STREAMING LIVE TABLE bronze AS (  SELECT  \*,  input\_file\_name() AS source\_file\_name  FROM cloud\_files( '${data\_source\_path}', 'csv',  map(""header"", ""true"")) )  ```       ``` import dlt from pyspark.sql.functions import input\_file\_name  data\_source\_path = spark.conf.get(""data\_source\_path"")  @dlt.table def bronze():     return (spark.readStream         .format(""cloudFiles"")         .option(""cloudFiles.format"", ""csv"")         .option(""header"", True)         .load(data\_source\_path )         .select(""\*"", input\_file\_name().alias(""source\_file\_name""))     )  ```       This pattern is especially useful if you need to test how ingestion logic might handle changes to schema or malformed data during initial ingestion. You can use the identical code throughout your entire pipeline in all environments while switching out datasets.                                                            "	No, Delta Live Tables cannot adjust source code of a running pipeline. However, Delta Live Tables provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start. Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.	1
no_reaction	['https://docs.databricks.com/dev-tools/sqltools-driver.html']	can I run sql from vs code	    Use a SQL database tool =========================     You can use these tools to run SQL commands and scripts and to browse database objects in Databricks.          |   Tool   |   Use this when you want to:   | | --- | --- | |  [Databricks SQL CLI](databricks-sql-cli.html)  |   Use a command line to run SQL commands and scripts on a Databricks SQL warehouse.   | |  [Databricks Driver for SQLTools](sqltools-driver.html)  |   Use Visual Studio Code to interact with Databricks SQL warehouses in remote Databricks workspaces.   | |  [DataGrip integration with Databricks](datagrip.html)  |   Use a query console, schema navigation, smart code completion, and other features to run SQL commands and scripts and to browse database objects in Databricks.   | |  [DBeaver integration with Databricks](dbeaver.html)  |   Run SQL commands and browse database objects in Databricks by using this client software application and database administration tool.   | |  [SQL Workbench/J](../partners/bi/workbenchj.html)  |   Run SQL scripts (either interactively or as a batch) in Databricks by using this SQL query tool.   |                                                       	Yes, you can use Visual Studio Code to run SQL commands and scripts on Databricks SQL warehouses in remote Databricks workspaces by using the Databricks Driver for SQLTools.	1
no_reaction	['https://docs.databricks.com/notebooks/run-notebook.html']	How can i prevent user to not run any notebooks ?	    Run Databricks notebooks ==========================     Before you can run any cell in a notebook, you must  [attach the notebook to a cluster](notebook-ui.html#attach)  .       To run all the cells in a notebook, select  **Run All**   in the notebook toolbar.        Important       Do not use  **Run All**   if steps for  [mount and unmount](../dbfs/mounts.html)  are in the same notebook. It could lead to a race condition and possibly corrupt the mount points.        To run a single cell, click in the cell and press  **shift+enter**   . You can also run a subset of lines in a cell; see  [Run selected text](notebooks-code.html#run-selected-text)  .       To run all cells before or after a cell, use the cell actions menu  ![Cell actions](../_images/cell-actions.png)  at the far right. Click  ![Run Menu](../_images/run-menu.png)  and select  **Run All Above**   or  **Run All Below**   .  **Run All Below**   includes the cell you are in;  **Run All Above**   does not.       When a notebook is running, the icon in the notebook tab changes from  ![notebook tab icon](../_images/nb-not-running-icon.png)  to  ![running notebook tab icon](../_images/nb-running-icon.png)  . If notifications are enabled in your browser and you navigate to a different tab while a notebook is running, a notification appears when the notebook finishes.       To stop or interrupt a running notebook, select  ![the interrupt button](../_images/nb-interrupt-button.png)  in the notebook toolbar. You can also select  **Run > Interrupt execution**   , or use the keyboard shortcut  `I     I`  .        Schedule a notebook run -------------------------     To automatically run a notebook on a regular schedule,  [create a notebook job](schedule-notebook-jobs.html)  .           Run a Delta Live Tables pipeline from a notebook --------------------------------------------------     For information about starting a Delta Live Tables run from a notebook, see  [Open or run a Delta Live Tables pipeline from a notebook](notebooks-dlt-pipeline.html)  .         Notifications ---------------     Notifications alert you to certain events, such as which command is currently running and which commands are in error state. When your notebook is showing multiple error notifications, the first one will have a link that allows you to clear all notifications.      ![Notebook notifications](../_images/notification.png)    Notebook notifications are enabled by default. You can disable them in  [user settings](notebooks-manage.html#configure-notebook-settings)  .      ###   Background notifications     If you start a notebook run and then navigate away from the tab or window that the notebook is running in, a notification appears when the notebook is completed. You can disable this notification in your browser settings.         ###   Databricks Advisor     Databricks Advisor automatically analyzes commands every time they are run and displays appropriate advice in the notebooks. The advice notices provide information that can assist you in improving the performance of workloads, reducing costs, and avoiding common mistakes.      ####   View advice     A blue box with a lightbulb icon signals that advice is available for a command. The box displays the number of distinct pieces of advice.      ![Databricks advice](../_images/advice-collapsed.png)    Click the lightbulb to expand the box and view the advice. One or more pieces of advice will become visible.      ![View advice](../_images/advice-expanded.png)    Click the  **Learn more**   link to view documentation providing more information related to the advice.       Click the  **Don’t show me this again**   link to hide the piece of advice. The advice of this type will no longer be displayed. This action can be  [reversed in Editor settings](#advice-settings)  .       Click the lightbulb again to collapse the advice box.          ###   Advice settings     To enable or disable Databricks Advisor, go to  [user settings](notebooks-manage.html#configure-notebook-settings)  or click the gear icon in the expanded advice box.       Toggle the  **Turn on Databricks Advisor**   option to enable or disable advice.       The  **Reset hidden advice**   link is displayed if one or more types of advice is currently hidden. Click the link to make that advice type visible again.                                                         	To prevent users from running any notebooks, you can revoke their access to the cluster or workspace. This can be done through the Databricks UI by modifying the user's permissions.	0
no_reaction	['https://docs.databricks.com/archive/legacy/data-tab.html']	how to create database in dbfs location in sql	    Explore and create tables in DBFS ===================================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [Load data using the add data UI](../../ingestion/add-data/index.html)  ,  [Upload data to Databricks](../../ingestion/add-data/upload-data.html)  , and  [Discover and manage data using Data Explorer](../../data/index.html)  .        Access the legacy DBFS file upload and table creation UI through the  [add data UI](../../ingestion/add-data/index.html)  . Click  ![New Icon](../../_images/create-icon.png) **New > Data > DBFS**   .       You can also access the UI from notebooks by clicking  **File > Upload Data**   .       Databricks recommends using  [Data Explorer](../../data/index.html)  for an improved experience for viewing data objects and managing ACLs and the  [upload data UI](../../ingestion/add-data/upload-data.html)  to easily ingest small files into Delta Lake.        Note       Availability of some elements described in this article varies based on workspace configurations. Contact your workspace administrator or Databricks representative.           Import data -------------     If you have small data files on your local machine that you want to analyze with Databricks, you can import them to  [DBFS](../../dbfs/index.html)  using the UI.        Note       Workspace admins can disable this feature. For more information, see  [Manage data upload](../../administration-guide/workspace/settings/disable-upload-data-ui.html)  .        Files imported to DBFS using these methods are stored in  [FileStore](../../dbfs/filestore.html#filestore)  .           Create a table ----------------     You can launch the DBFS create table UI either by clicking  ![New Icon](../../_images/create-icon.png) **New**   in the sidebar or the  **DBFS**   button in the  [add data UI](../../ingestion/add-data/index.html)  . You can populate a table from files in  [DBFS](../../dbfs/index.html)  or upload files.       With the UI, you can only create  [external tables](../../lakehouse/data-objects.html#unmanaged-table)  .     1. Choose a data source and follow the steps in the corresponding section to configure the table.       If a Databricks workspace administrator has  [disabled the Upload File option](../../administration-guide/workspace/settings/disable-upload-data-ui.html)  , you do not have the option to upload files; you can create tables using one of the other data sources.      **Instructions for Upload File**      1. Drag files to the   **Files**    dropzone or click the dropzone to browse and choose files. After upload, a path displays for each file. The path will be something like   `/FileStore/tables/<filename>-<integer>.<file-type>`   . You can use this path in a notebook to read data.  2. Click   **Create Table with UI**    .  3. In the Cluster drop-down, choose a cluster. **Instructions for DBFS**      1. Select a file.  2. Click   **Create Table with UI**    .  3. In the Cluster drop-down, choose a cluster. 2. Click  **Preview Table**   to view the table. 3. In the  **Table Name**   field, optionally override the default table name. A table name can contain only lowercase alphanumeric characters and underscores and must start with a lowercase letter or underscore. 4. In the  **Create in Database**   field, optionally override the selected  `default`  database. 5. In the  **File Type**   field, optionally override the inferred file type. 6. If the file type is CSV:      1. In the   **Column Delimiter**    field, select whether to override the inferred delimiter.  2. Indicate whether to use the first row as the column titles.  3. Indicate whether to infer the schema. 7. If the file type is JSON, indicate whether the file is multi-line. 8. Click  **Create Table**   .       View databases and tables ---------------------------      Note       Workspaces with  [Data Explorer](../../data/index.html)  enabled do not have access to the legacy behavior described below.        Click  ![Data Icon](../../_images/data-icon.png) **Data**   in the sidebar. Databricks selects a running cluster to which you have access. The Databases folder displays the list of databases with the  `default`  database selected. The Tables folder displays the list of tables in the  `default`  database.      ![Create tables list](../../_images/default-database.png)    You can change the cluster from the Databases menu,  [create table UI](#create-table-ui)  , or  [view table UI](#view-table-ui)  . For example, from the Databases menu:     1. Click the  ![Down Caret](../../_images/down-caret.png)  down arrow at the top of the Databases folder. 2. Select a cluster.      ![Select cluster](../../_images/cluster-select.png)         View table details --------------------     The table details view shows the table schema and sample data.     1. Click  ![Data Icon](../../_images/data-icon.png) **Data**   in the sidebar. 2. In the Databases folder, click a database. 3. In the Tables folder, click the table name. 4. In the Cluster drop-down, optionally select another cluster to render the table preview.      ![Table details](../../_images/tables-view.png)     Note       To display the table preview, a Spark SQL query runs on the cluster selected in the  **Cluster**   drop-down. If the cluster already has a workload running on it, the table preview may take longer to load.         Delete a table using the UI -----------------------------   1. Click  ![Data Icon](../../_images/data-icon.png) **Data**   in the sidebar. 2. Click the  ![Menu Dropdown](../../_images/menu-dropdown.png)  next to the table name and select  **Delete**   .                                                      	"To create a database in DBFS (Databricks File Store) location in SQL, you can use the DBFS create table UI either by clicking ""New"" in the sidebar or the ""DBFS"" button in the add data UI. You can populate a table from files in DBFS or upload files. With the UI, you can only create external tables. Choose a data source and follow the steps in the corresponding section to configure the table. If a Databricks workspace administrator has disabled the Upload File option, you do not have the option to upload files; you can create tables using one of the other data sources."	0
no_reaction	['https://docs.databricks.com/runtime/dbr.html']	runtime versioning	        Databricks Runtime ====================     Databricks Runtime includes Apache Spark but also adds a number of components and updates that substantially improve the usability, performance, and security of big data analytics:     * [Delta Lake](../delta/index.html)  , a next-generation storage layer built on top of Apache Spark that provides ACID transactions, optimized layouts and indexes, and execution engine improvements for building data pipelines. * Installed Java, Scala, Python, and R libraries * Ubuntu and its accompanying system libraries * GPU libraries for GPU-enabled clusters * Databricks services that integrate with other components of the platform, such as notebooks, jobs, and cluster manager     For information about the contents of each runtime version, see the  [release notes](../release-notes/runtime/releases.html)  .        Runtime versioning --------------------     Databricks Runtime versions are released on a regular basis:     * **Major**   versions are represented by an increment to the version number that precedes the decimal point (the jump from 3.5 to 4.0, for example). They are released when there are major changes, some of which may not be backwards-compatible. * **Feature**   versions are represented by an increment to the version number that follows the decimal point (the jump from 3.4 to 3.5, for example). Each major release includes multiple feature releases. Feature releases are always backwards compatible with previous releases within their major release. * **Long Term Support**   versions are represented by an  **LTS**   qualifier (for example,  **3.5 LTS**   ). For each major release, we declare a “canonical” feature version, for which we provide two full years of support. See  [Databricks runtime support lifecycles](../release-notes/runtime/databricks-runtime-ver.html#runtime-support)  for more information.                                                      	The context is about Databricks Runtime, which includes Apache Spark and additional components and updates to improve the usability, performance, and security of big data analytics. Runtime versions are released regularly, with major versions representing major changes that may not be backwards-compatible, feature versions being backwards-compatible with previous releases within their major release, and Long Term Support versions providing two full years of support for a canonical feature version within each major release.	1
no_reaction	['https://docs.databricks.com/release-notes/runtime/11.x-migration.html']	how do I upgrade from dbr 10.4 to 11.3?	      Databricks Runtime 11.x migration guide =========================================     This guide helps you migrate your Databricks workloads to the latest version of Databricks Runtime 11.x. To do so, Databricks recommends that you migrate your workloads in the following order:     * If your workloads are currently running on Databricks Runtime 10.4 LTS or above, you can migrate directly to the latest version of Databricks Runtime 11.x, as described later in this article. * If your workloads are currently running on Databricks Runtime 10.3 or below, do the following:      1. Migrate to Databricks Runtime 10.4 LTS first. See the   [Databricks Runtime 10.x migration guide](10.x-migration.html)   .  2. Follow the guidance in this article to migrate from Databricks Runtime 10.4 LTS to the latest version of Databricks Runtime 11.x.      Note      *LTS*   means this version is under  *long-term support*   . See  [Long-term support (LTS) lifecycle](databricks-runtime-ver.html#lts)  .             Apache Spark migration guidance ---------------------------------     The most recent version of the Databricks Runtime is 11.3 LTS, powered by Apache Spark 3.3.0. For Spark-specific migration information, click on one or more of the following links by Databricks Runtime version to view the corresponding Apache Spark Migration Guide. For reference, Databricks Runtime 10.4 LTS is powered by Apache Spark 3.2.1.          |   Databricks Runtime version   |   Apache Spark version   | | --- | --- | |   11.0 - 11.3   |  [3.3.0](https://spark.apache.org/docs/3.3.0/migration-guide.html)   | |   10.4 LTS - 10.5   |  [3.2.1](https://spark.apache.org/docs/3.2.1/migration-guide.html)   |           Databricks Runtime behavior changes -------------------------------------     The following notable behavior changes were introduced into each of the following DBR 11.x releases.         * [Databricks Runtime 11.3 LTS](11.3.html#behavior-changes) * [Databricks Runtime 11.1](11.1.html#behavior-changes) * [Databricks Runtime 11.0](11.0.html#behavior-changes)           Databricks Runtime system environment properties, features, and libraries ---------------------------------------------------------------------------     For information about system environment properties as well as new, changed, and deprecated features and libraries in Databricks Runtime releases from Databricks Runtime 10.4 LTS to the latest version of Databricks Runtime 11.x, see the following:     * [Databricks Runtime 11.3 LTS for Machine Learning](11.3ml.html) * [Databricks Runtime 11.3 LTS](11.3.html) * [Databricks Runtime 11.2 for Machine Learning](11.2ml.html) * [Databricks Runtime 11.2 (Unsupported)](11.2.html) * [Databricks Runtime 11.1 for Machine Learning](11.1ml.html) * [Databricks Runtime 11.1 (unsupported)](11.1.html) * [Databricks Runtime 11.0 for Machine Learning (Unsupported)](11.0ml.html) * [Databricks Runtime 11.0 (Unsupported)](11.0.html) * [Databricks Runtime 10.5 for Machine Learning (Unsupported)](10.5ml.html) * [Databricks Runtime 10.5 (Unsupported)](10.5.html) * [Databricks Runtime 10.4 LTS for Machine Learning](10.4ml.html) * [Databricks Runtime 10.4 LTS](10.4.html)     Post-release maintenance updates are listed in  [Databricks runtime maintenance updates](maintenance-updates.html)  .                                                        	To upgrade from DBR 10.4 to 11.3, you can migrate directly to the latest version of Databricks Runtime 11.x. The migration process is described in the Databricks Runtime 11.x migration guide.	1
no_reaction	['https://docs.databricks.com/machine-learning/reference-solutions/hugging-face-transformer.html']	how to use hugging face?	"    Use Hugging Face Transformers for natural language processing (NLP) =====================================================================     This article shows you how to use Hugging Face Transformers for natural language processing (NLP) model training and inference. Example notebooks and code demonstrate:     * Text classification with MLflow logging * Fine-tuning a text classification model on a single GPU      Why use Hugging Face Transformers? ------------------------------------     Hugging Face Transformers pipelines encode best practices and have default models selected for the different tasks, making it easy to get started. Pipelines make it easy to use GPUs when available and allow batching of items sent to the GPU for better throughput.       For many applications, such as sentiment analysis and text summarization, pretrained models work well without any additional model training. Hugging Face has the following:     * A  [model hub](https://huggingface.co/models)   containing many pretrained models. * The  [🤗 Transformers library](https://huggingface.co/docs/transformers/index)   that supports the download and use of these models for NLP applications and fine-tuning. It is common to need both a tokenizer and a model for natural language processing tasks. * [🤗 Transformers pipelines](https://huggingface.co/docs/transformers/v4.26.1/en/pipeline_tutorial)   that have a simple interface for most natural language processing tasks.       Batch inference: Hugging Face Transformers on CPUs or GPUs ------------------------------------------------------------     You can use Hugging Face Transformers models on Spark to scale out your NLP batch applications. The following sections describe best practices for using Hugging Face Transformers pipelines:     * Using  [Pandas UDFs](../../udf/pandas.html)  to distribute the model for computation on a cluster. * Understanding and tuning performance. * Saving models using MLflow for improved governance or deployment to  [Model serving on Databricks](../model-serving/index.html)  .       Cluster requirements ----------------------     Any cluster with the Hugging Face  `transformers`  library installed can be used for batch inference. The  `transformers`  library comes preinstalled on Databricks Runtime 10.4 LTS ML and above. Many of the popular NLP models work best on GPU hardware, so you may get the best performance using recent GPU hardware unless you use a model specifically optimized for use on CPUs.         Notebook: Hugging Face Transformers inference and MLflow logging ------------------------------------------------------------------     To get started quickly with example code, this notebook is an end-to-end example for text summarization by using Hugging Face Transformers pipelines inference and MLflow logging. The subsequent sections of this article go into more detail around using Hugging Face transformers on Databricks.        ###   Hugging Face Transformers pipelines inference notebook     [Open notebook in new tab](/_extras/notebooks/source/deep-learning/hugging-face-transformers-batch-nlp.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import               ###   Use Pandas UDFs to distribute model computation     When experimenting with pretrained models you can use  [Pandas UDFs](../../udf/pandas.html)  to wrap the model and perform computation on worker CPUs or GPUs. Pandas UDFs distribute the model to each worker. For example, you can create a 🤗 Transformers pipeline for machine translation as follows:        ``` from transformers import pipeline import torch device = 0 if torch.cuda.is\_available() else -1 translation\_pipeline = pipeline(task=""translation\_en\_to\_fr"", model=""t5-base"", device=device)  ```      Setting the  `device`  in this manner ensures that GPUs are used if they are available on the cluster. While this example is for machine translation, 🤗 Transformers pipelines support a  [wide range of NLP tasks](https://huggingface.co/docs/transformers/main_classes/pipelines#natural-language-processing)   that you can easily use on Databricks.       ###   Use a Pandas UDF to run the pipeline on a Databricks cluster     Use a Pandas UDF to run the pipeline on the workers of a Spark cluster:        ``` import pandas as pd from pyspark.sql.functions import pandas\_udf @pandas\_udf('string') def translation\_udf(texts: pd.Series) -> pd.Series:   translations = [result['translation\_text'] for result in translation\_pipeline(texts.to\_list(), batch\_size=1)]   return pd.Series(translations)  ```      The Hugging Face pipelines for translation return a list of Python  `dict`  objects, each with a single key  `translation\_text`  and a value containing the translated text. This UDF extracts the translation from the results to return a Pandas series with just the translated text. If your pipeline was constructed to use GPUs by setting  `device=0`  , then Spark automatically reassigns GPUs on the worker nodes if your cluster has instances with multiple GPUs.       To use the UDF to translate a text column, you can call the UDF in a  `select`  statement:        ``` texts = [""Hugging Face is a French company based in New York City."", ""Databricks is based in San Francisco.""] df = spark.createDataFrame(pd.DataFrame(texts, columns=[""text""])) display(df.select(df.text, translation\_udf(df.text).alias('translation')))  ```         Return complex result types -----------------------------     Using Pandas UDFs you can also return more structured output. For example, in named-entity recognition, pipelines return a list of  `dict`  objects containing the entity, its span, type, and an associated score. While similar to the example for translation, the return type for the  `@pandas\_udf`  annotation is more complex in the case of named-entity recognition. You can get a sense of the return types to use through inspection of pipeline results, for example by running the pipeline on the driver. In this example, use the following code:        ``` from transformers import pipeline import torch  device = 0 if torch.cuda.is\_available() else -1 ner\_pipeline = pipeline(task=""ner"", model=""Davlan/bert-base-multilingual-cased-ner-hrl"", aggregation\_strategy=""simple"", device=device)  ner\_pipeline(texts)  ```      To yield the annotations:        ``` [[{'entity\_group': 'ORG',    'score': 0.99933606,    'word': 'Hugging Face',    'start': 0,    'end': 12},   {'entity\_group': 'LOC',    'score': 0.99967843,    'word': 'New York City',    'start': 42,    'end': 55}],  [{'entity\_group': 'ORG',    'score': 0.9996372,    'word': 'Databricks',    'start': 0,    'end': 10},   {'entity\_group': 'LOC',    'score': 0.999588,    'word': 'San Francisco',    'start': 23,    'end': 36}]]  ```      To represent this as a return type, you can use an  `array`  of  `struct`  fields, listing the  `dict`  entries as the fields of the  `struct`  :        ``` import pandas as pd from pyspark.sql.functions import pandas\_udf @pandas\_udf('array<struct<word string, entity\_group string, score float, start integer, end integer>>') def ner\_udf(texts: pd.Series) -> pd.Series:   return pd.Series(ner\_pipeline(texts.to\_list(), batch\_size=1))  display(df.select(df.texts, ner\_udf(df.texts).alias('entities')))  ```        Tune performance ------------------     There are several key aspects to tuning performance of the UDF. The first is to use each GPU effectively, which you can adjust by changing the size of batches sent to the GPU by the Transformers pipeline. The second is to make sure the DataFrame is well-partitioned to utilize the entire cluster. Finally, you may wish to cache the Hugging Face model to save model load time or ingress costs.      ###   Choose a batch size     While the UDFs described above should work out-of-the box with a  `batch\_size`  of 1, this may not use the resources available to the workers efficiently. To improve performance, tune the batch size to the model and hardware in the cluster. Databricks recommends trying various batch sizes for the pipeline on your cluster to find the best performance. Read more about  [pipeline batching](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)   and other  [performance options](https://huggingface.co/docs/transformers/performance)   in Hugging Face documentation.       Try finding a batch size that is large enough so that it drives the full GPU utilization but does not result in  `CUDA     out     of     memory`  errors. When you receive  `CUDA     out     of     memory`  errors during tuning, you need to detach and reattach the notebook to release the memory used by the model and data in the GPU.       Monitor GPU performance by viewing the live  [ganglia metrics](../../clusters/clusters-manage.html#ganglia-metrics)  for a cluster, and choosing a metric, such as  `gpu0-util`  for GPU processor utilization or  `gpu0\_mem\_util`  for GPU memory utilization.       ###   Repartition data to use all available hardware     The second consideration for performance is making full use of the hardware in your cluster. Generally a small multiple of the number of GPUs on your workers (for GPU clusters) or number of cores across the workers in your cluster (for CPU clusters) works well. Your input DataFrame may already have enough partitions to take advantage of the cluster’s parallelism. To see how manypartitions the DataFrame contains, use  `df.rdd.getNumPartitions()`  . You can repartition a DataFrame using  `repartitioned\_df     =     df.repartition(desired\_partition\_count)`      ###   Cache the model in DBFS or on mount points     If you are frequently loading a model from different or restarted clusters, you may also wish to cache the Hugging Face model in the  [DBFS root volume](/dbfs)   or on  [a mount point](../../dbfs/mounts.html)  . This can decrease ingress costs and reduce the time to load the model on a new or restarted cluster. To do this, set the  `TRANSFORMERS\_CACHE`  environment variable in your code before loading the pipeline. For example:        ``` import os os.environ['TRANSFORMERS\_CACHE'] = '/dbfs/hugging\_face\_transformers\_cache/'  ```      Alternatively, you can achieve similar results by logging the model to MLflow, as described below.          Log to MLflow ---------------     You can log 🤗 Transformers  `pipelines`  models to MLflow using  [a custom pyfunc model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models)   . These models can be used for batch or  [real-time inference](../model-serving/index.html)  . First, create a custom MLflow model that wraps the pipeline loading and inference. For example, with the machine translation example, the class could be written as follows:        ``` import mlflow from transformers import pipeline import torch "	To use Hugging Face Transformers, you can use the Transformers library that supports the download and use of pre-trained models for NLP applications and fine-tuning. It is common to need both a tokenizer and a model for natural language processing tasks. Hugging Face Transformers pipelines encode best practices and have default models selected for the different tasks, making it easy to get started. Pipelines make it easy to use GPUs when available and allow batching of items sent to the GPU for better throughput.	1
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/create-tables.html']	can i convert managed tables to external tables	"    Create tables ===============     This article introduces the concept of  *managed*   and  *external*   tables in Unity Catalog and describes how to create tables in Unity Catalog.        Note       When you create a table, be sure to reference a catalog that is governed by Unity Catalog. The catalog  `hive\_metastore`  appears in Data Explorer but is not considered governed by Unity Catalog. It is managed by your Databricks workspace’s Hive metastore. All other catalogs listed are governed by Unity Catalog.       You can use the Unity Catalog table upgrade interface to upgrade existing tables registered in the Hive metastore to Unity Catalog. See  [Upgrade tables and views to Unity Catalog](migrate.html)  .           Managed tables ----------------     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.           External tables -----------------     External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.       When you run  `DROP     TABLE`  on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a  `LOCATION`  path in your  `CREATE     TABLE`  statement. External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     To manage access to the underlying cloud storage for an external table, you must set up  [storage credentials and external locations](manage-external-locations-and-credentials.html)  .       To learn more, see  [Create an external table](#create-an-external-table)  .         Requirements --------------     You must have the  `CREATE     TABLE`  privilege on the schema in which you want to create the table, as well as the  `USE     SCHEMA`  privilege on the schema and the  `USE     CATALOG`  privilege on the parent catalog.       If you are creating an external table, see  [Create an external table](#create-an-external-table)  for additional requirements.         Create a managed table ------------------------     To create a managed table, run the following SQL command. You can also use the  [example notebook](#example-notebook-managed-table)  to create a table. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. This cannot be the  `hive\_metastore`  catalog that is created automatically for the Hive metastore associated with your Databricks workspace. * `<schema\_name>`  : The name of the schema. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column.       ``` CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> (  <column\_specification> );  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> ""   ""(""   "" <column\_specification>""   "")"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "",  ""("",  "" <column\_specification>"",  "")"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "" +  ""("" +  "" <column\_specification>"" +  "")"")  ```       You can also create a managed table by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of table full names by using  [databricks\_tables](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/tables)   .       For example, to create the table  `main.default.department`  and insert five rows into it:         ``` CREATE TABLE main.default.department (  deptcode INT,  deptname STRING,  location STRING );  INSERT INTO main.default.department VALUES  (10, 'FINANCE', 'EDINBURGH'),  (20, 'SOFTWARE', 'PADDINGTON'),  (30, 'SALES', 'MAIDSTONE'),  (40, 'MARKETING', 'DARLINGTON'),  (50, 'ADMIN', 'BIRMINGHAM');  ```       ``` spark.sql(""CREATE TABLE main.default.department ""   ""(""   "" deptcode INT,""   "" deptname STRING,""   "" location STRING""   "")""   ""INSERT INTO main.default.department VALUES ""   "" (10, 'FINANCE', 'EDINBURGH'),""   "" (20, 'SOFTWARE', 'PADDINGTON'),""   "" (30, 'SALES', 'MAIDSTONE'),""   "" (40, 'MARKETING', 'DARLINGTON'),""   "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE main.default.department "",  ""("",  "" deptcode INT,"",  "" deptname STRING,"",  "" location STRING"",  "")"",  ""INSERT INTO main.default.department VALUES "",  "" (10, 'FINANCE', 'EDINBURGH'),"",  "" (20, 'SOFTWARE', 'PADDINGTON'),"",  "" (30, 'SALES', 'MAIDSTONE'),"",  "" (40, 'MARKETING', 'DARLINGTON'),"",  "" (50, 'ADMIN', 'BIRMINGHAM')"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE main.default.department "" +  ""("" +  "" deptcode INT,"" +  "" deptname STRING,"" +  "" location STRING"" +  "")"" +  ""INSERT INTO main.default.department VALUES "" +  "" (10, 'FINANCE', 'EDINBURGH'),"" +  "" (20, 'SOFTWARE', 'PADDINGTON'),"" +  "" (30, 'SALES', 'MAIDSTONE'),"" +  "" (40, 'MARKETING', 'DARLINGTON'),"" +  "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```        ###   Example notebooks for creating managed tables     You can use the following example notebooks to create a catalog, schema, and managed table, and to manage permissions on them.        ####   Create and manage a table in Unity Catalog with SQL     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-example-notebook.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Create and manage a table in Unity Catalog with Python     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-quickstart-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                   Drop a managed table ----------------------     You must be the table’s owner to drop a table. To drop a managed table, run the following SQL command:        ``` DROP TABLE IF EXISTS catalog\_name.schema\_name.table\_name;  ```      When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.         Create an external table --------------------------     The data in an external table is stored in a path on your cloud tenant. To work with external tables, Unity Catalog introduces two objects to access and work with external cloud storage:     * A  *storage credential*   contains an authentication method for accessing a cloud storage location. The storage credential does not contain a mapping to the path to which it grants access. Storage credentials are access-controlled to determine which users can use the credential. * An  *external location*   maps a storage credential with a cloud storage path to which it grants access. The external location grants access only to that cloud storage path and its contents. External locations are access-controlled to determine which users can use them. An external location is used automatically when your SQL command contains a  `LOCATION`  clause.      ###   Requirements     To create an external table, you must have:     * The  `CREATE     EXTERNAL     TABLE`  privilege on an external location or storage credential, that grants access to the  `LOCATION`  accessed by the external table. * The  `USE     SCHEMA`  permission on the table’s parent schema. * The  `USE     CATALOG`  permission on the table’s parent catalog. * The  `CREATE     TABLE`  permission on the table’s parent schema.     External locations and storage credentials are stored in the top level of the metastore, rather than in a catalog. To create a storage credential or an external location, you must be the metastore admin or an account-level admin. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .       ###   Create a table     Use one of the following command examples in a notebook or the SQL query editor to create an external table.       You can also use an  [example notebook](#example-notebook-external-table)  to create the storage credential, external location, and external table, and also manage permissions for them.       In the following examples, replace the placeholder values:     * `<catalog>`  : The name of the catalog that will contain the table. * `<schema>`  : The name of the schema that will contain the table. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column. * `<bucket\_path>`  : The path on your cloud tenant where the table will be created. * `<table\_directory>`  : A directory where the table will be created. Use a unique directory for each table.      Important       Once a table is created in a path, users can no longer directly access the files in that path from Databricks even if they have been given privileges on an external location or storage credential to do so. This is to ensure that users cannot circumvent access controls applied to tables by reading files from your cloud tenant directly.          ``` CREATE TABLE <catalog>.<schema>.<table\_name> (  <column\_specification> ) LOCATION 's3://<bucket\_path>/<table\_directory>';  ```       ``` spark.sql(""CREATE TABLE <catalog>.<schema>.<table\_name> ""   ""(""   "" <column\_specification>""   "") ""   ""LOCATION 's"	Yes, you can convert managed tables to external tables. To do so, you can create an external table with the same schema as the managed table and then insert the data from the managed table into the external table. Alternatively, you can use the CTAS (Create Table As Select) statement to create an external table from a managed table.	0
no_reaction	['https://docs.databricks.com/getting-started/admin-get-started.html']	How do I get started with Databricks?	    Get started articles, tutorials, and best practices =====================================================     Databricks documentation includes many tutorials, Get started articles, and best practices guides.        Get started articles vs. tutorials ------------------------------------     Get started articles provide a shortcut to understanding Databricks features or typical tasks you can perform in Databricks. Most of our Get started articles are intended for new users trying out Databricks.       Tutorials are slightly more complex, prescriptive steps for typical workflows in Databricks that you can use as examples for your projects.         Video tours -------------   * [Video: Databricks Data Science and Engineering workspace](https://www.youtube.com/watch?v=YGq150jtc58&index=5) * [Video: Databricks SQL workspace](https://www.youtube.com/watch?v=zDOVr-Wxyd0&list=PLTPXxbhUt-YWPEK3NYh433T8TwrpVbyce&index=6) * [Video: Databricks Machine Learning workspace](https://www.youtube.com/watch?v=JvXg1iycvQc&list=PLTPXxbhUt-YWPEK3NYh433T8TwrpVbyce&index=7) * [Video: Notebook basics](https://youtu.be/zhUtu0J8LWg)         Get started with Databricks Data Science & Engineering --------------------------------------------------------   * [Get started: Query data from a notebook](../quick-start.html) * [Get started: Build a basic ETL pipeline](../etl-quick-start.html) * [Tutorial: Run an end-to-end lakehouse analytics pipeline](/getting-started/lakehouse-e2e.html) * [Tutorial: Build an end-to-end data pipeline](../data-pipeline-get-started.html)         Get started with Databricks Machine Learning ----------------------------------------------   * [Get started: Databricks Machine Learning in-product quickstart](../../machine-learning/tutorial/ml-quickstart.html) * [10-min tutorials: ML notebooks](../../machine-learning/tutorial/index.html) * [Get started: MLflow quickstart notebooks](../../mlflow/quick-start.html)         Get started with Databricks SQL ---------------------------------   * [Databricks SQL user quickstart: Import and explore sample dashboards](../../sql/get-started/sample-dashboards.html) * [Databricks SQL user quickstart: Run and visualize a query](../../sql/get-started/user-quickstart.html)       Best practices for Databricks -------------------------------     The Databricks documentation includes a number of best practices articles to help you get the best performance at the lowest cost when using and administering Databricks.      ###   Data science and engineering best practices   * [Delta Lake](../../delta/best-practices.html) * [Hyperparameter tuning with Hyperopt](../../machine-learning/automl-hyperparam-tuning/hyperopt-best-practices.html) * [Deep learning in Databricks](../../machine-learning/train-model/dl-best-practices.html) * [Delta Lake Structured Streaming with Amazon Kinesis](../../structured-streaming/kinesis-best-practices.html) * [CI/CD](../../dev-tools/index-ci-cd.html#dev-tools-ci-cd) * [MLOps worflows](../../machine-learning/mlops/mlops-workflow.html)     ###   Best practices for Databricks admins   * [Cluster configuration](../../clusters/cluster-config-best-practices.html) * [Pools](../../clusters/pool-best-practices.html) * [Cluster policies](../../administration-guide/clusters/policies-best-practices.html) * [Data governance](../../data-governance/best-practices.html) * [GDPR and CCPA compliance using Delta Lake](../../security/privacy/gdpr-delta.html)                                                       	To get started with Databricks, you can refer to the Get started articles, tutorials, and best practices guides available in the Databricks documentation. The Get started articles provide a shortcut to understanding Databricks features or typical tasks you can perform in Databricks, while tutorials are slightly more complex, prescriptive steps for typical workflows in Databricks that you can use as examples for your projects. Additionally, there are video tours available for Databricks Data Science and Engineering workspace, Databricks SQL workspace, Databricks Machine Learning workspace, and Notebook basics. You can also refer to the specific Get started guides for Databricks Data Science & Engineering, Databricks Machine Learning, and Databricks SQL. Finally, there are best practices articles available for both data science and engineering and Databricks admins.	1
no_reaction	['https://docs.databricks.com/security/secrets/secrets.html']	when i create a secret, who can see it?	"    Secrets =========     A secret is a key-value pair that stores secret material, with a key name unique within a  [secret scope](secret-scopes.html)  . Each scope is limited to 1000 secrets. The maximum allowed secret value size is 128 KB.       See also  [Secrets API 2.0](../../dev-tools/api/latest/secrets.html)  .        Create a secret -----------------     Secret names are case insensitive.      ###   Create a secret in a Databricks-backed scope     To create a secret in a Databricks-backed scope using the  [Databricks CLI setup & documentation](../../dev-tools/cli/index.html)  (version 0.7.1 and above):        ``` databricks secrets put --scope <scope-name> --key <key-name>  ```      An editor opens and displays content like this:        ``` # ---------------------------------------------------------------------- # Do not edit the above line. Everything that follows it will be ignored. # Please input your secret value above the line. Text will be stored in # UTF-8 (MB4) form and any trailing new line will be stripped. # Exit without saving will abort writing secret.  ```      Paste your secret value above the line and save and exit the editor. Your input is stripped of the comments and stored associated with the key in the scope.       If you issue a write request with a key that already exists, the new value overwrites the existing value.       You can also provide a secret from a file or from the command line. For more information about writing secrets, see  [Secrets CLI](../../dev-tools/cli/secrets-cli.html)  .          List secrets --------------     To list secrets in a given scope:        ``` databricks secrets list --scope <scope-name>  ```      The response displays metadata information about the secret, such as the secret key name and last updated at timestamp (in milliseconds since epoch). You use the  [Secrets utility (dbutils.secrets)](../../dev-tools/databricks-utils.html#dbutils-secrets)  in a notebook or job to read a secret. For example:        ``` databricks secrets list --scope jdbc  ```       ``` Key name    Last updated ----------  -------------- password    1531968449039 username    1531968408097  ```        Read a secret ---------------     You create secrets using the REST API or CLI, but you must use the  [Secrets utility (dbutils.secrets)](../../dev-tools/databricks-utils.html#dbutils-secrets)  in a notebook or job to read a secret.         Delete a secret -----------------     To delete a secret from a scope with the Databricks CLI:        ``` databricks secrets delete --scope <scope-name> --key <key-name>  ```      You can also use the  [Secrets API 2.0](../../dev-tools/api/latest/secrets.html)  .           Use a secret in a Spark configuration property or environment variable ------------------------------------------------------------------------      Preview       This feature is in  [Public Preview](../../release-notes/release-types.html)  .         Note       Available in Databricks Runtime 6.4 Extended Support and above.        You can reference a secret in a Spark configuration property or environment variable. Retrieved secrets are redacted from notebook output and Spark driver and executor logs.        Important       Keep the following security implications in mind when referencing secrets in a Spark configuration property or environment variable:     * If table access control is not enabled on a cluster, any user with Can Attach To permissions on a cluster or Run permissions on a notebook can read Spark configuration properties from within the notebook. This includes users who do not have direct permission to read a secret. Databricks recommends enabling  [table access control](../../administration-guide/access-control/table-acl.html)  on all clusters or managing access to secrets using  [secret scopes](secret-scopes.html#create-a-databricks-backed-secret-scope)  . * Even when table access control is enabled, users with Can Attach To permissions on a cluster or Run permissions on a notebook can read cluster environment variables from within the notebook. Databricks does not recommend storing secrets in cluster environment variables if they must not be available to all users on the cluster. * Secrets  *are not*   redacted from the Spark driver log  `stdout`  and  `stderr`  streams. By default, Spark driver logs are viewable by users with any of the following  [cluster level permissions](../auth-authz/access-control/cluster-acl.html#cluster-level-permissions)  :      + Can Attach To  + Can Restart  + Can Manage  You can optionally limit who can read Spark driver logs to users with the Can Manage permission by setting the cluster’s Spark configuration property  `spark.databricks.acl.needAdminPermissionToViewLogs     true`     ###   Requirements and limitations     The following requirements and limitations apply to referencing secrets in Spark configuration properties and environment variables:     * Cluster owners must have  **Can Read**   permission on the secret scope. * Only cluster owners can add a reference to a secret in a Spark configuration property or environment variable and edit the existing scope and name. Owners change a secret using the  [Put secret](../../dev-tools/api/latest/secrets.html#secretsecretserviceputsecret)  API. You must restart your cluster to fetch the secret again. * Users with the  **Can Manage**   permission on the cluster can delete a secret Spark configuration property or environment variable.       ###   Syntax for referencing secrets in a Spark configuration property or environment variable     You can refer to a secret using any valid variable name or Spark configuration property. Databricks enables special behavior for variables referencing secrets based on the syntax of the value being set, not the variable name.       The syntax of the Spark configuration property or environment variable value must be  `{{secrets/<scope-name>/<secret-name>}}`  . The value must start with  `{{secrets/`  and end with  `}}`  .       The variable portions of the Spark configuration property or environment variable are:     * `<scope-name>`  : The name of the scope in which the secret is associated. * `<secret-name>`  : The unique name of the secret in the scope.     For example,  `{{secrets/scope1/key1}}`  .        Note     * There should be no spaces between the curly brackets. If there are spaces, they are treated as part of the scope or secret name.        ###   Reference a secret with a Spark configuration property     You specify a reference to a secret in a  [Spark configuration property](../../clusters/configure.html#spark-configuration)  in the following format:        ``` spark.<property-name> {{secrets/<scope-name>/<secret-name>}}  ```      Any Spark configuration  `<property-name>`  can reference a secret. Each Spark configuration property can only reference one secret, but you can configure multiple Spark properties to reference secrets.      **Example**       You set a Spark configuration to reference a secret:        ``` spark.password {{secrets/scope1/key1}}  ```      To fetch the secret in the notebook and use it:         ``` spark.conf.get(""spark.password"")  ```       ``` SELECT ${spark.password};  ```       ###   Reference a secret in an environment variable     You specify a secret path in an  [environment variable](../../clusters/configure.html#environment-variables)  in the following format:        ``` <variable-name>={{secrets/<scope-name>/<secret-name>}}  ```      You can use any valid variable name when you reference a secret. Access to secrets referenced in environment variables is determined by the permissions of the user who configured the cluster. Secrets stored in environmental variables are accessible by all users of the cluster, but are redacted from plaintext display like secrets referenced elsewhere.       Environment variables that reference secrets are accessible from a  [cluster-scoped init script](../../clusters/init-scripts.html#environment-variables)  .      **Example**       You set an environment variable to reference a secret:        ``` SPARKPASSWORD={{secrets/scope1/key1}}  ```      To fetch the secret in an init script, access  `$SPARKPASSWORD`  using the following pattern:        ``` if [ -n ""$SPARKPASSWORD"" ]; then  # code to use ${SPARKPASSWORD} fi  ```                                                        "	When you create a secret in a Databricks-backed scope, only users with appropriate permissions to the scope can see it. The secret is stored associated with the key in the scope.	1
no_reaction	['https://docs.databricks.com/workflows/index.html']	how to orchestrate multi-task ml workflows using databricks jobs	    What is Databricks Workflows? ===============================     Databricks Workflows orchestrates data processing, machine learning, and analytics pipelines in the Databricks Lakehouse Platform. Workflows has fully managed orchestration services integrated with the Databricks platform, including Databricks Jobs to run non-interactive code in your Databricks workspace and Delta Live Tables to build reliable and maintainable ETL pipelines. The following diagram illustrates a workflow that is orchestrated by a Databricks job to:     1. Run a Delta Live Tables pipeline that ingests raw clickstream data from cloud storage, cleans and prepares the data, sessionizes the data, and persists the final sessionized data set to Delta Lake. 2. Run a Delta Live Tables pipeline that ingests order data from cloud storage, cleans and transforms the data for processing, and persist the final data set to Delta Lake. 3. Join the order and sessionized clickstream data to create a new data set for analysis. 4. Extract features from the prepared data. 5. Perform tasks in parallel to persist the features and train a machine learning model.    ![Diagram illustrating an example workflow](../_images/example-workflow-diagram.png)     What is Databricks Jobs? --------------------------     A Databricks job is a way to run your data processing and analysis applications in a Databricks workspace. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately, periodically through an easy-to-use scheduling system, whenever new files arrive in an external location, or continuously to ensure an instance of the job is always running. You can also run jobs interactively in the  [notebook UI](../notebooks/index.html)  .       You can create and run a job using the Jobs UI, the Databricks CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and notifications (for example, email, webhook destination, or Slack notifications).       To learn about using the Databricks CLI, see  [Jobs CLI](../dev-tools/cli/jobs-cli.html)  . To learn about using the Jobs API, see  [Jobs API 2.1](../dev-tools/api/latest/jobs.html)  .       The following sections cover important features of Databricks Jobs.        Important     * You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. * A workspace is limited to 1000 concurrent task runs. A  `429     Too     Many     Requests`  response is returned when you request a run that cannot start immediately. * The number of jobs a workspace can create in an hour is limited to 10000 (includes “runs submit”). This limit also affects jobs created by the REST API and notebook workflows.         ###   Implement data processing and analysis with job tasks     You implement your data processing and analysis workflow using  *tasks*   . A job is composed of one or more tasks. You can create job tasks that run notebooks, JARS, Delta Live Tables pipelines, or Python, Scala, Spark submit, and Java applications. Your job tasks can also orchestrate Databricks SQL queries, alerts and dashboards to create analyses and visualizations, or you can use the dbt task to run dbt transformations in your workflow. Legacy Spark Submit applications are also supported.       You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel.       ###   Run jobs interactively, continuously, or using job triggers     You can run your jobs interactively from the Jobs UI, API, or CLI or you can run a  [continuous job](jobs/jobs.html#continuous-jobs)  . You can  [create a schedule](jobs/jobs.html#job-schedule)  to run your job periodically or run your job when  [new files arrive](jobs/file-arrival-triggers.html)  in an external location such as Amazon S3 or Azure storage.       ###   Monitor job progress with notifications     You can receive notifications when a job or task starts, completes, or fails. You can send notifications to one or more email addresses or system destinations (for example, webhook destinations or Slack). See  [Add email and system notifications for job events](jobs/job-notifications.html)  .       ###   Run your jobs with Databricks compute resources     Databricks clusters and SQL warehouses provide the computation resources for your jobs. You can run your jobs with a job cluster, an all-purpose cluster, or a SQL warehouse:     * A job cluster is a dedicated cluster for your job or individual job tasks. Your job can use a job cluster that’s shared by all tasks or you can configure a cluster for individual tasks when you create or edit a task. An job cluster is created when the job or task starts and terminated when the job or task ends. * An all-purpose cluster is a shared cluster that is manually started and terminated and can be shared by multiple users and jobs.     To optimize resource usage, Databricks recommends using a job cluster for your jobs. To reduce the time spent waiting for cluster startup, consider using an all-purpose cluster. See  [Cluster configuration tips](jobs/jobs.html#cluster-config-tips)  .       You use a SQL warehouse to run Databricks SQL tasks such as queries, dashboards, or alerts. You can also use a SQL warehouse to run dbt transformations with the dbt task.       ###   Next steps     To get started with Databricks Jobs:     * Create your first Databricks jobs workflow with the  [quickstart](jobs/jobs-quickstart.html)  . * Learn how to create, view, and run workflows with the Databricks jobs  [user interface](jobs/jobs.html)  .     Learn more about building, managing, and troubleshooting workflows with Databricks Jobs:     * Learn how to communicate information between tasks in a Databricks job with  [task values](jobs/share-task-context.html)  . * Learn how to  [troubleshoot and fix failed](jobs/repair-job-failures.html)  jobs. * Get notified when your job runs start, complete or fail with  [job run notifications](jobs/job-notifications.html)  . * Learn how to run your Databricks job when new data arrives with  [file arrival triggers](jobs/file-arrival-triggers.html)  . * Learn about  [Jobs API updates](jobs/jobs-api-updates.html)  to support creating and managing workflows with Databricks jobs. * Use  [how-to guides and tutorials](jobs/how-to/index.html)  to learn more about implementing data workflows with Databricks Jobs.          What is Delta Live Tables? ----------------------------     Delta Live Tables is a framework that simplifies ETL and streaming data processing. Delta Live Tables provides efficient ingestion of data with built-in support for  [Auto Loader](../ingestion/auto-loader/index.html)  , SQL and Python interfaces that support declarative implementation of data transformations, and support for writing transformed data to Delta Lake. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.       To get started, see  [What is Delta Live Tables?](../delta-live-tables/index.html)  .           Databricks Jobs and Delta Live Tables ---------------------------------------     Databricks Jobs and Delta Live Tables provide a comprehensive framework for building and deploying end-to-end data processing and analysis workflows.       Use Delta Live Tables for all ingestion and transformation of data. Use Databricks Jobs to orchestrate workloads composed of a single task or multiple data processing and analysis tasks in the Lakehouse platform, including Delta Live Tables ingestion and transformation.       As a workflow orchestration system, Databricks Jobs also supports:     * Running jobs on a triggered basis, for example, running a workflow on a schedule. * Data analysis through SQL queries, machine learning and data analysis with notebooks, scripts, or external libraries, and so forth. * Running a job composed of a single task, for example, running an Apache Spark job packaged in a JAR.       Workflow orchestration with Apache AirFlow --------------------------------------------     Although Databricks recommends using Databricks Jobs to orchestrate your data workflows, you can also use  [Apache Airflow](https://airflow.apache.org/)   to manage and schedule your data workflows. With Airflow, you define your workflow in a Python file, and Airflow manages scheduling and running the workflow. See  [Orchestrate Databricks jobs with Apache Airflow](jobs/how-to/use-airflow-with-jobs.html)  .                                                              	To orchestrate multi-task ML workflows using Databricks Jobs, you can create a job composed of one or more tasks that run notebooks, JARS, Delta Live Tables pipelines, or Python, Scala, Spark submit, and Java applications. You can control the execution order of tasks by specifying dependencies between the tasks and configure tasks to run in sequence or parallel. Databricks Jobs manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. Additionally, Delta Live Tables provides efficient ingestion of data with built-in support for Auto Loader, SQL and Python interfaces that support declarative implementation of data transformations, and support for writing transformed data to Delta Lake. Use Delta Live Tables for all ingestion and transformation of data and use Databricks Jobs to orchestrate workloads composed of a single task or multiple data processing and analysis tasks in the Lakehouse platform, including Delta Live Tables ingestion and transformation.	1
no_reaction	['https://docs.databricks.com/notebooks/schedule-notebook-jobs.html']	How to schedule a notebook	    Create and manage scheduled notebook jobs ===========================================     You can create and manage notebook jobs directly in the notebook UI. If a notebook is already assigned to one or more jobs, you can create and manage schedules for those jobs. If a notebook is not assigned to a job, you can create a job and a schedule to run the notebook.        Schedule a notebook job -------------------------     To schedule a notebook job to run periodically:     1. In the notebook, click  ![Notebook schedule button](../_images/schedule-button.png)  at the top right. If no jobs exist for this notebook, the Schedule dialog appears.      ![Schedule notebook dialog](../_images/schedule-dialog.png)    If jobs already exist for the notebook, the Jobs List dialog appears. To display the Schedule dialog, click  **Add a schedule**   .      ![Job list dialog](../_images/job-list-dialog.png) 2. In the Schedule dialog, optionally enter a name for the job. The default name is the name of the notebook. 3. Select  **Manual**   to run your job only when manually triggered, or  **Scheduled**   to define a schedule for running the job. If you select  **Scheduled**   , use the drop-downs to specify the frequency, time, and time zone. 4. In the  **Cluster**   drop-down, select the cluster to run the task.       If you have  **Allow Cluster Creation**   permissions, by default the job runs on a  [new job cluster](../workflows/jobs/jobs.html#choose-the-correct-cluster-type-for-your-job)  . To edit the configuration of the default job cluster, click  **Edit**   at the right of the field to display the  [cluster configuration dialog](../clusters/configure.html)  .       If you do not have  **Allow Cluster Creation**   permissions, by default the job runs on the cluster that the notebook is attached to. If the notebook is not attached to a cluster, you must select a cluster from the  **Cluster**   drop-down. 5. Optionally, enter any  **Parameters**   to pass to the job. Click  **Add**   and specify the key and value of each parameter. Parameters set the value of the  [notebook widget](widgets.html)  specified by the key of the parameter. Use  [Task parameter variables](../workflows/jobs/jobs.html#task-parameter-variables)  to pass a limited set of dynamic values as part of a parameter value. 6. Optionally, specify email addresses to receive  **Alerts**   on job events. See  [Add email and system notifications for job events](../workflows/jobs/job-notifications.html)  . 7. Click  **Submit**   .       Run a notebook job --------------------     To manually run a notebook job:     1. In the notebook, click  ![Notebook schedule button](../_images/schedule-button.png)  at the top right. 2. Click  **Run now**   . 3. To view the  [job run details](../workflows/jobs/jobs.html#job-run-details)  , click  ![New Tab Icon](../_images/open-in-new-tab.png)  .       Manage scheduled notebook jobs --------------------------------     To display jobs associated with this notebook, click the  **Schedule**   button. The jobs list dialog appears, showing all jobs currently defined for this notebook. To manage jobs, click  ![Jobs Vertical Ellipsis](../_images/jobs-vertical-ellipsis.png)  at the right of a job in the list.      ![Job list menu](../_images/job-list-menu.png)    From this menu, you can edit the schedule,  [clone](../workflows/jobs/jobs.html#clone-job)  the job, view  [job run details](../workflows/jobs/jobs.html#view-job-run-list)  , pause the job, resume the job, or delete a scheduled job.       When you clone a scheduled job, a new job is created with the same parameters as the original. The new job appears in the list with the name  `Clone     of     <initial     job     name>`  .       How you edit a job depends on the complexity of the job’s schedule. Either the Schedule dialog or the  [Job details panel](../workflows/jobs/jobs.html#job-edit)  displays, allowing you to edit the schedule, cluster, parameters, and so on.                                                        	"To schedule a notebook job, click on the schedule button at the top right of the notebook and select ""Scheduled"" to define a schedule for running the job. Then, use the drop-downs to specify the frequency, time, and time zone. Finally, select the cluster to run the task and click ""Submit""."	1
no_reaction	['https://spark.apache.org/docs/latest/']	What is Spark?	"   Spark Overview ================   Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including  [Spark SQL](sql-programming-guide.html)   for SQL and structured data processing,  [pandas API on Spark](api/python/getting_started/quickstart_ps.html)   for pandas workloads,  [MLlib](ml-guide.html)   for machine learning,  [GraphX](graphx-programming-guide.html)   for graph processing, and  [Structured Streaming](structured-streaming-programming-guide.html)   for incremental computation and stream processing.     Downloading =============   Get Spark from the  [downloads page](https://spark.apache.org/downloads.html)   of the project website. This documentation is for Spark version 3.4.0. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version  [by augmenting Spark’s classpath](hadoop-provided.html)   . Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI.     If you’d like to build Spark from  source, visit  [Building Spark](building-spark.html)   .     Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86\_64 and ARM64. It’s easy to run locally on one machine — all you need is to have  `java`   installed on your system  `PATH`   , or the  `JAVA_HOME`   environment variable pointing to a Java installation.     Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+, and R 3.5+. Python 3.7 support is deprecated as of Spark 3.4.0. Java 8 prior to version 8u362 support is deprecated as of Spark 3.4.0. When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.     For Java 11, setting  `-Dio.netty.tryReflectionSetAccessible=true`   is required for the Apache Arrow library. This prevents the  `java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available`   error when Apache Arrow uses Netty internally.     Running the Examples and Shell ================================   Spark comes with several sample programs. Python, Scala, Java, and R examples are in the  `examples/src/main`   directory.     To run Spark interactively in a Python interpreter, use  `bin/pyspark`   :    ``` ./bin/pyspark --master ""local[2]""  ```   Sample applications are provided in Python. For example:    ``` ./bin/spark-submit examples/src/main/python/pi.py 10  ```   To run one of the Scala or Java sample programs, use  `bin/run-example <class> [params]`   in the top-level Spark directory. (Behind the scenes, this invokes the more general  [`spark-submit`   script](submitting-applications.html)   for launching applications). For example,    ``` ./bin/run-example SparkPi 10  ```   You can also run Spark interactively through a modified version of the Scala shell. This is a great way to learn the framework.    ``` ./bin/spark-shell --master ""local[2]""  ```   The  `--master`   option specifies the  [master URL for a distributed cluster](submitting-applications.html#master-urls)   , or  `local`   to run locally with one thread, or  `local[N]`   to run locally with N threads. You should start by using  `local`   for testing. For a full list of options, run the Spark shell with the  `--help`   option.     Since version 1.4, Spark has provided an  [R API](sparkr.html)   (only the DataFrame APIs are included). To run Spark interactively in an R interpreter, use  `bin/sparkR`   :    ``` ./bin/sparkR --master ""local[2]""  ```   Example applications are also provided in R. For example:    ``` ./bin/spark-submit examples/src/main/r/dataframe.R  ```   Running Spark Client Applications Anywhere with Spark Connect ---------------------------------------------------------------   Spark Connect is a new client-server architecture introduced in Spark 3.4 that decouples Spark client applications and allows remote connectivity to Spark clusters. The separation between client and server allows Spark and its open ecosystem to be leveraged from anywhere, embedded in any application. In Spark 3.4, Spark Connect provides DataFrame API coverage for PySpark and DataFrame/Dataset API support in Scala.     To learn more about Spark Connect and how to use it, see  [Spark Connect Overview](spark-connect-overview.html)   .     Launching on a Cluster ========================   The Spark  [cluster mode overview](cluster-overview.html)   explains the key concepts in running on a cluster. Spark can run both by itself, or over several existing cluster managers. It currently provides several options for deployment:    * [Standalone Deploy Mode](spark-standalone.html)   : simplest way to deploy Spark on a private cluster * [Apache Mesos](running-on-mesos.html)   (deprecated) * [Hadoop YARN](running-on-yarn.html) * [Kubernetes](running-on-kubernetes.html)   Where to Go from Here =======================  **Programming Guides:**   * [Quick Start](quick-start.html)   : a quick introduction to the Spark API; start here! * [RDD Programming Guide](rdd-programming-guide.html)   : overview of Spark basics - RDDs (core but old API), accumulators, and broadcast variables * [Spark SQL, Datasets, and DataFrames](sql-programming-guide.html)   : processing structured data with relational queries (newer API than RDDs) * [Structured Streaming](structured-streaming-programming-guide.html)   : processing structured data streams with relation queries (using Datasets and DataFrames, newer API than DStreams) * [Spark Streaming](streaming-programming-guide.html)   : processing data streams using DStreams (old API) * [MLlib](ml-guide.html)   : applying machine learning algorithms * [GraphX](graphx-programming-guide.html)   : processing graphs * [SparkR](sparkr.html)   : processing data with Spark in R * [PySpark](api/python/getting_started/index.html)   : processing data with Spark in Python * [Spark SQL CLI](sql-distributed-sql-engine-spark-sql-cli.html)   : processing data with SQL on the command line  **API Docs:**   * [Spark Scala API (Scaladoc)](api/scala/org/apache/spark/index.html) * [Spark Java API (Javadoc)](api/java/index.html) * [Spark Python API (Sphinx)](api/python/index.html) * [Spark R API (Roxygen2)](api/R/index.html) * [Spark SQL, Built-in Functions (MkDocs)](api/sql/index.html)  **Deployment Guides:**   * [Cluster Overview](cluster-overview.html)   : overview of concepts and components when running on a cluster * [Submitting Applications](submitting-applications.html)   : packaging and deploying applications * Deployment modes:  + [Amazon EC2](https://github.com/amplab/spark-ec2)    : scripts that let you launch a cluster on EC2 in about 5 minutes  + [Standalone Deploy Mode](spark-standalone.html)    : launch a standalone cluster quickly without a third-party cluster manager  + [Mesos](running-on-mesos.html)    : deploy a private cluster using   [Apache Mesos](https://mesos.apache.org)  + [YARN](running-on-yarn.html)    : deploy Spark on top of Hadoop NextGen (YARN)  + [Kubernetes](running-on-kubernetes.html)    : deploy Spark on top of Kubernetes  **Other Documents:**   * [Configuration](configuration.html)   : customize Spark via its configuration system * [Monitoring](monitoring.html)   : track the behavior of your applications * [Tuning Guide](tuning.html)   : best practices to optimize performance and memory use * [Job Scheduling](job-scheduling.html)   : scheduling resources across and within Spark applications * [Security](security.html)   : Spark security support * [Hardware Provisioning](hardware-provisioning.html)   : recommendations for cluster hardware * Integration with other storage systems:  + [Cloud Infrastructures](cloud-integration.html)  + [OpenStack Swift](storage-openstack-swift.html) * [Migration Guide](migration-guide.html)   : Migration guides for Spark components * [Building Spark](building-spark.html)   : build Spark using the Maven system * [Contributing to Spark](https://spark.apache.org/contributing.html) * [Third Party Projects](https://spark.apache.org/third-party-projects.html)   : related third party Spark projects  **External Resources:**   * [Spark Homepage](https://spark.apache.org) * [Spark Community](https://spark.apache.org/community.html)   resources, including local meetups * [StackOverflow tag  `apache-spark`](http://stackoverflow.com/questions/tagged/apache-spark) * [Mailing Lists](https://spark.apache.org/mailing-lists.html)   : ask questions about Spark here * AMP Camps: a series of training camps at UC Berkeley that featured talks and exercises about Spark, Spark Streaming, Mesos, and more.  [Videos](https://www.youtube.com/user/BerkeleyAMPLab/search?query=amp%20camp)   , are available online for free. * [Code Examples](https://spark.apache.org/examples.html)   : more are also available in the  `examples`   subfolder of Spark (  [Scala](https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples)   ,  [Java](https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples)   ,  [Python](https://github.com/apache/spark/tree/master/examples/src/main/python)   ,  [R](https://github.com/apache/spark/tree/master/examples/src/main/r)   )  "	Spark is a unified analytics engine for large-scale data processing that provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.	1
no_reaction	['https://docs.databricks.com/query-federation/snowflake.html']	is databricks better than snowflake	"    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format(""snowflake"")   .option(""dbtable"", table\_name)   .option(""sfUrl"", database\_host\_url)   .option(""sfUser"", username)   .option(""sfPassword"", password)   .option(""sfDatabase"", database\_name)   .option(""sfSchema"", schema\_name)   .option(""sfWarehouse"", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format(""snowflake"")  .option(""dbtable"", table\_name)  .option(""sfUrl"", database\_host\_url)  .option(""sfUser"", username)  .option(""sfPassword"", password)  .option(""sfDatabase"", database\_name)  .option(""sfSchema"", schema\_name)  .option(""sfWarehouse"", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         "	Based on the context, it seems that the document is providing information on how to read and write data from Snowflake using Databricks. It does not provide a direct comparison between Databricks and Snowflake, so it is difficult to say whether one is better than the other.	0
no_reaction	['https://docs.databricks.com/repos/repos-setup.html']	how do i connect a git repo to databricks repos	    Git integration with Databricks Repos =======================================     Databricks Repos is a visual Git client in Databricks. It supports common Git operations such a cloning a repository, committing and pushing, pulling, branch management, and visual comparison of diffs when committing.       Within Repos you can develop code in notebooks or other files and follow data science and engineering code development best practices using Git for version control, collaboration, and CI/CD.        What can you do with Databricks Repos? ----------------------------------------     Databricks Repos provides source control for data and AI projects by integrating with Git providers.       In Databricks Repos, you can use Git functionality to:     * Clone, push to, and pull from a remote Git repository. * Create and manage branches for development work. * Create notebooks, and edit notebooks and other files. * Visually compare differences upon commit.     For step-by-step instructions, see  [Clone a Git repo & other common Git operations](git-operations-with-repos.html)  . Databricks Repos also has an  [API](../dev-tools/api/latest/repos.html)  that you can integrate with your CI/CD pipeline. For example, you can programmatically update a Databricks repo so that it always has the most recent version of the code. For information about best practices for code development using Databricks Repos, see  [CI/CD workflows with Git integration and Databricks Repos](ci-cd-techniques-with-repos.html)  .       For following tasks, work in your Git provider:     * Create a pull request. * Resolve merge conflicts. * Merge or delete branches. * Rebase a branch.       Supported Git providers -------------------------     Databricks supports the following Git providers:     * GitHub * Bitbucket Cloud * GitLab * Azure DevOps * AWS CodeCommit * GitHub AE     See  [Get a Git access token & connect a remote repo to Databricks](get-access-tokens-from-git-provider.html)  .       Databricks Repos also supports Bitbucket Server, GitHub Enterprise Server, and GitLab self-managed integration, if the server is internet accessible. To integrate with a private Git server instance that is not internet-accessible, get in touch with your Databricks representative.       Support for arbitrary files in Databricks Repos is available in Databricks Runtime 8.4 and above. See  [What are workspace files?](../files/workspace.html)  .         Next step -----------    [Set up Databricks Repos](repos-setup.html)                                                         	"To connect a Git repo to Databricks Repos, you can use the Git functionality within Repos to clone, push to, and pull from a remote Git repository. Databricks Repos supports several Git providers, including GitHub, Bitbucket Cloud, GitLab, Azure DevOps, AWS CodeCommit, and GitHub AE. You can also integrate Databricks Repos with your CI/CD pipeline using the Repos API. For step-by-step instructions, you can refer to the ""Clone a Git repo & other common Git operations"" section in the provided context."	1
thumb_down	['https://docs.databricks.com/sql/api/index.html']	Hey, I am looking for the SQL gateway REST api documentation. Do you know where it is?	      Databricks SQL API reference ==============================     The Databricks SQL REST API supports services to manage queries and dashboards, query history, and SQL warehouses.       This article provides an overview of how to use the REST API. Links to each API reference are listed at the end of the article.       For information about authenticating to the REST API, see  [Authenticate with personal access tokens in Databricks SQL](authentication.html)  .        Rate limits -------------     The Databricks REST API supports a maximum of 30 requests/second per workspace. Requests that exceed the rate limit will receive a  [429 response status code](https://developer.mozilla.org/docs/Web/HTTP/Status/429)   .         Parse output --------------     It can be useful to parse out parts of the JSON output. In these cases, we recommend that you to use the utility  `jq`  . For more information, see the  [jq Manual](https://stedolan.github.io/jq/manual/)   . You can install  `jq`  on MacOS using Homebrew by running  `brew     install     jq`  .       Some  `STRING`  fields (which contain error and descriptive messaging intended to be consumed by the UI) are unstructured, and you should not depend on the format of these fields in programmatic workflows.         Invoke a GET using a query string -----------------------------------     While most API calls require that you specify a JSON body, for  `GET`  calls you can specify the object to be retrieved. For example, to get the details for a SQL warehouse, run:        ``` curl ... https://<databricks-instance>/api/2.0/sql/warehouses/<warehouse-id>  ```          Authenticate to Databricks SQL APIs -------------------------------------    * [Authenticate with personal access tokens in Databricks SQL](authentication.html)                                                          Databricks SQL APIs ---------------------    * [REST API Explorer (Beta)](/api-explorer/workspace/databricks_sql?http://) * [Queries, Dashboards, and Alerts API 2.0](queries-dashboards.html) * [Query History API 2.0](query-history.html) * [Statement Execution API 2.0](statements.html) * [SQL Warehouses APIs 2.0](sql-endpoints.html) * [Statement Execution API 2.0](statements.html) * [Databricks SQL Statement Execution API 2.0 tutorial](sql-execution-tutorial.html)       	Yes, the SQL gateway REST API documentation can be found in the Databricks SQL API reference. You can find the link to the API reference at the end of the article.	1
no_reaction	['https://docs.databricks.com/ingestion/index.html']	how do I ingest files using databricks autoloader?	    Auto Loader FAQ =================     Commonly asked questions about Databricks Auto Loader.        Does Auto Loader process the file again when the file gets appended or overwritten? -------------------------------------------------------------------------------------     Files are processed exactly once unless  `cloudFiles.allowOverwrites`  is enabled. If a file is appended to or overwritten, Databricks does not guarantee which version of the file is processed. Databricks recommends you use Auto Loader to ingest only immutable files. If this does not meet your requirements, contact your Databricks representative.         If my data files do not arrive continuously, but in regular intervals, for example, once a day, should I still use this source and are there any benefits? ------------------------------------------------------------------------------------------------------------------------------------------------------------     In this case, you can set up a  `Trigger.Once`  or  `Trigger.AvailableNow`  (available in Databricks Runtime 10.2 and later) Structured Streaming job and schedule to run after the anticipated file arrival time. Auto Loader works well with both infrequent or frequent updates. Even if the eventual updates are very large, Auto Loader scales well to the input size. Auto Loader’s efficient file discovery techniques and schema evolution capabilities make Auto Loader the recommended method for incremental data ingestion.         What happens if I change the checkpoint location when restarting the stream? ------------------------------------------------------------------------------     A checkpoint location maintains important identifying information of a stream. Changing the checkpoint location effectively means that you have abandoned the previous stream and started a new stream.         Do I need to create event notification services beforehand? -------------------------------------------------------------     No. If you choose file notification mode and provide the required permissions, Auto Loader can create file notification services for you. See  [What is Auto Loader file notification mode?](file-notification-mode.html)        How do I clean up the event notification resources created by Auto Loader? ----------------------------------------------------------------------------     You can use the  [cloud resource manager](file-notification-mode.html#cloud-resource-management)  to list and tear down resources. You can also delete these resources manually using the cloud provider’s UI or APIs.         Can I run multiple streaming queries from different input directories on the same bucket/container? -----------------------------------------------------------------------------------------------------     Yes, as long as they are not parent-child directories; for example,  `prod-logs/`  and  `prod-logs/usage/`  would not work because  `/usage`  is a child directory of  `/prod-logs`  .         Can I use this feature when there are existing file notifications on my bucket or container? ----------------------------------------------------------------------------------------------     Yes, as long as your input directory does not conflict with the existing notification prefix (for example, the above parent-child directories).         How does Auto Loader infer schema? ------------------------------------     When the DataFrame is first defined, Auto Loader lists your source directory and chooses the most recent (by file modification time) 50 GB of data or 1000 files, and uses those to infer your data schema.       Auto Loader also infers partition columns by examining the source directory structure and looks for file paths that contain the  `/key=value/`  structure. If the source directory has an inconsistent structure, for example:        ``` base/path/partition=1/date=2020-12-31/file1.json // inconsistent because date and partition directories are in different orders base/path/date=2020-12-31/partition=2/file2.json // inconsistent because the date directory is missing base/path/partition=3/file3.json  ```      Auto Loader infers the partition columns as empty. Use  `cloudFiles.partitionColumns`  to explicitly parse columns from the directory structure.         How does Auto Loader behave when the source folder is empty? --------------------------------------------------------------     If the source directory is empty, Auto Loader requires you to provide a schema as there is no data to perform inference.         When does Autoloader infer schema? Does it evolve automatically after every micro-batch? ------------------------------------------------------------------------------------------     The schema is inferred when the DataFrame is first defined in your code. During each micro-batch, schema changes are evaluated on the fly; therefore, you don’t need to worry about performance hits. When the stream restarts, it picks up the evolved schema from the schema location and starts executing without any overhead from inference.         What’s the performance impact on ingesting the data when using Auto Loader schema inference? ----------------------------------------------------------------------------------------------     You should expect schema inference to take a couple of minutes for very large source directories during initial schema inference. You shouldn’t observe significant performance hits otherwise during stream execution. If you run your code in a Databricks notebook, you can see status updates that specify when Auto Loader will be listing your directory for sampling and inferring your data schema.         Due to a bug, a bad file has changed my schema drastically. What should I do to roll back a schema change? ------------------------------------------------------------------------------------------------------------     Contact Databricks support for help.                                                        	To ingest files using Databricks Auto Loader, you can use Auto Loader to ingest only immutable files. If your data files do not arrive continuously, but in regular intervals, you can set up a Trigger.Once or Trigger.AvailableNow Structured Streaming job and schedule to run after the anticipated file arrival time. Auto Loader works well with both infrequent or frequent updates. Even if the eventual updates are very large, Auto Loader scales well to the input size. Auto Loader’s efficient file discovery techniques and schema evolution capabilities make Auto Loader the recommended method for incremental data ingestion.	1
no_reaction	['https://docs.databricks.com/ingestion/add-data/upload-data.html']	How do I load csv files into Delta table 	"      CSV file ==========     This article provides examples for reading and writing to CSV files with Databricks using Python, Scala, R, and SQL.        Note       You can use SQL to read CSV data directly or by using a temporary view. Databricks recommends using a temporary view. Reading the CSV file directly has the following drawbacks:     * You can’t  [specify data source options](#options)  . * You can’t  [specify the schema](#specify-schema)  for the data.     See  [Examples](#examples)  .         Options ---------     You can configure several options for CSV file data sources. See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#csv(path:String):Unit) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html#pyspark.sql.DataFrameWriter.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#csv(path:String):Unit)         Work with malformed CSV records ---------------------------------     When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:     * `PERMISSIVE`  (default): nulls are inserted for fields that could not be parsed correctly * `DROPMALFORMED`  : drops lines that contain fields that could not be parsed * `FAILFAST`  : aborts the reading if any malformed data is found     To set the mode, use the  `mode`  option.        ``` diamonds\_df = (spark.read   .format(""csv"")   .option(""mode"", ""PERMISSIVE"")   .load(""/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv"") )  ```      In the  `PERMISSIVE`  mode it is possible to inspect the rows that could not be parsed correctly using one of the following methods:     * You can provide a custom path to the option  `badRecordsPath`  to record corrupt records to a file. * You can add the column  `\_corrupt\_record`  to the schema provided to the DataFrameReader to review corrupt records in the resultant DataFrame.      Note       The  `badRecordsPath`  option takes precedence over  `\_corrupt\_record`  , meaning that malformed rows written to the provided path do not appear in the resultant DataFrame.       Default behavior for malformed records changes when using the  [rescued data column](#rescued-data)  .         ###   Find malformed rows notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-corrupt-record.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                    Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.3 (Unsupported)](../release-notes/runtime/8.3.html)  and above.        When using the  `PERMISSIVE`  mode, you can enable the rescued data column to capture any data that wasn’t parsed because one or more fields in a record have one of the following issues:     * Absent from the provided schema. * Does not match the data type of the provided schema. * Has a case mismatch with the field names in the provided schema.     The rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set(""spark.databricks.sql.rescuedDataColumn.filePath.enabled"",     ""false"")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name when reading data, such as  `\_rescued\_data`  with  `spark.read.option(""rescuedDataColumn"",     ""\_rescued\_data"").format(""csv"").load(<path>)`  .       The CSV parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors.       When  `rescuedDataColumn`  is used in  `PERMISSIVE`  mode, the following rules apply to  [corrupt records](#corrupt-records)  :     * The first row of the file (either a header row or a data row) sets the expected row length. * A row with a different number of columns is considered incomplete. * Data type mismatches are not considered corrupt records. * Only incomplete and malformed CSV records are considered corrupt and recorded to the  `\_corrupt\_record`  column or  `badRecordsPath`  .       Examples ----------     These examples use the  [diamonds dataset](../dbfs/databricks-datasets.html)  . Specify the path to the dataset as well as any options that you would like.        In this section:     * [Read file in any language](#read-file-in-any-language) * [Specify schema](#specify-schema) * [Pitfalls of reading a subset of columns](#pitfalls-of-reading-a-subset-of-columns)     ###  [Read file in any language](#id1)     This notebook shows how to read a file, display sample data, and print the data schema using Scala, R, Python, and SQL.        ####   Read CSV files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Specify schema](#id2)     When the schema of the CSV file is known, you can specify the desired schema to the CSV reader with the  `schema`  option.        ####   Read CSV files with schema notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-schema.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Pitfalls of reading a subset of columns](#id3)     The behavior of the CSV parser depends on the set of columns that are read. If the specified schema is incorrect, the results might differ considerably depending on the subset of columns that is accessed. The following notebook presents the most common pitfalls.        ####   Caveats of reading a subset of columns of a CSV file notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-column-subset.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                  "	"To load CSV files into Delta table, you can use the `spark.read.format(""csv"").load()` method to read the CSV file and then use the `DataFrame.write.format(""delta"").save()` method to save the data as a Delta table. You can also specify options for the CSV file data source, such as the mode for handling malformed records, and the schema for the data. It is recommended to use a temporary view to read CSV data directly."	0
no_reaction	['https://kb.databricks.com/sql/set-nullability-when-using-saveastable-with-delta-tables']	does using .saveastable() on dataframe save it as a delta table by default	"   pyspark.sql.DataFrameWriter.saveAsTable  [¶](#pyspark-sql-dataframewriter-saveastable ""Permalink to this headline"") =====================================================================================================================  `DataFrameWriter.`  `saveAsTable`    (   *name     :     str*  ,  *format     :     Optional    [    str    ]     =     None*  ,  *mode     :     Optional    [    str    ]     =     None*  ,  *partitionBy     :     Union[str, List[str], None]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → None  [[source]](../../../_modules/pyspark/sql/readwriter.html#DataFrameWriter.saveAsTable) [¶](#pyspark.sql.DataFrameWriter.saveAsTable ""Permalink to this definition"")    Saves the content of the  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  as the specified table.     In the case the table already exists, behavior of this function depends on the save mode, specified by the    mode    function (default to throwing an exception). When    mode    is    Overwrite    , the schema of the  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  does not need to be the same as that of the existing table.    * append    : Append contents of this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  to existing data. * overwrite    : Overwrite existing data. * error    or    errorifexists    : Throw an exception if data already exists. * ignore    : Silently ignore this operation if data already exists.   New in version 1.4.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **name**    str     the table name    **format**    str, optional     the format used to save    **mode**    str, optional     one of    append    ,    overwrite    ,    error    ,    errorifexists    ,    ignore    (default: error)    **partitionBy**    str or list     names of partitioning columns    **\*\*options**    dict     all other string options     Notes     When    mode    is    Append    , if there is an existing table, we will use the format and options of the existing table. The column order in the schema of the  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  doesn’t need to be the same as that of the existing table. Unlike  [`DataFrameWriter.insertInto()`](pyspark.sql.DataFrameWriter.insertInto.html#pyspark.sql.DataFrameWriter.insertInto ""pyspark.sql.DataFrameWriter.insertInto"")  ,  [`DataFrameWriter.saveAsTable()`](#pyspark.sql.DataFrameWriter.saveAsTable ""pyspark.sql.DataFrameWriter.saveAsTable"")  will use the column names to find the correct column positions.     Examples     Creates a table from a DataFrame, and read it back.    ``` >>> \_ = spark.sql(""DROP TABLE IF EXISTS tblA"") >>> spark.createDataFrame([ ...     (100, ""Hyukjin Kwon""), (120, ""Hyukjin Kwon""), (140, ""Haejoon Lee"")], ...     schema=[""age"", ""name""] ... ).write.saveAsTable(""tblA"") >>> spark.read.table(""tblA"").sort(""age"").show() +---+------------+ |age| name| +---+------------+ |100|Hyukjin Kwon| |120|Hyukjin Kwon| |140| Haejoon Lee| +---+------------+ >>> \_ = spark.sql(""DROP TABLE tblA"")  ```  [pyspark.sql.DataFrameWriter.save](pyspark.sql.DataFrameWriter.save.html ""previous page"")  [pyspark.sql.DataFrameWriter.sortBy](pyspark.sql.DataFrameWriter.sortBy.html ""next page"")   "	No, using `.saveAsTable()` on a DataFrame does not save it as a Delta table by default. The format used to save the table can be specified as an optional argument.	0
no_reaction	['https://docs.databricks.com/delta-live-tables/create-multiple-tables.html']	How do I create a new delta table by selecting from a different delta table in Python?	"    Programmatically create multiple tables =========================================     You can use Python with Delta Live Tables to programmatically create multiple tables to reduce code redundancy.       You might have pipelines containing multiple flows or dataset definitions that differ only by a small number of parameters. This redundancy results in pipelines that are error-prone and difficult to maintain. For example, the following diagram shows the graph of a pipeline that uses a fire department dataset to find neighborhoods with the fastest response times for different categories of emergency calls. In this example, the parallel flows differ by only a few parameters.      ![Fire dataset flow diagram](../_images/fire-dataset-flows.png)       Delta Live Tables metaprogramming with Python example -------------------------------------------------------     You can use a metaprogramming pattern to reduce the overhead of generating and maintaining redundant flow definitions. Metaprogramming in Delta Live Tables is done using Python inner functions. Because these functions are lazily evaluated, you can use them to create flows that are identical except for input parameters. Each invocation can include a different set of parameters that controls how each table should be generated, as shown in the following example:        ``` import dlt from pyspark.sql.functions import \*  @dlt.table(   name=""raw\_fire\_department"",   comment=""raw table for fire department response"" ) @dlt.expect\_or\_drop(""valid\_received"", ""received IS NOT NULL"") @dlt.expect\_or\_drop(""valid\_response"", ""responded IS NOT NULL"") @dlt.expect\_or\_drop(""valid\_neighborhood"", ""neighborhood != 'None'"") def get\_raw\_fire\_department():   return (     spark.read.format('csv')       .option('header', 'true')       .option('multiline', 'true')       .load('/databricks-datasets/timeseries/Fires/Fire\_Department\_Calls\_for\_Service.csv')       .withColumnRenamed('Call Type', 'call\_type')       .withColumnRenamed('Received DtTm', 'received')       .withColumnRenamed('Response DtTm', 'responded')       .withColumnRenamed('Neighborhooods - Analysis Boundaries', 'neighborhood')     .select('call\_type', 'received', 'responded', 'neighborhood')   )  all\_tables = []  def generate\_tables(call\_table, response\_table, filter):   @dlt.table(     name=call\_table,     comment=""top level tables by call type""   )   def create\_call\_table():     return (       spark.sql(""""""  SELECT  unix\_timestamp(received,'M/d/yyyy h:m:s a') as ts\_received,  unix\_timestamp(responded,'M/d/yyyy h:m:s a') as ts\_responded,  neighborhood  FROM LIVE.raw\_fire\_department  WHERE call\_type = '{filter}'  """""".format(filter=filter))     )    @dlt.table(     name=response\_table,     comment=""top 10 neighborhoods with fastest response time ""   )   def create\_response\_table():     return (       spark.sql(""""""  SELECT  neighborhood,  AVG((ts\_received - ts\_responded)) as response\_time  FROM LIVE.{call\_table}  GROUP BY 1  ORDER BY response\_time  LIMIT 10  """""".format(call\_table=call\_table))     )    all\_tables.append(response\_table)  generate\_tables(""alarms\_table"", ""alarms\_response"", ""Alarms"") generate\_tables(""fire\_table"", ""fire\_response"", ""Structure Fire"") generate\_tables(""medical\_table"", ""medical\_response"", ""Medical Incident"")  @dlt.table(   name=""best\_neighborhoods"",   comment=""which neighbor appears in the best response time list the most"" ) def summary():   target\_tables = [dlt.read(t) for t in all\_tables]   unioned = functools.reduce(lambda x,y: x.union(y), target\_tables)   return (     unioned.groupBy(col(""neighborhood""))       .agg(count(""\*"").alias(""score""))       .orderBy(desc(""score""))   )  ```                                                       "	"To create a new delta table by selecting from a different delta table in Python, you can use the `dlt.read()` function to read the data from the existing delta table and then use the `dlt.write()` function to write the data to a new delta table. Here is an example:  ``` import dlt  # Read data from existing delta table df = dlt.read(""existing_delta_table"")  # Select data from existing delta table new_df = df.select(""column1"", ""column2"")  # Write data to new delta table dlt.write(""new_delta_table"", new_df) ```  Replace ""existing_delta_table"" with the name of the existing delta table you want to select data from, and replace ""new_delta_table"" with the name of the new delta table you want to create."	1
no_reaction	['https://docs.databricks.com/migration/index.html']	What is the best way to migrate workloads from Synapse to Databricks?	"    Connecting Databricks and Azure Synapse with PolyBase (legacy) ================================================================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [Query data in Azure Synapse Analytics](../../external-data/synapse-analytics.html)  .        Databricks recommends using the default  `COPY`  functionality with Azure Data Lake Storage Gen2 for connections to Azure Synapse. This article includes legacy documentation around PolyBase and blob storage.      [Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/)   (formerly SQL Data Warehouse) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data. Use Azure as a key component of a big data solution. Import big data into Azure with simple  [PolyBase](https://learn.microsoft.com/azure/synapse-analytics/sql-data-warehouse/load-data-wideworldimportersdw)   T-SQL queries, or  [COPY](https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql)   statement and then use the power of MPP to run high-performance analytics. As you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights.       You can access Azure Synapse from Databricks using the Azure Synapse connector, a data source implementation for Apache Spark that uses  [Azure Blob storage](https://azure.microsoft.com/services/storage/blobs/)   , and PolyBase or the  `COPY`  statement in Azure Synapse to transfer large volumes of data efficiently between a Databricks cluster and an Azure Synapse instance.       Both the Databricks cluster and the Azure Synapse instance access a common Blob storage container to exchange data between these two systems. In Databricks, Apache Spark jobs are triggered by the Azure Synapse connector to read data from and write data to the Blob storage container. On the Azure Synapse side, data loading and unloading operations performed by PolyBase are triggered by the Azure Synapse connector through JDBC. In Databricks Runtime 7.0 and above,  `COPY`  is used by default to load data into Azure Synapse by the Azure Synapse connector through JDBC.        Note      `COPY`  is available only on Azure Synapse Gen2 instances, which provide  [better performance](https://learn.microsoft.com/azure/sql-data-warehouse/upgrade-to-latest-generation)   . If your database still uses Gen1 instances, we recommend that you migrate the database to Gen2.        The Azure Synapse connector is more suited to ETL than to interactive queries, because each query execution can extract large amounts of data to Blob storage. If you plan to perform several queries against the same Azure Synapse table, we recommend that you save the extracted data in a format such as Parquet.        Requirements --------------     An Azure Synapse  [database master key](https://learn.microsoft.com/sql/relational-databases/security/encryption/create-a-database-master-key)   .           Authentication ----------------     The Azure Synapse connector uses three types of network connections:     * Spark driver to Azure Synapse * Spark driver and executors to Azure storage account * Azure Synapse to Azure storage account      ```                                  ┌─────────┐       ┌─────────────────────────>│ STORAGE │<────────────────────────┐       │   Storage acc key /      │ ACCOUNT │  Storage acc key /      │       │   Managed Service ID /   └─────────┘  OAuth 2.0 /            │       │                               │                              │       │                               │                              │       │                               │ Storage acc key /            │       │                               │ OAuth 2.0 /                  │       │                               │                              │       v                               v                       ┌──────v────┐ ┌──────────┐                      ┌──────────┐                │┌──────────┴┐ │ Synapse  │                      │  Spark   │                ││ Spark     │ │ Analytics│<────────────────────>│  Driver  │<───────────────>│ Executors │ └──────────┘  JDBC with           └──────────┘    Configured   └───────────┘               username & password /                in Spark  ```      The following sections describe each connection’s authentication configuration options.      ###   Spark driver to Azure Synapse     The Spark driver can connect to Azure Synapse using JDBC with a username and password or OAuth 2.0 with a service principal for authentication.      ####   Username and password     We recommend that you use the connection strings provided by Azure portal for both authentication types, which enable Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for  `encrypt=true`  in the connection string.       To allow the Spark driver to reach Azure Synapse, we recommend that you set  **Allow Azure services and resources to access this workspace**   to  **ON**   on the Networking pane under Security of the Azure Synapse workspace through the Azure portal. This setting allows communications from all Azure IP addresses and all Azure subnets, which allows Spark drivers to reach the Azure Synapse instance.       ####   OAuth 2.0 with a service principal     You can authenticate to Azure Synapse Analytics using a service principal with access to the underlying storage account. For more information on using service principal credentials to access an Azure storage account, see  [Access Azure Data Lake Storage Gen2 and Blob Storage](../../storage/azure-storage.html)  . You must set the  `enableServicePrincipalAuth`  option to  `true`  in the connection configuration  [Parameters](#parameters)  to enable the connector to authenticate with a service principal.       You can optionally use a different service principal for the Azure Synapse Analytics connection. An example that configures service principal credentials for the storage account and optional service principal credentials for Synapse:         ``` ; Defining the Service Principal credentials for the Azure storage account fs.azure.account.auth.type OAuth fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider fs.azure.account.oauth2.client.id <application-id> fs.azure.account.oauth2.client.secret <service-credential> fs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/<directory-id>/oauth2/token  ; Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials) spark.databricks.sqldw.jdbc.service.principal.client.id <application-id> spark.databricks.sqldw.jdbc.service.principal.client.secret <service-credential>  ```       ``` // Defining the Service Principal credentials for the Azure storage account spark.conf.set(""fs.azure.account.auth.type"", ""OAuth"") spark.conf.set(""fs.azure.account.oauth.provider.type"", ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"") spark.conf.set(""fs.azure.account.oauth2.client.id"", ""<application-id>"") spark.conf.set(""fs.azure.account.oauth2.client.secret"", ""<service-credential>"") spark.conf.set(""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")  // Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials) spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"") spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")  ```       ``` # Defining the service principal credentials for the Azure storage account spark.conf.set(""fs.azure.account.auth.type"", ""OAuth"") spark.conf.set(""fs.azure.account.oauth.provider.type"",  ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"") spark.conf.set(""fs.azure.account.oauth2.client.id"", ""<application-id>"") spark.conf.set(""fs.azure.account.oauth2.client.secret"", ""<service-credential>"") spark.conf.set(""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")  # Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials) spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"") spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")  ```       ``` # Load SparkR library(SparkR) conf <- sparkR.callJMethod(sparkR.session(), ""conf"")  # Defining the service principal credentials for the Azure storage account sparkR.callJMethod(conf, ""set"", ""fs.azure.account.auth.type"", ""OAuth"") sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth.provider.type"", ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")"	Databricks recommends using the default `COPY` functionality with Azure Data Lake Storage Gen2 for connections to Azure Synapse. Both the Databricks cluster and the Azure Synapse instance access a common Blob storage container to exchange data between these two systems. In Databricks Runtime 7.0 and above, `COPY` is used by default to load data into Azure Synapse by the Azure Synapse connector through JDBC.	0
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/migrate.html']	How do i set up unity catalog with an external hive metastore	"      Work with Unity Catalog and the legacy Hive metastore =======================================================     This article explains how to use a Unity Catalog metastore with the per-workspace Hive metastore in Databricks.       If your workspace was in service before it was enabled for Unity Catalog, it likely has a Hive metastore that contains data that you want to continue to use. Databricks recommends that you  [migrate the tables managed by the Hive metastore to the Unity Catalog metastore](migrate.html)  , but if you choose not to, this article explains how to work with data managed by both metastores.          Query the Hive metastore in Unity Catalog -------------------------------------------     The Unity Catalog metastore is additive, meaning it can be used with the per-workspace Hive metastore in Databricks. The Hive metastore appears as a top-level catalog called  `hive\_metastore`  in the three-level namespace.       For example, you can refer to a table called  `sales\_raw`  in the  `sales`  schema in the legacy Hive metastore by using the following notation:         ``` SELECT \* from hive\_metastore.sales.sales\_raw;  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       ``` library(SparkR)  display(tableToDF(""hive\_metastore.sales.sales\_raw""))  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       You can also specify the catalog and schema with a  `USE`  statement:         ``` USE hive\_metastore.sales; SELECT \* from sales\_raw;  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```       ``` library(SparkR)  sql(""USE hive\_metastore.sales"") display(tableToDF(""sales\_raw""))  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```           Access control in Unity Catalog and the Hive metastore --------------------------------------------------------     If you configured  [table access control](../table-acls/index.html)  on the Hive metastore, Databricks continues to enforce those access controls for data in the  `hive\_metastore`  catalog for clusters running in the shared access mode. The Unity Catalog access model differs slightly from legacy access controls, like no  `DENY`  statements. The Hive metastore is a workspace-level object. Permissions defined within the  `hive\_metastore`  catalog always refer to the local users and groups in the workspace. See  [Differences from table access control](#differences-from-table-access-control)  .         Differences from table access control ---------------------------------------     Unity Catalog has the following key differences from using  [table access controls](../table-acls/index.html)  in the legacy Hive metastore in each workspace.       The access control model in Unity Catalog has the following differences from table access control:     * **Account groups**   : Access control policies in Unity Catalog are applied to account groups, while access control policies for the Hive metastore are applied to workspace-local groups. See  [Special considerations for groups](../../administration-guide/users-groups/index.html#special-groups)  . * **`USE     CATALOG`  and  `USE     SCHEMA`  permissions are required on the catalog and schema for all operations on objects inside the catalog or schema**   : Regardless of a principal’s privileges on a table, the principal must also have the  `USE     CATALOG`  privilege on its parent catalog to access the schema and the  `USE     SCHEMA`  privilege to access objects within the schema. With workspace-level table access controls, on the other hand, granting  `USAGE`  on the root catalog automatically grants  `USAGE`  on all databases, but  `USAGE`  on the root catalog is not required. * **Views**   : In Unity Catalog, the owner of a view does not need to be an owner of the view’s referenced tables and views. Having the  `SELECT`  privilege is sufficient, along with  `USE     SCHEMA`  on the views’ parent schema and  `USE     CATALOG`  on the parent catalog. With workspace-level table access controls, a view’s owner needs to be an owner of all referenced tables and views. * **No support for  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  s**   : In Unity Catalog, there is no concept of an  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  permission. These permissions could be used to circumvent access control restrictions by allowing an unprivileged user to run privileged code.         Joins between Unity Catalog and Hive metastore objects --------------------------------------------------------     By using three-level namespace notation, you can join data in a Unity Catalog metastore with data in the legacy Hive metastore.        Note       A join with data in the legacy Hive metastore will only work on the workspace where that data resides. Trying to run such a join in another workspace results in an error. Databricks recommends that you  [upgrade](migrate.html)  legacy tables and views to Unity Catalog.        The following example joins results from the  `sales\_current`  table in the legacy Hive metastore with the  `sales\_historical`  table in the Unity Catalog metastore when the  `order\_id`  fields are equal.         ``` SELECT \* FROM hive\_metastore.sales.sales\_current JOIN main.shared\_sales.sales\_historical ON hive\_metastore.sales.sales\_current.order\_id = main.shared\_sales.sales\_historical.order\_id;  ```       ``` dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(   other = dfHistorical,   on = dfCurrent.order\_id == dfHistorical.order\_id ))  ```       ``` library(SparkR)  dfCurrent = tableToDF(""hive\_metastore.sales.sales\_current"") dfHistorical = tableToDF(""main.shared\_sales.sales\_historical"")  display(join(  x = dfCurrent,  y = dfHistorical,  joinExpr = dfCurrent$order\_id == dfHistorical$order\_id))  ```       ``` val dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") val dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(  right = dfHistorical,  joinExprs = dfCurrent(""order\_id"") === dfHistorical(""order\_id"") ))  ```         Default catalog -----------------     If you omit the top-level catalog name when you perform data operations, and there is no  `USE     CATALOG`  statement, the default catalog is assumed. Of course, if you are supplying the catalog name in another way, such as setting it on a JDBC string, then that catalog is assumed instead of the default catalog when you omit the catalog name in data operations.       By default, the default catalog is  `hive\_metastore`  . To configure a different default catalog for a workspace, you can set a Spark configuration or run a  [Databricks CLI](../../dev-tools/cli/unity-catalog-cli.html)  command:         Set the  `spark.databricks.sql.initial.catalog.name`  value.         This CLI command updates the default catalog only for SQL warehouses. To specify the default catalog for clusters, use the Spark config.        ``` databricks unity-catalog metastores assign --workspace-id 1234567890123456 \  --metastore-id 12a345b6-7890-1cd2-3456-e789f0a12b34 \  --default-catalog-name my_catalog  ```        If your Databricks workspace used the per-workspace Hive metastore before the workspace was enabled for Unity Catalog, Databricks recommends that you keep the default catalog value as  `hive\_metastore`  so that your existing code can operate on current Hive metastore data without any change.        Warning       Changing the default catalog can break existing data operations that depend on it.          Cluster instance profile --------------------------     When using the Hive metastore alongside Unity Catalog, the instance profile on the cluster is used to access Hive metastore data but not data registered in Unity Catalog.       If users access paths that are outside Unity Catalog (such as a path not registered as a table or external location) then the instance profile on the cluster is used.           Upgrade legacy tables to Unity Catalog ----------------------------------------     Tables in the Hive metastore do not benefit from the full set of security and governance features that Unity Catalog introduces, such as built-in auditing and access control. Databricks recommends that you  [upgrade your legacy tables](migrate.html)  by adding them to Unity Catalog.                                                        "	To set up Unity Catalog with an external Hive metastore, you can use the three-level namespace notation to join data in a Unity Catalog metastore with data in the legacy Hive metastore. However, if your workspace was in service before it was enabled for Unity Catalog, it likely has a Hive metastore that contains data that you want to continue to use. Databricks recommends that you migrate the tables managed by the Hive metastore to the Unity Catalog metastore, but if you choose not to, you can work with data managed by both metastores.	1
no_reaction	['https://docs.databricks.com/sql/language-manual/functions/contains.html']	check string contain	"   pyspark.pandas.Series.str.contains  [¶](#pyspark-pandas-series-str-contains ""Permalink to this headline"") ===========================================================================================================  `str.`  `contains`    (   *pat     :     str*  ,  *case     :     bool     =     True*  ,  *flags     :     int     =     0*  ,  *na     :     Any     =     None*  ,  *regex     :     bool     =     True*   )    → pyspark.pandas.series.Series  [¶](#pyspark.pandas.Series.str.contains ""Permalink to this definition"")    Test if pattern or regex is contained within a string of a Series.     Return boolean Series based on whether a given pattern or regex is contained within a string of a Series.     Analogous to  [`match()`](pyspark.pandas.Series.str.match.html#pyspark.pandas.Series.str.match ""pyspark.pandas.Series.str.match"")  , but less strict, relying on  `re.search()`  instead of  `re.match()`  .     Parameters    **pat**    str     Character sequence or regular expression.    **case**    bool, default True     If True, case sensitive.    **flags**    int, default 0 (no flags)     Flags to pass through to the re module, e.g. re.IGNORECASE.    **na**    default None     Fill value for missing values. NaN converted to None.    **regex**    bool, default True     If True, assumes the pat is a regular expression. If False, treats the pat as a literal string.     Returns     Series of boolean values or object     A Series of boolean values indicating whether the given pattern is contained within the string of each element of the Series.     Examples     Returning a Series of booleans using only a literal pattern.    ``` >>> s1 = ps.Series(['Mouse', 'dog', 'house and parrot', '23', np.NaN]) >>> s1.str.contains('og', regex=False) 0 False 1 True 2 False 3 False 4 None dtype: object  ```   Specifying case sensitivity using case.    ``` >>> s1.str.contains('oG', case=True, regex=True) 0 False 1 False 2 False 3 False 4 None dtype: object  ```   Specifying na to be False instead of NaN replaces NaN values with False. If Series does not contain NaN values the resultant dtype will be bool, otherwise, an object dtype.    ``` >>> s1.str.contains('og', na=False, regex=True) 0 False 1 True 2 False 3 False 4 False dtype: bool  ```   Returning ‘house’ or ‘dog’ when either expression occurs in a string.    ``` >>> s1.str.contains('house|dog', regex=True) 0 False 1 True 2 True 3 False 4 None dtype: object  ```   Ignoring case sensitivity using flags with regex.    ``` >>> import re >>> s1.str.contains('PARROT', flags=re.IGNORECASE, regex=True) 0 False 1 False 2 True 3 False 4 None dtype: object  ```   Returning any digit using regular expression.    ``` >>> s1.str.contains('[0-9]', regex=True) 0 False 1 False 2 False 3 True 4 None dtype: object  ```   Ensure pat is a not a literal pattern when regex is set to True. Note in the following example one might expect only s2[1] and s2[3] to return True. However, ‘.0’ as a regex matches any character followed by a 0.    ``` >>> s2 = ps.Series(['40','40.0','41','41.0','35']) >>> s2.str.contains('.0', regex=True) 0 True 1 True 2 False 3 True 4 False dtype: bool  ```  [pyspark.pandas.Series.str.center](pyspark.pandas.Series.str.center.html ""previous page"")  [pyspark.pandas.Series.str.count](pyspark.pandas.Series.str.count.html ""next page"")   "	The `pyspark.pandas.Series.str.contains` function tests if a pattern or regex is contained within a string of a Series and returns a boolean Series based on whether the pattern or regex is contained within the string of a Series. It takes in parameters such as `pat`, `case`, `flags`, `na`, and `regex`. An example usage is shown in the context.	1
no_reaction	['https://kb.databricks.com/delta/optimize-delta-sink-structured-streaming']	how to optimize delta table	    Optimization recommendations on Databricks ============================================     Databricks provides many optimizations supporting a variety of workloads on the lakehouse, ranging from large-scale ETL processing to ad-hoc, interactive queries. Many of these optimizations take place automatically. You get their benefits simply by using Databricks. Additionally, most Databricks Runtime features require Delta Lake, the default storage layer used to create tables in Databricks.       Databricks configures default values that optimize most workloads. But, in some cases, changing configuration settings improves performance.            Databricks Runtime performance enhancements ---------------------------------------------      Note       Use the latest Databricks Runtime to leverage the newest performance enhancements. All behaviors documented here are enabled by default in Databricks Runtime 10.4 LTS and above.      * [Disk caching](disk-cache.html)  accelerates repeated reads against Parquet data files by loading data to disk volumes attached to compute clusters. * [Dynamic file pruning](dynamic-file-pruning.html)  improves query performance by skipping directories that do not contain data files that match query predicates. * [Low shuffle merge](low-shuffle-merge.html)  reduces the number of data files rewritten by  `MERGE`  operations and reduces the need to recaculate  `ZORDER`  clusters. * Apache Spark 3.0 introduced  [adaptive query execution](aqe.html)  , which provides enhanced performance for many operations.       Databricks recommendations for enhanced performance -----------------------------------------------------   * You can  [clone](../delta/clone.html)  tables on Databricks to make deep or shallow copies of source datasets. * The  [cost-based optimizer](cbo.html)  accelerates query performance by leveraging table statistics. * You can  [auto optimize](auto-optimize.html)  Delta tables using optimized writes and automatic file compaction; this is especially useful for long-running Structured Streaming jobs. * You can use Spark SQL to interact with  [semi-structured JSON data](semi-structured.html)  without parsing strings. * [Higher order functions](higher-order-lambda-functions.html)  provide built-in, optimized performance for many operations that do not have common Spark operators. Higher order functions provide a performance benefit over user defined functions. * Databricks provides a number of built-in operators and special syntax for working with  [complex data types](complex-types.html)  , including arrays, structs, and JSON strings. * You can manually tune settings for joins that include  [ranges](range-join.html)  or contain data with substanial  [skew](skew-join.html)  .       Opt-in behaviors ------------------   * Databricks provides a write serializable isolation guarantee by default; changing the  [isolation level](isolation-level.html)  to serializable can reduce throughput for concurrent operations, but might be necessary when read serializability is required. * You can use  [bloom filter indexes](bloom-filters.html)  to reduce the likelihood of scanning data files that don’t contain records matching a given condition.                                                      	To optimize Delta tables, Databricks provides many optimizations supporting a variety of workloads on the lakehouse, ranging from large-scale ETL processing to ad-hoc, interactive queries. Many of these optimizations take place automatically. Additionally, most Databricks Runtime features require Delta Lake, the default storage layer used to create tables in Databricks. Databricks configures default values that optimize most workloads. But, in some cases, changing configuration settings improves performance. Databricks also provides recommendations for enhanced performance, such as cloning tables, using the cost-based optimizer, auto-optimizing Delta tables, using Spark SQL to interact with semi-structured JSON data, using higher order functions, and manually tuning settings for joins that include ranges or contain data with substantial skew.	1
no_reaction	['https://docs.databricks.com/dev-tools/python-sql-connector.html']	how do I connect to a databricks database from python	"      Databricks SQL Connector for Python =====================================     The  [Databricks SQL Connector for Python](https://github.com/databricks/databricks-sql-python)   is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses. The Databricks SQL Connector for Python is easier to set up and use than similar Python libraries such as  [pyodbc](pyodbc.html)  . This library follows  [PEP 249 – Python Database API Specification v2.0](https://www.python.org/dev/peps/pep-0249/)   .        Requirements --------------   * A development machine running Python >=3.7 and <=3.11. * An existing  [cluster](../clusters/index.html)  or  [SQL warehouse](../sql/admin/create-sql-warehouse.html)  .       Get started -------------   * Gather the following information for the cluster or SQL warehouse that you want to use:         + The server hostname of the cluster. You can get this from the   **Server Hostname**    value in the   **[Advanced Options > JDBC/ODBC](../integrations/jdbc-odbc-bi.html#connection-details-cluster)**   tab for your cluster.  + The HTTP path of the cluster. You can get this from the   **HTTP Path**    value in the   **[Advanced Options > JDBC/ODBC](../integrations/jdbc-odbc-bi.html#connection-details-cluster)**   tab for your cluster.  + A Databricks   [personal access token](auth.html#pat)   for the workspace.    Note       As a security best practice, you should not hard-code this information into your code. Instead, you should retrieve this information from a secure location. For example, the code examples later in this article use environment variables.          + The server hostname of the SQL warehouse. You can get this from the   **Server Hostname**    value in the   [Connection Details](../integrations/jdbc-odbc-bi.html#connection-details-sql-warehouse)   tab for your SQL warehouse.  + The HTTP path of the SQL warehouse. You can get this from the   **HTTP Path**    value in the   [Connection Details](../integrations/jdbc-odbc-bi.html#connection-details-sql-warehouse)   tab for your SQL warehouse.  + A Databricks   [personal access token](auth.html#pat)   for the workspace.    Note       As a security best practice, you should not hard-code this information into your code. Instead, you should retrieve this information from a secure location. For example, the code examples later in this article use environment variables. * Install the Databricks SQL Connector for Python library on your development machine by running  `pip     install     databricks-sql-connector`  .       Examples ----------     The following code examples demonstrate how to use the Databricks SQL Connector for Python to query and insert data, query metadata, manage cursors and connections, and configure logging.       These code example retrieve their  `server\_hostname`  ,  `http\_path`  , and  `access\_token`  connection variable values from these environment variables:     * `DATABRICKS\_SERVER\_HOSTNAME`  , which represents the  **Server Hostname**   value from the requirements. * `DATABRICKS\_HTTP\_PATH`  , which represents the  **HTTP Path**   value from the requirements. * `DATABRICKS\_TOKEN`  , which represents your access token from the requirements.     You can use other approaches to retrieving these connection variable values. Using environment variables is just one approach among many.      * [Query data](#query-data) * [Insert data](#insert-data) * [Query metadata](#query-metadata) * [Manage cursors and connections](#manage-cursors-and-connections) * [Configure logging](#configure-logging)     ###  [Query data](#id6)     The following code example demonstrates how to call the Databricks SQL Connector for Python to run a basic SQL command on a cluster or SQL warehouse. This command returns the first two rows from the  `diamonds`  table.       The  `diamonds`  table is included in  [Sample datasets](../dbfs/databricks-datasets.html)  . This table is also featured in  [Tutorial: Query data with notebooks](../getting-started/quick-start.html)  .        ``` from databricks import sql import os  with sql.connect(server\_hostname = os.getenv(""DATABRICKS\_SERVER\_HOSTNAME""),                  http\_path       = os.getenv(""DATABRICKS\_HTTP\_PATH""),                  access\_token    = os.getenv(""DATABRICKS\_TOKEN"")) as connection:    with connection.cursor() as cursor:     cursor.execute(""SELECT \* FROM default.diamonds LIMIT 2"")     result = cursor.fetchall()      for row in result:       print(row)  ```      ###  [Insert data](#id7)     The following example demonstrate how to insert small amounts of data (thousands of rows):        ``` from databricks import sql import os  with sql.connect(server\_hostname = os.getenv(""DATABRICKS\_SERVER\_HOSTNAME""),                  http\_path       = os.getenv(""DATABRICKS\_HTTP\_PATH""),                  access\_token    = os.getenv(""DATABRICKS\_TOKEN"")) as connection:    with connection.cursor() as cursor:     cursor.execute(""CREATE TABLE IF NOT EXISTS squares (x int, x\_squared int)"")      squares = [(i, i \* i) for i in range(100)]     values = "","".join([f""({x}, {y})"" for (x, y) in squares])      cursor.execute(f""INSERT INTO squares VALUES {values}"")      cursor.execute(""SELECT \* FROM squares LIMIT 10"")      result = cursor.fetchall()      for row in result:       print(row)  ```      For large amounts of data, you should first upload the data to cloud storage and then execute the  [COPY INTO](../sql/language-manual/delta-copy-into.html)  command.       ###  [Query metadata](#id8)     There are dedicated methods for retrieving metadata. The following example retrieves metadata about columns in a sample table:        ``` from databricks import sql import os  with sql.connect(server\_hostname = os.getenv(""DATABRICKS\_SERVER\_HOSTNAME""),                  http\_path       = os.getenv(""DATABRICKS\_HTTP\_PATH""),                  access\_token    = os.getenv(""DATABRICKS\_TOKEN"")) as connection:    with connection.cursor() as cursor:     cursor.columns(schema\_name=""default"", table\_name=""squares"")     print(cursor.fetchall())  ```      ###  [Manage cursors and connections](#id9)     It is best practice to close any connections and cursors that have been finished with. This frees resources on Databricks clusters and Databricks SQL warehouses.       You can use a context manager (the  `with`  syntax used in previous examples) to manage the resources, or explicitly call  `close`  :        ``` from databricks import sql import os  connection = sql.connect(server\_hostname = os.getenv(""DATABRICKS\_SERVER\_HOSTNAME""),                          http\_path       = os.getenv(""DATABRICKS\_HTTP\_PATH""),                          access\_token    = os.getenv(""DATABRICKS\_TOKEN""))  cursor = connection.cursor()  cursor.execute(""SELECT \* from range(10)"") print(cursor.fetchall())  cursor.close() connection.close()  ```      ###  [Configure logging](#id10)     The Databricks SQL Connector uses Python’s  [standard logging module](https://docs.python.org/3/library/logging.html)   . You can configure the logging level similar to the following:        ``` from databricks import sql import os, logging  logging.getLogger(""databricks.sql"").setLevel(logging.DEBUG) logging.basicConfig(filename = ""results.log"",                     level    = logging.DEBUG)  connection = sql.connect(server\_hostname = os.getenv(""DATABRICKS\_SERVER\_HOSTNAME""),                          http\_path       = os.getenv(""DATABRICKS\_HTTP\_PATH""),                          access\_token    = os.getenv(""DATABRICKS\_TOKEN""))  cursor = connection.cursor()  cursor.execute(""SELECT \* from range(10)"")  result = cursor.fetchall()  for row in result:    logging.debug(row)  cursor.close() connection.close()  ```         API reference ---------------    * [Package](#package) * [Module](#module)      + [Methods](#methods)          - [`connect`    method](#connect-method) * [Classes](#classes)      + [`Connection`   class](#connection-class)          - [Methods](#methods)              * [`close`     method](#close-method)    * [`cursor`     method](#cursor-method)  + [`Cursor`   class](#cursor-class)          - [Attributes](#attributes)              * [`arraysize`     attribute](#arraysize-attribute)    * [`description`     attribute](#description-attribute)   - [Methods](#methods)              * [`cancel`     method](#cancel-method)    * [`close`     method](#close-method)    * [`execute`     method](#execute-method)    * [`executemany`     method](#executemany-method)    * [`cat"	To connect to a Databricks database from Python, you can use the Databricks SQL Connector for Python library. First, you need to gather the server hostname, HTTP path, and personal access token for the cluster or SQL warehouse that you want to use. Then, you can install the Databricks SQL Connector for Python library on your development machine by running `pip install databricks-sql-connector`. Finally, you can use the `connect` method from the library to establish a connection to the database.	1
no_reaction	['https://docs.databricks.com/security/auth-authz/access-control/dashboard-acl.html']	How to share a dashboard?	    Dashboard access control ==========================     With dashboard access control, individual permissions determine a user’s abilities. This article describes the individual permissions and how to configure dashboard access control.        Dashboard permissions -----------------------     There are four permission levels for a dashboard:  **No Permissions**   ,  **Can Run**   ,  **Can Edit**   , and  **Can Manage**   . Only the dashboard owner has  **Can Manage**   permission; this permission  *cannot*   be shared.       The table lists the abilities for each permission.              |   Ability   |   No Permissions   |   Can View   |   Can Run   |   Can Edit   |   Can Manage   | | --- | --- | --- | --- | --- | --- | |   See in dashboard list   |  |   x   |   x   |   x   |   x   | |   View dashboard and results   |  |   x   |   x   |   x   |   x   | |   Refresh query results in the dashboard (or choose different parameters)   |  |  |   x   |   x   |   x   | |   Edit dashboard   |  |  |  |   x  [(1)](#1)  |   x   | |   Modify permissions   |  |  |  |  |   x   | |   Delete dashboard   |  |  |  |  |   x   |    **(1)**   Requires the  **Run as viewer**  [sharing setting](#sharing-settings)  .        Note       The principal used to execute a query is the user that created the query, not the user that clicks the  **Refresh**   button.          Sharing settings ------------------     Dashboards support two types of sharing settings:  **Run as viewer**   and  **Run as owner**   .       To configure sharing settings:     1. Click  ![Dashboards Icon](../../../_images/dashboards-icon.png) **Dashboards**   in the sidebar. 2. Click a dashboard. 3. Click the  ![Share Button](../../../_images/share-button.png)  button at the top right. The Manage Permissions dialog appears. 4. In the  **Sharing settings > Credentials**   field at the bottom, select:      * **Run as viewer**    : The viewer’s credential is used to execute the queries in the dashboard. The viewer must also have at least   **Can Use**    permissions on the warehouse.  * **Run as owner**    : The owner’s credential is used to execute the queries in the dashboard.       Manage dashboard permission with folders ------------------------------------------     You can manage dashboard permissions by adding dashboards to folders. Dashboard in a folder inherit all permissions settings of that folder. For example, a user that has  **Can Run**   permission on a folder has  **Can Run**   permission on the dashboards in that folder. To learn about configuring permissions on folders, see  [Folder permissions](workspace-acl.html#folder-permissions)  .       To learn more about organizing dashboards into folders, see  [Workspace browser in Databricks SQL](../../../sql/user/workspace-browser/index.html)  .         Manage dashboard permissions using the UI -------------------------------------------   1. Click  ![Dashboards Icon](../../../_images/dashboards-icon.png) **Dashboards**   in the sidebar. 2. Click a dashboard. 3. Click the  ![Share Button](../../../_images/share-button.png)  button at the top right. The Manage Permissions dialog appears.      ![Manage dashboard permissions](../../../_images/manage-permissions1.png) 4. Follow the steps based on the permission type you want to grant:      * **Can Edit**    permission          1. Select the    **Run as viewer**    [sharing setting](#sharing-settings)    .   2. Click the top field to add a user or group.   3. Select the    **Can Edit**     permission.   4. Click    **Add**     .  * **Can Run**    permission          1. Click the top field to add a user or group.   2. Select the    **Can Run**     permission.   3. Click    **Add**     . 5. Dismiss the dialog.                                                      	To share a dashboard, you need to click on the Share button at the top right of the dashboard and then select the sharing settings. There are two types of sharing settings: Run as viewer and Run as owner. You can manage dashboard permissions by adding dashboards to folders. Dashboard in a folder inherit all permissions settings of that folder.	1
no_reaction	['https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html']	How to transition a model stage using the mlflow client api	"      Log, load, register, and deploy MLflow models ===============================================     An MLflow  [Model](https://mlflow.org/docs/latest/models.html)   is a standard format for packaging machine learning models that can be used in a variety of downstream tools—for example, batch inference on Apache Spark or real-time serving through a REST API. The format defines a convention that lets you save a model in different  [flavors](https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors)   (python-function, pytorch, sklearn, and so on), that can be understood by different model  [serving and inference platforms](https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools)   .          Log and load models ---------------------     When you log a model, MLflow automatically logs  `requirements.txt`  and  `conda.yaml`  files. You can use these files to recreate the model development environment and reinstall dependencies using  `virtualenv`  (recommended) or  `conda`  .        Important       Anaconda Inc. updated their  [terms of service](https://www.anaconda.com/terms-of-service)   for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See  [Anaconda Commercial Edition FAQ](https://www.anaconda.com/blog/anaconda-commercial-edition-faq)   for more information. Your use of any Anaconda channels is governed by their terms of service.       MLflow models logged before  [v1.18](https://mlflow.org/news/2021/06/18/1.18.0-release/index.html)   (Databricks Runtime 8.3 ML or earlier) were by default logged with the conda  `defaults`  channel (  <https://repo.anaconda.com/pkgs/>  ) as a dependency. Because of this license change, Databricks has stopped the use of the  `defaults`  channel for models logged using MLflow v1.18 and above. The default channel logged is now  `conda-forge`  , which points at the community managed  <https://conda-forge.org/>  .       If you logged a model before MLflow v1.18 without excluding the  `defaults`  channel from the conda environment for the model, that model may have a dependency on the  `defaults`  channel that you may not have intended. To manually confirm whether a model has this dependency, you can examine  `channel`  value in the  `conda.yaml`  file that is packaged with the logged model. For example, a model’s  `conda.yaml`  with a  `defaults`  channel dependency may look like this:        ``` channels: - defaults dependencies: - python=3.8.8 - pip - pip:  - mlflow  - scikit-learn==0.23.2  - cloudpickle==1.6.0  name: mlflow-env  ```      Because Databricks can not determine whether your use of the Anaconda repository to interact with your models is permitted under your relationship with Anaconda, Databricks is not forcing its customers to make any changes. If your use of the Anaconda.com repo through the use of Databricks is permitted under Anaconda’s terms, you do not need to take any action.       If you would like to change the channel used in a model’s environment, you can re-register the model to the model registry with a new  `conda.yaml`  . You can do this by specifying the channel in the  `conda\_env`  parameter of  `log\_model()`  .       For more information on the  `log\_model()`  API, see the MLflow documentation for the model flavor you are working with, for example,  [log\_model for scikit-learn](https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)   .       For more information on  `conda.yaml`  files, see the  [MLflow documentation](https://www.mlflow.org/docs/latest/models.html#additional-logged-files)   .       ###   API commands     To log a model to the MLflow  [tracking server](tracking.html)  , use  `mlflow.<model-type>.log\_model(model,     ...)`  .       To load a previously logged model for inference or further development, use  `mlflow.<model-type>.load\_model(modelpath)`  , where  `modelpath`  is one of the following:     * a run-relative path (such as  `runs:/{run\_id}/{model-path}`  ) * a DBFS path * a  [registered model](model-registry.html)  path (such as  `models:/{model\_name}/{model\_stage}`  ).     For a complete list of options for loading MLflow models, see  [Referencing Artifacts in the MLflow documentation](https://www.mlflow.org/docs/latest/concepts.html#artifact-locations)   .       For Python MLflow models, an additional option is to use  `mlflow.pyfunc.load\_model()`  to load the model as a generic Python function. You can use the following code snippet to load the model and score data points.        ``` model = mlflow.pyfunc.load\_model(model\_path) model.predict(model\_input)  ```      As an alternative, you can export the model as an Apache Spark UDF to use for scoring on a Spark cluster, either as a batch job or as a real-time  [Spark Streaming](../structured-streaming/index.html)  job.        ``` # load input data table as a Spark DataFrame input\_data = spark.table(input\_table\_name) model\_udf = mlflow.pyfunc.spark\_udf(model\_path) df = input\_data.withColumn(""prediction"", model\_udf())  ```      ###   Log model dependencies     To accurately load a model, you should make sure the model dependencies are loaded with the correct versions into the notebook environment. In Databricks Runtime 10.5 ML and above, MLflow warns you if a mismatch is detected between the current environment and the model’s dependencies.       Additional functionality to simplify restoring model dependencies is included in Databricks Runtime 11.0 ML and above. In Databricks Runtime 11.0 ML and above, for  `pyfunc`  flavor models, you can call  `mlflow.pyfunc.get\_model\_dependencies`  to retrieve and download the model dependencies. This function returns a path to the dependencies file which you can then install by using  `%pip     install     <file-path>`  . When you load a model as a PySpark UDF, specify  `env\_manager=""virtualenv""`  in the  `mlflow.pyfunc.spark\_udf`  call. This restores model dependencies in the context of the PySpark UDF and does not affect the outside environment.       You can also use this functionality in Databricks Runtime 10.5 or below by manually installing  [MLflow version 1.25.0 or above](https://www.mlflow.org/docs/latest/index.html)   :        ``` %pip install ""mlflow>=1.25.0""  ```      For additional information on how to log model dependencies (Python and non-Python) and artifacts, see  [Log model dependencies](log-model-dependencies.html)  .       Learn how to log model dependencies and custom artifacts for model serving:     * [Deploy models with dependencies](log-model-dependencies.html#deploy-dependencies) * [Use custom Python libraries with Model Serving](../machine-learning/model-serving/private-libraries-model-serving.html) * [Package custom artifacts for Model Serving](../machine-learning/model-serving/model-serving-custom-artifacts.html)    * [Log model dependencies](log-model-dependencies.html)      ###   Automatically generated code snippets in the MLflow UI     When you log a model in a Databricks notebook, Databricks automatically generates code snippets that you can copy and use to load and run the model. To view these code snippets:     1. Navigate to the Runs screen for the run that generated the model. (See  [View notebook experiment](tracking.html#view-notebook-experiment)  for how to display the Runs screen.) 2. Scroll to the  **Artifacts**   section. 3. Click the name of the logged model. A panel opens to the right showing code you can use to load the logged model and make predictions on Spark or pandas DataFrames.    ![Artifact panel code snippets](../_images/code-snippets.png)    ###   Examples     For examples of logging models, see the examples in  [Track machine learning training runs examples](tracking.html#tracking-examples)  . For an example of loading a logged model for inference, see the following example.      * [Model inference example](model-example.html)         Register models in the Model Registry ---------------------------------------     You can register models in the  [MLflow Model Registry](model-registry.html)  , a centralized model store that provides a UI and set of APIs to manage the full lifecycle of MLflow Models. For general information about the Model Registry, see  [MLflow Model Registry on Databricks](model-registry.html)  . For instructions on how to use the Model Registry to manage models in Databricks, see  [Manage model lifecycle](../machine-learning/manage-model-lifecycle/index.html)  .       To register a model using the API, use  `mlflow.register\_model(""runs:/{run\_id}/{model-path}"",     ""{registered-model-name}"")`  .         Save models to DBFS ---------------------     To save a model locally, use  `mlflow.<model-type>.save\_model(model,     modelpath)`  .  `modelpath`  must be a  [DBFS](../dbfs/index.html)  path. For example, if you use a DBFS location  `dbfs:/my\_project\_models`  to store your project work, you must use the model path  `/dbfs/my\_project\_models`  :        ```   modelpath = ""/dbfs/my\_project\_models/model-%f-%f"" % (alpha, l1\_ratio)   mlflow.sklearn.save\_model(lr, modelpath)  ```      For MLlib models, use  [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines)   .         Download model artifacts --------------------------     You can download the logged model artifacts (such as model files, plots, and metrics) for a registered model with various APIs.      [Python API](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_model_version_download_uri)   example:        ``` from mlflow.store.artifact.models\_artifact\_repo import ModelsArtifactRepository  model\_uri = MlflowClient.get\_model\_version\_download\_uri(model\_name, model\_version) ModelsArtifactRepository(model\_uri).download\_artifacts(artifact\_path="""")  ```     [Java API](https://mlflow.org/docs/latest/java_api/org/mlflow/tracking/MlflowClient.html#downloadModelVersion-java.lang"	"To transition a model stage using the mlflow client API, you can use the `transition_model_version_stage` function from the `mlflow.tracking.client` module. This function allows you to transition a registered model version to a new stage, such as from ""Staging"" to ""Production"". Here is an example code snippet:  ``` import mlflow  # Set the tracking URI to the MLflow server mlflow.set_tracking_uri(""http://localhost:5000"")  # Set the model name and version model_name = ""my_model"" model_version = ""1""  # Transition the model version to the ""Production"" stage mlflow.tracking.client.MlflowClient().transition_model_version_stage(     name=model_name,     version=model_version,     stage=""Production"" ) ```  Note that you need to set the tracking URI to the MLflow server and have the appropriate permissions to transition the model stage."	0
no_reaction	['https://docs.databricks.com/delta-live-tables/index.html']	how do i transition to delta live tables	    Release 2022.42 =================    **October 17 - October 21, 2022**       These features and improvements were released with the 2022.42 release of Delta Live Tables.          Databricks Runtime versions used by this release --------------------------------------------------    **Channel:**     * CURRENT (default): Databricks Runtime 11.0.8 * PREVIEW: Databricks Runtime 11.0.8       New features and improvements in this release -----------------------------------------------   * Setting attributes that are not supported with Delta Live Tables clusters now results in an error before the pipeline begins running. * You can now use cloud-specific cluster attributes for all supported clouds.       * A new onboarding widget provides a guide to create a Delta Live Tables pipeline when you launch a new workspace.       Bug Fixes in this release ---------------------------   * With this release, Delta Live Tables does not interrupt triggered updates during a service upgrade. Previously, Delta Live Tables incorrectly restarted triggered updates after a service upgrade.   * This release fixes a bug causing the Delta streaming source to not initialize correctly when restarting a query using  `AvailableNow()`  .                                                      	To transition to Delta Live Tables, you can create a new Delta Live Tables pipeline using the onboarding widget provided in the workspace. The 2022.42 release of Delta Live Tables also includes improvements such as setting attributes that are not supported resulting in an error before the pipeline begins running, and the ability to use cloud-specific cluster attributes for all supported clouds.	0
no_reaction	['https://docs.databricks.com/machine-learning/feature-store/workflow-overview-and-notebook.html']	can you write me an example of how to use the feature store?	    Databricks Feature Store ==========================     This page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.       The Databricks Feature Store library is available only on Databricks Runtime for Machine Learning and is accessible through Databricks notebooks and workflows.        Note       At this time, Feature Store does not support writing to a Unity Catalog metastore. In Unity Catalog-enabled workspaces, you can write feature tables only to the default Hive metastore.         What is a feature store? --------------------------     A feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.       Machine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.       Developing features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.         Why use Databricks Feature Store? -----------------------------------     Databricks Feature Store is fully integrated with other components of Databricks.     * Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features. * Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature. * Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier. * Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.       Start using Feature Store ---------------------------     See the following articles to get started with Feature Store:     * Try one of the  [example notebooks](example-notebooks.html)  that illustrate feature store capabilities. * See the reference material for the  [Feature Store Python API](python-api.html)  . * Learn about  [training models with Feature Store](train-models-with-feature-store.html)  . * Learn about  [working with feature tables](feature-tables.html)  . * Use  [time series feature tables and point-in-time lookups](time-series.html)  to retrieve the latest feature values as of a particular time for training or scoring a model. * Learn about  [publishing features to online feature stores](online-feature-stores.html)  for real-time serving and automatic feature lookup. * [Share feature tables across workspaces](multiple-workspaces.html)  . * [Control access to feature tables](access-control.html)  .       More information ------------------     For more information on best practices for using Feature Store, download  [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores)   .                                                          	To use the Databricks Feature Store, you can start by trying one of the example notebooks that illustrate feature store capabilities, or you can see the reference material for the Feature Store Python API. You can also learn about training models with Feature Store, working with feature tables, using time series feature tables and point-in-time lookups, publishing features to online feature stores for real-time serving and automatic feature lookup, sharing feature tables across workspaces, and controlling access to feature tables. Databricks Feature Store is fully integrated with other components of Databricks, providing discoverability, lineage, integration with model scoring and serving, and point-in-time lookups.	1
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.to_table.html']	How do you write a Spark DataFrame to a table?	"   pyspark.pandas.DataFrame.to\_table  [¶](#pyspark-pandas-dataframe-to-table ""Permalink to this headline"") ==========================================================================================================  `DataFrame.`  `to_table`    (   *name     :     str*  ,  *format     :     Optional    [    str    ]     =     None*  ,  *mode     :     str     =     'w'*  ,  *partition\_cols     :     Union[str, List[str], None]     =     None*  ,  *index\_col     :     Union[str, List[str], None]     =     None*  ,  *\*\*     options     :     Any*   )    → None  [[source]](../../../_modules/pyspark/pandas/frame.html#DataFrame.to_table) [¶](#pyspark.pandas.DataFrame.to_table ""Permalink to this definition"")    Write the DataFrame into a Spark table.  [`DataFrame.spark.to\_table()`](pyspark.pandas.DataFrame.spark.to_table.html#pyspark.pandas.DataFrame.spark.to_table ""pyspark.pandas.DataFrame.spark.to_table"")  is an alias of  [`DataFrame.to\_table()`](#pyspark.pandas.DataFrame.to_table ""pyspark.pandas.DataFrame.to_table"")  .     Parameters    **name**    str, required     Table name in Spark.    **format**    string, optional     Specifies the output data source format. Some common ones are:    * ‘delta’ * ‘parquet’ * ‘orc’ * ‘json’ * ‘csv’  **mode**    str {‘append’, ‘overwrite’, ‘ignore’, ‘error’, ‘errorifexists’}, default     ‘overwrite’. Specifies the behavior of the save operation when the table exists already.    * ‘append’: Append the new data to existing data. * ‘overwrite’: Overwrite existing data. * ‘ignore’: Silently ignore this operation if data already exists. * ‘error’ or ‘errorifexists’: Throw an exception if data already exists.  **partition\_cols**    str or list of str, optional, default None     Names of partitioning columns    **index\_col: str or list of str, optional, default: None**    Column names to be used in Spark to represent pandas-on-Spark’s index. The index name in pandas-on-Spark is ignored. By default, the index is always lost.    **options**    Additional options passed directly to Spark.     Returns     None     See also    [`read\_table`](pyspark.pandas.read_table.html#pyspark.pandas.read_table ""pyspark.pandas.read_table"")  [`DataFrame.to\_spark\_io`](pyspark.pandas.DataFrame.to_spark_io.html#pyspark.pandas.DataFrame.to_spark_io ""pyspark.pandas.DataFrame.to_spark_io"")  [`DataFrame.spark.to\_spark\_io`](pyspark.pandas.DataFrame.spark.to_spark_io.html#pyspark.pandas.DataFrame.spark.to_spark_io ""pyspark.pandas.DataFrame.spark.to_spark_io"")  [`DataFrame.to\_parquet`](pyspark.pandas.DataFrame.to_parquet.html#pyspark.pandas.DataFrame.to_parquet ""pyspark.pandas.DataFrame.to_parquet"")   Examples    ``` >>> df = ps.DataFrame(dict( ...    date=list(pd.date\_range('2012-1-1 12:00:00', periods=3, freq='M')), ...    country=['KR', 'US', 'JP'], ...    code=[1, 2 ,3]), columns=['date', 'country', 'code']) >>> df  date country code 0 2012-01-31 12:00:00 KR 1 1 2012-02-29 12:00:00 US 2 2 2012-03-31 12:00:00 JP 3  ```  ``` >>> df.to\_table('%s.my\_table' % db, partition\_cols='date')  ```  [pyspark.pandas.read\_table](pyspark.pandas.read_table.html ""previous page"")  [pyspark.pandas.read\_delta](pyspark.pandas.read_delta.html ""next page"")   "	To write a Spark DataFrame to a table, you can use the `to_table()` method of the DataFrame. This method writes the DataFrame into a Spark table. The required parameter is the table name in Spark, and there are optional parameters such as format, mode, partition_cols, and index_col. The `to_table()` method is an alias of `spark.to_table()`.	1
no_reaction	['https://docs.databricks.com/security/keys/index.html']	How do i encrypt data?	    Data security and encryption ==============================     This article introduces data security configurations to help protect your data       For information about securing access to your data, see  [Data governance guide](../../data-governance/index.html)  .          Overview of data security and encryption ------------------------------------------     Databricks provides encryption features to help protect your data. Not all security features are available on all pricing tiers. The following table contains an overview of the features and how they align to pricing plans.              |   Feature   |   Pricing tier   | | --- | --- | |   Customer-managed keys for encryption   |   Enterprise   | |   Encrypt traffic between cluster worker nodes   |   Enterprise   | |   Encrypt queries, query history, and query results   |   Enterprise   | |   Workspace settings for data security   |   Standard   |             Enable customer-managed keys for encryption ---------------------------------------------     Databricks supports adding a customer-managed key to help protect and control access to data. There are three customer-managed key features for different types of data:     * **Customer-managed keys for managed services**   : Managed services data in the Databricks control plane is encrypted at rest. You can add a customer-managed key for managed services to help protect and control access to the following types of encrypted data:      + Notebook source files that are stored in the control plane.  + Notebook results for notebooks that are stored in the control plane.  + Secrets stored by the secret manager APIs.  + Databricks SQL queries and query history.  + Personal access tokens or other credentials used to set up Git integration with Databricks Repos.     For more information, see  [Customer-managed keys for managed services](customer-managed-keys-managed-services-aws.html)  .     * **Customer-managed keys for workspace storage**   : You can configure your own key to encrypt the data on the Amazon S3 bucket in your AWS account that you specified when you created your workspace. You can optionally use the same key to encrypt your cluster’s EBS volumes. For more information, see  [Customer-managed keys for workspace storage](customer-managed-keys-storage-aws.html)     For more details of which customer-managed key features in Databricks protect different types kinds of data, see  [Customer-managed keys for encryption](customer-managed-keys.html)  .         Encrypt queries, query history, and query results ---------------------------------------------------     You can use your own key from AWS KMS to encrypt the Databricks SQL queries and your query history stored in the Databricks  [control plane](../../getting-started/overview.html)  . For more details, see  [Encrypt queries, query history, and query results](sql-encryption.html)        Manage workspace settings ---------------------------     Databricks workspace administrators can manage their workspace’s security settings, such as the ability to download notebooks and enforcing the user isolation cluster access mode. For more information, see  [Manage workspace settings](../../administration-guide/workspace/settings/index.html)  .                                                        	"To encrypt data in Databricks, you can use customer-managed keys for encryption, encrypt traffic between cluster worker nodes, and encrypt queries, query history, and query results. Customer-managed keys for encryption are available on the Enterprise pricing tier, while encrypting traffic between cluster worker nodes and encrypting queries, query history, and query results are available on the Standard pricing tier. For more information on how to enable customer-managed keys for encryption, see the ""Customer-managed keys for encryption"" section in the context."	1
no_reaction	['https://kb.databricks.com/training/how-can-i-locate-all-of-the-courses-that-are-available-to-me']	How do I access Databricks academy?	Title: How do I create an Academy account?  Description: Customer, Partner, and Microsoft Academy  Answer: Step 1:  For customers, click on: https://customer-academy.databricks.com/learn For partners, click on:  https://partner-academy.databricks.com/learn   For Microsoft employees, click on:  https://microsoft-academy.databricks.com/learn Step 2: Click on Register and fill in all the mandatory details. Step 3: Select the next option to enter the company email address. Step 4: Click on register to complete the registration process.  	"To access Databricks Academy, you can visit the following links based on your status:  - For customers: https://customer-academy.databricks.com/learn  - For partners: https://partner-academy.databricks.com/learn  - For Microsoft employees: https://microsoft-academy.databricks.com/learn  Once you are on the Academy website, you can register by clicking on the ""Register"" button and filling in all the mandatory details. Then, select the option to enter your company email address and complete the registration process by clicking on ""Register""."	1
no_reaction	['https://docs.databricks.com/administration-guide/account-settings/audit-logs.html']	How do i find the account console audit logs?	"      Configure audit logging =========================      Note       This feature is available on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   .        Databricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns.       There are two types of logs:     * Workspace-level audit logs with workspace-level events. * Account-level audit logs with account-level events.     For a list of each of these types of events and the associated services, see  [Audit events](#events)  .          Configure verbose audit logs ------------------------------     In addition to the default  [events](#events)  , you can configure a workspace to generate additional events by enabling  *verbose audit logs*   .       To enable verbose audit logs, your account and workspace must be on the E2 version of the platform. To confirm the version of the platform you are using, contact your Databricks representative.      ###   Enable or disable verbose audit logs   1. As an admin, go to the Databricks  [admin console](../workspace/index.html#admin-console)  . 2. Click  **Workspace settings**   . 3. Next to  **Verbose Audit Logs**   , enable or disable the feature.     When you enable or disable verbose logging, an auditable event is emitted in the category  `workspace`  with action  `workspaceConfKeys`  . The  `workspaceConfKeys`  request parameter is  `enableVerboseAuditLogs`  . The request parameter  `workspaceConfValues`  is  `true`  (feature enabled) or  `false`  (feature disabled).       ###   Additional verbose notebook action     Additional verbose action in audit log category  `notebook`  :           |   Action   |   Description   |   Request Parameters   | | --- | --- | --- | |  `runCommand`  |   Emitted after Databricks runs a command in a notebook. A command corresponds to a cell in a notebook.   |   [“notebookId”, “executionTime”, “status”, “commandId”, “commandText”]   |       ###   Additional verbose Databricks SQL actions     Additional actions in audit log category  `databrickssql`  :           |   Action   |   Description   |   Request Parameters   | | --- | --- | --- | |  `commandSubmit`  |   Runs when a command is submitted to Databricks SQL.   |   [“commandText”, “warehouseId”, “commandId”]   | |  `commandFinish`  |   Runs when a command completes or a command is cancelled.   |   [“warehouseId”, “commandId”]   |     Check the  `response`  field for additional information related to the command result:     * `statusCode`  - The HTTP response code. This will be error 400 if it is a general error.      + `errorMessage`   - Error message. In some cases for certain long-running commands, the   `errorMessage`   field may not populate on failure.  + `result`   :   *This field is empty*    .          Configure audit log delivery ------------------------------     As a Databricks account admin, you can configure low-latency delivery of audit logs in JSON file format to an AWS S3 storage bucket, where you can make the data available for  [usage analysis](#analyze)  . Databricks delivers a separate JSON file for each workspace in your account and a separate file for account-level events.       After initial setup or other log delivery configuration changes, expect a delay of up to one hour until changes take effect. After logging delivery begins, auditable events are typically logged within 15 minutes. For the file naming, delivery rules, and schema, see  [Audit delivery details and format](#details)  .       The API to configure low-latency delivery of audit logs is  [Account API 2.0](../../dev-tools/api/latest/account.html)  , which is the same API used to configure  [billable usage log delivery](billable-usage-delivery.html)  .       You can optionally deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery. This allows flexibility, for example setting up workspaces from multiple AWS accounts to deliver to the same S3 bucket. This option requires that you configure an S3 bucket policy that references a cross-account IAM role. Instructions and a policy template are provided in this article.       Access to the logs depends on how you set up the S3 bucket. Databricks delivers logs to your S3 bucket with AWS’s built-in  [BucketOwnerFullControl Canned ACL](https://docs.aws.amazon.com/AmazonS3/latest/dev/about-object-ownership.html)   so that account owners and designees can download the logs directly. To support bucket ownership for newly-created objects, you must set your bucket’s  **S3 Object Ownership**   setting to the value  **Bucket owner preferred**   .        Important       If instead you set your bucket’s  **S3 Object Ownership**   setting to  **Object writer**   , new objects such as your logs remain owned by the uploading account, which is by default the IAM role you created and specified to access your bucket. This can make it difficult to access the logs, because you cannot access them from the AWS console or automation tools that you authenticated with as the bucket owner.        Databricks recommends that you review  [Security Best Practices for S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html)   for guidance around protecting the data in your bucket from unwanted access.      ###   Configuration options     To configure audit log delivery, you have the following options.     * If you have one workspace in your Databricks account, follow the instructions in the sections that follow, creating a single configuration object with a common configuration for your workspace. * If you have multiple workspaces in the same Databricks account, you can do any of the following:      + Share the same configuration (log delivery S3 bucket and IAM role) for all workspaces in the account. This is the only configuration option that also delivers account-level audit logs. It is the default option.  + Use separate configurations for each workspace in the account.  + Use separate configurations for different groups of workspaces, each sharing a configuration. * If you have multiple workspaces, each associated with a separate Databricks account, you must create unique storage and credential configuration objects for each account, but you can reuse an S3 bucket or IAM role between these configuration objects.      Note       Even though you use the Account API to configure log delivery, you can configure log delivery for any workspace, including workspaces that were not created using the Account API.        ###   High-level flow     The high-level flow of audit log delivery:     1. [Configure storage](#configure-storage)  : In AWS, create a new AWS S3 bucket. Using Databricks APIs, call the Account API to create a storage configuration object that uses the bucket name.        Note       To deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery, you need to add an S3 bucket policy. You do not add the policy in this step. 2. [Configure credentials](#configure-credentials)  : In AWS, create the appropriate AWS IAM role. Using Databricks APIs, call the Account API to create a credentials configuration object that uses the IAM role’s ARN. The role policy can specify a path prefix for log delivery within your S3 bucket. You can choose to define an IAM role to include multiple path prefixes if you want log delivery configurations for different workspaces that share the S3 bucket but use different path prefixes. 3. [Optional cross-account support](#cross-account-policy)  To deliver logs to an AWS account other than the account of the IAM role that you create for log delivery, add an S3 bucket policy. This policy references IDs for the cross-account IAM role that you created in the previous step. 4. [Call the log delivery API](#log-delivery-api)  : Call the Account API to create a log delivery configuration that uses the credential and storage configuration objects from previous steps. This step lets you specify if you want to associate the log delivery configuration for all workspaces in your account (current and future workspaces) or for a specific set of workspaces. For a list of account-level events, see  [Audit events](#events)  .     After you complete these steps, you can access the JSON files. The delivery location is:        ``` bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs\_<internal-id>.json  ```      If you configure audit log delivery for the entire account, account-level audit events that are not associated with any single workspace are delivered to the  `workspaceId=0`  partition.       New JSON files are delivered every few minutes, potentially overwriting existing files for each workspace. When you initially set up audit log delivery, it can take up to one hour for log delivery to begin. After audit log delivery begins, auditable events are typically logged within 15 minutes. Additional configuration changes typically take an hour to take effect.       For more information about accessing these files and analyzing them using Databricks, see  [Analyze audit logs](#analyze)  .        Important       There is a limit on the number of log delivery configurations available per account (each limit applies separately to each log type including billable usage and audit logs). You can create a maximum of two enabled account-level delivery configurations (configurations without a workspace filter) per type. Additionally, you can create and enable two workspace level delivery configurations per workspace for each log type, meaning the same workspace ID can occur in the workspace filter for no more than two delivery configurations per log type. You cannot delete a log delivery configuration, but you can disable it. You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.        ###   Requirements   * Account admin email address and password to authenticate with the APIs. The email address and password are both case sensitive. * Account ID. Get your account ID from  [the account console](index.html#account-id)  .     ###   How to authenticate to the APIs     The APIs described in this article are published on the  `accounts.cloud.databricks.com`  base endpoint for all AWS regional deployments.       Use the following base URL for API requests:  `https://accounts.cloud.databricks.com/api/2.0/`      This REST API requires HTTP basic authentication, which involves setting the HTTP header  `Authorization`  . In this article,  *username*   refers to your account admin email address. The email address is case sensitive. There are several ways to provide your credentials to tools such as curl.     * Pass your username and account password separately in the headers of each request in  `<username>:<password>`  syntax.       For example:        ``` curl -X GET -u `<username>:<password>` -H ""Content-Type: application/json"" \  'https://accounts.cloud.databricks.com/api/2.0/accounts/<account-id>/<endpoint>'  ``` * Apply base64 encoding to your  `<username>:<password>`  string and provide it directly in the HTTP header:        ``` curl -X GET -H ""Content-Type: application/json"" \  -H 'Authorization: Basic <base64-username-pw>'  'https://accounts.cloud.databricks.com/api/2.0/accounts/<account-id>/<endpoint>'  ``` * Create a  [.netrc](https://everything.curl.dev/usingcurl/netrc)   file with  `machine`  ,  `login`  , and  `password`  properties:        ``` machine accounts.cloud.databricks.com login <username>"	"To find the account console audit logs, you can configure low-latency delivery of audit logs in JSON file format to an AWS S3 storage bucket. Databricks delivers a separate JSON file for each workspace in your account and a separate file for account-level events. As a Databricks account admin, you can configure this by following the steps outlined in the ""Configure audit log delivery"" section of the document."	1
no_reaction	['https://docs.databricks.com/dev-tools/api/2.0/jobs.html']	How to install Maven library in  with jobs api	        Jobs API 2.1 ==============                                                     	The context provided is incomplete and does not provide any information on how to install Maven library in Jobs API 2.1. Can you please provide more information or context related to the question?	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.mode.html']	Describe when replacing missing values with the mode value is an appropriate way to handle missing values.	"   pyspark.pandas.DataFrame.mode  [¶](#pyspark-pandas-dataframe-mode ""Permalink to this headline"") =================================================================================================  `DataFrame.`  `mode`    (   *axis     :     Union    [    int    ,    str    ]     =     0*  ,  *numeric\_only     :     bool     =     False*  ,  *dropna     :     bool     =     True*   )    → pyspark.pandas.frame.DataFrame  [[source]](../../../_modules/pyspark/pandas/frame.html#DataFrame.mode) [¶](#pyspark.pandas.DataFrame.mode ""Permalink to this definition"")    Get the mode(s) of each element along the selected axis.     The mode of a set of values is the value that appears most often. It can be multiple values.     New in version 3.4.0.     Parameters    **axis**    {0 or ‘index’}, default 0     Axis for the function to be applied on.    **numeric\_only**    bool, default False     If True, only apply to numeric columns.    **dropna**    bool, default True     Don’t consider counts of NaN/NaT.     Returns     DataFrame     The modes of each column or row.     See also    [`Series.mode`](pyspark.pandas.Series.mode.html#pyspark.pandas.Series.mode ""pyspark.pandas.Series.mode"")   Return the highest frequency value in a Series.    [`Series.value\_counts`](pyspark.pandas.Series.value_counts.html#pyspark.pandas.Series.value_counts ""pyspark.pandas.Series.value_counts"")   Return the counts of values in a Series.     Examples    ``` >>> df = ps.DataFrame([('bird', 2, 2), ...                    ('mammal', 4, np.nan), ...                    ('arthropod', 8, 0), ...                    ('bird', 2, np.nan)], ...                   index=('falcon', 'horse', 'spider', 'ostrich'), ...                   columns=('species', 'legs', 'wings')) >>> df  species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN  ```   By default missing values are not considered, and the mode of wings are both 0 and 2. Because the resulting DataFrame has two rows, the second row of  `species`  and  `legs`  contains  `NaN`  .    ``` >>> df.mode()  species legs wings 0 bird 2.0 0.0 1 None NaN 2.0  ```   Setting  `dropna=False` `NaN`  values are considered and they can be the mode (like for wings).    ``` >>> df.mode(dropna=False)  species legs wings 0 bird 2 NaN  ```   Setting  `numeric\_only=True`  , only the mode of numeric columns is computed, and columns of other types are ignored.    ``` >>> df.mode(numeric\_only=True)  legs wings 0 2.0 0.0 1 NaN 2.0  ```  [pyspark.pandas.DataFrame.median](pyspark.pandas.DataFrame.median.html ""previous page"")  [pyspark.pandas.DataFrame.pct\_change](pyspark.pandas.DataFrame.pct_change.html ""next page"")   "	Replacing missing values with the mode value is an appropriate way to handle missing values when dealing with categorical data. The mode is the value that appears most often in a set of values, and it can be multiple values. The `DataFrame.mode` function in PySpark can be used to get the mode(s) of each element along the selected axis. By default, missing values are not considered, but they can be considered by setting `dropna=False`.	0
no_reaction	['https://docs.databricks.com/sql/language-manual/sql-ref-functions-builtin-alpha.html']	What is the list of databricks functions?	    Alphabetical list of built-in functions =========================================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       This article provides an alphabetically-ordered list of built-in functions and operators in Databricks.      * [`abs`  function](functions/abs.html) * [`acos`  function](functions/acos.html) * [`acosh`  function](functions/acosh.html) * [`add\_months`  function](functions/add_months.html) * [`aes\_decrypt`  function](functions/aes_decrypt.html) * [`aes\_encrypt`  function](functions/aes_encrypt.html) * [`aggregate`  function](functions/aggregate.html) * [`&`  (ampersand sign) operator](functions/ampersandsign.html) * [`and`  predicate](functions/and.html) * [`any`  aggregate function](functions/any.html) * [`any\_value`  aggregate function](functions/any_value.html) * [`approx\_count\_distinct`  aggregate function](functions/approx_count_distinct.html) * [`approx\_percentile`  aggregate function](functions/approx_percentile.html) * [`approx\_top\_k`  aggregate function](functions/approx_top_k.html) * [`array`  function](functions/array.html) * [`array\_agg`  aggregate function](functions/array_agg.html) * [`array\_append`  function](functions/array_append.html) * [`array\_compact`  function](functions/array_compact.html) * [`array\_contains`  function](functions/array_contains.html) * [`array\_distinct`  function](functions/array_distinct.html) * [`array\_except`  function](functions/array_except.html) * [`array\_intersect`  function](functions/array_intersect.html) * [`array\_join`  function](functions/array_join.html) * [`array\_max`  function](functions/array_max.html) * [`array\_min`  function](functions/array_min.html) * [`array\_position`  function](functions/array_position.html) * [`array\_remove`  function](functions/array_remove.html) * [`array\_repeat`  function](functions/array_repeat.html) * [`array\_size`  function](functions/array_size.html) * [`array\_sort`  function](functions/array_sort.html) * [`array\_union`  function](functions/array_union.html) * [`arrays\_overlap`  function](functions/arrays_overlap.html) * [`arrays\_zip`  function](functions/arrays_zip.html) * [`ascii`  function](functions/ascii.html) * [`asin`  function](functions/asin.html) * [`asinh`  function](functions/asinh.html) * [`assert\_true`  function](functions/assert_true.html) * [`\*`  (asterisk sign) operator](functions/asterisksign.html) * [`atan`  function](functions/atan.html) * [`atan2`  function](functions/atan2.html) * [`atanh`  function](functions/atanh.html) * [`avg`  aggregate function](functions/avg.html) * [`!=`  (bangeq sign) operator](functions/bangeqsign.html) * [`!`  (bang sign) operator](functions/bangsign.html) * [`base64`  function](functions/base64.html) * [`between`  predicate](functions/between.html) * [`bigint`  function](functions/bigint.html) * [`bin`  function](functions/bin.html) * [`binary`  function](functions/binary.html) * [`bit\_and`  aggregate function](functions/bit_and.html) * [`bit\_count`  function](functions/bit_count.html) * [`bit\_get`  function](functions/bit_get.html) * [`bit\_length`  function](functions/bit_length.html) * [`bit\_or`  aggregate function](functions/bit_or.html) * [`bit\_reverse`  function](functions/bit_reverse.html) * [`bit\_xor`  aggregate function](functions/bit_xor.html) * [`bool\_and`  aggregate function](functions/bool_and.html) * [`bool\_or`  aggregate function](functions/bool_or.html) * [`boolean`  function](functions/boolean.html) * [`[     ]`  (bracket sign) operator](functions/bracketsign.html) * [`bround`  function](functions/bround.html) * [`btrim`  function](functions/btrim.html) * [`cardinality`  function](functions/cardinality.html) * [`^`  (caret sign) operator](functions/caretsign.html) * [`case`  expression](functions/case.html) * [`cast`  function](functions/cast.html) * [`cbrt`  function](functions/cbrt.html) * [`ceil`  function](functions/ceil.html) * [`ceiling`  function](functions/ceiling.html) * [`char`  function](functions/char.html) * [`char\_length`  function](functions/char_length.html) * [`character\_length`  function](functions/character_length.html) * [`charindex`  function](functions/charindex.html) * [`chr`  function](functions/chr.html) * [`cloud\_files\_state`  table-valued function](functions/cloud_files_state.html) * [`coalesce`  function](functions/coalesce.html) * [`collect\_list`  aggregate function](functions/collect_list.html) * [`collect\_set`  aggregate function](functions/collect_set.html) * [`::`  (colon colon sign) operator](functions/coloncolonsign.html) * [`:`  (colon sign) operator](functions/colonsign.html) * [`concat`  function](functions/concat.html) * [`concat\_ws`  function](functions/concat_ws.html) * [`contains`  function](functions/contains.html) * [`conv`  function](functions/conv.html) * [`corr`  aggregate function](functions/corr.html) * [`cos`  function](functions/cos.html) * [`cosh`  function](functions/cosh.html) * [`cot`  function](functions/cot.html) * [`count`  aggregate function](functions/count.html) * [`count\_if`  aggregate function](functions/count_if.html) * [`count\_min\_sketch`  aggregate function](functions/count_min_sketch.html) * [`covar\_pop`  aggregate function](functions/covar_pop.html) * [`covar\_samp`  aggregate function](functions/covar_samp.html) * [`crc32`  function](functions/crc32.html) * [`csc`  function](functions/csc.html) * [`cube`  function](functions/cube.html) * [`cume\_dist`  analytic window function](functions/cume_dist.html) * [`curdate`  function](functions/curdate.html) * [`current\_catalog`  function](functions/current_catalog.html) * [`current\_database`  function](functions/current_database.html) * [`current\_date`  function](functions/current_date.html) * [`current\_metastore`  function](functions/current_metastore.html) * [`current\_schema`  function](functions/current_schema.html) * [`current\_timestamp`  function](functions/current_timestamp.html) * [`current\_timezone`  function](functions/current_timezone.html) * [`current\_user`  function](functions/current_user.html) * [`current\_version`  function](functions/current_version.html) * [`date`  function](functions/date.html) * [`date\_add`  function](functions/date_add.html) * [`date\_format`  function](functions/date_format.html) * [`date\_from\_unix\_date`  function](functions/date_from_unix_date.html) * [`date\_part`  function](functions/date_part.html) * [`date\_sub`  function](functions/date_sub.html) * [`date\_trunc`  function](functions/date_trunc.html) * [`dateadd`  function](functions/dateadd.html) * [`dateadd`  (days) function](functions/dateadd2.html) * [`datediff`  function](functions/datediff.html) * [`datediff`  (timestamp) function](functions/datediff3.html) * [`day`  function](functions/day.html) * [`dayofmonth`  function](functions/dayofmonth.html) * [`dayofweek`  function](functions/dayofweek.html) * [`dayofyear`  function](functions/dayofyear.html) * [`decimal`  function](functions/decimal.html) * [`decode`  function](functions/decode.html) * [`decode`  (character set) function](functions/decode_cs.html) * [`degrees`  function](functions/degrees.html) * [`dense\_rank`  ranking window function](functions/dense_rank.html) * [`div`  operator](functions/div.html) * [`.`  (dot sign) operator](functions/dotsign.html) * [`double`  function](functions/double.html) * [`e`  function](functions/e.html) * [`element\_at`  function](functions/element_at.html) * [`elt`  function](functions/elt.html) * [`encode`  function](functions/encode.html) * [`endswith`  function](functions/endswith.html) * [`==`  (eq eq sign) operator](functions/eqeqsign.html) * [`=`  (eq sign) operator](functions/eqsign.html) * [`equal\_null`  function](functions/equal_null.html) * [`every`  aggregate function](functions/every.html) * [`exists`  function](functions/exists.html) * [`exp`  function](functions/exp.	The list of Databricks functions can be found in an alphabetically-ordered list in the Databricks documentation. The list includes functions such as `abs`, `acos`, `add_months`, `aggregate`, `array`, `ascii`, `avg`, `base64`, `ceil`, `concat`, `count`, `date`, `decode`, `double`, `element_at`, `endswith`, `exp`, and many more.	1
no_reaction	['https://spark.apache.org/docs/latest/api/R/reference/summary.html']	Compute summary statistics on a Spark DataFrame using .summary()	"  ![]()   summary =========  `summary.Rd`    Computes specified statistics for numeric and string columns. Available statistics are:    * count * mean * stddev * min * max * arbitrary approximate percentiles specified as a percentage (e.g., ""75%"")   If no statistics are given, this function computes count, mean, stddev, min, approximate quartiles (percentiles at 25%, 50%, and 75%), and max. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting Dataset. If you want to programmatically compute summary statistics, use the  `agg`   function instead.     Usage -------  ``` summary(object, ...)  # S4 method for SparkDataFrame summary(object, ...) ```   Arguments -----------   object     a SparkDataFrame to be summarized.     ...     (optional) statistics to be computed for all columns.     Value -------   A SparkDataFrame.     Note ------   summary(SparkDataFrame) since 1.5.0     The statistics provided by  `summary`   were change in 2.3.0 use  [describe](describe.html)   for  previous defaults.     See also ----------  [describe](describe.html)    Other SparkDataFrame functions:  `[SparkDataFrame-class](SparkDataFrame.html)`  ,  `[agg](summarize.html)   ()`   ,  `[alias](alias.html)   ()`   ,  `[arrange](arrange.html)   ()`   ,  `[as.data.frame](as.data.frame.html)   ()`   ,  `[attach,SparkDataFrame-method](attach.html)`  ,  `[broadcast](broadcast.html)   ()`   ,  `[cache](cache.html)   ()`   ,  `[checkpoint](checkpoint.html)   ()`   ,  `[coalesce](coalesce.html)   ()`   ,  `[collect](collect.html)   ()`   ,  `[colnames](columns.html)   ()`   ,  `[coltypes](coltypes.html)   ()`   ,  `[createOrReplaceTempView](createOrReplaceTempView.html)   ()`   ,  `[crossJoin](crossJoin.html)   ()`   ,  `[cube](cube.html)   ()`   ,  `[dapplyCollect](dapplyCollect.html)   ()`   ,  `[dapply](dapply.html)   ()`   ,  `[describe](describe.html)   ()`   ,  `[dim](dim.html)   ()`   ,  `[distinct](distinct.html)   ()`   ,  `[dropDuplicates](dropDuplicates.html)   ()`   ,  `[dropna](nafunctions.html)   ()`   ,  `[drop](drop.html)   ()`   ,  `[dtypes](dtypes.html)   ()`   ,  `[exceptAll](exceptAll.html)   ()`   ,  `[except](except.html)   ()`   ,  `[explain](explain.html)   ()`   ,  `[filter](filter.html)   ()`   ,  `[first](first.html)   ()`   ,  `[gapplyCollect](gapplyCollect.html)   ()`   ,  `[gapply](gapply.html)   ()`   ,  `[getNumPartitions](getNumPartitions.html)   ()`   ,  `[group\_by](groupBy.html)   ()`   ,  `[head](head.html)   ()`   ,  `[hint](hint.html)   ()`   ,  `[histogram](histogram.html)   ()`   ,  `[insertInto](insertInto.html)   ()`   ,  `[intersectAll](intersectAll.html)   ()`   ,  `[intersect](intersect.html)   ()`   ,  `[isLocal](isLocal.html)   ()`   ,  `[isStreaming](isStreaming.html)   ()`   ,  `[join](join.html)   ()`   ,  `[limit](limit.html)   ()`   ,  `[localCheckpoint](localCheckpoint.html)   ()`   ,  `[merge](merge.html)   ()`   ,  `[mutate](mutate.html)   ()`   ,  `[ncol](ncol.html)   ()`   ,  `[nrow](nrow.html)   ()`   ,  `[persist](persist.html)   ()`   ,  `[printSchema](printSchema.html)   ()`   ,  `[randomSplit](randomSplit.html)   ()`   ,  `[rbind](rbind.html)   ()`   ,  `[rename](rename.html)   ()`   ,  `[repartitionByRange](repartitionByRange.html)   ()`   ,  `[repartition](repartition.html)   ()`   ,  `[rollup](rollup.html)   ()`   ,  `[sample](sample.html)   ()`   ,  `[saveAsTable](saveAsTable.html)   ()`   ,  `[schema](schema.html)   ()`   ,  `[selectExpr](selectExpr.html)   ()`   ,  `[select](select.html)   ()`   ,  `[showDF](showDF.html)   ()`   ,  `[show](show.html)   ()`   ,  `[storageLevel](storageLevel.html)   ()`   ,  `[str](str.html)   ()`   ,  `[subset](subset.html)   ()`   ,  `[take](take.html)   ()`   ,  `[toJSON](toJSON.html)   ()`   ,  `[unionAll](unionAll.html)   ()`   ,  `[unionByName](unionByName.html)   ()`   ,  `[union](union.html)   ()`   ,  `[unpersist](unpersist.html)   ()`   ,  `[unpivot](unpivot.html)   ()`   ,  `[withColumn](withColumn.html)   ()`   ,  `[withWatermark](withWatermark.html)   ()`   ,  `[with](with.html)   ()`   ,  `[write.df](write.df.html)   ()`   ,  `[write.jdbc](write.jdbc.html)   ()`   ,  `[write.json](write.json.html)   ()`   ,  `[write.orc](write.orc.html)   ()`   ,  `[write.parquet](write.parquet.html)   ()`   ,  `[write.stream](write.stream.html)   ()`   ,  `[write.text](write.text.html)   ()`    Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() path <- ""path/to/file.json"" df <- [read.json](read.json.html)(path) summary(df) summary(df, ""min"", ""25%"", ""75%"", ""max"") summary([select](select.html)(df, ""age"", ""height"")) }  ```  "	"The `summary()` function in Spark computes specified statistics for numeric and string columns in a Spark DataFrame. Available statistics include count, mean, stddev, min, max, and arbitrary approximate percentiles specified as a percentage (e.g., ""75%""). If no statistics are given, this function computes count, mean, stddev, min, approximate quartiles (percentiles at 25%, 50%, and 75%), and max. This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting Dataset. If you want to programmatically compute summary statistics, use the `agg` function instead."	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.toTable.html']	in a CDC pipeline, do i have to use streaming table or live table ? 	"   pyspark.sql.streaming.DataStreamWriter.toTable  [¶](#pyspark-sql-streaming-datastreamwriter-totable ""Permalink to this headline"") ===================================================================================================================================  `DataStreamWriter.`  `toTable`    (   *tableName     :     str*  ,  *format     :     Optional    [    str    ]     =     None*  ,  *outputMode     :     Optional    [    str    ]     =     None*  ,  *partitionBy     :     Union[str, List[str], None]     =     None*  ,  *queryName     :     Optional    [    str    ]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → pyspark.sql.streaming.query.StreamingQuery  [[source]](../../../_modules/pyspark/sql/streaming/readwriter.html#DataStreamWriter.toTable) [¶](#pyspark.sql.streaming.DataStreamWriter.toTable ""Permalink to this definition"")    Starts the execution of the streaming query, which will continually output results to the given table as new data arrives.     The returned  [`StreamingQuery`](pyspark.sql.streaming.StreamingQuery.html#pyspark.sql.streaming.StreamingQuery ""pyspark.sql.streaming.StreamingQuery"")  object can be used to interact with the stream.     New in version 3.1.0.     Parameters    **tableName**    str     string, for the name of the table.    **format**    str, optional     the format used to save.    **outputMode**    str, optional     specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.    * append    : Only the new rows in the streaming DataFrame/Dataset will be written to the sink * complete    : All the rows in the streaming DataFrame/Dataset will be written to the sink every time these are some updates * update    : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to    append    mode.  **partitionBy**    str or list, optional     names of partitioning columns    **queryName**    str, optional     unique name for the query    **\*\*options**    dict     All other string options. You may want to provide a    checkpointLocation    .     Notes     This API is evolving.     For v1 table, partitioning columns provided by    partitionBy    will be respected no matter the table exists or not. A new table will be created if the table not exists.     For v2 table,    partitionBy    will be ignored if the table already exists.    partitionBy    will be respected only if the v2 table does not exist. Besides, the v2 table created by this API lacks some functionalities (e.g., customized properties, options, and serde info). If you need them, please create the v2 table manually before the execution to avoid creating a table with incomplete information.     Examples     Save a data stream to a table.    ``` >>> import tempfile >>> import time >>> \_ = spark.sql(""DROP TABLE IF EXISTS my\_table2"") >>> with tempfile.TemporaryDirectory() as d: ...     # Create a table with Rate source. ...     q = spark.readStream.format(""rate"").option( ...         ""rowsPerSecond"", 10).load().writeStream.toTable( ...             ""my\_table2"", ...             queryName='that\_query', ...             outputMode=""append"", ...             format='parquet', ...             checkpointLocation=d) ...     time.sleep(3) ...     q.stop() ...     spark.read.table(""my\_table2"").show() ...     \_ = spark.sql(""DROP TABLE my\_table2"") +...---------+-----+ |...timestamp|value| +...---------+-----+ ...  ```  [pyspark.sql.streaming.DataStreamWriter.start](pyspark.sql.streaming.DataStreamWriter.start.html ""previous page"")  [pyspark.sql.streaming.DataStreamWriter.trigger](pyspark.sql.streaming.DataStreamWriter.trigger.html ""next page"")   "	Based on the context, if you want to continually output results to a given table as new data arrives, you should use the `toTable` method of `DataStreamWriter`. This method starts the execution of the streaming query and returns a `StreamingQuery` object that can be used to interact with the stream. Therefore, you should use a streaming table.	0
no_reaction	['https://kb.databricks.com/delta/delta-cache-autoscaling']	does spark scale down nodes that have RDD cache?	"  ![]()   Cache =======  `cache.Rd`    Persist with the default storage level (MEMORY\_ONLY).     Usage -------  ``` cache(x)  # S4 method for SparkDataFrame cache(x) ```   Arguments -----------   x     A SparkDataFrame     Note ------   cache since 1.4.0     See also ----------   Other SparkDataFrame functions:  `[SparkDataFrame-class](SparkDataFrame.html)`  ,  `[agg](summarize.html)   ()`   ,  `[alias](alias.html)   ()`   ,  `[arrange](arrange.html)   ()`   ,  `[as.data.frame](as.data.frame.html)   ()`   ,  `[attach,SparkDataFrame-method](attach.html)`  ,  `[broadcast](broadcast.html)   ()`   ,  `[checkpoint](checkpoint.html)   ()`   ,  `[coalesce](coalesce.html)   ()`   ,  `[collect](collect.html)   ()`   ,  `[colnames](columns.html)   ()`   ,  `[coltypes](coltypes.html)   ()`   ,  `[createOrReplaceTempView](createOrReplaceTempView.html)   ()`   ,  `[crossJoin](crossJoin.html)   ()`   ,  `[cube](cube.html)   ()`   ,  `[dapplyCollect](dapplyCollect.html)   ()`   ,  `[dapply](dapply.html)   ()`   ,  `[describe](describe.html)   ()`   ,  `[dim](dim.html)   ()`   ,  `[distinct](distinct.html)   ()`   ,  `[dropDuplicates](dropDuplicates.html)   ()`   ,  `[dropna](nafunctions.html)   ()`   ,  `[drop](drop.html)   ()`   ,  `[dtypes](dtypes.html)   ()`   ,  `[exceptAll](exceptAll.html)   ()`   ,  `[except](except.html)   ()`   ,  `[explain](explain.html)   ()`   ,  `[filter](filter.html)   ()`   ,  `[first](first.html)   ()`   ,  `[gapplyCollect](gapplyCollect.html)   ()`   ,  `[gapply](gapply.html)   ()`   ,  `[getNumPartitions](getNumPartitions.html)   ()`   ,  `[group\_by](groupBy.html)   ()`   ,  `[head](head.html)   ()`   ,  `[hint](hint.html)   ()`   ,  `[histogram](histogram.html)   ()`   ,  `[insertInto](insertInto.html)   ()`   ,  `[intersectAll](intersectAll.html)   ()`   ,  `[intersect](intersect.html)   ()`   ,  `[isLocal](isLocal.html)   ()`   ,  `[isStreaming](isStreaming.html)   ()`   ,  `[join](join.html)   ()`   ,  `[limit](limit.html)   ()`   ,  `[localCheckpoint](localCheckpoint.html)   ()`   ,  `[merge](merge.html)   ()`   ,  `[mutate](mutate.html)   ()`   ,  `[ncol](ncol.html)   ()`   ,  `[nrow](nrow.html)   ()`   ,  `[persist](persist.html)   ()`   ,  `[printSchema](printSchema.html)   ()`   ,  `[randomSplit](randomSplit.html)   ()`   ,  `[rbind](rbind.html)   ()`   ,  `[rename](rename.html)   ()`   ,  `[repartitionByRange](repartitionByRange.html)   ()`   ,  `[repartition](repartition.html)   ()`   ,  `[rollup](rollup.html)   ()`   ,  `[sample](sample.html)   ()`   ,  `[saveAsTable](saveAsTable.html)   ()`   ,  `[schema](schema.html)   ()`   ,  `[selectExpr](selectExpr.html)   ()`   ,  `[select](select.html)   ()`   ,  `[showDF](showDF.html)   ()`   ,  `[show](show.html)   ()`   ,  `[storageLevel](storageLevel.html)   ()`   ,  `[str](str.html)   ()`   ,  `[subset](subset.html)   ()`   ,  `[summary](summary.html)   ()`   ,  `[take](take.html)   ()`   ,  `[toJSON](toJSON.html)   ()`   ,  `[unionAll](unionAll.html)   ()`   ,  `[unionByName](unionByName.html)   ()`   ,  `[union](union.html)   ()`   ,  `[unpersist](unpersist.html)   ()`   ,  `[unpivot](unpivot.html)   ()`   ,  `[withColumn](withColumn.html)   ()`   ,  `[withWatermark](withWatermark.html)   ()`   ,  `[with](with.html)   ()`   ,  `[write.df](write.df.html)   ()`   ,  `[write.jdbc](write.jdbc.html)   ()`   ,  `[write.json](write.json.html)   ()`   ,  `[write.orc](write.orc.html)   ()`   ,  `[write.parquet](write.parquet.html)   ()`   ,  `[write.stream](write.stream.html)   ()`   ,  `[write.text](write.text.html)   ()`    Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() path <- ""path/to/file.json"" df <- [read.json](read.json.html)(path) cache(df) }  ```  "	Yes, Spark can scale down nodes that have RDD cache. When a node is removed from the cluster, its cached data is lost and the RDDs that were cached on that node will be rebuilt on demand. However, if the RDDs were previously persisted to disk or replicated across the cluster, they will not be lost.	0
no_reaction	['https://docs.databricks.com/data-sharing/share-data-open.html']	how can I share data	      Access data shared with you using Delta Sharing =================================================     This article shows how to to access data that has been shared with you using Delta Sharing.          Delta Sharing and data recipients -----------------------------------     Delta Sharing is an open standard for secure data sharing. A Databricks user, called a  *data provider*   , can use Delta Sharing to share data with a person or group outside of their organization, called a  *data recipient*   .        ###   Databricks-to-Databricks sharing and open sharing     How you access the data depends on whether you yourself are a Databricks user and whether or not your data provider configured the data being shared with you for  *Databricks-to-Databricks*   sharing or  *open sharing*   .      **In the Databricks-to-Databricks model**   , you must be a user on a Databricks workspace that is enabled for  [Unity Catalog](../data-governance/unity-catalog/index.html)  . A member of your team provides the data provider with a unique identifier for your Databricks workspace, and the data provider uses that to create a secure sharing connection. The shared data simply becomes available for access in your workspace. If necessary, a member of your team configures granular access control on that data.      **In the open sharing model**   , you can use any tool you like (including Databricks) to access the shared data. The data provider sends you an activation URL over a secure channel. You follow it to download a credential file that lets you access the data shared with you.       ###   Terms of use     The shared data is not provided by Databricks directly but by data providers running on Databricks.        Note       By accessing a data provider’s shared data as a data recipient, data recipient represents that it has been authorized to access the data share(s) provided to it by the data provider and acknowledges that (1) Databricks has no liability for such data or data recipient’s use of such shared data, and (2) Databricks may collect information about data recipient’s use of and access to the shared data (including identifying any individual or company who accesses the data using the credential file in connection with such information) and may share it with the applicable data provider.           Get access to the data shared with you ----------------------------------------     How you access the data depends on whether your data provider shared data with you using the open sharing protocol or the Databricks-to-Databricks sharing protocol. See  [Databricks-to-Databricks sharing and open sharing](#open-sharing-vs-db-to-db)  .        ###   Get access in the Databricks-to-Databricks model     In the Databricks-to-Databricks model:     1. The data provider sends you instructions for finding a unique identifier for the  [Unity Catalog](../data-governance/unity-catalog/index.html)  metastore associated with your Databricks workspace, and you send it to them.       The sharing identifier is a string consisting of the metastore’s cloud, region, and UUID (the unique identifier for the metastore), in the format  `<cloud>:<region>:<uuid>`  . For example,  `aws:eu-west-1:b0c978c8-3e68-4cdf-94af-d05c120ed1ef`  .       To get the sharing identifier using Data Explorer:      1. In your Databricks workspace, click   ![Data Icon](../_images/data-icon.png)  **Data**    .  2. In the left pane, expand the   **Delta Sharing**    menu and select   **Shared with me**    .  3. Above the Providers tab, click the   **Sharing identifier**    copy icon.  To get the sharing identifier using a notebook or Databricks SQL query, use the default SQL function  `CURRENT\_METASTORE`  . If you use a notebook, it must run on a  [Unity-Catalog-capable cluster](../data-governance/unity-catalog/compute.html#access-mode)  in the workspace you will use to access the shared data.        ``` SELECT CURRENT\_METASTORE();  ``` 2. The data provider creates:      * A   *recipient*    in their Databricks account to represent you and the users in your organization who will access the data.  * A   *share*    , which is a representation of the tables and views to be shared with you. 3. You access the data shared with you. You or someone on your team can, if necessary, configure granular data access on that data for your users.       ###   Get access in the open sharing model     In the open sharing model:     1. The data provider creates:      * A   *recipient*    in their Databricks account to represent you and the users in your organization who will access the data. A token and credential file are generated as part of this configuration.  * A   *share*    , which is a representation of the tables and partitions to be shared with you. 2. The data provider sends you an activation URL over a secure channel. You follow it to download a credential file that lets you access the data shared with you.        Important       Don’t share the activation link with anyone. You can download a credential file only once. If you visit the activation link again after the credential file has already downloaded, the  **Download Credential File**   button is disabled.        If you lose the activation link before you use it, contact the data provider. 3. Store the credential file in a secure location.       Don’t share the credential file with anyone outside the group of users who should have access to the shared data. If you need to share it with someone in your organization, Databricks recommends using a password manager.        Read the shared data ----------------------     How you read data that has been shared securely with you using Delta Sharing depends on whether you received a credential file (the open sharing model) or you are using a Databricks workspace and you provided the data provider with your sharing identifier (the Databricks-to-Databricks model).      ###   Read shared data using a credential file (open sharing)     If data has been shared with you using the Delta Sharing open sharing protocol, you use the credential file that you downloaded to authenticate to the data provider’s Databricks account and read the shared data. Access persists until the provider stops sharing the data with you. Updates to the data are available to you in near real time. You can read and make copies of the shared data, but you can’t modify the source data.       To learn how to access and read shared data using the credential file in Databricks, Apache Spark, pandas, and Power BI, see  [Read data shared using Delta Sharing open sharing](read-data-open.html)  .       ###   Read shared data using Databricks-to-Databricks sharing     If data has been shared with you using the Databricks-to-Databricks model, then no credential file is required to access the shared data. Databricks takes care of the secure connection, and the shared data is automatically discoverable in your Databricks workspace.       To learn how to find, read, and manage that shared data in your Databricks workspace, see  [Read data shared using Databricks-to-Databricks Delta Sharing](read-data-databricks.html)  .          Audit usage of shared data ----------------------------     If you have access to a Databricks workspace, you can use Databricks audit logs to understand who in your organization is accessing which data using Delta Sharing. See  [Audit and monitor data access using Delta Sharing (for recipients)](audit-logging-recipient.html)  .         Next steps ------------   * [Learn more about Databricks](../getting-started/index.html) * [Learn more about Delta Sharing](https://delta.io/sharing/) * [Learn more about Unity Catalog](../data-governance/unity-catalog/index.html)                                                      	To share data using Delta Sharing, a data provider can use Delta Sharing to share data with a person or group outside of their organization, called a data recipient. The data recipient can access the shared data either through the Databricks-to-Databricks sharing model or the open sharing model. In the Databricks-to-Databricks model, the data recipient must be a user on a Databricks workspace that is enabled for Unity Catalog, and a member of their team provides the data provider with a unique identifier for their Databricks workspace. In the open sharing model, the data provider sends the data recipient an activation URL over a secure channel, and the data recipient follows it to download a credential file that lets them access the data shared with them.	1
no_reaction	['https://docs.databricks.com/workflows/jobs/how-to/index.html']	what's the difference between jobs and dlt pipelines	    Implement data processing and analysis workflows with Jobs ============================================================     You can use a Databricks job to orchestrate your data processing, machine learning, or data analytics pipelines in the Databricks Lakehouse. Databricks Jobs support a number of workload types, including notebooks, scripts, Delta Live Tables pipelines, Databricks SQL queries, and  [dbt](../../../partners/prep/dbt.html)  projects. The following articles guide you in using the features and options in the Databricks Jobs platform to implement your data pipelines.        Transform, analyze, and visualize your data with a Databricks job -------------------------------------------------------------------     You can use a job to create a data pipeline that ingests, transforms, analyzes, and visualizes data. The example in  [Use Databricks SQL in a Databricks job](use-dbsql-in-workflows.html)  builds a pipeline that:     1. Uses a Python script to fetch data using a REST API. 2. Uses  [Delta Live Tables](../../../delta-live-tables/index.html)  to ingest and transform the fetched data and save the transformed data to Delta Lake. 3. Uses the Jobs integration with Databricks SQL to analyze the transformed data and create graphs to visualize the results.       Use dbt transformations in a job ----------------------------------     Use the  `dbt`  task type if you are doing data transformation with a dbt core project and want to integrate that project into a Databricks job, or you want to create new dbt transformations and run those transformations in a job. See  [Use dbt transformations in a Databricks job](use-dbt-in-workflows.html)  .         Use a Python package in a job -------------------------------     Python wheels are a standard way to package and distribute the files required to run a Python application. You can easily create a job that uses Python code packaged as a wheel with the  `Python     wheel`  task type. See  [Use a Python wheel in a Databricks job](use-python-wheels-in-workflows.html)  .         Use code packaged in a JAR ----------------------------     Libraries and applications implemented in a JVM language such as Java and Scala are commonly packaged in a Java archive (JAR) file. Databricks Jobs supports code packaged in a JAR with the  `JAR`  task type. See  [Use a JAR in a Databricks job](use-jars-in-workflows.html)  .         Use notebooks maintained in a central repository --------------------------------------------------     A common way to manage version control and collaboration for production artifacts is to use a central repository such as GitHub. Databricks Jobs supports creating and running jobs using notebooks imported from a repository, including GitHub or Databricks Repos. See  [Use version controlled notebooks in a Databricks job](use-repos.html)  .         Orchestrate your jobs with Apache Airflow -------------------------------------------     Databricks recommends using Databricks Jobs to orchestrate your workflows. However, Apache Airflow is commonly used as a workflow orchestration system and provides native support for Databricks Jobs. While Databricks Jobs provides a visual UI to create your workflows, Airflow uses Python files to define and deploy your data pipelines. For an example of creating and running a job with Airflow, see  [Orchestrate Databricks jobs with Apache Airflow](use-airflow-with-jobs.html)  .                                                          	Jobs are used to orchestrate data processing, machine learning, or data analytics pipelines in the Databricks Lakehouse, while Delta Live Table (DLT) pipelines are a workload type that can be used in Databricks Jobs to ingest and transform data using Delta Live Tables and save the transformed data to Delta Lake. In summary, DLT pipelines are a type of workload that can be used within Jobs to process data, while Jobs are a more general way to orchestrate various types of data processing pipelines.	0
no_reaction	['https://docs.databricks.com/spark/index.html']	Why use Apache Spark on Databricks?	    Apache Spark on Databricks ============================     This article describes the how Apache Spark is related to Databricks and the Databricks Lakehouse Platform.       Apache Spark is at the heart of the Databricks Lakehouse Platform and is the technology powering compute clusters and SQL warehouses on the platform. Databricks is an optimized platform for Apache Spark, providing an efficient and simple platform for running Apache Spark workloads.            What is the relationship of Apache Spark to Databricks? ---------------------------------------------------------     The Databricks company was founded by the orginal creators of Apache Spark. As an open source software project, Apache Spark has  [committers from many top companies](https://spark.apache.org/committers.html)   , including Databricks.       Databricks continues to develop and release features to Apache Spark. The Databricks Runtime includes additional optimizations and proprietary features that build upon and extend Apache Spark, including  [Photon](../runtime/photon.html)  , an optimized version of Apache Spark rewritten in C++.           How does Apache Spark work on Databricks? -------------------------------------------     Whenever you deploy a compute cluster or SQL warehouse on Databricks, Apache Spark is configured and deployed to virtual machines. You don’t need to worry about configuring or initializing a Spark context or Spark session, as these are managed for you by Databricks.           Can I use Databricks without using Apache Spark? --------------------------------------------------     Databricks supports a variety of workloads and includes a number of other open source libraries in the Databricks Runtime. Databricks SQL uses Apache Spark under the hood, but end users use standard SQL syntax to create and query database objects.       Databricks Runtime for Machine Learning is optimized for ML workloads, and many data scientists use primary open source libraries like TensorFlow and SciKit Learn while working on Databricks. You can use  [workflows](../workflows/index.html)  to schedule arbitrary workloads against compute resources deployed and managed by Databricks.           Why use Apache Spark on Databricks? -------------------------------------     The Databricks Lakehouse Platform provides a secure, collaborative environment for developing and deploying enterprise solutions that scale with your business. Databricks employees representative many of the most knowledgeable Apache Spark maintainers and users in the world, and the company continuously develops and releases new optimizations to ensure that user have access to the fastest environment for running Apache Spark.                                                        	You would use Apache Spark on Databricks because Databricks is an optimized platform for Apache Spark, providing an efficient and simple platform for running Apache Spark workloads. The Databricks Lakehouse Platform provides a secure, collaborative environment for developing and deploying enterprise solutions that scale with your business. Additionally, Databricks employees represent many of the most knowledgeable Apache Spark maintainers and users in the world, and the company continuously develops and releases new optimizations to ensure that users have access to the fastest environment for running Apache Spark.	1
no_reaction	['https://docs.databricks.com/administration-guide/account-settings-e2/private-access-settings.html']	"how to create ""private network""?"	    Manage private access settings ================================         This article describes a process that is available only for  [accounts on the E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.       This article discusses how to create private access settings objects, which are a required object as part of enabling  [AWS PrivateLink](https://aws.amazon.com/privatelink)   . This article does not contain all the information necessary to configure PrivateLink for your workspace. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see  [Enable AWS PrivateLink](../cloud-configurations/aws/privatelink.html)  .       The following related sections discuss updating existing network and configuration objects:     * [Update a running or failed workspace](../workspace/update-workspace.html)  . * [Updates of existing PrivateLink configuration objects](../cloud-configurations/aws/privatelink.html#update-related)  .      What is a private access settings object? -------------------------------------------     A private access settings object is a Databricks object that describes a workspace’s PrivateLink connectivity. Create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region.       This object serves several purposes:     * It expresses your intent to use AWS PrivateLink with your workspace. * It controls your settings for the front-end use case of AWS PrivateLink for public network access. * It controls which VPC endpoints are permitted to access your workspace.     Create a private access settings object using the account console or the  [Account API 2.0](../../dev-tools/api/latest/account.html)  . You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you  *must*   attach a private access settings object to the workspace during workspace creation.           Create a private access settings object -----------------------------------------      Note       These instructions show you how to create the private access object from the  **Cloud resources**   page in the account console before you create a new workspace. You can also create the private access settings in a similar way as part of the flow of creating a new workspace and choosing  **Add a new private access object**   from the picker instead of choosing an existing object. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .      1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. Click  **Add private access settings**   .      ![private access settings object](../../_images/privatelink-vpc-pas.png) 5. Enter a name for your new private access settings object. 6. For the region, be sure to match the region of your workspace as this is not validated immediately and workspace deployment will fail if it does not match. It is validated only during the actual creation of the workspace. 7. Set the  **Public access enabled**   field, which configures public access to the front-end connection (the web application and REST APIs) for your workspace.      * If set to   **False**    (the default), the front-end connection can be accessed only using PrivateLink connectivity and not from the public internet. When public access is disabled, the   [IP access lists for workspaces](../../security/network/ip-access-list-workspace.html)   feature is unsupported.  * If set to   **True**    , the front-end connection can be accessed either from PrivateLink connectivity or from the public internet. Any IP access lists only limit connections from the public internet but not traffic through the PrivateLink connection. 8. Set the  **Private Access Level**   field to the value that best represents which VPC endpoints to allow for your workspace.      * Set to   **Account**    to limit connections to those VPC endpoints that are registered in your Databricks account.  * Set to   **Endpoint**    to limit connections to an explicit set of VPC endpoints, which you can enter in a field that appears. It lets you select VPC endpoint registrations that you’ve already created. Be sure to include your   *front-end*    VPC endpoint registration if you created one. 9. Click  **Add**   .         Update a private access settings object -----------------------------------------     To update fields on a private access object:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Update**   . 5. Change any fields. For guidance on specific fields, see  [Create a private access settings object](#create)  .        Note       The private access access level  `ANY`  is deprecated. If the object previously had this value and you use the account console to update the private access settings for any fields, you must change the private access level to a non-deprecated value. To make changes to other fields without changing the  `ANY`  private access level at this time, use the  [Account API](../../dev-tools/api/latest/account.html)  . See  [AWS PrivateLink private access level ANY is deprecated](../../release-notes/product/2022/august.html#privatelink-private-access-level-any-deprecated)  . 6. Click  **Update private access setting**   .         Delete a private access settings object -----------------------------------------     Private access settings object cannot be edited after creation. If the configuration has incorrect data or if you no longer need it for any workspaces, delete it:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Delete**   . 5. In the confirmation dialog, click  **Confirm Delete**   .                                                      	To create a private network, you need to create a private access settings object, which is a required object as part of enabling AWS PrivateLink. This object controls your settings for the front-end use case of AWS PrivateLink for public network access and which VPC endpoints are permitted to access your workspace. You can create a private access settings object using the account console or the Account API 2.0. To create a private access settings object from the Cloud resources page in the account console, you need to click Cloud resources, then Network, then Private access settings, and finally Add private access settings. Then, you need to enter a name for your new private access settings object, set the Public access enabled field, and set the Private Access Level field to the value that best represents which VPC endpoints to allow for your workspace. Finally, you need to click Add.	1
no_reaction	['https://docs.databricks.com/delta/vacuum.html']	I am trying to vacuum the data to optimize the storage cost. However, I observed that the older versions still have the data even after running the vacuum command.	    Remove unused data files with vacuum ======================================     You can remove data files no longer referenced by a Delta table that are older than the retention threshold by running the  `vacuum`  command on the table. Running  `vacuum`  regularly is important for cost and compliance because of the following considerations:     * Deleting unused data files reduces cloud storage costs. * Data files removed by  `vacuum`  might contain records that have been modified or deleted. Permanently removing these files from cloud storage ensures these records are no longer accessible.    `vacuum`  is not triggered automatically. The default retention threshold for data files is 7 days. To change this behavior, see  [Configure data retention for time travel](history.html#data-retention)  .       Some Delta Lake features use metadata files to mark data as deleted rather than rewriting data files. You can use  `REORG     TABLE     ...     APPLY     (PURGE)`  to commit these deletions and rewrite data files. See  [Purge metadata-only deletes to force data rewrite](#purge)  .        Important     * `vacuum`  removes all files from directories not managed by Delta Lake, ignoring directories beginning with  `\_`  . If you are storing additional metadata like Structured Streaming checkpoints within a Delta table directory, use a directory name such as  `\_checkpoints`  . * `vacuum`  deletes only data files, not log files. Log files are deleted automatically and asynchronously after checkpoint operations. The default retention period of log files is 30 days, configurable through the  `delta.logRetentionDuration`  property which you set with the  `ALTER     TABLE     SET     TBLPROPERTIES`  SQL method. See  [Delta table properties reference](table-properties.html)  . * The ability to  [time travel](history.html)  back to a version older than the retention period is lost after running  `vacuum`  .       Note       When disk caching is enabled, a cluster might contain data from Parquet files that have been deleted with  `vacuum`  . Therefore, it may be possible to query the data of previous table versions whose files have been deleted. Restarting the cluster will remove the cached data. See  [Configure the disk cache](../optimizations/disk-cache.html#configure-cache)  .         Example syntax for vacuum ---------------------------      ``` VACUUM eventsTable -- vacuum files not required by versions older than the default retention period  VACUUM '/data/events' -- vacuum files in path-based table  VACUUM delta.`/data/events/`  VACUUM delta.`/data/events/` RETAIN 100 HOURS -- vacuum files not required by versions more than 100 hours old  VACUUM eventsTable DRY RUN -- do dry run to get the list of files to be deleted  ```      For Spark SQL syntax details, see  [VACUUM](../sql/language-manual/delta-vacuum.html)  .       See the  [Delta Lake API documentation](index.html#delta-api)  for Scala, Java, and Python syntax details.           Purge metadata-only deletes to force data rewrite ---------------------------------------------------     The  `REORG     TABLE`  command provides the  `APPLY     (PURGE)`  syntax to rewrite data to apply soft-deletes. Soft-deletes do not rewrite data or delete data files, but rather use metadata files to indicate that some data values have changed. See  [REORG TABLE](../sql/language-manual/delta-reorg-table.html)  .       Operations that create soft-deletes in Delta Lake include the following:     * Dropping columns with  [column mapping](delta-column-mapping.html)  enabled. * Deleting rows with  [deletion vectors](deletion-vectors.html)  enabled. * Any data modifications on Photon-enabled clusters when deletion vectors are enabled.     With soft-deletes enabled, old data may remain physically present in the table’s current files even after the data has been deleted or updated. To remove this data physically from the table, complete the following steps:     1. Run  `REORG     TABLE     ...     APPLY     (PURGE)`  . After doing this, the old data is no longer present in the table’s  *current*   files, but it is still present in the older files that are used for time travel. 2. Run  `VACUUM`  to delete these older files.    `REORG     TABLE`  creates a new version of the table as the operation completes. All table versions in the history prior to this transaction refer to older data files. Conceptually, this is similar to the  `OPTIMIZE`  command, where data files are rewritten even though data in the current table version stays consistent.        Important       Data files are only deleted when the files have  *expired*   according to the  `VACUUM`  retention period. This means that the  `VACUUM`  must be done with a delay after the  `REORG`  so that the older files have expired. The retention period of  `VACUUM`  can be reduced to shorten the required waiting time, at the cost of reducing the maximum time travel history that is retained.          What size cluster does vacuum need? -------------------------------------     To select the correct cluster size for  `VACUUM`  , it helps to understand that the operation occurs in two phases:     1. The job begins by using all available executor nodes to list files in the source directory in parallel. This list is compared to all files currently referenced in the Delta transaction log to identify files to be deleted. The driver sits idle during this time. 2. The driver then issues deletion commands for each file to be deleted. File deletion is a driver-only operation, meaning that all operations occur in a single node while the worker nodes sit idle.     To optimize cost and performance, Databricks recommends the following, especially for long-running vacuum jobs:     * Run vacuum on a cluster with auto-scaling set for 1-4 workers, where each worker has 8 cores. * Select a driver with between 8 and 32 cores. Increase the size of the driver to avoid out-of-memory (OOM) errors.     If  `VACUUM`  operations are regularly deleting more than 10 thousand files or taking over 30 minutes of processing time, you might want to increase either the size of the driver or the number of workers.       If you find that the slowdown occurs while identifying files to be removed, add more worker nodes. If the slowdown occurs while delete commands are running, try increasing the size of the driver.         How frequently should you run vacuum? ---------------------------------------     Databricks recommends regularly running  `VACUUM`  on all tables to reduce excess cloud data storage costs. The default retention threshold for vacuum is 7 days. Setting a higher threshold gives you access to a greater history for your table, but increases the number of data files stored and, as a result, incurs greater storage costs from your cloud provider.         Why can’t you vacuum a Delta table with a low retention threshold? --------------------------------------------------------------------      Warning       It is recommended that you set a retention interval to be at least 7 days, because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table. If  `VACUUM`  cleans up active files, concurrent readers can fail or, worse, tables can be corrupted when  `VACUUM`  deletes files that have not yet been committed. You must choose an interval that is longer than the longest running concurrent transaction and the longest period that any stream can lag behind the most recent update to the table.        Delta Lake has a safety check to prevent you from running a dangerous  `VACUUM`  command. If you are certain that there are no operations being performed on this table that take longer than the retention interval you plan to specify, you can turn off this safety check by setting the Spark configuration property  `spark.databricks.delta.retentionDurationCheck.enabled`  to  `false`  .           Audit information -------------------    `VACUUM`  commits to the Delta transaction log contain audit information. You can query the audit events using  `DESCRIBE     HISTORY`  .       To capture audit information, enable  `spark.databricks.delta.vacuum.logging.enabled`  . Audit logging is not enabled by default for AWS S3 tables due to the limited consistency guarantees provided by S3 with regard to multi-workspace writes. If you enable it on S3, make sure there are no workflows that involve multi-workspace writes. Failing to do so may result in data loss.                                                        	It seems that the user is trying to optimize storage cost by vacuuming the data, but even after running the vacuum command, the older versions still have the data.	1
no_reaction	['https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html']	how to use autoloader with kafka	"    Configure Auto Loader for production workloads ================================================     Databricks recommends that you follow  [the streaming best practices](../../structured-streaming/production.html)  for running Auto Loader in production.       Databricks recommends using  [Auto Loader in Delta Live Tables](dlt.html)  for incremental data ingestion. Delta Live Tables extends functionality in Apache Spark Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:     * Autoscaling compute infrastructure for cost savings * Data quality checks with  [expectations](../../delta-live-tables/expectations.html) * Automatic  [schema evolution](schema.html)  handling * Monitoring via metrics in the  [event log](../../delta-live-tables/observability.html#event-log)      Monitoring Auto Loader ------------------------      ###   Querying files discovered by Auto Loader      Note       The  `cloud\_files\_state`  function is available in Databricks Runtime 10.5 and above.        Auto Loader provides a SQL API for inspecting the state of a stream. Using the  `cloud\_files\_state`  function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from  `cloud\_files\_state`  , providing the checkpoint location associated with an Auto Loader stream.        ``` SELECT \* FROM cloud\_files\_state('path/to/checkpoint');  ```      ###   Listen to stream updates     To further monitor Auto Loader streams, Databricks recommends using Apache Spark’s  [Streaming Query Listener interface](../../structured-streaming/stream-monitoring.html)  .       Auto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the  `numFilesOutstanding`  and  `numBytesOutstanding`  metrics under the  **Raw Data**   tab in the streaming query progress dashboard:        ``` {  ""sources"" : [  {  ""description"" : ""CloudFilesSource[/path/to/source]"",  ""metrics"" : {  ""numFilesOutstanding"" : ""238"",  ""numBytesOutstanding"" : ""163939124006""  }  }  ] }  ```      In Databricks Runtime 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as  `approximateQueueSize`  for AWS and Azure.          Cost considerations ---------------------     When running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.       To reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using  `Trigger.AvailableNow`  instead of running it continuously as long as you don’t have low latency requirements. See  [Configure Structured Streaming trigger intervals](../../structured-streaming/triggers.html)  .       File discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:     * Providing a  `ProcessingTime`  trigger when running Auto Loader continuously in directory listing mode * Architecting file uploads to your storage account in lexical ordering to leverage  [Incremental Listing](directory-listing-mode.html#incremental-listing)  when possible * Using Databricks Runtime 9.0 or later in directory listing mode, especially for deeply nested directories * [Leveraging file notifications](file-notification-mode.html)  when incremental listing is not possible * Using  [resource tags](options.html#file-notification-options)  to tag resources created by Auto Loader to track your costs       Using Trigger.AvailableNow and rate limiting ----------------------------------------------      Note       Available in Databricks Runtime 10.1 for Scala only.       Available in Databricks Runtime 10.2 and above for Python and Scala.        Auto Loader can be scheduled to run in Databricks Jobs as a batch job by using  `Trigger.AvailableNow`  . The  `AvailableNow`  trigger will instruct Auto Loader to process all files that arrived  **before**   the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.       With  `Trigger.AvailableNow`  , file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure  `cloudFiles.maxFilesPerTrigger`  and  `cloudFiles.maxBytesPerTrigger`  to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided  `maxBytesPerTrigger`  . When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.           Event retention -----------------      Note       Available in Databricks Runtime 8.4 and above.        Auto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the  `cloudFiles.maxFileAge`  option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for  `cloudFiles.maxFileAge`  is  `""14     days""`  . Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.        Warning      `cloudFiles.maxFileAge`  is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning  `cloudFiles.maxFileAge`  incorrectly can lead to data quality issues. Therefore, Databricks doesn’t recommend tuning this parameter unless absolutely required.        Trying to tune the  `cloudFiles.maxFileAge`  option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a  `cloudFiles.maxFileAge`  :     * If your stream restarts after a long time, file notification events that are pulled from the queue that are older than  `cloudFiles.maxFileAge`  are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than  `cloudFiles.maxFileAge`  are ignored. * If you use directory listing mode and use  `cloudFiles.maxFileAge`  , for example set to  `""1     month""`  , you stop your stream and restart the stream with  `cloudFiles.maxFileAge`  set to  `""2     months""`  , all files that are older than 1 month, but more recent than 2 months are reprocessed.     The best approach to tuning  `cloudFiles.maxFileAge`  would be to start from a generous expiration, for example,  `""1     year""`  and working downwards to something like  `""9     months""`  . If you set this option the first time you start the stream, you will not ingest data older than  `cloudFiles.maxFileAge`  , therefore, if you want to ingest old data you should not set this option as you start your stream.                                                        "	To use Auto Loader with Kafka, you can configure the Kafka source as the input source for Auto Loader. You can use the `cloudFiles.format` option to specify the format of the Kafka messages, and the `cloudFiles.schema` option to specify the schema of the messages. Additionally, you can use the `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` options to configure how many files or how many bytes should be processed in a micro-batch. For more information on using Auto Loader in production, you can refer to the streaming best practices guide provided by Databricks.	0
no_reaction	['https://kb.databricks.com/notebooks/troubleshoot-cancel-command']	"what could be the reason for "" Fatal error: The Python kernel is unresponsive."""	Title: Troubleshooting unresponsive Python notebooks or canceled commands  Description: Learn how to troubleshoot unresponsive Python notebooks and cancelled commands in Databricks notebooks.  Answer: This article provides an overview of troubleshooting steps you can take if a notebook is unresponsive or cancels commands. Check metastore connectivity Problem Simple commands in newly-attached notebooks fail, but succeed in notebooks that were attached to the same cluster earlier. Troubleshooting steps Check metastore connectivity. The inability to connect to the Hive metastore can cause REPL initialization to hang, making the cluster appear unresponsive. Are you are using the Databricks metastore or your own external metastore? If you are using an external metastore, have you changed anything recently? Did you upgrade your metastore version? Rotate passwords or configurations? Change security group rules? See Metastore for more troubleshooting tips and solutions. Check for conflicting libraries Problem Python library conflicts can result in cancelled commands. The Databricks support organization sees conflicts most often with versions of ipython, numpy, scipy, and pandas. Troubleshooting steps Review the Cluster cancels Python command execution due to library conflict KB article for more information. For more notebook troubleshooting information, see Notebooks.	"The reason for ""Fatal error: The Python kernel is unresponsive"" could be due to unresponsive Python notebooks or cancelled commands. This can be caused by a variety of issues, including metastore connectivity problems or conflicting libraries. To troubleshoot, you can check for metastore connectivity and review the Cluster cancels Python command execution due to library conflict KB article for more information."	1
no_reaction	['https://docs.databricks.com/sql/user/alerts/index.html']	How can i set an alert on my databricks usage	"    Alerts ========     Use  [SQL editor](../sql-editor/index.html)  to define alerts to notify you when a field returned by a  [scheduled query](../queries/schedule-query.html)  meets a threshold. You can use alerts to monitor your business or integrate alerts with tools to start workflows such as user onboarding or support tickets. Alerts complement scheduled queries, but their criteria are checked after every execution.        Important     * Alerts leveraging queries with  [parameters](../queries/query-parameters.html)  use the default value specified in the SQL editor for each parameter.     * [View and organize alerts](#view-and-organize-alerts) * [Create an alert](#create-an-alert) * [Alert on multiple columns](#alert-on-multiple-columns) * [Notification frequency](#notification-frequency) * [Configure alert permissions](#configure-alert-permissions) * [Transfer ownership of an alert](#transfer-ownership-of-an-alert)        [View and organize alerts](#id1) ---------------------------------     Alerts can be viewed in one of two ways:     * New alerts can now be viewed in the workspace browser by clicking  ![Workspace Icon](../../../_images/workspace-icon.png) **Workspace**   in the sidebar. These alerts are viewable, by default, in the  **Home**   folder. Users can organize alerts into folders in the workspace browser along with other Databricks objects. To view and organize currently existing alerts in the workspace browser, users (or admins) must migrate them into the workspace browser. For information on the Databricks SQL workspace browser, including migration, see  [Workspace browser in Databricks SQL](../workspace-browser/index.html)  . * All alerts, regardless of creation time or migration status, can be viewed by clicking  ![Alerts Icon](../../../_images/alerts-icon.png) **Alerts**   in the sidebar.     By default, objects are sorted in reverse chronological order. You can reorder the list by clicking the column headings.     * **Name**   shows the string name of each alert. * **State**   shows whether the alert status is  `TRIGGERED`  ,  `OK`  , or  `UNKNOWN`  . * **Last Updated**   shows the last updated time or date. * **Created at**   shows the date and time the alert was created.      + `TRIGGERED`   means that on the most recent execution, the Value column in your target query met the Condition and Threshold you configured. If your alert checks whether “cats” is above 1500, your alert will be triggered as long as “cats” is above 1500.  + `OK`   means that on the most recent query execution, the Value column did not meet the Condition and Threshold you configured. This doesn’t mean that the Alert was not previously triggered. If your “cats” value is now 1470, your alert will show as   `OK`   .  + `UNKNOWN`   means Databricks SQL does not have enough data to evaluate the alert  criteria. You will see this status immediately after creating your Alert and until the query has executed. You will also see this status if there was no data in the query result or if the most recent query result doesn’t include the   *Value Column*    you configured.    ![Alert list](../../../_images/alerts.png)       [Create an alert](#id2) ------------------------     Follow these steps to create an alert on a single column of a query.     1. Do one of the following:      * Click   ![New Icon](../../../_images/create-icon.png)  **New**    in the sidebar and select   **Alert**    .  * Click   ![Alerts Icon](../../../_images/alerts-icon.png)  **Alerts**    in the sidebar and click the   **+ New Alert**    button.  * Click   ![Workspace Icon](../../../_images/workspace-icon.png)  **Workspace**    in the sidebar and click   **+ Create Alert**    . 2. Search for a target query.      ![Target query](../../../_images/new-alert-query-search.png)    To alert on multiple columns, you need to modify your query. See  [Alert on multiple columns](#alert-multiple-columns)  . 3. In the  **Trigger when**   field, configure the alert.      * The   **Value column**    drop-down controls which field of your query result is evaluated.  * The   **Condition**    drop-down controls the logical operation to be applied.  * The   **Threshold**    text input is compared against the Value column using the Condition you specify. ![Alert settings](../../../_images/alert_settings.png)     Note       If a target query returns multiple records, Databricks SQL alerts act on the first one. As you change the Value column setting, the current value of that field in the top row is shown beneath it. 4. In the  **When triggered, send notification**   field, select how many notifications are sent when your alert is triggered:      * **Just once**    : Send a notification when the   [alert status](#view-alerts)   changes from   `OK`   to   `TRIGGERED`   .  * **Each time alert is evaluated**    : Send a notification whenever the alert status is   `TRIGGERED`   regardless of its status at the previous evaluation.  * **At most every**    : Send a notification whenever the alert status is   `TRIGGERED`   at a specific interval. This choice lets you avoid notification spam for alerts that trigger often.  Regardless of which notification setting you choose, you receive a notification whenever the status goes from  `OK`  to  `TRIGGERED`  or from  `TRIGGERED`  to  `OK`  . The schedule settings affect how many notifications you will receive if the status remains  `TRIGGERED`  from one execution to the next. For details, see  [Notification frequency](#notification-frequency)  . 5. In the  **Template**   drop-down, choose a template:      * **Use default template**    : Alert notification is a message with links to the Alert configuration screen and the Query screen.  * **Use custom template**    : Alert notification includes more specific information about the alert.          1. A box displays, consisting of input fields for subject and body. Any static content is valid, and you can incorporate built-in template variables:              + `ALERT\_STATUS`     : The evaluated alert status (string).    + `ALERT\_CONDITION`     : The alert condition operator (string).    + `ALERT\_THRESHOLD`     : The alert threshold (string or number).    + `ALERT\_NAME`     : The alert name (string).    + `ALERT\_URL`     : The alert page URL (string).    + `QUERY\_NAME`     : The associated query name (string).    + `QUERY\_URL`     : The associated query page URL (string).    + `QUERY\_RESULT\_VALUE`     : The query result value (string or number).    + `QUERY\_RESULT\_ROWS`     : The query result rows (value array).    + `QUERY\_RESULT\_COLS`     : The query result columns (string array).    An example subject, for instance, could be:    `Alert           ""{{ALERT\_NAME}}""           changed           status           to           {{ALERT\_STATUS}}`    .   2. You can use HTML to format messages in a custom template. The following tags and attributes are allowed in templates:              + Tags:     `<a>`     ,     `<abbr>`     ,     `<acronym>`     ,     `<b>`     ,     `<blockquote>`     ,     `<body>`     ,     `<br>`     ,     `<code>`     ,     `<div>`     ,     `<em>`     ,     `<h1>`     ,     `<h2>`     ,     `<h3>`     ,     `<h4>`     ,     `<h5`     >,     `<h6`     >,     `<head`     >,     `<hr>`     ,     `<html>`     ,     `<i>`     ,     `<li>`     ,     `<ol>`     ,     `<p>`     ,     `<span>`     ,     `<strong>`     ,     `<table>`     ,     `<tbody>`     ,     `<td>`     ,     `<th>`     ,     `<tr>`     ,     `<ul>`    + Attributes: href (for     `<a>`     ), title (for     `<a>`     ,     `<abbr>`     ,     `<acronym>`     )   3. Click the    **Preview**     toggle button to preview the rendered result.                    Important                 The preview is useful for verifying that template variables are rendered correctly. It is not an accurate representation of the eventual notification content, as each notification destination can display notifications differently.   4. Click the    **Save Changes**     button. 6. In  **Refresh**   , set a refresh schedule. An alert’s refresh schedule is independent of the query’s refresh schedule.      * If the query is a   **Run as owner**    query, the query runs using the query owner’s credential on the alert’s refresh schedule.  * If the query is a   **Run as viewer**    query, the query runs using the alert creator’s credential on the alert’s refresh schedule. 7. Click  **Create Alert**   . 8. Choose a  [notification destination](../../admin/notification-destinations.html)  .        Important       If you skip this step you  *will not*   be notified when the alert is triggered.       ![Notification destination](../../../_images/alert_destination.png)        [Alert on multiple columns](#id3) ----------------------------------     To set an alert based on multiple columns of a query, your query can implement the alert logic and return a boolean value for the alert to trigger on. For example:        ``` SELECT CASE WHEN drafts\_count > 10000 AND archived\_count > 5000 THEN 1 ELSE 0 END FROM ( SELECT sum(CASE WHEN is\_archived THEN 1 ELSE 0"	To set an alert on your Databricks usage, you can use the SQL editor to define alerts to notify you when a field returned by a scheduled query meets a threshold. Follow the steps outlined in the context to create an alert on a single column of a query. You can also set an alert based on multiple columns of a query by implementing the alert logic and returning a boolean value for the alert to trigger on. Once you have created the alert, choose a notification destination to receive notifications when the alert is triggered.	1
no_reaction	['https://docs.databricks.com/error-messages/user-defined-functions-error-class.html']	can I trigger a user defined function based upon an update to a table	    Functions ===========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Spark SQL provides two function features to meet a wide range of needs: built-in functions and user-defined functions (UDFs).        Built-in functions --------------------     This article presents the usages and descriptions of categories of frequently used built-in functions for aggregation, arrays and maps, dates and timestamps, and JSON data.      * [Built-in functions](sql-ref-functions-builtin.html)        SQL user-defined functions ----------------------------     SQL user-defined functions (UDFs) are functions you can define yourself which can return scalar values or result sets.       See  [CREATE FUNCTION (SQL)](sql-ref-syntax-ddl-create-sql-function.html)  for more information.      ###   User-defined functions     UDFs allow you to define your own functions when the system’s built-in functions are not enough to perform the desired task. To use UDFs, you first define the function, then register the function with Spark, and finally call the registered function. A UDF can act on a single row or act on multiple rows at once. Spark SQL also supports integration of existing Hive implementations of UDFs, user defined aggregate functions (UDAF), and user defined table functions (UDTF).      * [User-defined aggregate functions (UDAFs)](sql-ref-functions-udf-aggregate.html) * [Integration with Hive UDFs, UDAFs, and UDTFs](sql-ref-functions-udf-hive.html) * [User-defined scalar functions (UDFs)](sql-ref-functions-udf-scalar.html)                                                        	Yes, you can trigger a user-defined function based upon an update to a table. However, the process for doing so may vary depending on the specific database or platform being used. The context provides information on user-defined functions (UDFs) in Spark SQL, which can be defined and registered with Spark to act on single or multiple rows.	0
no_reaction	['https://docs.databricks.com/delta-live-tables/index.html']	I meant Delta Live Tables	"      Delta Live Tables CLI =======================     You run Databricks Delta Live Tables CLI subcommands by appending them to  `databricks     pipelines`  . These subcommands call the  [Delta Live Tables API](../../delta-live-tables/api-guide.html)  .        ``` databricks pipelines -h  ```       ``` Usage: databricks pipelines [OPTIONS] COMMAND [ARGS]...    Utility to interact with Databricks Delta Live Tables Pipelines.  Options:   -v, --version   0.17.0   --debug         Debug Mode. Shows full stack trace on error.   --profile TEXT  CLI connection profile to use. The default profile is                   ""DEFAULT"".    -h, --help      Show this message and exit.  Commands:   create  Creates a pipeline.   delete  Deletes the pipeline and cancels any active updates.   deploy  [Deprecated] This command is deprecated, use create and edit           commands instead.  Creates or edits a pipeline specified by the           pipeline settings.    edit    Edits a pipeline.   get     Gets a pipeline's current settings and status.   list    Lists all pipelines and their statuses.   reset   [Deprecated] Use the ""start --full-refresh"" command instead. Resets           a pipeline so that data can be reprocessed from the beginning.    run     [Deprecated] Use the ""start"" command instead. Starts a pipeline           update.    start   Starts a pipeline update.   stop    Stops the pipeline by cancelling any active update.  ```       Create a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     create     --help`  .      ###   Usage      ``` databricks pipelines create --settings create-pipeline.json  ```     `create-pipeline.json`  :        ``` {  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 5  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_analysis"",  ""continuous"": false }  ```       ``` Successfully created pipeline: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003 with ID: 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Edit a pipeline -----------------     To display usage documentation, run  `databricks     pipelines     edit     --help`  .        ###   Usage      ``` databricks pipelines edit --settings edit-pipeline.json  ```     `edit-pipeline.json`  :        ``` {  ""id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 3  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_quickstart\_data"",  ""continuous"": false }  ```       ``` Successfully edited pipeline settings: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Delete a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     delete     --help`  .        ###   Usage      ``` databricks pipelines delete --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003 deleted  ```         Get information about a pipeline ----------------------------------     To display usage documentation, run  `databricks     pipelines     get     --help`  .        ###   Usage      ``` databricks pipelines get --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` {  ""pipeline\_id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""spec"": {  ""id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 5  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_analysis"",  ""continuous"": false  },  ""state"": ""IDLE"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""creator\_user\_name"": ""username@databricks.com"",  ""last\_modified"": 1641600056265 }  ```         List all pipelines and information on their status ----------------------------------------------------     To display usage documentation, run  `databricks     pipelines     list     --help`  .        ###   Usage      ``` databricks pipelines list  ```       ``` [  {  ""pipeline\_id"": ""8a93e1b4-7024-11ec-90d6-0242ac120003"",  ""state"": ""IDLE"",  ""name"": ""Retail sales pipeline (SQL)"",  ""latest\_updates"": [  {  ""update\_id"": ""9df03924-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T16:18:57.982Z""  },  {  ""update\_id"": ""a57e601c-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-28T18:19:30.371Z""  }  ],  ""creator\_user\_name"": ""user@databricks.com""  },  {  ""pipeline\_id"": ""b46e2670-7024-11ec-90d6-0242ac120003"",  ""state"": ""IDLE"",  ""name"": ""DLT quickstart (Python)"",  ""latest\_updates"": [  {  ""update\_id"": ""bcd8fa2e-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-12-16T18:19:25.827Z""  },  {  ""update\_id"": ""c2c7a2c8-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T22:22:32.586Z""  },  {  ""update\_id"": ""c88d2818-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T22:16:18.258Z""  }  ],  ""creator\_user\_name"": ""user@databricks.com""  } ]  ```         Start a pipeline update -------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Started an update c6462e2e-beb7-4c24-a9d3-962eaa19781b for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```        Start a pipeline update with full refresh -------------------------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ###   Usage      ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003 --full-refresh  ```       ``` Started an update ed885d3d-4fc0-44fd-b5dd-59471c789327 with full refresh for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Stop a pipeline update ------------------------     To display usage documentation, run  `databricks     pipelines     stop     --help`  .        ###   Usage      ``` databricks pipelines stop --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Stopped pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```                                                        "	The context is about Delta Live Tables CLI, which is a utility to interact with Databricks Delta Live Tables Pipelines. It provides subcommands to create, edit, delete, get information about, list, start, and stop pipelines.	1
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/index.html']	Does unity catalog work with Informatica?	    What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	Based on the context, there is no clear information on whether Unity Catalog works with Informatica.	0
no_reaction	['https://docs.databricks.com/archive/admin-guide/aws-storage.html']	is it possible to change the default location of aws s3 data storage	    Manage storage configurations using the account console =========================================================     This article describes how to:     * Create and configure an S3 bucket to store a limited set of Databricks workspace information such as libraries, some logs, and notebook version history. * Use the Databricks  [account console](../account-settings/index.html#account-console)  to create a  *storage configuration*   that references the S3 bucket.      Note       This article describes the process using the account console. To learn how to create storage configurations using the Account API, see  [Create a workspace using the Account API](../workspace/create-workspace-api.html)  . For legacy account types, see  [Configure AWS storage (legacy)](../../archive/admin-guide/aws-storage.html)  .        The bucket you include in your storage configuration is referred to as your workspace’s root storage. Do not use your root storage to store production customer data. Instead, create additional S3 buckets or other data sources for production data and optionally  [create DBFS mount points](../../storage/amazon-s3.html)  for them.        Define a storage configuration and generate a bucket policy -------------------------------------------------------------     These instructions show you how to create the storage configuration from the account console before you create a new workspace. You can also create the storage configuration as part of the flow of creating a new workspace. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .     1. In to the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Storage configuration**   . 3. Click  **Add storage configuration**   . 4. In the  **Storage configuration name**   field, enter a human-readable name for your new storage configuration. 5. In the  **Bucket Name**   field, enter the exact name of the S3 bucket you will create.        Important       The bucket name cannot include dot notation (  `.`  ). For more bucket naming guidance, see the  [AWS bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)   . 6. Click  **Generate Policy**   and copy the policy that is generated. You will add this to your S3 bucket configuration in AWS in the next task. 7. Click  **Add**   .       Create the S3 bucket ----------------------   1. Log into your AWS Console as a user with administrator privileges and go to the  **S3**   service. 2. Create an S3 bucket, using the name that you entered in the Databricks storage configuration.       See  [Create a Bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html)   in the AWS documentation.        Important       * The S3 bucket must be in the same AWS region as the Databricks workspace deployment.  * Databricks recommends as a best practice that you use an S3 bucket that is dedicated to Databricks, unshared with other resources or services.  * The bucket name cannot include dot notation (   `.`   ).  * Do not reuse a bucket from legacy workspaces. For example, if you are migrating to E2, create a new AWS bucket for your E2 setup. 3. Click the  **Permissions**   tab. 4. In the  **Bucket policy**   section, click  **Edit**   . 5. Paste the bucket policy that you generated and copied from the  **Add Storage Configuration**   dialog in Databricks. 6. Save the bucket.       Enable object-level logging (recommended) -------------------------------------------     Databricks strongly recommends that you enable S3 object-level logging for your root storage bucket. This enables faster investigation of any issues that may come up. Be aware that S3 object-level logging can increase AWS usage costs.       For instructions, see the  [AWS documentation on CloudTrail event logging for S3 buckets and objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-cloudtrail-logging-for-s3.html)   .         View storage configurations -----------------------------   1. In to the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Storage configuration**   .       All storage configurations are listed, with  **Bucket Name**   and  **Created**   date displayed for each. 3. Click the storage configuration name to view more details.         Delete a storage configuration --------------------------------     Storage configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the storage configuration:     1. In to the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Storage configuration**   . 3. On the storage configuration row, click the Actions menu icon, and select  **Delete**   .       You can also click the storage configuration name and click  **Delete**   on the pop-up dialog. 4. In the confirmation dialog, click  **Confirm Delete**   .       Encrypt your root S3 bucket using customer-managed keys (optional) --------------------------------------------------------------------     You can encrypt your root S3 bucket using customer-managed keys, which requires using the  [Account API 2.0](../../dev-tools/api/latest/account.html)  .       You can either add an encryption key when you  [create a new workspace using the Account API](../workspace/create-workspace-api.html#cmk)  or add the key later. For more information, see  [Customer-managed keys for workspace storage](../../security/keys/customer-managed-keys-storage-aws.html)  .                                                        	Yes, it is possible to change the default location of AWS S3 data storage. This can be done by creating and configuring an S3 bucket to store a limited set of Databricks workspace information such as libraries, some logs, and notebook version history. Then, using the Databricks account console, a storage configuration can be created that references the S3 bucket. The bucket you include in your storage configuration is referred to as your workspace’s root storage.	1
no_reaction	['https://docs.databricks.com/introduction/index.html']	is databricks good	    What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .                                                          	Based on the context, Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used for processing, storing, cleaning, sharing, analyzing, modeling, and monetizing datasets with solutions from BI to machine learning. Databricks provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks has a strong commitment to the open-source community and maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use. Databricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks is used for various use cases such as building an enterprise data lakehouse, ETL and data engineering, machine learning, AI, and data science, data warehousing, analytics, and BI, data governance, and secure data sharing, DevOps, CI/CD, and task orchestration, and real-time and streaming analytics.	1
no_reaction	['https://docs.databricks.com/structured-streaming/initial-state-map-groups-with-state.html']	how do I do stateful streaming in Python?	      Optimize stateful Structured Streaming queries ================================================     Managing the intermediate state information of stateful Structured Streaming queries can help prevent unexpected latency and production problems.        Preventing slow down from garbage collection (GC) pause in stateful streaming -------------------------------------------------------------------------------     If you have stateful operations in your streaming query (such as streaming aggregation) and you want to maintain millions of keys in the state, then you may face issues related to large JVM garbage collection (GC) pauses. This causes high variations in the micro-batch processing times. This occurs because your JVM’s memory maintains your state data by default. Having a large number of state objects puts pressure on your JVM memory, which causes high GC pauses.       In such cases, you can choose to use a more optimized state management solution based on  [RocksDB](https://rocksdb.org/)   . This solution is available in Databricks Runtime. Rather than keeping the state in the JVM memory, this solution uses RocksDB to efficiently manage the state in the native memory and the local SSD. Furthermore, any changes to this state are automatically saved by Structured Streaming to the checkpoint location you have provided, thus providing full fault-tolerance guarantees (the same as default state management). For instructions for configuring RocksDB as state store, see  [Configure RocksDB state store on Databricks](rocksdb-state-store.html)  .           Recommended configurations for stateful Structured Streaming on Databricks ----------------------------------------------------------------------------     Databricks recommends:     * Use compute-optimized instances as workers. For example, AWS c3.4xlarge instances. * Set the number of shuffle partitions to 1-2 times number of cores in the cluster. * Set the  `spark.sql.streaming.noDataMicroBatches.enabled`  configuration to  `false`  in the SparkSession. This prevents the streaming micro-batch engine from processing micro-batches that do not contain data. Note also that setting this configuration to  `false`  could result in stateful operations that leverage watermarks or processing time timeouts to not get data output until new data arrives instead of immediately.     Regarding performance benefits, RocksDB-based state management can maintain 100 times more state keys than the default one. For example, in a Spark cluster with AWS c3.4xlarge instances as workers, the default state management can maintain up to 1-2 million state keys per executor after which the JVM GC starts affecting performance significantly. In contrast, the RocksDB-based state management can easily maintain 100 million state keys per executor without any GC issues.        Note       The state management scheme cannot be changed between query restarts. That is, if a query has been started with the default management, then it cannot changed without starting the query from scratch with a new checkpoint location.                                                         	To do stateful streaming in Python, you can use Structured Streaming in Apache Spark. To optimize stateful Structured Streaming queries, you can use RocksDB as a more efficient state management solution. Databricks recommends using compute-optimized instances as workers, setting the number of shuffle partitions to 1-2 times the number of cores in the cluster, and setting the `spark.sql.streaming.noDataMicroBatches.enabled` configuration to `false`. Note that the state management scheme cannot be changed between query restarts.	1
thumb_down	['https://docs.databricks.com/data/manage-permissions.html']	HOw to add groups in Unity Catalog	    Manage groups ===============     This article explains how admins create and manage Databricks groups. For an overview of the Databricks identity model, see  [Databricks identities and roles](index.html#identity-model)  .        Overview of group management ------------------------------     Groups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.        ###   Difference between account groups and workspace-local groups     While users and service principals created at the workspace level are  [automatically synchronized to the account](index.html#account-level-identity-sync)  , groups created at the workspace level are not. Instead, Databricks has the concept of  *account groups*   and  *workspace-local groups*   .     * **Account groups**   can be granted access to data in a  [Unity Catalog](../../data-governance/unity-catalog/index.html)  metastore and permissions to  [identity federated](index.html#assign-users-to-workspaces)  workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API. * **Workspace-local groups**   are legacy groups. These groups are identified as  *workspace-local*   in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a  [Unity Catalog](../../data-governance/unity-catalog/index.html)  metastore.      Note       In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the  [SCIM API 2.0 (Groups) for workspaces](../../dev-tools/api/latest/scim/scim-groups.html)  API.        Databricks recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but Databricks recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using Unity Catalog. See  [Migrate workspace-local groups to account groups](#migrate)  .       ###   Who can manage groups?     To manage groups in Databricks, you must be either an  *account admin*   or a  *workspace admin*   .     * **Account admins**   can add groups to the account and manage group members. They can also assign groups to workspaces and configure data access for them across workspaces, as long as those workspaces use  [identity federation](index.html#assign-users-to-workspaces)  . * **Workspace admins**   can add account groups to their identity federated workspace and manage access to objects and functionality in the workspace, such as the ability to create clusters or pools. Workspace admins cannot create or manage account groups. Workspace admins can create and manage workspace-local groups. In identity federated workspaces, workspace-local groups can only be managed using the  [SCIM API 2.0 (Groups) for workspaces](../../dev-tools/api/latest/scim/scim-groups.html)  API.     Workspace admins are members of the  `admins`  group in the workspace, which is a reserved group that cannot be deleted.            Add groups to your account ----------------------------     As an account admin, you can add groups to your Databricks account using the account console, a provisioning connector for your identity provider, or the SCIM (Account) API.        ###   Add groups to your account using the account console     To add a group to the account using the account console, do the following:     1. As an account admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Account Console user management icon](../../_images/user-management.png) **User management**   . 3. On the  **Groups**   tab, click  **Add group**   . 4. Enter a name for the group. 5. Click  **Confirm**   . 6. When prompted, add users, service principals, and groups to the group.       ###   Add users, service principals, and groups to an existing group using the account console     To add users, service principals, and groups to an existing group using the account console, do the following:     1. As an account admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Account Console user management icon](../../_images/user-management.png) **User management**   . 3. On the  **Groups**   tab, select the group you want to update. 4. Click  **Add members**   . 5. Search for the user, group, or service principal you want to add and select it. 6. Click  **Add**   .     To give group members access to a workspace, you need to add the group to the workspace using identity federation. See  [Add groups to workspaces](#add-groups-workspace)  .         ###   Sync groups to your Databricks account from an identity provider     You can sync groups from your identity provider (IdP) to your Databricks account using a SCIM provisioning connector. For instructions, see  [Provision identities to your Databricks account](scim/index.html#provision-account)  .        Important       If you already have SCIM connectors that sync identities directly to your workspaces  **and those workspaces are enabled for identity federation**   , we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.        ###   Add groups to your account using the SCIM APIs     Account admins can add and manage groups in the Databricks account using the SCIM API for Accounts.       Workspace admins can’t add groups using this API, but they can list and view groups. To do this, they must invoke the API using a different endpoint URL:     * Account admins use  `accounts.cloud.databricks.com/api/2.0/accounts/{account\_id}/scim/v2/`  . * Workspace admins use  `{workspace-domain}/api/2.0/account/scim/v2/`  .     Workspace admins cannot create groups using the SCIM API for Accounts. For details, see  [SCIM API 2.0 (Accounts)](../../dev-tools/api/latest/scim/account-scim.html)  .            Assign the account admin role to a group ------------------------------------------     You cannot assign the account admin role to a group using the account console, but you can assign it to groups using the SCIM API for Accounts. See  [SCIM API 2.0 (Accounts)](../../dev-tools/api/latest/scim/account-scim.html)  .         Remove groups from your Databricks account --------------------------------------------     Account admins can remove groups from a Databricks account. Workspace admins cannot.        Important       When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to,  *unless they are members of another group or have been directly granted access to the account or any workspaces*   . We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:     * Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API * Jobs owned by the user will fail * Clusters owned by the user will stop * Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing      To remove a group using the account console, do the following:     1. As an account admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Account Console user management icon](../../_images/user-management.png) **User management**   . 3. On the  **Groups**   tab, find the group you want to remove. 4. Click the  ![Kebab menu](../../_images/kebab-menu.png)  kebab menu at the far right of the user row and select  **Delete**   . 5. In the confirmation dialog box, click  **Confirm delete**   .     If you remove a group using the account console, you must ensure that you also remove the group using any SCIM provisioning connectors or SCIM API applications that have been set up for the account. If you don’t, SCIM provisioning will simply add the group and its members back the next time it syncs. See  [Sync users and groups from your identity provider](scim/index.html)  .       To remove a group from a Databricks account using SCIM APIs, see  [Provision identities to your Databricks account](scim/index.html#provision-account)  and  [SCIM API 2.0 (Accounts)](../../dev-tools/api/latest/scim/account-scim.html)  .           Migrate workspace-local groups to account groups --------------------------------------------------     This section applies only to workspaces that are enabled for  [identity federation](index.html#enable-identity-federation)  .       Groups created at the workspace level (workspace-local groups) are not automatically synchronized to the account as account groups. You can use workspace-local groups in the workspace they are defined in, but you cannot manage them using account-level interfaces, and you cannot use them to manage data access across workspaces using Unity Catalog. Therefore Databricks recommends that you convert them to account groups.       You can use any of the following methods to migrate workspace-local groups to the account level:     * **Convert them manually.**   Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.       See  [Add groups to your account using the account console](#add-group-account-console)  . * **Convert them using a SCIM provisioning connector.**   Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.       See  [Sync users and groups from your identity provider](scim/index.html)  . * **Convert them using the SCIM APIs.**   Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.       See  [SCIM API 2.0 (Accounts)](../../dev-tools/api/latest/scim/account-scim.html)  and  [SCIM API 2.0 (Groups) for workspaces](../../dev-tools/api/latest/scim/scim-groups.html)  .     After you migrate the workspace-local group to the account, you need to grant the new account group access to the workspace and the objects, and the functionality that the workspace-local group originally had access to so that the group members maintains that access. Follow  [Add groups to workspaces](#add-groups-workspace)  to assign workspace permissions to the new account groups, and use  [Permissions API 2.0](../../dev-tools/api/latest/permissions.html)  to grant the group access to objects within the workspace.           Add groups to workspaces --------------------------     Workspace admins can add account groups to identity-federated workspaces using the workspace admin console and the Workspace Assignment API. Workspace admins can add and manage workspace-local groups in non-identity federated workspaces using the workspace admin console and the workspace-level SCIM (Groups) API.      ###   Assign a group to a workspace	To assign a group to a workspace in Databricks, you need to use identity federation. Workspace admins can add account groups to identity-federated workspaces using the workspace admin console and the Workspace Assignment API. To do this, you need to be a workspace admin and the group needs to be an account group. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a Unity Catalog metastore.	1
thumb_down	['https://spark.apache.org/docs/latest/sql-ref-number-pattern.html']	how do I convert a string to a number in pyspark?	"   pyspark.pandas.to\_numeric  [¶](#pyspark-pandas-to-numeric ""Permalink to this headline"") ==========================================================================================  `pyspark.pandas.`  `to_numeric`    (   *arg*  ,  *errors     =     'raise'*   )   [[source]](../../../_modules/pyspark/pandas/namespace.html#to_numeric) [¶](#pyspark.pandas.to_numeric ""Permalink to this definition"")    Convert argument to a numeric type.     Parameters    **arg**    scalar, list, tuple, 1-d array, or Series     Argument to be converted.    **errors**    {‘raise’, ‘coerce’}, default ‘raise’    * If ‘coerce’, then invalid parsing will be set as NaN. * If ‘raise’, then invalid parsing will raise an exception. * If ‘ignore’, then invalid parsing will return the input.   Note     ‘ignore’ doesn’t work yet when    arg    is pandas-on-Spark Series.     Returns    **ret**    numeric if parsing succeeded.     See also    [`DataFrame.astype`](pyspark.pandas.DataFrame.astype.html#pyspark.pandas.DataFrame.astype ""pyspark.pandas.DataFrame.astype"")   Cast argument to a specified dtype.    [`to\_datetime`](pyspark.pandas.to_datetime.html#pyspark.pandas.to_datetime ""pyspark.pandas.to_datetime"")   Convert argument to datetime.    [`to\_timedelta`](pyspark.pandas.to_timedelta.html#pyspark.pandas.to_timedelta ""pyspark.pandas.to_timedelta"")   Convert argument to timedelta.    `numpy.ndarray.astype`   Cast a numpy array to a specified type.     Examples    ``` >>> psser = ps.Series(['1.0', '2', '-3']) >>> psser 0 1.0 1 2 2 -3 dtype: object  ```  ``` >>> ps.to\_numeric(psser) 0 1.0 1 2.0 2 -3.0 dtype: float32  ```   If given Series contains invalid value to cast float, just cast it to    np.nan    when    errors    is set to “coerce”.    ``` >>> psser = ps.Series(['apple', '1.0', '2', '-3']) >>> psser 0 apple 1 1.0 2 2 3 -3 dtype: object  ```  ``` >>> ps.to\_numeric(psser, errors=""coerce"") 0 NaN 1 1.0 2 2.0 3 -3.0 dtype: float32  ```   Also support for list, tuple, np.array, or a scalar    ``` >>> ps.to\_numeric(['1.0', '2', '-3']) array([ 1., 2., -3.])  ```  ``` >>> ps.to\_numeric(('1.0', '2', '-3')) array([ 1., 2., -3.])  ```  ``` >>> ps.to\_numeric(np.array(['1.0', '2', '-3'])) array([ 1., 2., -3.])  ```  ``` >>> ps.to\_numeric('1.0') 1.0  ```  [pyspark.pandas.notnull](pyspark.pandas.notnull.html ""previous page"")  [pyspark.pandas.to\_datetime](pyspark.pandas.to_datetime.html ""next page"")   "	To convert a string to a number in PySpark, you can use the `to_numeric` function from the `pyspark.pandas` module. This function can convert a scalar, list, tuple, 1-d array, or Series to a numeric type. You can specify the `errors` parameter as `'coerce'` to set invalid parsing as NaN, `'raise'` to raise an exception for invalid parsing, or `'ignore'` to return the input for invalid parsing.	1
no_reaction	['https://docs.databricks.com/partner-connect/walkthrough-fivetran.html']	how can I get data from Fivetran	    Connect to Fivetran =====================     Fivetran automated data integration adapts as schemas and APIs change, ensuring reliable data access and simplified analysis with ready-to-query schemas.       You can integrate your Databricks SQL  [warehouses](../../sql/admin/create-sql-warehouse.html)  (formerly Databricks SQL endpoints) and Databricks  [clusters](../../clusters/index.html)  with Fivetran. The Fivetran integration with Databricks helps you centralize data from disparate data sources into Delta Lake.            Connect to Fivetran using Partner Connect -------------------------------------------     To connect your Databricks workspace to Fivetran using Partner Connect, see  [Connect to ingestion partners using Partner Connect](../../partner-connect/ingestion.html)  .        Note       Partner Connect does not integrate Fivetran with Databricks clusters. To integrate a cluster with Fivetran, connect to Fivetran manually.        For an overview of the Partner Connect procedure, watch this YouTube video (3 minutes).                     Connect to Fivetran manually ------------------------------     For an overview of the manual connection procedure, watch this YouTube video (2 minutes).                  Note       To connect a SQL warehouse with Fivetran faster, use Partner Connect.         ###   Requirements     Before you connect to Fivetran manually, you must have the following:     * A cluster or SQL warehouse in your Databricks workspace.      + [Create a cluster](../../clusters/configure.html)   .  + [Configure SQL warehouses](../../sql/admin/create-sql-warehouse.html)   . * The connection details for your cluster or SQL warehouse, specifically the  **Server Hostname**   ,  **Port**   , and  **HTTP Path**   values.      + [Get connection details for a cluster](../../integrations/jdbc-odbc-bi.html#connection-details-cluster)   .  + [Get connection details for a SQL warehouse](../../integrations/jdbc-odbc-bi.html#sql)   . * A Databricks  [personal access token](../../dev-tools/auth.html#pat)  .        Note       As a security best practice, when authenticating with automated tools, systems, scripts, and apps, Databricks recommends you use access tokens belonging to  [service principals](../../administration-guide/users-groups/service-principals.html)  instead of workspace users. To create access tokens for service principals, see  [Manage access tokens for a service principal](../../administration-guide/users-groups/service-principals.html#personal-access-tokens)  .      Tip       If the  **Fivetran**   tile  [in Partner Connect](#partner-connect)  in your workspace has a check mark icon inside of it, you can get the connection details for the connected SQL warehouse by clicking the tile and then expanding  **Connection details**   . The  **Personal access token**   is hidden; you must  [create a replacement personal access token](../../partner-connect/index.html#how-to-create-token)  and enter that new token instead when Fivetran asks you for it.          ###   Steps to connect     To connect to Fivetran manually, do the following:     1. Sign in to your Fivetran account, or create a new Fivetran account, at  <https://fivetran.com/login>  .        Important       If you sign in to your organization’s Fivetran account, a  **Choose Destination**   page may display, listing one or more existing destination entries with the Databricks logo.  *These entries might contain connection details for compute resources in workspaces that are separate from yours.*   If you still want to reuse one of these connections, and you trust the compute resource and have access to it, choose that destination and then skip ahead to  [Next steps](#next-steps)  . Otherwise, choose any available destination to get past this page, and then go to  <https://fivetran.com/account>  . 2. In your  **Dashboard**   page in Fivetran, click the  **Destinations**   tab. (If the  **Dashboard**   page is not displayed, go to  <https://fivetran.com/account>  .) 3. Click  **Add Destination**   . 4. Enter a  **Destination name**   and click  **Add**   . 5. On the  **Fivetran is modern ELT**   page, click  **Set up a connector**   . 6. Click a data source, and then click  **Next**   . 7. Follow the on-screen instructions in the  **Setup Guide**   in Fivetran to finish setting up the connector. 8. Click  **Save & Test**   . 9. After the test succeeds, click  **Continue**   . 10. On the  **Select your data’s destination**   page, click  **Databricks on AWS**   . 11. Click  **Continue Setup**   . 12. Complete the on-screen instructions in Fivetran to enter the connection details for your existing Databricks compute resource, specifically the  **Server Hostname**   and  **HTTP Path**   field values, and the token that you generated earlier. 13. Click  **Save & Test**   . 14. After the test succeeeds, click  **Continue**   . 15. Continue with  [Next steps](#next-steps)  .          Next steps ------------     Explore one or more of the following resources on the Fivetran website:     * [Getting Started](https://fivetran.com/docs/getting-started) * [Architecture](https://fivetran.com/docs/getting-started/architecture) * [Connectors](https://fivetran.com/docs/getting-started/fivetran-dashboard/connectors) * [Destinations](https://fivetran.com/docs/getting-started/fivetran-dashboard/destination) * [Sync Overview](https://fivetran.com/docs/getting-started/syncoverview) * [Transformations](https://fivetran.com/docs/transformations) * [Fivetran Documentation](https://fivetran.com/docs) * [Fivetran Support](https://support.fivetran.com/hc)                                                      	To get data from Fivetran, you can integrate your Databricks SQL warehouses and Databricks clusters with Fivetran. You can connect to Fivetran using Partner Connect or manually. To connect manually, you need a cluster or SQL warehouse in your Databricks workspace, the connection details for your cluster or SQL warehouse, and a Databricks personal access token. You can follow the steps provided in the context to connect to Fivetran manually.	1
no_reaction	['https://kb.databricks.com/streaming/sqs-stream-partition-values']	how to read parquet file from s3 insql ?	    Parquet file ==============    [Apache Parquet](https://parquet.apache.org/)   is a columnar file format that provides optimizations to speed up queries. It is a far more efficient file format than  [CSV](csv.html)  or  [JSON](json.html)  .       For more information, see  [Parquet Files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#parquet(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#parquet(path:String):Unit)     The following notebook shows how to read and write data to Parquet files.        ###   Reading Parquet files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-parquet-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 	To read a Parquet file from S3 in SQL, you can use the following syntax:  ``` SELECT * FROM parquet.`s3://<bucket>/<path_to_file>` ```  Replace `<bucket>` with the name of your S3 bucket and `<path_to_file>` with the path to your Parquet file within the bucket.	0
no_reaction	['https://spark.apache.org/docs/latest/spark-connect-overview.html']	What is Spark Connect? Answer briefly using only emoji.	"   Spark Connect Overview ========================  **Building client-side Spark applications**    In Apache Spark 3.4, Spark Connect introduced a decoupled client-server architecture that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks and programming languages.     To get started, see  [Quickstart: Spark Connect](api/python/getting_started/quickstart_connect.html)   .    ![Spark Connect API Diagram](img/spark-connect-api.png ""Spark Connect API"")   How Spark Connect works =========================   The Spark Connect client library is designed to simplify Spark application development. It is a thin API that can be embedded everywhere: in application servers, IDEs, notebooks, and programming languages. The Spark Connect API builds on Spark’s DataFrame API using unresolved logical plans as a language-agnostic protocol between the client and the Spark driver.     The Spark Connect client translates DataFrame operations into unresolved logical query plans which are encoded using protocol buffers. These are sent to the server using the gRPC framework.     The Spark Connect endpoint embedded on the Spark Server receives and translates unresolved logical plans into Spark’s logical plan operators. This is similar to parsing a SQL query, where attributes and relations are parsed and an initial parse plan is built. From there, the standard Spark execution process kicks in, ensuring that Spark Connect leverages all of Spark’s optimizations and enhancements. Results are streamed back to the client through gRPC as Apache Arrow-encoded row batches.    ![Spark Connect communication](img/spark-connect-communication.png ""Spark Connect communication"")   Operational benefits of Spark Connect =======================================   With this new architecture, Spark Connect mitigates several multi-tenant operational issues:    **Stability**   : Applications that use too much memory will now only impact their own environment as they can run in their own processes. Users can define their own dependencies on the client and don’t need to worry about potential conflicts with the Spark driver.    **Upgradability**   : The Spark driver can now seamlessly be upgraded independently of applications, for example to benefit from performance improvements and security fixes. This means applications can be forward-compatible, as long as the server-side RPC definitions are designed to be backwards compatible.    **Debuggability and observability**   : Spark Connect enables interactive debugging during development directly from your favorite IDE. Similarly, applications can be monitored using the application’s framework native metrics and logging libraries.     How to use Spark Connect ==========================   Starting with Spark 3.4, Spark Connect is available and supports PySpark and Scala applications. We will walk through how to run an Apache Spark server with Spark Connect and connect to it from a client application using the Spark Connect client library.     Download and start Spark server with Spark Connect ----------------------------------------------------   First, download Spark from the  [Download Apache Spark](https://spark.apache.org/downloads.html)   page. Spark Connect was introduced in Apache Spark version 3.4 so make sure you choose 3.4.0 or newer in the release drop down at the top of the page. Then choose your package type, typically “Pre-built for Apache Hadoop 3.3 and later”, and click the link to download.     Now extract the Spark package you just downloaded on your computer, for example:    ``` tar -xvf spark-3.4.0-bin-hadoop3.tgz ```   In a terminal window, go to the  `spark`   folder in the location where you extracted Spark before and run the  `start-connect-server.sh`   script to start Spark server with Spark Connect, like in this example:    ``` ./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.4.0 ```   Note that we include a Spark Connect package (  `spark-connect_2.12:3.4.0`   ), when starting Spark server. This is required to use Spark Connect. Make sure to use the same version of the package as the Spark version you downloaded previously. In this example, Spark 3.4.0 with Scala 2.12.     Now Spark server is running and ready to accept Spark Connect sessions from client applications. In the next section we will walk through how to use Spark Connect when writing client applications.     Use Spark Connect in client applications ------------------------------------------   When creating a Spark session, you can specify that you want to use Spark Connect and there are a few ways to do that outlined as follows.     If you do not use one of the mechanisms outlined here, your Spark session will work just like before, without leveraging Spark Connect, and your application code will run on the Spark driver node.    ###   Set SPARK\_REMOTE environment variable   If you set the  `SPARK_REMOTE`   environment variable on the client machine where your Spark client application is running and create a new Spark Session as in the following example, the session will be a Spark Connect session. With this approach, there is no code change needed to start using Spark Connect.     In a terminal window, set the  `SPARK_REMOTE`   environment variable to point to the local Spark server you started previously on your computer:    ``` export SPARK\_REMOTE=""sc://localhost"" ```   And start the Spark shell as usual:    ``` ./bin/pyspark ```   The PySpark shell is now connected to Spark using Spark Connect as indicated in the welcome message:    ``` Client connected to the Spark Connect server at localhost ```   And if you write your own program, create a Spark session as shown in this example:    ``` from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() ```   This will create a Spark Connect session from your application by reading the  `SPARK_REMOTE`   environment variable we set previously.    ###   Specify Spark Connect when creating Spark session   You can also specify that you want to use Spark Connect explicitly when you create a Spark session.     For example, you can launch the PySpark shell with Spark Connect as illustrated here.     To launch the PySpark shell with Spark Connect, simply include the  `remote`   parameter and specify the location of your Spark server. We are using  `localhost`   in this example to connect to the local Spark server we started previously:    ``` ./bin/pyspark --remote ""sc://localhost"" ```   And you will notice that the PySpark shell welcome message tells you that you have connected to Spark using Spark Connect:    ``` Client connected to the Spark Connect server at localhost ```   You can also check the Spark session type. If it includes  `.connect.`   you are using Spark Connect as shown in this example:    ``` SparkSession available as 'spark'. >>> type(spark) <class 'pyspark.sql.connect.session.SparkSession'> ```   Now you can run PySpark code in the shell to see Spark Connect in action:    ``` >>> columns = [""id"",""name""] >>> data = [(1,""Sarah""),(2,""Maria"")] >>> df = spark.createDataFrame(data).toDF(\*columns) >>> df.show() +---+-----+ | id| name| +---+-----+ |  1|Sarah| |  2|Maria| +---+-----+ ```   Or, when writing your own code, include the  `remote`   function with a reference to your Spark server when you create a Spark session, as in this example:    ``` from pyspark.sql import SparkSession spark = SparkSession.builder.remote(""sc://localhost"").getOrCreate() ```  ``` import org.apache.spark.sql.SparkSession val spark = SparkSession.builder().remote(""sc://localhost"").build() ```   Client application authentication ===================================   While Spark Connect does not have built-in authentication, it is designed to work seamlessly with your existing authentication infrastructure. Its gRPC HTTP/2 interface allows for the use of authenticating proxies, which makes it possible to secure Spark Connect without having to implement authentication logic in Spark directly.     What is supported in Spark 3.4 ================================  **PySpark**   : In Spark 3.4, Spark Connect supports most PySpark APIs, including  [DataFrame](api/python/reference/pyspark.sql/dataframe.html)   ,  [Functions](api/python/reference/pyspark.sql/functions.html)   , and  [Column](api/python/reference/pyspark.sql/column.html)   . However, some APIs such as  [SparkContext](api/python/reference/api/pyspark.SparkContext.html)   and  [RDD](api/python/reference/api/pyspark.RDD.html)   are not supported. You can check which APIs are currently supported in the  [API reference](api/python/reference/index.html)   documentation. Supported APIs are labeled “Supports Spark Connect” so you can check whether the APIs you are using are available before migrating existing code to Spark Connect.    **Scala**   : In Spark 3.4, Spark Connect supports most Scala APIs, including  [Dataset](api/scala/org/apache/spark/sql/Dataset.html)   ,  [functions](api/scala/org/apache/spark/sql/functions$.html)   , and  [Column](api/scala/org/apache/spark/sql/Column.html)   .     Support for more APIs is planned for upcoming Spark releases.    "	🤖 Spark Connect is a decoupled client-server architecture that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. It simplifies Spark application development and enables interactive debugging during development directly from your favorite IDE. It supports PySpark and Scala applications.	0
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat.html']	how do I concatenate string columns in pyspark?	"   pyspark.sql.functions.concat  [¶](#pyspark-sql-functions-concat ""Permalink to this headline"") ===============================================================================================  `pyspark.sql.functions.`  `concat`    (   *\*     cols     :     ColumnOrName*   )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#concat) [¶](#pyspark.sql.functions.concat ""Permalink to this definition"")    Concatenates multiple input columns together into a single column. The function works with strings, numeric, binary and compatible array columns.     New in version 1.5.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **cols**   [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")  or str     target column or columns to work on.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")   concatenated values. Type of the    Column    depends on input columns’ type.     See also    [`pyspark.sql.functions.array\_join()`](pyspark.sql.functions.array_join.html#pyspark.sql.functions.array_join ""pyspark.sql.functions.array_join"")   to concatenate string columns with delimiter     Examples    ``` >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd']) >>> df = df.select(concat(df.s, df.d).alias('s')) >>> df.collect() [Row(s='abcd123')] >>> df DataFrame[s: string]  ```  ``` >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c']) >>> df = df.select(concat(df.a, df.b, df.c).alias(""arr"")) >>> df.collect() [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)] >>> df DataFrame[arr: array<bigint>]  ```  [pyspark.sql.functions.slice](pyspark.sql.functions.slice.html ""previous page"")  [pyspark.sql.functions.array\_position](pyspark.sql.functions.array_position.html ""next page"")   "	To concatenate string columns in PySpark, you can use the `concat` function from the `pyspark.sql.functions` module. The function takes in multiple input columns and concatenates them into a single column. Here is an example:  ``` from pyspark.sql.functions import concat  df = spark.createDataFrame([('hello', 'world')], ['col1', 'col2']) df = df.select(concat(df.col1, df.col2).alias('concatenated')) df.show() ```  This will output a DataFrame with a new column named `concatenated` that contains the concatenated values of `col1` and `col2`.	1
no_reaction	['https://docs.databricks.com/structured-streaming/delta-lake.html']	How can I do multiple writes to the same delta table in streaming?	"    Table streaming reads and writes ==================================     Delta Lake is deeply integrated with  [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)   through  `readStream`  and  `writeStream`  . Delta Lake overcomes many of the limitations typically associated with streaming systems and files, including:     * Coalescing small files produced by low latency ingest * Maintaining “exactly-once” processing with more than one stream (or concurrent batch jobs) * Efficiently discovering which files are new when using files as the source for a stream        Delta table as a source -------------------------     When you load a Delta table as a stream source and use it in a streaming query, the query processes all of the data present in the table as well as any new data that arrives after the stream is started.       You can load both paths and tables as a stream.        ``` spark.readStream.format(""delta"")  .load(""/tmp/delta/events"")  import io.delta.implicits.\_ spark.readStream.delta(""/tmp/delta/events"")  ```      or        ``` import io.delta.implicits.\_  spark.readStream.format(""delta"").table(""events"")  ```        Limit input rate ------------------     The following options are available to control micro-batches:     * `maxFilesPerTrigger`  : How many new files to be considered in every micro-batch. The default is 1000. * `maxBytesPerTrigger`  : How much data gets processed in each micro-batch. This option sets a “soft max”, meaning that a batch processes approximately this amount of data and may process more than the limit in order to make the streaming query move forward in cases when the smallest input unit is larger than this limit. If you use  `Trigger.Once`  for your streaming, this option is ignored. This is not set by default.     If you use  `maxBytesPerTrigger`  in conjunction with  `maxFilesPerTrigger`  , the micro-batch processes data until either the  `maxFilesPerTrigger`  or  `maxBytesPerTrigger`  limit is reached.        Note       In cases when the source table transactions are cleaned up due to the  `logRetentionDuration` [configuration](../delta/history.html#data-retention)  and the stream lags in processing, Delta Lake processes the data corresponding to the latest available transaction history of the source table but does not fail the stream. This can result in data being dropped.            Stream a Delta Lake change data capture (CDC) feed ----------------------------------------------------     Delta Lake  [change data feed](../delta/delta-change-data-feed.html)  records changes to a Delta table, including updates and deletes. When enabled, you can stream from a change data feed and write logic to process inserts, updates, and deletes into downstream tables. Although change data feed data output differs slightly from the Delta table it describes, this provides a solution for propagating incremental changes to downstream tables in a  [medallion architecture](../lakehouse/medallion.html)  .           Ignore updates and deletes ----------------------------     Structured Streaming does not handle input that is not an append and throws an exception if any modifications occur on the table being used as a source. There are two main strategies for dealing with changes that cannot be automatically propagated downstream:     * You can delete the output and checkpoint and restart the stream from the beginning. * You can set either of these two options:      + `ignoreDeletes`   : ignore transactions that delete data at partition boundaries.  + `skipChangeCommits`   : ignore transactions that delete or modify existing records.   `skipChangeCommits`   subsumes   `ignoreDeletes`   .      Note       In Databricks Runtime 12.1 and above,  `skipChangeCommits`  deprecates the previous setting  `ignoreChanges`  . In Databricks Runtime 12.0 and lower,  `ignoreChanges`  is the only supported option.       The semantics for  `ignoreChanges`  differ greatly from  `skipChangeCommits`  . With  `ignoreChanges`  enabled, rewritten data files in the source table are re-emitted after a data changing operation such as  `UPDATE`  ,  `MERGE     INTO`  ,  `DELETE`  (within partitions), or  `OVERWRITE`  . Unchanged rows are often emitted alongside new rows, so downstream consumers must be able to handle duplicates. Deletes are not propagated downstream.  `ignoreChanges`  subsumes  `ignoreDeletes`  .      `skipChangeCommits`  disregards file changing operations entirely. Data files that are rewritten in the source table due to data changing operation such as  `UPDATE`  ,  `MERGE     INTO`  ,  `DELETE`  , and  `OVERWRITE`  are ignored entirely. In order to reflect changes in upstream source tables, you must implement separate logic to propagate these changes.       ###   Example     For example, suppose you have a table  `user\_events`  with  `date`  ,  `user\_email`  , and  `action`  columns that is partitioned by  `date`  . You stream out of the  `user\_events`  table and you need to delete data from it due to GDPR.       When you delete at partition boundaries (that is, the  `WHERE`  is on a partition column), the files are already segmented by value so the delete just drops those files from the metadata. Thus, if you just want to delete data from some partitions, you can use:        ``` spark.readStream.format(""delta"")  .option(""ignoreDeletes"", ""true"")  .load(""/tmp/delta/user\_events"")  ```      However, if you have to delete data based on  `user\_email`  , then you will need to use:        ``` spark.readStream.format(""delta"")  .option(""ignoreChanges"", ""true"")  .load(""/tmp/delta/user\_events"")  ```      If you update a  `user\_email`  with the  `UPDATE`  statement, the file containing the  `user\_email`  in question is rewritten. When you use  `ignoreChanges`  , the new record is propagated downstream with all other unchanged records that were in the same file. Your logic should be able to handle these incoming duplicate records.          Specify initial position --------------------------     You can use the following options to specify the starting point of the Delta Lake streaming source without processing the entire table.     * `startingVersion`  : The Delta Lake version to start from. All table changes starting from this version (inclusive) will be read by the streaming source. You can obtain the commit versions from the  `version`  column of the  [DESCRIBE HISTORY](../delta/history.html)  command output.       In Databricks Runtime 7.4 and above, to return only the latest changes, specify  `latest`  . * `startingTimestamp`  : The timestamp to start from. All table changes committed at or after the timestamp (inclusive) will be read by the streaming source. One of:      + A timestamp string. For example,   `""2019-01-01T00:00:00.000Z""`   .  + A date string. For example,   `""2019-01-01""`   .     You cannot set both options at the same time; you can use only one of them. They take effect only when starting a new streaming query. If a streaming query has started and the progress has been recorded in its checkpoint, these options are ignored.        Important       Although you can start the streaming source from a specified version or timestamp, the schema of the streaming source is always the latest schema of the Delta table. You must ensure there is no incompatible schema change to the Delta table after the specified version or timestamp. Otherwise, the streaming source may return incorrect results when reading the data with an incorrect schema.         ###   Example     For example, suppose you have a table  `user\_events`  . If you want to read changes since version 5, use:        ``` spark.readStream.format(""delta"")  .option(""startingVersion"", ""5"")  .load(""/tmp/delta/user\_events"")  ```      If you want to read changes since 2018-10-18, use:        ``` spark.readStream.format(""delta"")  .option(""startingTimestamp"", ""2018-10-18"")  .load(""/tmp/delta/user\_events"")  ```         Process initial snapshot without data being dropped -----------------------------------------------------      Note       This feature is available on Databricks Runtime 11.1 and above. This feature is in  [Public Preview](../release-notes/release-types.html)  .        When using a Delta table as a stream source, the query first processes all of the data present in the table. The Delta table at this version is called the initial snapshot. By default, the Delta table’s data files are processed based on which file was last modified. However, the last modification time does not necessarily represent the record event time order.       In a stateful streaming query with a defined watermark, processing files by modification time can result in records being processed in the wrong order. This could lead to records dropping as late events by the watermark.       You can avoid the data drop issue by enabling the following option:     * withEventTimeOrder: Whether the initial snapshot should be processed with event time order.     With event time order enabled, the event time range of initial snapshot data is divided into time buckets. Each micro batch processes a bucket by filtering data within the time range. The maxFilesPerTrigger and maxBytesPerTrigger configuration options are still applicable to control the microbatch size but only in an approximate way due to the nature of the processing.       The graphic below shows this process:      ![Initial Snapshot](../_images/delta-initial-snapshot-data-drop.png)    Notable information about this feature:     * The data drop issue only happens when the initial Delta snapshot of a stateful streaming query is processed in the default order. * You cannot change  `withEventTimeOrder`  once the stream query is started while the initial snapshot is still being processed. To restart with  `withEventTimeOrder`  changed, you need to delete the checkpoint. * If you are running a stream query with withEventTimeOrder enabled, you cannot downgrade it to a DBR version which doesn’t support this feature until the initial snapshot processing is completed. If you need to downgrade, you can wait for the initial snapshot to finish, or delete the checkpoint and restart the query. * This feature is not supported in the following uncommon scenarios:      + The event time column is a generated column and there are non-projection transformations between the Delta source and watermark.  + There is a watermark that has more than one Delta source in the stream query. * With event time order enabled, the performance of the Delta initial snapshot processing might be slower. * Each micro batch scans the initial snapshot to filter data within the corresponding event time range. For faster filter action, it is advised to use a Delta source column as the event time so that data skipping can be applied (check  [Data skipping with Z-order indexes for Delta Lake](../delta/data-skipping.html)  for when it’s applicable). Additionally, table partitioning along the event time column can further speed the processing. You can check Spark UI to see how many delta files are scanned for a specific micro batch.      ###   Example     Suppose you have a table  `user\_events`  with an  `event\_time`  column. Your streaming query is an aggregation query. If you want to ensure no data drop during the initial snapshot processing, you can use:        ``` spark.readStream.format(""delta"")  .option(""withEventTimeOrder"", ""true"")  .load(""/tmp/delta/user\_events"")  .withWatermark(""event\_time"", ""10 seconds"")  ```       Note       You can also enable this with Spark config on the cluster which will apply to all streaming queries:  `spark.databricks.delta.withEventTimeOrder.enabled     true`            Delta table as a sink -----------------------     You can also write data into a Delta table using Structured Streaming. The transaction log enables Delta Lake to guarantee exactly"	To do multiple writes to the same Delta table in streaming, you can use the `foreachBatch` method in Spark Structured Streaming. This method allows you to apply any arbitrary operation on the output of a streaming query, including writing to a Delta table. You can use the `foreachBatch` method to write the output of a streaming query to a Delta table in batches, which can improve performance and reduce the number of small files produced by low latency ingest.	1
no_reaction	['https://docs.databricks.com/files/download-internet-files.html']	how to download a csv file from the dbfs to a local machine?	"      CSV file ==========     This article provides examples for reading and writing to CSV files with Databricks using Python, Scala, R, and SQL.        Note       You can use SQL to read CSV data directly or by using a temporary view. Databricks recommends using a temporary view. Reading the CSV file directly has the following drawbacks:     * You can’t  [specify data source options](#options)  . * You can’t  [specify the schema](#specify-schema)  for the data.     See  [Examples](#examples)  .         Options ---------     You can configure several options for CSV file data sources. See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#csv(path:String):Unit) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html#pyspark.sql.DataFrameWriter.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#csv(path:String):Unit)         Work with malformed CSV records ---------------------------------     When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:     * `PERMISSIVE`  (default): nulls are inserted for fields that could not be parsed correctly * `DROPMALFORMED`  : drops lines that contain fields that could not be parsed * `FAILFAST`  : aborts the reading if any malformed data is found     To set the mode, use the  `mode`  option.        ``` diamonds\_df = (spark.read   .format(""csv"")   .option(""mode"", ""PERMISSIVE"")   .load(""/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv"") )  ```      In the  `PERMISSIVE`  mode it is possible to inspect the rows that could not be parsed correctly using one of the following methods:     * You can provide a custom path to the option  `badRecordsPath`  to record corrupt records to a file. * You can add the column  `\_corrupt\_record`  to the schema provided to the DataFrameReader to review corrupt records in the resultant DataFrame.      Note       The  `badRecordsPath`  option takes precedence over  `\_corrupt\_record`  , meaning that malformed rows written to the provided path do not appear in the resultant DataFrame.       Default behavior for malformed records changes when using the  [rescued data column](#rescued-data)  .         ###   Find malformed rows notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-corrupt-record.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                    Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.3 (Unsupported)](../release-notes/runtime/8.3.html)  and above.        When using the  `PERMISSIVE`  mode, you can enable the rescued data column to capture any data that wasn’t parsed because one or more fields in a record have one of the following issues:     * Absent from the provided schema. * Does not match the data type of the provided schema. * Has a case mismatch with the field names in the provided schema.     The rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set(""spark.databricks.sql.rescuedDataColumn.filePath.enabled"",     ""false"")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name when reading data, such as  `\_rescued\_data`  with  `spark.read.option(""rescuedDataColumn"",     ""\_rescued\_data"").format(""csv"").load(<path>)`  .       The CSV parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors.       When  `rescuedDataColumn`  is used in  `PERMISSIVE`  mode, the following rules apply to  [corrupt records](#corrupt-records)  :     * The first row of the file (either a header row or a data row) sets the expected row length. * A row with a different number of columns is considered incomplete. * Data type mismatches are not considered corrupt records. * Only incomplete and malformed CSV records are considered corrupt and recorded to the  `\_corrupt\_record`  column or  `badRecordsPath`  .       Examples ----------     These examples use the  [diamonds dataset](../dbfs/databricks-datasets.html)  . Specify the path to the dataset as well as any options that you would like.        In this section:     * [Read file in any language](#read-file-in-any-language) * [Specify schema](#specify-schema) * [Pitfalls of reading a subset of columns](#pitfalls-of-reading-a-subset-of-columns)     ###  [Read file in any language](#id1)     This notebook shows how to read a file, display sample data, and print the data schema using Scala, R, Python, and SQL.        ####   Read CSV files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Specify schema](#id2)     When the schema of the CSV file is known, you can specify the desired schema to the CSV reader with the  `schema`  option.        ####   Read CSV files with schema notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-schema.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Pitfalls of reading a subset of columns](#id3)     The behavior of the CSV parser depends on the set of columns that are read. If the specified schema is incorrect, the results might differ considerably depending on the subset of columns that is accessed. The following notebook presents the most common pitfalls.        ####   Caveats of reading a subset of columns of a CSV file notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-column-subset.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                  "	To download a CSV file from the DBFS to a local machine, you can use the Databricks CLI command `databricks fs cp` followed by the path of the file in the DBFS and the path of the destination on the local machine. For example, to download a file named `example.csv` from the DBFS to the current directory on the local machine, you can run the command `databricks fs cp dbfs:/path/to/example.csv ./example.csv`.	0
no_reaction	['https://docs.databricks.com/dev-tools/python-api.html']	Write me python code to start a databricks cluster using the Databricks REST API 	"              Clusters API 2.0 ==================     The Clusters API allows you to create, start, edit, list, terminate, and delete clusters. The maximum allowed size of a request to the Clusters API is 10MB.       Cluster lifecycle methods require a cluster ID, which is returned from  [Create](#clusterclusterservicecreatecluster)  . To obtain a list of clusters, invoke  [List](#clusterclusterservicelistclusters)  .       Databricks maps cluster node instance types to compute units known as DBUs. See the  [instance type pricing page](https://databricks.com/product/aws-pricing/instance-types)   for a list of the supported instance types and their corresponding DBUs. For instance provider information, see  [AWS instance type specifications](https://aws.amazon.com/ec2/instance-types/)   and  [pricing](https://aws.amazon.com/ec2/pricing/)   .       Databricks always provides one year’s deprecation notice before ceasing support for an instance type.        Warning       You should never hard code secrets or store them in plain text. Use the  [Secrets API 2.0](secrets.html)  to manage secrets in the  [Databricks CLI setup & documentation](../../cli/index.html)  . Use the  [Secrets utility (dbutils.secrets)](../../databricks-utils.html#dbutils-secrets)  to reference secrets in notebooks and jobs.         Important       To access Databricks REST APIs, you must  [authenticate](authentication.html)  .           Create --------        |   Endpoint   |   HTTP Method   | | --- | --- | |  `2.0/clusters/create`  |  `POST`  |     Create a new Apache Spark cluster. This method acquires new instances from the cloud provider if necessary. This method is asynchronous; the returned  `cluster\_id`  can be used to poll the cluster state. When this method returns, the cluster is in a  `PENDING`  state. The cluster is usable once it enters a  `RUNNING`  state. See  [ClusterState](#clusterclusterstate)  .        Note       Databricks may not be able to acquire some of the requested nodes, due to cloud provider limitations or transient network issues. If Databricks acquires at least 85% of the requested on-demand nodes, cluster creation will succeed. Otherwise the cluster will terminate with an informative error message.           ###   Examples      ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""my-cluster"",  ""spark\_version"": ""7.3.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""spark\_conf"": {  ""spark.speculation"": true  },  ""aws\_attributes"": {  ""availability"": ""SPOT"",  ""zone\_id"": ""us-west-2a""  },  ""num\_workers"": 25 }  ```       ``` { ""cluster\_id"": ""1234-567890-cited123"" }  ```      Here is an example for an autoscaling cluster. This cluster will start with two nodes, the minimum.        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""autoscaling-cluster"",  ""spark\_version"": ""7.3.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""autoscale"" : {  ""min\_workers"": 2,  ""max\_workers"": 50  } }  ```       ``` { ""cluster\_id"": ""1234-567890-batch123"" }  ```          This example creates a  [Single Node cluster](../../../clusters/single-node.html)  . To create a Single Node cluster:     * Set  `spark\_conf`  and  `custom\_tags`  to the exact values in the example. * Set  `num\_workers`  to  `0`  .      ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""single-node-cluster"",  ""spark\_version"": ""7.6.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""num\_workers"": 0,  ""spark\_conf"": {  ""spark.databricks.cluster.profile"": ""singleNode"",  ""spark.master"": ""[\*, 4]""  },  ""custom\_tags"": {  ""ResourceClass"": ""SingleNode""  } }  ```       ``` { ""cluster\_id"": ""1234-567890-ruins123"" }  ```          This example creates a cluster and mounts an Amazon EFS file system.        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""cluster\_name"": ""efs-cluster"",  ""spark\_version"": ""7.6.x-scala2.12"",  ""node\_type\_id"": ""i3.xlarge"",  ""instance\_type"": ""i3.xlarge"",  ""cluster\_mount\_infos"":[  {  ""network\_filesystem\_info"":{  ""server\_address"":""hostname.efs.us-east-1.amazonaws.com"",  ""mount\_options"": ""rsize=1048576,wsize=1048576,hard,timeo=600""  },  ""remote\_mount\_dir\_path"": ""/"",  ""local\_mount\_dir\_path"": ""/mnt/efs-mount""  }  ],  ""aws\_attributes"":{  ""availability"": ""SPOT"",  ""zone\_id"": ""us-east-2""  },  ""num\_workers"": 25 }  ```       ``` { ""cluster\_id"": ""1234-567890-pouch123"" }  ```      To create a job or submit a run with a new cluster using a policy and the policy’s default values, set  `policy\_id`  to the policy ID and  `apply\_policy\_default\_values`  to  `true`  :        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/clusters/create \ --data @create-cluster.json  ```     `create-cluster.json`  :        ``` {  ""num\_workers"": null,  ""autoscale"": {  ""min\_workers"": 2,  ""max\_workers"": 8  },  ""cluster\_name"": ""my-cluster"",  ""spark\_version"": ""7.3.x-scala2.12"",  ""spark\_conf"": {},  ""aws\_attributes"": {  ""first\_on\_demand"": 1,  ""availability"": ""SPOT\_WITH\_FALLBACK"",  ""zone\_id"": ""us-west-2a"",  ""instance\_profile\_arn"": null,  ""spot\_bid\_price\_percent"": 100,  ""ebs\_volume\_count"": 0  },  ""node\_type\_id"": ""i3.xlarge"",  ""ssh\_public\_keys"": [],  ""custom\_tags"": {},  ""spark\_env\_vars"": {  ""PYSPARK\_PYTHON"": ""/databricks/python3/bin/python3""  },  ""autotermination\_minutes"": 120,  ""init\_scripts"": [],  ""policy\_id"": ""C65B864F02000008"",  ""apply\_policy\_default\_values"": true }  ```       ``` { ""cluster\_id"": ""1234-567890-buyer123"" }  ```      To create a new cluster, define the cluster’s properties in  `new\_cluster`  :        ``` curl --netrc -X POST \ https://dbc-a1b2345c-d6e7.cloud.databricks.com/api/2.0/jobs/create \ --data @create-job.json  ```     `create-job.json`  :        ``` {  ""run\_name"": ""my spark task"",  ""new\_cluster"": {  ""spark\_version"": ""7.3.x-scala2.12"",  ""node\_type\_id"": ""r3.xlarge"",  ""aws\_attributes"": {  ""availability"": ""ON\_DEMAND""  },  ""num\_workers"": 10,  ""policy\_id"": ""ABCD000000000000""  },  ""libraries"": [  {  ""jar"": ""dbfs:/my-jar.jar""  },  {  ""maven"": {  ""coordinates"": ""org.jsoup:jsoup:1.7.2""  }  }  ],  ""spark\_jar\_task"": {  ""main\_class\_name"": ""com.databricks.ComputeModels""  } }  ```       ``` { ""job\_id"": 244 }  ```            ###   Request structure of the cluster definition         |   Field Name   |   Type   |   Description   | | --- | --- | --- | |   num\_workers OR autoscale   |  `INT32`  OR  [AutoScale](#clusterautoscale)  |   If num\_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num\_workers executors for a total of num\_workers + 1 Spark nodes.    **Note:**   When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in  `executors`  will gradually increase from 5 to 10 as the new nodes are provisioned.     If autoscale, parameters needed in order to automatically scale clusters up and down based on load.   | |   cluster\_name   |  `STRING`  |   Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.   | |   spark\_version   |  `STRING"	"Here is the Python code to start a Databricks cluster using the Databricks REST API:  ``` import requests import json  # Set the API endpoint and token api_endpoint = ""https://<databricks-instance>/api/2.0/clusters/create"" api_token = ""<your-api-token>""  # Set the request headers headers = {     ""Authorization"": ""Bearer "" + api_token,     ""Content-Type"": ""application/json"" }  # Set the request body request_body = {     ""cluster_name"": ""my-cluster"",     ""spark_version"": ""7.3.x-scala2.12"",     ""node_type_id"": ""i3.xlarge"",     ""spark_conf"": {         ""spark.speculation"": True     },     ""aws_attributes"": {         ""availability"": ""SPOT"",         ""zone_id"": ""us-west-2a""     },     ""num_workers"": 25 }  # Send the request response = requests.post(api_endpoint, headers=headers, data=json.dumps(request_body))  # Print the response print(response.json()) ```  Note that you need to replace `<databricks-instance>` and `<your-api-token>` with your own values. Also, make sure to set the `request_body` variable according to your needs."	1
no_reaction	['https://docs.databricks.com/administration-guide/account-settings-e2/private-access-settings.html']	How to set up private link?	    Manage private access settings ================================         This article describes a process that is available only for  [accounts on the E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.       This article discusses how to create private access settings objects, which are a required object as part of enabling  [AWS PrivateLink](https://aws.amazon.com/privatelink)   . This article does not contain all the information necessary to configure PrivateLink for your workspace. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see  [Enable AWS PrivateLink](../cloud-configurations/aws/privatelink.html)  .       The following related sections discuss updating existing network and configuration objects:     * [Update a running or failed workspace](../workspace/update-workspace.html)  . * [Updates of existing PrivateLink configuration objects](../cloud-configurations/aws/privatelink.html#update-related)  .      What is a private access settings object? -------------------------------------------     A private access settings object is a Databricks object that describes a workspace’s PrivateLink connectivity. Create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region.       This object serves several purposes:     * It expresses your intent to use AWS PrivateLink with your workspace. * It controls your settings for the front-end use case of AWS PrivateLink for public network access. * It controls which VPC endpoints are permitted to access your workspace.     Create a private access settings object using the account console or the  [Account API 2.0](../../dev-tools/api/latest/account.html)  . You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you  *must*   attach a private access settings object to the workspace during workspace creation.           Create a private access settings object -----------------------------------------      Note       These instructions show you how to create the private access object from the  **Cloud resources**   page in the account console before you create a new workspace. You can also create the private access settings in a similar way as part of the flow of creating a new workspace and choosing  **Add a new private access object**   from the picker instead of choosing an existing object. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .      1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. Click  **Add private access settings**   .      ![private access settings object](../../_images/privatelink-vpc-pas.png) 5. Enter a name for your new private access settings object. 6. For the region, be sure to match the region of your workspace as this is not validated immediately and workspace deployment will fail if it does not match. It is validated only during the actual creation of the workspace. 7. Set the  **Public access enabled**   field, which configures public access to the front-end connection (the web application and REST APIs) for your workspace.      * If set to   **False**    (the default), the front-end connection can be accessed only using PrivateLink connectivity and not from the public internet. When public access is disabled, the   [IP access lists for workspaces](../../security/network/ip-access-list-workspace.html)   feature is unsupported.  * If set to   **True**    , the front-end connection can be accessed either from PrivateLink connectivity or from the public internet. Any IP access lists only limit connections from the public internet but not traffic through the PrivateLink connection. 8. Set the  **Private Access Level**   field to the value that best represents which VPC endpoints to allow for your workspace.      * Set to   **Account**    to limit connections to those VPC endpoints that are registered in your Databricks account.  * Set to   **Endpoint**    to limit connections to an explicit set of VPC endpoints, which you can enter in a field that appears. It lets you select VPC endpoint registrations that you’ve already created. Be sure to include your   *front-end*    VPC endpoint registration if you created one. 9. Click  **Add**   .         Update a private access settings object -----------------------------------------     To update fields on a private access object:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Update**   . 5. Change any fields. For guidance on specific fields, see  [Create a private access settings object](#create)  .        Note       The private access access level  `ANY`  is deprecated. If the object previously had this value and you use the account console to update the private access settings for any fields, you must change the private access level to a non-deprecated value. To make changes to other fields without changing the  `ANY`  private access level at this time, use the  [Account API](../../dev-tools/api/latest/account.html)  . See  [AWS PrivateLink private access level ANY is deprecated](../../release-notes/product/2022/august.html#privatelink-private-access-level-any-deprecated)  . 6. Click  **Update private access setting**   .         Delete a private access settings object -----------------------------------------     Private access settings object cannot be edited after creation. If the configuration has incorrect data or if you no longer need it for any workspaces, delete it:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Delete**   . 5. In the confirmation dialog, click  **Confirm Delete**   .                                                      	To set up private link, you need to create a private access settings object, which is a required object as part of enabling AWS PrivateLink. This object controls your settings for the front-end use case of AWS PrivateLink for public network access and which VPC endpoints are permitted to access your workspace. You can create a private access settings object using the account console or the Account API 2.0.	1
thumb_down	['https://docs.databricks.com/release-notes/cluster-images/1.6.3-db1.html']	What version of Python does DBR 12.1 use?	    1.6.3-db1 cluster image =========================     Databricks released this update in November, 2016.        Important       This release has been deprecated. For more information about the Databricks Runtime deprecation policy and schedule, see  [Databricks runtime support lifecycles](../runtime/databricks-runtime-ver.html#runtime-support)  .        The following release notes provide information about the Spark 1.6.3-db1 cluster image powered by Apache Spark.        Apache Spark --------------     In this release, Spark refers to Apache Spark 1.6.3. For more information, please see  [Apache Spark 1.6.3 release notes](https://spark.apache.org/releases/spark-release-1-6-3.html)   .         Changes and Improvements --------------------------   * Operating system upgraded to Ubuntu 16.04.1 LTS from Ubuntu 15.10. * Python upgraded to 2.7.12 from 2.7.10. * Most pre-installed Python libraries upgraded. For the list of Python libraries and their versions, see  [Pre-installed Python Libraries](#spark-1-6-3-db1-python-libs)  . * Fixed issues around unicode characters in Python notebooks * For Hadoop 2 version, Apache Hadoop upgraded to 2.7.3 from 2.7.2. * For Hadoop 2 version, the default value of  `mapreduce.fileoutputcommitter.algorithm.version`  changed to 2 from 1.       System Environment --------------------   * **Operating System**   : Ubuntu 16.04.1 LTS * **Java**   : 1.8.0\_91 * **Scala**   : 2.10.6 * **Python**   : 2.7.12 * **R**   : R version 3.2.3 (2015-12-10)      ###   Pre-installed Python Libraries            |   Library   |   Version   |   Library   |   Version   |   Library   |   Version   | | --- | --- | --- | --- | --- | --- | |   ansi2html   |   1.1.1   |   argparse   |   1.2.1   |   boto   |   2.42.0   | |   boto3   |   1.4.1   |   botocore   |   1.4.70   |   brewer2mpl   |   1.4.1   | |   certifi   |   2016.2.28   |   cffi   |   1.7.0   |   chardet   |   2.3.0   | |   colorama   |   0.3.7   |   configobj   |   5.0.6   |   cryptography   |   1.5   | |   cycler   |   0.10.0   |   Cython   |   0.24.1   |   decorator   |   4.0.10   | |   docutils   |   0.12   |   enum34   |   1.1.6   |   et-xmlfile   |   1.0.1   | |   freetype-py   |   1.0.2   |   funcsigs   |   1.0.2   |   fusepy   |   2.0.4   | |   futures   |   3.0.5   |   ggplot   |   0.6.8   |   html5lib   |   0.999   | |   idna   |   2.1   |   ipaddress   |   1.0.16   |   ipython   |   2.2.0   | |   ipython-genutils   |   0.1.0   |   jdcal   |   1.2   |   Jinja2   |   2.8   | |   jmespath   |   0.9.0   |   llvmlite   |   0.13.0   |   lxml   |   3.6.4   | |   MarkupSafe   |   0.23   |   matplotlib   |   1.5.3   |   mpld3   |   0.2   | |   msgpack-python   |   0.4.7   |   ndg-httpsclient   |   0.3.3   |   numba   |   0.28.1   | |   numpy   |   1.11.1   |   openpyxl   |   2.3.2   |   pandas   |   0.18.1   | |   pathlib2   |   2.1.0   |   patsy   |   0.4.1   |   pexpect   |   4.0.1   | |   pickleshare   |   0.7.4   |   Pillow   |   3.3.1   |   pip   |   8.1.2   | |   pkg\_resources   |   0.0.0   |   ply   |   3.9   |   prompt-toolkit   |   1.0.7   | |   psycopg2   |   2.6.2   |   ptyprocess   |   0.5.1   |   py4j   |   0.10.3   | |   pyasn1   |   0.1.9   |   pycparser   |   2.14   |   Pygments   |   2.1.3   | |   PyGObject   |   3.20.0   |   pyOpenSSL   |   16.0.0   |   pyparsing   |   2.1.4   | |   pypng   |   0.0.18   |   Python   |   2.7.12   |   python-dateutil   |   2.5.3   | |   python-geohash   |   0.8.5   |   pytz   |   2016.6.1   |   requests   |   2.11.1   | |   s3transfer   |   0.1.9   |   scikit-learn   |   0.17.1   |   scipy   |   0.18.1   | |   scour   |   0.32   |   seaborn   |   0.7.1   |   setuptools   |   28.6.0   | |   simplejson   |   3.8.2   |   simples3   |   1.0   |   singledispatch   |   3.4.0.3   | |   six   |   1.10.0   |   statsmodels   |   0.6.1   |   traitlets   |   4.3.0   | |   urllib3   |   1.19.1   |   virtualenv   |   15.0.1   |   wcwidth   |   0.1.7   | |   wheel   |   0.30.0a0   |   wsgiref   |   0.1.2   |  |  |       ###   Pre-installed R Libraries            |   Library   |   Version   |   Library   |   Version   |   Library   |   Version   | | --- | --- | --- | --- | --- | --- | |   abind   |   1.4-3   |   assertthat   |   0.1   |   base   |   3.2.3   | |   BH   |   1.60.0-2   |   bitops   |   1.0-6   |   boot   |   1.3-17   | |   brew   |   1.0-6   |   car   |   2.1-3   |   caret   |   6.0-73   | |   chron   |   2.3-47   |   class   |   7.3-14   |   cluster   |   2.0.5   | |   codetools   |   0.2-14   |   colorspace   |   1.2-4   |   compiler   |   3.2.3   | |   crayon   |   1.3.1   |   curl   |   2.2   |   data.table   |   1.9.6   | |   datasets   |   3.2.3   |   DBI   |   0.5-1   |   devtools   |   1.12.0   | |   dichromat   |   2.0-0   |   digest   |   0.6.9   |   doMC   |   1.3.4   | |   dplyr   |   0.5.0   |   foreach   |   1.4.3   |   foreign   |   0.8-66   | |   gbm   |   2.1.1   |   ggplot2   |   2.2.0   |   git2r   |   0.15.0   | |   glmnet   |   2.0-5   |   graphics   |   3.2.3   |   grDevices   |   3.2.3   | |   grid   |   3.2.3   |   gsubfn   |   0.6-6   |   gtable   |   0.1.2   | |   h2o   |   3.10.0.8   |   httr   |   1.2.1   |   hwriter   |   1.3.2   | |   hwriterPlus   |   1.0-3   |   iterators   |   1.0.8   |   jsonlite   |   1.1   | |   KernSmooth   |   2.23-15   |   labeling   |   0.3   |   lattice   |   0.20-34   | |   lazyeval   |   0.2.0   |   littler   |   0.3.0   |   lme4   |   1.1-12   | |   lubridate   |   1.6.0   |   magrittr   |   1.5 	DBR 12.1 is not mentioned in the provided context.	0
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/index.html']	how do I implement unity catalog	    What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	To implement Unity Catalog, you need to follow these steps:  1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. Create and grant access to catalogs, schemas, and tables.  For complete setup instructions, please refer to the documentation.	1
no_reaction	['https://docs.databricks.com/notebooks/notebooks-dlt-pipeline.html']	write a dlt notebook with apply_changes	"   `table\_changes`  table-valued function ========================================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime       Returns a log of changes to a Delta Lake table with Change Data Feed enabled.       To invoke this function you need to have at least one of the following:     * `SELECT`  privilege on the specified table * Be the owner of the table * Have administrative privileges      Syntax --------      ``` table\_changes ( table\_str, start [, end ] )  ```        Arguments -----------   * `table\_str`  : A STRING literal representing the optionally qualified name of the table. * `start`  : A BIGINT or TIMESTAMP literal, representing the first version or timestamp of change to return. * `end`  : An optional BIGINT or TIMESTAMP literal, representing the last version or timestamp of change to return. If not specified all changes from  `start`  up to the current change are returned.       Returns ---------     A table including all columns of the table identified in  `table\_str`  , plus the following columns:     * `\_change\_type     STRING     NOT     NULL`      Specifies the change:  `delete`  ,  `insert`  ,  `update\_preimage`  , or  `update\_postimage` * `\_commit\_version     BIGINT     NOT     NULL`      Specifies the commit version of the table associated with the change. * `\_commit\_timestamp     TIMESTAMP     NOT     NULL`      Specifies the commit timestamp associated with the change.     If  `table\_str`  does not represent a qualified table name the name is qualified with the value of  `current\_schema`  . If the table name contains spaces or dots use back quotes within the string to quote that part of the name.         Examples ----------      ``` -- Create a Delta table with Change Data Feed; > CREATE TABLE myschema.t(c1 INT, c2 STRING) TBLPROPERTIES(delta.enableChangeDataFeed=true);  -- Modify the table > INSERT INTO myschema.t VALUES (1, 'Hello'), (2, 'World'); > INSERT INTO myschema.t VALUES (3, '!'); > UPDATE myschema.t SET c2 = upper(c2) WHERE c1 < 3; > DELETE FROM myschema.t WHERE c1 = 3;  -- Show the history of table change events > DESCRIBE HISTORY myschema.t;  version timestamp                    userId           userName      operation    operationParameters                                            ...        4 2022-09-01T18:32:35.000+0000 6167625779053302 alf@melmak.et DELETE       {""predicate"":""[\""(spark_catalog.myschema.t.c1 = 3)\""]""}        3 2022-09-01T18:32:32.000+0000 6167625779053302 alf@melmak.et UPDATE       {""predicate"":""(c1#3195878 < 3)""}        2 2022-09-01T18:32:28.000+0000 6167625779053302 alf@melmak.et WRITE        {""mode"":""Append"",""partitionBy"":""[]""}        1 2022-09-01T18:32:26.000+0000 6167625779053302 alf@melmak.et WRITE        {""mode"":""Append"",""partitionBy"":""[]""}        0 2022-09-01T18:32:23.000+0000 6167625779053302 alf@melmak.et CREATE TABLE {""isManaged"":""true"",""description"":null,""partitionBy"":""[]"",""properties"":""{\""delta.enableChangeDataFeed\"":\""true\""}""}  -- Show the change table feed using a the commit timestamp retrieved from the history. > SELECT * FROM table_changes('`myschema`.`t`', 2);  c1 c2     _change_type    _commit_version _commit_timestamp   3 !      insert                        2 2022-09-01T18:32:28.000+0000   2 WORLD  update_postimage              3 2022-09-01T18:32:32.000+0000   2 World  update_preimage               3 2022-09-01T18:32:32.000+0000   1 Hello  update_preimage               3 2022-09-01T18:32:32.000+0000   1 HELLO  update_postimage              3 2022-09-01T18:32:32.000+0000   3 !      delete                        4 2022-09-01T18:32:35.000+0000  -- Show the ame change table feed using a point in time. > SELECT * FROM table_changes('`myschema`.`t`', '2022-09-01T18:32:27.000+0000') ORDER BY _commit_version;  c1 c2     _change_type    _commit_version _commit_timestamp   3 !      insert                        2 2022-09-01T18:32:28.000+0000   2 WORLD  update_postimage              3 2022-09-01T18:32:32.000+0000   2 World  update_preimage               3 2022-09-01T18:32:32.000+0000   1 Hello  update_preimage               3 2022-09-01T18:32:32.000+0000   1 HELLO  update_postimage              3 2022-09-01T18:32:32.000+0000   3 !      delete                        4 2022-09-01T18:32:35.000+0000  ```        Related functions -------------------   * [Change Data Feed](../../../delta/delta-change-data-feed.html) * [DESCRIBE HISTORY](../delta-describe-history.html)                                                      "	The `table_changes` function is used to return a log of changes to a Delta Lake table with Change Data Feed enabled. It returns a table including all columns of the table identified in `table_str`, plus the following columns: `_change_type`, `_commit_version`, and `_commit_timestamp`. To invoke this function, you need to have at least one of the following: `SELECT` privilege on the specified table, be the owner of the table, or have administrative privileges. The syntax for the function is `table_changes(table_str, start [, end])`.	0
no_reaction	['https://kb.databricks.com/clusters/use-audit-logs-to-identify-who-deleted-a-cluster']	how can i delete a cluster	    Manage clusters =================     This article describes how to manage Databricks clusters, including displaying, editing, starting, terminating, deleting, controlling access, and monitoring performance and logs.          Display clusters ------------------     To view the clusters in your workspace, click  ![compute icon](../_images/clusters-icon.png) **Compute**   in the sidebar.       At the left side are two columns indicating if the cluster has been pinned and the status of the cluster. Hover over the status to get more information.           Pin a cluster ---------------     30 days after a cluster is terminated, it is permanently deleted. To keep an all-purpose cluster configuration even after a cluster has been  [terminated](#cluster-terminate)  for more than 30 days, an administrator can pin the cluster. Up to 100 clusters can be pinned.       Admins can pin a cluster from the cluster list or the cluster detail page by clicking the pin icon.       You can also invoke the  [Pin](../dev-tools/api/latest/clusters.html#pin)  API endpoint to programmatically pin a cluster.           View a cluster configuration as a JSON file ---------------------------------------------     Sometimes it can be helpful to view your cluster configuration as JSON. This is especially useful when you want to create similar clusters using the  [Clusters API 2.0](../dev-tools/api/latest/clusters.html)  . When you view an existing cluster, simply go to the  **Configuration**   tab, click  **JSON**   in the top right of the tab, copy the JSON, and paste it into your API call. JSON view is ready-only.           Edit a cluster ----------------     You can edit a cluster configuration from the cluster details UI. You can also invoke the  [Edit](../dev-tools/api/latest/clusters.html#clusterclusterserviceeditcluster)  API endpoint to programmatically edit the cluster.        Note     * Notebooks and jobs that were attached to the cluster remain attached after editing. * Libraries installed on the cluster remain installed after editing. * If you edit any attribute of a running cluster (except for the cluster size and permissions), you must restart it. This can disrupt users who are currently using the cluster. * You can edit only running or terminated clusters. You can, however, update  *permissions*   for clusters that are not in those states on the cluster details page.          Clone a cluster -----------------     To clone an existing cluster, select  **Clone**   from the cluster’s  ![Kebab menu](../_images/kebab-menu.png)  kebab menu (also known as the three-dot menu).       After you select clone, the cluster creation UI opens pre-populated with the cluster configuration. The following attributes are not included in the clone:     * Cluster permissions * Installed libraries * Attached notebooks         Control access to clusters ----------------------------     Cluster access control within the  [admin console](../administration-guide/access-control/cluster-acl.html)  allows admins to give fine-grained cluster access to other users. There are two types of cluster access control:     * Cluster-creation permission: Admins can choose which users are allowed to create clusters. * Cluster-level permissions: A user who has the  **Can manage**   permission for a cluster can configure whether other users can attach to, restart, resize, and manage that cluster.     To edit permissions for a cluster, select  **Edit Permissions**   from the cluster’s  ![Kebab menu](../_images/kebab-menu.png)  kebab menu.       For more on cluster access control and cluster-level permissions, see  [Cluster access control](../security/auth-authz/access-control/cluster-acl.html)  .           Terminate a cluster ---------------------     To save cluster resources, you can terminate a cluster. The terminated cluster’s configuration is stored so that it can be  [reused](#cluster-start)  (or, in the case of jobs,  [autostarted](#autostart-clusters)  ) at a later time. You can manually terminate a cluster or configure the cluster to automatically terminate after a specified period of inactivity. When the number of terminated clusters exceeds 150, the oldest clusters are deleted.       Unless a cluster is  [pinned](#pin-a-cluster)  or restarted, it is automatically and permanently deleted 30 days after termination.       Terminated clusters appear in the cluster list with a gray circle at the left of the cluster name.        Note       When you run a  [job](../workflows/jobs/jobs.html)  on a  *New Job Cluster*   (which is usually recommended), the cluster terminates and is unavailable for restarting when the job is complete. On the other hand, if you schedule a job to run on an  *Existing All-Purpose Cluster*   that has been terminated, that cluster will  [autostart](#autostart-clusters)  .         ###   Manual termination     You can manually terminate a cluster from the cluster list (by clicking the square on the cluster’s row) or the cluster detail page (by clicking  **Terminate**   ).         ###   Automatic termination     You can also set auto termination for a cluster. During cluster creation, you can specify an inactivity period in minutes after which you want the cluster to terminate.       If the difference between the current time and the last command run on the cluster is more than the inactivity period specified, Databricks automatically terminates that cluster.       A cluster is considered inactive when all commands on the cluster, including Spark jobs, Structured Streaming, and JDBC calls, have finished executing. This does not include commands run by SSH-ing into the cluster and running bash commands.        Warning     * Clusters do not report activity resulting from the use of DStreams. This means that an autoterminating cluster may be terminated while it is running DStreams. Turn off auto termination for clusters running DStreams or consider using Structured Streaming. * The auto termination feature monitors only Spark jobs, not user-defined local processes. Therefore, if all Spark jobs have completed, a cluster may be terminated even if local processes are running. * Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination.     ####   Configure automatic termination     You can configure automatic termination in create cluster UI. Ensure the box is checked and enter a number of minutes in the  **Terminate after \_\_\_ of minutes of inactivity**   setting.       You can opt out of auto termination by clearing the Auto Termination checkbox or by specifying an inactivity period of  `0`  .        Note       Auto termination is best supported in the latest Spark versions. Older Spark versions have known limitations which can result in inaccurate reporting of cluster activity. For example, clusters running JDBC, R, or streaming commands can report a stale activity time that leads to premature cluster termination. Please upgrade to the most recent Spark version to benefit from bug fixes and improvements to auto termination.             ###   Unexpected termination     Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination.       For a list of termination reasons and remediation steps, see the  [Knowledge Base](https://kb.databricks.com/clusters/termination-reasons.html)   .          Delete a cluster ------------------     Deleting a cluster terminates the cluster and removes its configuration. To delete a cluster, select  **Delete**   from the cluster’s  ![Kebab menu](../_images/kebab-menu.png)  kebab menu.        Warning       You cannot undo this action.        To delete a pinned cluster, it must first be unpinned by an administrator.       You can also invoke the  [Permanent delete](../dev-tools/api/latest/clusters.html#clusterclusterservicepermanentdeletecluster)  API endpoint to programmatically delete a cluster.             Restart a cluster -------------------     You can restart a previously terminated cluster from the cluster list, the cluster detail page, or a notebook. You can also invoke the  [Start](../dev-tools/api/latest/clusters.html#clusterclusterservicestartcluster)  API endpoint to programmatically start a cluster.       Databricks identifies a cluster using its unique  [cluster ID](../dev-tools/api/latest/clusters.html#clusterclusterinfo)  . When you start a terminated cluster, Databricks re-creates the cluster with the same ID, automatically installs all the libraries, and reattaches the notebooks.        ###   Cluster autostart for jobs     When a job assigned to a terminated cluster is scheduled to run, or you connect to a terminated cluster from a JDBC/ODBC interface, the cluster is automatically restarted. See  [Create a job](../workflows/jobs/jobs.html#job-create)  and  [JDBC connect](../integrations/jdbc-odbc-bi.html)  .       Cluster autostart allows you to configure clusters to autoterminate without requiring manual intervention to restart the clusters for scheduled jobs. Furthermore, you can schedule cluster initialization by scheduling a job to run on a terminated cluster.       Before a cluster is restarted automatically,  [cluster](../security/auth-authz/access-control/cluster-acl.html)  and  [job](../security/auth-authz/access-control/jobs-acl.html)  access control permissions are checked.        Note       If your cluster was created in Databricks platform version 2.70 or earlier, there is no autostart: jobs scheduled to run on terminated clusters will fail.        ###   Restart a cluster to update it with the latest images     When you restart a cluster, it gets the latest images for the compute resource containers and the VM hosts. It is particularly important to schedule regular restarts for long-running clusters such as those used for processing streaming data.       It is your responsibility to restart all compute resources regularly to keep the image up-to-date with the latest image version.        Important       If you enable the  [compliance security profile](../security/privacy/security-profile.html)  for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins restart clusters manually during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job.          ###   Run a script that determines how many days your clusters have been running, and optionally restart them     If you are a workspace admin, you can run a script that determines how long each of your clusters has been running, and optionally restart them if they are older than a specified number of days. Databricks provides this script as a notebook.       The first lines of the script define configuration parameters:     * `min\_age\_output`  : The maximum number of days that a cluster can run. Default is 1. * `perform\_restart`  : If  `True`  , the script restarts clusters with age greater than the number of days specified by  `min\_age\_output`  . The default is  `False`  , which identifies the long running clusters but does not restart them. * `secret\_configuration`  : Replace  `REPLACE\_WITH\_SCOPE`  and  `REPLACE\_WITH\_KEY`  with a  [secret scope and key name](../security/secrets/index.html)  . For more details of setting up the secrets, see the notebook.      Warning       If you set  `perform\_restart`  to  `True`  , the script automatically restarts eligible clusters, which can cause active jobs to fail and reset open notebooks. To reduce the risk of disrupting your workspace’s business critical jobs, plan a scheduled maintenance window and be sure to notify workspace users.         ####   Identify and optionally restart long-running clusters notebook     [Open notebook in new tab](/_extras/notebooks/source/clusters-long-running-optional-restart.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                       View cluster information in the Apache Spark UI -------------------------------------------------     You can view detailed information about Spark jobs by selecting the  **Spark UI**   tab on the cluster details page.       If you restart a terminated cluster, the Spark UI displays information for the restarted	"To delete a cluster, select ""Delete"" from the cluster’s kebab menu. This terminates the cluster and removes its configuration. To delete a pinned cluster, it must first be unpinned by an administrator."	1
no_reaction	['https://docs.databricks.com/structured-streaming/kafka.html']	how can i read from kafka?	"    Stream processing with Apache Kafka and Databricks ====================================================     This article describes how you can use Apache Kafka as either a source or a sink when running Structured Streaming workloads on Databricks.       For more Kafka, see the  [Kafka documentation](https://kafka.apache.org/documentation/)   .        Read data from Kafka ----------------------     The following is an example for reading data from Kafka:        ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""subscribe"", ""<topic>"")   .option(""startingOffsets"", ""latest"")   .load() )  ```        Write data to Kafka ---------------------     The following is an example for writing data to Kafka:        ``` (df.writeStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""topic"", ""<topic>"")   .option(""checkpointLocation"", ""<checkpoint\_path>"")   .start() )  ```            Configure Kafka for Apache Spark on Databricks ------------------------------------------------     Databricks provides the  `kafka`  keyword as a data format to configure connections to Kafka 0.10+.       The following are the most common configurations for Kafka:       There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:           |   Option   |   Value   |   Description   | | --- | --- | --- | |   subscribe   |   A comma-separated list of topics.   |   The topic list to subscribe to.   | |   subscribePattern   |   Java regex string.   |   The pattern used to subscribe to topic(s).   | |   assign   |   JSON string  `{""topicA"":[0,1],""topic"":[2,4]}`  .   |   Specific topicPartitions to consume.   |     Other notable configurations:            |   Option   |   Value   |   Default Value   |   Description   | | --- | --- | --- | --- | |   kafka.bootstrap.servers   |   Comma-separated list of host:port.   |   empty   |   [Required] The Kafka  `bootstrap.servers`  configuration. If you find there is no data from Kafka, check the broker address list first. If the broker address list is incorrect, there might not be any errors. This is because Kafka client assumes the brokers will become available eventually and in the event of network errors retry forever.   | |   failOnDataLoss   |  `true`  or  `false`  .   |  `true`  |   [Optional] Whether to fail the query when it’s possible that data was lost. Queries can permanently fail to read data from Kafka due to many scenarios such as deleted topics, topic truncation before processing, and so on. We try to estimate conservatively whether data was possibly lost or not. Sometimes this can cause false alarms. Set this option to  `false`  if it does not work as expected, or you want the query to continue processing despite data loss.   | |   minPartitions   |   Integer >= 0, 0 = disabled.   |   0 (disabled)   |   [Optional] Minimum number of partitions to read from Kafka. You can configure Spark to use an arbitrary minimum of partitions to read from Kafka using the  `minPartitions`  option. Normally Spark has a 1-1 mapping of Kafka topicPartitions to Spark partitions consuming from Kafka. If you set the  `minPartitions`  option to a value greater than your Kafka topicPartitions, Spark will divvy up large Kafka partitions to smaller pieces. This option can be set at times of peak loads, data skew, and as your stream is falling behind to increase processing rate. It comes at a cost of initializing Kafka consumers at each trigger, which may impact performance if you use SSL when connecting to Kafka.   | |   kafka.group.id   |   A Kafka consumer group ID.   |   not set   |   [Optional] Group ID to use while reading from Kafka. Use this with caution. By default, each query generates a unique group ID for reading data. This ensures that each query has its own consumer group that does not face interference from any other consumer, and therefore can read all of the partitions of its subscribed topics. In some scenarios (for example, Kafka group-based authorization), you may want to use specific authorized group IDs to read data. You can optionally set the group ID. However, do this with extreme caution as it can cause unexpected behavior.  * Concurrently running queries (both, batch and streaming) with the same group ID are likely interfere with each other causing each query to read only part of the data. * This may also occur when queries are started/restarted in quick succession. To minimize such issues, set the Kafka consumer configuration  `session.timeout.ms`  to be very small.  | |   startingOffsets   |   earliest , latest   |   latest   |   [Optional] The start point when a query is started, either “earliest” which is from the earliest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.   |     See  [Structured Streaming Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)   for other optional configurations.         Schema for Kafka records --------------------------     The schema of Kafka records is:          |   Column   |   Type   | | --- | --- | |   key   |   binary   | |   value   |   binary   | |   topic   |   string   | |   partition   |   int   | |   offset   |   long   | |   timestamp   |   long   | |   timestampType   |   int   |     The  `key`  and the  `value`  are always deserialized as byte arrays with the  `ByteArrayDeserializer`  . Use DataFrame operations (such as  `cast(""string"")`  ) to explicitly deserialize the keys and values.           Retrieve Kafka metrics ------------------------      Note       Available in Databricks Runtime 8.1 and above.        You can get the average, min, and max of the number of offsets that the streaming query is behind the latest available offset among all the subscribed topics with the  `avgOffsetsBehindLatest`  ,  `maxOffsetsBehindLatest`  , and  `minOffsetsBehindLatest`  metrics. See  [Reading Metrics Interactively](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reading-metrics-interactively)   .        Note       Available in Databricks Runtime 9.1 and above.        Get the estimated total number of bytes that the query process has not consumed from the subscribed topics by examining the value of  `estimatedTotalBytesBehindLatest`  . This estimate is based on the batches that were processed in the last 300 seconds. The timeframe that the estimate is based on can be changed by setting the option  `bytesEstimateWindowLength`  to a different value. For example, to set it to 10 minutes:        ``` df = (spark.readStream   .format(""kafka"")   .option(""bytesEstimateWindowLength"", ""10m"") # m for minutes, you can also use ""600s"" for 600 seconds )  ```      If you are running the stream in a notebook, you can see these metrics under the  **Raw Data**   tab in the streaming query progress dashboard:        ``` {  ""sources"" : [ {  ""description"" : ""KafkaV2[Subscribe[topic]]"",  ""metrics"" : {  ""avgOffsetsBehindLatest"" : ""4.0"",  ""maxOffsetsBehindLatest"" : ""4"",  ""minOffsetsBehindLatest"" : ""4"",  ""estimatedTotalBytesBehindLatest"" : ""80.0""  },  } ] }  ```        Use SSL to connect Databricks to Kafka ----------------------------------------     To enable SSL connections to Kafka, follow the instructions in the Confluent documentation  [Encryption and Authentication with SSL](https://docs.confluent.io/current/kafka/authentication_ssl.html#clients)   . You can provide the configurations described there, prefixed with  `kafka.`  , as options. For example, you specify the trust store location in the property  `kafka.ssl.truststore.location`  .       Databricks recommends that you:     * Store your certificates in cloud object storage. You can restrict access to the certificates only to clusters that can access Kafka. See  [Data governance guide](../data-governance/index.html)  . * Store your certificate passwords as  [secrets](../security/secrets/secrets.html)  in a  [secret scope](../security/secrets/secret-scopes.html)  .     The following example uses object storage locations and Databricks secrets to enable an SSL connection:        ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ...)   .option(""kafka.security.protocol"", ""SASL\_SSL"")   .option(""kafka.ssl.truststore.location"", <truststore-location>)   .option(""kafka.ssl.keystore.location"", <keystore-location>)   .option(""kafka.ssl.keystore.password"", dbutils.secrets.get(scope=<certificate-scope-name>,key=<keystore-password-key-name>))   .option(""kafka.ssl.truststore.password"", dbutils.secrets.get(scope=<certificate-scope-name>,key=<truststore-password-key-name>)) )  ```                                                       "	"To read data from Kafka, you can use the following example code:  ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""subscribe"", ""<topic>"")   .option(""startingOffsets"", ""latest"")   .load() ) ```  This code reads data from a Kafka topic and loads it into a DataFrame."	1
no_reaction	['https://docs.databricks.com/dev-tools/cli/secrets-cli.html']	what is the command in dbutils to grand someone access to a secret?	"    Secrets CLI =============     You run Databricks secrets CLI subcommands by appending them to  `databricks     secrets`  . These subcommands call the  [Secrets API 2.0](../api/latest/secrets.html)  .       For more information about secrets, see  [Secret management](../../security/secrets/index.html)  .        Note       The Secrets CLI requires Databricks CLI 0.7.1 or above.         ``` databricks secrets --help  ```       ``` Usage: databricks secrets [OPTIONS] COMMAND [ARGS]...    Utility to interact with secret API.  Options:   -v, --version   [VERSION]   --profile TEXT  CLI connection profile to use. The default profile is                   ""DEFAULT"".   -h, --help      Show this message and exit.  Commands:   create-scope  Creates a secret scope.     Options:       --scope SCOPE                  The name of the secret scope.       --initial-manage-principal     The initial principal that can manage the created secret scope.                                       If specified, the initial ACL with MANAGE permission applied                                       to the scope is assigned to the supplied principal (user or group).                                       The only supported principal is the group                                       ""users"", which contains all users in the workspace. If not                                       specified, the initial ACL with MANAGE permission applied to                                       the scope is assigned to request issuer's user identity.   delete        Deletes a secret.     Options:       --scope SCOPE                  The name of the secret scope.       --key KEY                      The name of secret key.   delete-acl    Deletes an access control rule for a principal.     Options:       --scope SCOPE                  The name of the scope.       --principal PRINCIPAL          The name of the principal.   delete-scope  Deletes a secret scope.     Options:       --scope SCOPE                  The name of the secret scope.   get-acl       Gets the details for an access control rule.     Options:       --scope SCOPE                  The name of the secret scope.       --principal PRINCIPAL          The name of the principal.       --output FORMAT                JSON or TABLE. Set to TABLE by default.   list          Lists all the secrets in a scope.     Options:       --scope SCOPE                  The name of the secret scope.       --output FORMAT                JSON or TABLE. Set to TABLE by default.   list-acls     Lists all access control rules for a given secret scope.     Options:       --scope SCOPE                  The name of the secret scope.       --output FORMAT                JSON or TABLE. Set to TABLE by default.   list-scopes   Lists all secret scopes.       --output FORMAT                JSON or TABLE. Set to TABLE by default.   put           Puts a secret in a scope.     Options:       --scope SCOPE                  The name of the secret scope.       --key KEY                      The name of the secret key.       --string-value TEXT            Read value from string and stored in UTF-8 (MB4) form       --binary-file PATH             Read value from binary-file and stored as bytes.   put-acl       Creates or overwrites an access control rule for a principal                 applied to a given secret scope.     Options:       --scope SCOPE                    The name of the secret scope.       --principal PRINCIPAL            The name of the principal.       --permission [MANAGE|WRITE|READ] The permission to apply.   write         Puts a secret in a scope. ""write"" is an alias for ""put"".     Options:       --scope SCOPE                  The name of the secret scope.       --key KEY                      The name of the secret key.       --string-value TEXT            Read value from string and stored in UTF-8 (MB4) form       --binary-file PATH             Read value from binary-file and stored as bytes.   write-acl     Creates or overwrites an access control rule for a principal                 applied to a given secret scope. ""write-acl"" is an alias for                 ""put-acl"".     Options:       --scope SCOPE                    The name of the secret scope.       --principal PRINCIPAL            The name of the principal.       --permission [MANAGE|WRITE|READ] The permission to apply.  ```       Create a secret scope -----------------------     To display usage documentation, run  `databricks     secrets     create-scope     --help`  .        ``` databricks secrets create-scope --scope my-scope  ```      If successful, no output is displayed.         Delete a secret -----------------     To display usage documentation, run  `databricks     secrets     delete     --help`  .        ``` databricks secrets delete --scope my-scope --key my-key  ```      If successful, no output is displayed.         Revoke an ACL for a principal -------------------------------     To display usage documentation, run  `databricks     secrets     delete-acl     --help`  .        ``` databricks secrets delete-acl --scope my-scope --principal someone@example.com  ```      If successful, no output is displayed.         Delete a secret scope -----------------------     To display usage documentation, run  `databricks     secrets     delete-scope     --help`  .        ``` databricks secrets delete-scope --scope my-scope  ```      If successful, no output is displayed.         Get an ACL for a principal ----------------------------     To display usage documentation, run  `databricks     secrets     get-acl     --help`  .        ``` databricks secrets get-acl --scope my-scope --principal someone@example.com --output JSON  ```       ``` {  ""principal"": ""sonmeone@example.com"",  ""permission"": ""MANAGE"" }  ```        List the secret keys stored within a secret scope ---------------------------------------------------     To display usage documentation, run  `databricks     secrets     list     --help`  .        ``` databricks secrets list --scope my-scope --output JSON  ```       ``` {  ""secrets"": [  {  ""key"": ""my-key"",  ""last\_updated\_timestamp"": 1621284092605  }  ] }  ```       Note       You cannot access secret values by using the Databricks CLI. To access secret values, you must use the Databricks Utilities  [secrets utility](../databricks-utils.html#dbutils-secrets)  within a Databricks notebook.          List the ACLs for a secret scope ----------------------------------     To display usage documentation, run  `databricks     secrets     list-acls     --help`  .        ``` databricks secrets list-acls --scope my-scope --output JSON  ```       ``` {  ""items"": [  {  ""principal"": ""someone@example.com"",  ""permission"": ""MANAGE""  }  ] }  ```        List all available secret scopes in the workspace ---------------------------------------------------     To display usage documentation, run  `databricks     secrets     list-scopes     --help`  .        ``` databricks secrets list-scopes --output JSON  ```       ``` {  ""scopes"": [  {  ""name"": ""my-scope"",  ""backend\_type"": ""DATABRICKS""  }  ] }  ```        Create or update a secret ---------------------------     To display usage documentation, run  `databricks     secrets     put     --help`  or  `databricks     secrets     write     --help`  .       There are three ways to store a secret. The easiest way is to use the  `--string-value`  option; the secret will be stored in UTF-8 (MB4) form. You should be careful with this option, because your secret may be stored in your command line history in plain text.        ``` databricks secrets put --scope my-scope --key my-key --string-value my-value  ```      Or:        ``` databricks secrets write --scope my-scope --key my-key --string-value my-value  ```      If successful, no output is displayed.       You can also use the  `--binary-file`  option to provide a secret"	To grant someone access to a secret in Databricks, you need to create or overwrite an access control rule for a principal applied to a given secret scope using the `databricks secrets put-acl` or `databricks secrets write-acl` command. The command requires the `--scope` option to specify the name of the secret scope, the `--principal` option to specify the name of the principal, and the `--permission` option to specify the permission to apply, which can be MANAGE, WRITE, or READ.	1
no_reaction	['https://docs.databricks.com/sql/language-manual/parameters/timezone.html']	how to change the time zone ? 	    TIMEZONE ==========    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL       The  `TIMEZONE`  configuration parameter controls the local timezone used for timestamp operations within a session.       You can set this parameter at the session level using the  [SET statement](../sql-ref-syntax-aux-conf-mgmt-set.html)  and at the global level using  [SQL configuration parameters](../../admin/sql-configuration-parameters.html)  or  [Global SQL Warehouses API](../../api/sql-endpoints.html#global-sql-warehouses-api)  .       An alternative way to set the session timezone is using the  [SET TIME ZONE statement](../sql-ref-syntax-aux-conf-mgmt-set-timezone.html)  .        Setting ---------     The parameter may be set to one of these values:     * **Region base zone ID**       Region IDs must have the form ‘area/city’, such as  `America/Los\_Angeles`  . * **Zone offset**       Zone offsets must be in the format ‘  `(+|-)HH`  ’,  `(+|-)HH:mm`  or  `(+|-)HH:mm:ss`  , e.g  `-08`  ,  `+01:00`  or  `-13:33:33`  . Also, ‘UTC’ is supported as an alias for  `+00:00`  .     Other short names are not recommended to use because they can be ambiguous.         System default ----------------     The system default is  `UTC`  .         Examples ----------      ``` -- Set time zone to the region-based zone ID. > SET timezone = America/Los\_Angeles; > SELECT current\_timezone();  America/Los\_Angeles  -- Set time zone to the Zone offset. > SET timezone = +08:00; > SELECT current\_timezone();  +08:00  ```        Related ---------   * [RESET](../sql-ref-syntax-aux-conf-mgmt-reset.html) * [SET statement](../sql-ref-syntax-aux-conf-mgmt-set.html) * [SET TIME ZONE statement](../sql-ref-syntax-aux-conf-mgmt-set-timezone.html) * [SQL configuration parameters](../../admin/sql-configuration-parameters.html) * [Global SQL Warehouses API](../../api/sql-endpoints.html#global-sql-warehouses-api)                                                      	To change the local timezone used for timestamp operations within a session, you can set the `TIMEZONE` configuration parameter at the session level using the `SET statement` and at the global level using SQL configuration parameters or Global SQL Warehouses API. An alternative way to set the session timezone is using the `SET TIME ZONE statement`. The parameter may be set to one of these values: Region base zone ID or Zone offset. The system default is UTC.	1
no_reaction	['https://docs.databricks.com/introduction/index.html']	What is databricks?	    What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .                                                          	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used to process, store, clean, share, analyze, model, and monetize datasets with solutions from BI to machine learning. The Databricks platform provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks is used by customers to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.	1
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.LongType.html']	using databricks sql, how can I cast a hex string to a long	"   LongType  [¶](#longtype ""Permalink to this headline"") =======================================================  *class*  `pyspark.sql.types.`  `LongType`  [[source]](../../../_modules/pyspark/sql/types.html#LongType) [¶](#pyspark.sql.types.LongType ""Permalink to this definition"")    Long data type, i.e. a signed 64-bit integer.     If the values are beyond the range of [-9223372036854775808, 9223372036854775807], please use  [`DecimalType`](pyspark.sql.types.DecimalType.html#pyspark.sql.types.DecimalType ""pyspark.sql.types.DecimalType"")  .     Methods    |  [`fromInternal`](#pyspark.sql.types.LongType.fromInternal ""pyspark.sql.types.LongType.fromInternal"")  (obj)   |   Converts an internal SQL object into a native Python object.   | |  [`json`](#pyspark.sql.types.LongType.json ""pyspark.sql.types.LongType.json"")  ()   |   | |  [`jsonValue`](#pyspark.sql.types.LongType.jsonValue ""pyspark.sql.types.LongType.jsonValue"")  ()   |   | |  [`needConversion`](#pyspark.sql.types.LongType.needConversion ""pyspark.sql.types.LongType.needConversion"")  ()   |   Does this type needs conversion between Python object and internal SQL object.   | |  [`simpleString`](#pyspark.sql.types.LongType.simpleString ""pyspark.sql.types.LongType.simpleString"")  ()   |   | |  [`toInternal`](#pyspark.sql.types.LongType.toInternal ""pyspark.sql.types.LongType.toInternal"")  (obj)   |   Converts a Python object into an internal SQL object.   | |  [`typeName`](#pyspark.sql.types.LongType.typeName ""pyspark.sql.types.LongType.typeName"")  ()   |   |   Methods Documentation    `fromInternal`    (   *obj     :     Any*   )    → Any  [¶](#pyspark.sql.types.LongType.fromInternal ""Permalink to this definition"")    Converts an internal SQL object into a native Python object.    `json`    (     )    → str  [¶](#pyspark.sql.types.LongType.json ""Permalink to this definition"")   `jsonValue`    (     )    → Union    [    str    ,    Dict    [    str    ,    Any    ]     ]   [¶](#pyspark.sql.types.LongType.jsonValue ""Permalink to this definition"")   `needConversion`    (     )    → bool  [¶](#pyspark.sql.types.LongType.needConversion ""Permalink to this definition"")    Does this type needs conversion between Python object and internal SQL object.     This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.    `simpleString`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#LongType.simpleString) [¶](#pyspark.sql.types.LongType.simpleString ""Permalink to this definition"")   `toInternal`    (   *obj     :     Any*   )    → Any  [¶](#pyspark.sql.types.LongType.toInternal ""Permalink to this definition"")    Converts a Python object into an internal SQL object.    *classmethod*  `typeName`    (     )    → str  [¶](#pyspark.sql.types.LongType.typeName ""Permalink to this definition"")   [IntegerType](pyspark.sql.types.IntegerType.html ""previous page"")  [MapType](pyspark.sql.types.MapType.html ""next page"")   "	You can use the `cast` function in Databricks SQL to cast a hex string to a long. Here is an example:  ``` SELECT CAST('0x123456789abcdef' AS BIGINT); ```  This will return the decimal representation of the hex string as a long.	0
no_reaction	['https://docs.databricks.com/delta-live-tables/testing.html']	How to For example, you can publish to a dev schema for development and a prod schema for production data?	"      Publish data from Delta Live Tables pipelines ===============================================      Note       You cannot publish tables to  [Unity Catalog](../data-governance/unity-catalog/index.html)  . Delta Live Tables supports publishing tables only to the workspace-level Hive metastore.        You can make the output data of your pipeline discoverable and available to query by publishing datasets to the Databricks  [metastore](../data/metastores/index.html)  . To publish datasets to the metastore, enter a schema name in the  **Target**   field when you create a pipeline. You can also add a target database to an existing pipeline.       By default, all tables and views created in Delta Live Tables are local to the pipeline. You must publish tables to a target schema to query or use Delta Live Tables datasets outside the pipeline in which they are declared.          How to publish Delta Live Tables datasets to a schema -------------------------------------------------------     You can declare a target schema for all tables in your Delta Live Tables pipeline using the  **Target schema**   field in the  **Pipeline settings**   and  **Create pipeline**   UIs.       You can also specify a schema in a JSON configuration by setting the  `target`  value.       You must run an update for the pipeline to publish results to the target schema.       You can use this feature with multiple environment configurations to publish to different schemas based on the environment. For example, you can publish to a  `dev`  schema for development and a  `prod`  schema for production data.           How to query datasets in Delta Live Tables --------------------------------------------     After an update completes, you can view the schema and tables, query the data, or use the data in downstream applications.       Once published, Delta Live Tables tables can be queried from any environment with access to the target schema. This includes Databricks SQL, notebooks, and other Delta Live Tables pipelines.        Important       When you create a  `target`  configuration, only tables and associated metadata are published. Views are not published to the metastore.          Exclude tables from target schema -----------------------------------     If you need to calculate intermediate tables that are not intended for external consumption, you can prevent them from being published to a schema using the  `TEMPORARY`  keyword. Temporary tables still store and process data according to Delta Live Tables semantics, but should not be accessed outside of the current pipeline. Use the following syntax to declare temporary tables:         ``` CREATE TEMPORARY LIVE TABLE temp\_table AS SELECT ... ;  ```       ``` @dlt.table(   temporary=True) def temp\_table():   return (""..."")  ```                                                        "	To publish Delta Live Tables datasets to a schema, you can declare a target schema for all tables in your Delta Live Tables pipeline using the Target schema field in the Pipeline settings and Create pipeline UIs. You can also specify a schema in a JSON configuration by setting the target value. You must run an update for the pipeline to publish results to the target schema. You can use this feature with multiple environment configurations to publish to different schemas based on the environment. For example, you can publish to a dev schema for development and a prod schema for production data.	1
no_reaction	['https://docs.databricks.com/data-governance/unity-catalog/hive-metastore.html']	how does unity catalog work with an external hive metastore?	"      Work with Unity Catalog and the legacy Hive metastore =======================================================     This article explains how to use a Unity Catalog metastore with the per-workspace Hive metastore in Databricks.       If your workspace was in service before it was enabled for Unity Catalog, it likely has a Hive metastore that contains data that you want to continue to use. Databricks recommends that you  [migrate the tables managed by the Hive metastore to the Unity Catalog metastore](migrate.html)  , but if you choose not to, this article explains how to work with data managed by both metastores.          Query the Hive metastore in Unity Catalog -------------------------------------------     The Unity Catalog metastore is additive, meaning it can be used with the per-workspace Hive metastore in Databricks. The Hive metastore appears as a top-level catalog called  `hive\_metastore`  in the three-level namespace.       For example, you can refer to a table called  `sales\_raw`  in the  `sales`  schema in the legacy Hive metastore by using the following notation:         ``` SELECT \* from hive\_metastore.sales.sales\_raw;  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       ``` library(SparkR)  display(tableToDF(""hive\_metastore.sales.sales\_raw""))  ```       ``` display(spark.table(""hive\_metastore.sales.sales\_raw""))  ```       You can also specify the catalog and schema with a  `USE`  statement:         ``` USE hive\_metastore.sales; SELECT \* from sales\_raw;  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```       ``` library(SparkR)  sql(""USE hive\_metastore.sales"") display(tableToDF(""sales\_raw""))  ```       ``` spark.sql(""USE hive\_metastore.sales"") display(spark.table(""sales\_raw""))  ```           Access control in Unity Catalog and the Hive metastore --------------------------------------------------------     If you configured  [table access control](../table-acls/index.html)  on the Hive metastore, Databricks continues to enforce those access controls for data in the  `hive\_metastore`  catalog for clusters running in the shared access mode. The Unity Catalog access model differs slightly from legacy access controls, like no  `DENY`  statements. The Hive metastore is a workspace-level object. Permissions defined within the  `hive\_metastore`  catalog always refer to the local users and groups in the workspace. See  [Differences from table access control](#differences-from-table-access-control)  .         Differences from table access control ---------------------------------------     Unity Catalog has the following key differences from using  [table access controls](../table-acls/index.html)  in the legacy Hive metastore in each workspace.       The access control model in Unity Catalog has the following differences from table access control:     * **Account groups**   : Access control policies in Unity Catalog are applied to account groups, while access control policies for the Hive metastore are applied to workspace-local groups. See  [Special considerations for groups](../../administration-guide/users-groups/index.html#special-groups)  . * **`USE     CATALOG`  and  `USE     SCHEMA`  permissions are required on the catalog and schema for all operations on objects inside the catalog or schema**   : Regardless of a principal’s privileges on a table, the principal must also have the  `USE     CATALOG`  privilege on its parent catalog to access the schema and the  `USE     SCHEMA`  privilege to access objects within the schema. With workspace-level table access controls, on the other hand, granting  `USAGE`  on the root catalog automatically grants  `USAGE`  on all databases, but  `USAGE`  on the root catalog is not required. * **Views**   : In Unity Catalog, the owner of a view does not need to be an owner of the view’s referenced tables and views. Having the  `SELECT`  privilege is sufficient, along with  `USE     SCHEMA`  on the views’ parent schema and  `USE     CATALOG`  on the parent catalog. With workspace-level table access controls, a view’s owner needs to be an owner of all referenced tables and views. * **No support for  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  s**   : In Unity Catalog, there is no concept of an  `ALL     FILES`  or  `ANONYMOUS     FUNCTION`  permission. These permissions could be used to circumvent access control restrictions by allowing an unprivileged user to run privileged code.         Joins between Unity Catalog and Hive metastore objects --------------------------------------------------------     By using three-level namespace notation, you can join data in a Unity Catalog metastore with data in the legacy Hive metastore.        Note       A join with data in the legacy Hive metastore will only work on the workspace where that data resides. Trying to run such a join in another workspace results in an error. Databricks recommends that you  [upgrade](migrate.html)  legacy tables and views to Unity Catalog.        The following example joins results from the  `sales\_current`  table in the legacy Hive metastore with the  `sales\_historical`  table in the Unity Catalog metastore when the  `order\_id`  fields are equal.         ``` SELECT \* FROM hive\_metastore.sales.sales\_current JOIN main.shared\_sales.sales\_historical ON hive\_metastore.sales.sales\_current.order\_id = main.shared\_sales.sales\_historical.order\_id;  ```       ``` dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(   other = dfHistorical,   on = dfCurrent.order\_id == dfHistorical.order\_id ))  ```       ``` library(SparkR)  dfCurrent = tableToDF(""hive\_metastore.sales.sales\_current"") dfHistorical = tableToDF(""main.shared\_sales.sales\_historical"")  display(join(  x = dfCurrent,  y = dfHistorical,  joinExpr = dfCurrent$order\_id == dfHistorical$order\_id))  ```       ``` val dfCurrent = spark.table(""hive\_metastore.sales.sales\_current"") val dfHistorical = spark.table(""main.shared\_sales.sales\_historical"")  display(dfCurrent.join(  right = dfHistorical,  joinExprs = dfCurrent(""order\_id"") === dfHistorical(""order\_id"") ))  ```         Default catalog -----------------     If you omit the top-level catalog name when you perform data operations, and there is no  `USE     CATALOG`  statement, the default catalog is assumed. Of course, if you are supplying the catalog name in another way, such as setting it on a JDBC string, then that catalog is assumed instead of the default catalog when you omit the catalog name in data operations.       By default, the default catalog is  `hive\_metastore`  . To configure a different default catalog for a workspace, you can set a Spark configuration or run a  [Databricks CLI](../../dev-tools/cli/unity-catalog-cli.html)  command:         Set the  `spark.databricks.sql.initial.catalog.name`  value.         This CLI command updates the default catalog only for SQL warehouses. To specify the default catalog for clusters, use the Spark config.        ``` databricks unity-catalog metastores assign --workspace-id 1234567890123456 \  --metastore-id 12a345b6-7890-1cd2-3456-e789f0a12b34 \  --default-catalog-name my_catalog  ```        If your Databricks workspace used the per-workspace Hive metastore before the workspace was enabled for Unity Catalog, Databricks recommends that you keep the default catalog value as  `hive\_metastore`  so that your existing code can operate on current Hive metastore data without any change.        Warning       Changing the default catalog can break existing data operations that depend on it.          Cluster instance profile --------------------------     When using the Hive metastore alongside Unity Catalog, the instance profile on the cluster is used to access Hive metastore data but not data registered in Unity Catalog.       If users access paths that are outside Unity Catalog (such as a path not registered as a table or external location) then the instance profile on the cluster is used.           Upgrade legacy tables to Unity Catalog ----------------------------------------     Tables in the Hive metastore do not benefit from the full set of security and governance features that Unity Catalog introduces, such as built-in auditing and access control. Databricks recommends that you  [upgrade your legacy tables](migrate.html)  by adding them to Unity Catalog.                                                        "	The article explains how to use a Unity Catalog metastore with the per-workspace Hive metastore in Databricks. It also explains how to work with data managed by both metastores. To query the Hive metastore in Unity Catalog, you can refer to a table in the Hive metastore by using the following notation: SELECT * from hive_metastore.sales.sales_raw. You can also specify the catalog and schema with a USE statement.	1
no_reaction	['https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.foreachPartition.html']	foreach or foreachpartition with a python function	"   pyspark.sql.DataFrame.foreachPartition  [¶](#pyspark-sql-dataframe-foreachpartition ""Permalink to this headline"") ===================================================================================================================  `DataFrame.`  `foreachPartition`    (   *f     :     Callable[[Iterator[pyspark.sql.types.Row]], None]*   )    → None  [[source]](../../../_modules/pyspark/sql/dataframe.html#DataFrame.foreachPartition) [¶](#pyspark.sql.DataFrame.foreachPartition ""Permalink to this definition"")    Applies the  `f`  function to each partition of this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  .     This a shorthand for  `df.rdd.foreachPartition()`  .     New in version 1.3.0.     Parameters    **f**    function     A function that accepts one parameter which will receive each partition to process.     Examples    ``` >>> df = spark.createDataFrame( ...     [(14, ""Tom""), (23, ""Alice""), (16, ""Bob"")], [""age"", ""name""]) >>> def func(itr): ...     for person in itr: ...         print(person.name) >>> df.foreachPartition(func)  ```  [pyspark.sql.DataFrame.foreach](pyspark.sql.DataFrame.foreach.html ""previous page"")  [pyspark.sql.DataFrame.freqItems](pyspark.sql.DataFrame.freqItems.html ""next page"")   "	The `foreachPartition` function in PySpark applies a given function to each partition of a DataFrame. It is a shorthand for `df.rdd.foreachPartition()`. The function passed to `foreachPartition` should accept one parameter which will receive each partition to process.	1
no_reaction	['https://docs.databricks.com/sql/admin/sql-configuration-parameters.html']	how do a pass a parameter from powerBI to a Databricks SQL warehouse?	    SET =====    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Sets a Databricks SQL  [parameter](sql-ref-parameters.html)  at the session level, returns the value of an existing parameter or returns all parameters with value and meaning. When using Databricks Runtime, parameters are known as SQL Conf properties.        Syntax --------      ``` SET  ```       ``` SET [ -v ]  ```       ``` SET parameter\_key [ = parameter\_value ]  ```        Parameters ------------   * **(none)**      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL       Outputs the key and value of changed Databricks SQL parameters. * **-v**       Outputs the key, value and meaning of existing parameters. * **parameter\_key**       Returns the value of the specified parameter. * **parameter\_key = parameter\_value**       Sets the value for a given parameter. If an old value exists for a given parameter, then it gets overridden by the new value.         Databricks SQL Examples -------------------------      ``` -- Set a property. > SET ansi\_mode = true;  -- List all configuration parameters with their value and description. > SET -v;  -- List all configuration parameters with a set value for the current session. > SET;  -- List the value of specified property key. > SET ansi\_mode;  key value  --------- -----  ansi\_mode true  ```          Databricks Runtime Examples -----------------------------      ``` -- Set a property. SET spark.sql.variable.substitute=false;  -- List all SQLConf properties with value and meaning. SET -v;  -- List all SQLConf properties with value for current session. SET;  -- List the value of specified property key. SET spark.sql.variable.substitute; +-----------------------------+-----+ | key|value| +-----------------------------+-----+ |spark.sql.variable.substitute|false| +-----------------------------+-----+  ```        Related articles ------------------   * [Configuration parameters](sql-ref-parameters.html) * [RESET](sql-ref-syntax-aux-conf-mgmt-reset.html)                                                      	To pass a parameter from Power BI to a Databricks SQL warehouse, you can use the SET command in Databricks SQL to set the parameter value at the session level. The parameter can then be referenced in your SQL queries.	0
no_reaction	['https://docs.databricks.com/machine-learning/feature-store/index.html']	Databricks Feature Store	    Databricks Feature Store ==========================     This page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.       The Databricks Feature Store library is available only on Databricks Runtime for Machine Learning and is accessible through Databricks notebooks and workflows.        Note       At this time, Feature Store does not support writing to a Unity Catalog metastore. In Unity Catalog-enabled workspaces, you can write feature tables only to the default Hive metastore.         What is a feature store? --------------------------     A feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.       Machine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.       Developing features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.         Why use Databricks Feature Store? -----------------------------------     Databricks Feature Store is fully integrated with other components of Databricks.     * Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features. * Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature. * Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier. * Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.       Start using Feature Store ---------------------------     See the following articles to get started with Feature Store:     * Try one of the  [example notebooks](example-notebooks.html)  that illustrate feature store capabilities. * See the reference material for the  [Feature Store Python API](python-api.html)  . * Learn about  [training models with Feature Store](train-models-with-feature-store.html)  . * Learn about  [working with feature tables](feature-tables.html)  . * Use  [time series feature tables and point-in-time lookups](time-series.html)  to retrieve the latest feature values as of a particular time for training or scoring a model. * Learn about  [publishing features to online feature stores](online-feature-stores.html)  for real-time serving and automatic feature lookup. * [Share feature tables across workspaces](multiple-workspaces.html)  . * [Control access to feature tables](access-control.html)  .       More information ------------------     For more information on best practices for using Feature Store, download  [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores)   .                                                          	A feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference. Databricks Feature Store is fully integrated with other components of Databricks and provides discoverability, lineage, integration with model scoring and serving, and point-in-time lookups. It is available only on Databricks Runtime for Machine Learning and is accessible through Databricks notebooks and workflows.	1
no_reaction	['https://kb.databricks.com/clusters/legacy-global-init-script-migration-notebook']	how do I prevent Init scripts ?	"      Cluster node initialization scripts =====================================     An init script is a shell script that runs during startup of each cluster node  *before*   the Apache Spark driver or worker JVM starts.       Some examples of tasks performed by init scripts include:     * Install packages and libraries not included in Databricks Runtime. To install Python packages, use the Databricks  `pip`  binary located at  `/databricks/python/bin/pip`  to ensure that Python packages install into the Databricks Python virtual environment rather than the system Python environment. For example,  `/databricks/python/bin/pip     install     <package-name>`  . * Modify the JVM system classpath in  [special cases](https://spark.apache.org/docs/latest/sql-programming-guide.html#troubleshooting)   . * Set system properties and environment variables used by the JVM. * Modify Spark configuration parameters.      Warning       Databricks scans the reserved location  `/databricks/init`  for  [legacy global init scripts](#migrate-legacy-scripts)  . Databricks recommends you avoid storing init scripts in this location to avoid unexpected behavior.           Init script types -------------------     Databricks supports two kinds of init scripts: cluster-scoped and global.     * **Cluster-scoped**   : run on every cluster configured with the script. This is the recommended way to run an init script. * **Global**   : run on every cluster in the workspace. They can help you to enforce consistent cluster configurations across your workspace. Use them carefully because they can cause unanticipated impacts, like library conflicts. Only admin users can create global init scripts. Global init scripts are not run on  [model serving clusters](../archive/legacy-model-serving/model-serving.html)  .      Warning       Legacy global init scripts and cluster-named init scripts are  *deprecated*   and cannot be used in new workspaces starting February 21, 2023:     * **Cluster-named**   : run on a cluster with the same name as the script. Cluster-named init scripts are best-effort (silently ignore failures), and attempt to continue the cluster launch process. Use Cluster-scoped init scripts instead, they are a complete replacement. * **Legacy global**   : run on every cluster. They are less secure than the new global init script framework, silently ignore failures, and cannot reference  [environment variables](#env-var)  . Migrate existing legacy global init scripts to the new global init script framework. See  [Migrate from legacy to new global init scripts](#migrate-legacy-scripts)  .      Whenever you change any type of init script, you must restart all clusters affected by the script.               Init script execution order -----------------------------     The order of execution of init scripts is:     1. Legacy global (deprecated - do not use) 2. Cluster-named (deprecated - do not use) 3. Global 4. Cluster-scoped       Environment variables -----------------------     Cluster-scoped and global init scripts support the following environment variables:     * `DB\_CLUSTER\_ID`  : the ID of the cluster on which the script is running. See  [Clusters API 2.0](../dev-tools/api/latest/clusters.html)  . * `DB\_CONTAINER\_IP`  : the private IP address of the container in which Spark runs. The init script is run inside this container. See  [SparkNode](../dev-tools/api/latest/clusters.html#clustersparkinfosparknode)  . * `DB\_IS\_DRIVER`  : whether the script is running on a driver node. * `DB\_DRIVER\_IP`  : the IP address of the driver node. * `DB\_INSTANCE\_TYPE`  : the instance type of the host VM. * `DB\_CLUSTER\_NAME`  : the name of the cluster the script is executing on. * `DB\_IS\_JOB\_CLUSTER`  : whether the cluster was created to run a job. See  [Create a job](../workflows/jobs/jobs.html#create-a-job)  .     For example, if you want to run part of a script only on a driver node, you could write a script like:        ``` echo $DB\_IS\_DRIVER if [[ $DB\_IS\_DRIVER = ""TRUE"" ]]; then  <run this part only on driver> else  <run this part only on workers> fi <run this part on both driver and workers>  ```      You can also configure  [custom environment variables](configure.html#env-var)  for a cluster and reference those variables in init scripts.      ###   Use secrets in environment variables     You can use any valid variable name when you reference a secret. Access to secrets referenced in environment variables is determined by the permissions of the user who configured the cluster. Secrets stored in environmental variables are accessible by all users of the cluster, but are redacted from plaintext display in the normal fashion as secrets referenced elsewhere.       For more details, see  [Reference a secret in an environment variable](../security/secrets/secrets.html#reference-a-secret-in-an-environment-variable)  .          Logging ---------     Init script start and finish events are captured in cluster event logs. Details are captured in cluster logs. Global init script create, edit, and delete events are also captured in account-level audit logs.      ###   Init script events    [Cluster event logs](clusters-manage.html#event-log)  capture two init script events:  `INIT\_SCRIPTS\_STARTED`  and  `INIT\_SCRIPTS\_FINISHED`  , indicating which scripts are scheduled for execution and which have completed successfully.  `INIT\_SCRIPTS\_FINISHED`  also captures execution duration.       Global init scripts are indicated in the log event details by the key  `""global""`  and cluster-scoped init scripts are indicated by the key  `""cluster""`  .        Note       Cluster event logs do not log init script events for each cluster node; only one node is selected to represent them all.          ###   Init script logs     If  [cluster log delivery](configure.html#cluster-log-delivery)  is configured for a cluster, the init script logs are written to  `/<cluster-log-path>/<cluster-id>/init\_scripts`  . Logs for each container in the cluster are written to a subdirectory called  `init\_scripts/<cluster\_id>\_<container\_ip>`  . For example, if  `cluster-log-path`  is set to  `cluster-logs`  , the path to the logs for a specific container would be:  `dbfs:/cluster-logs/<cluster-id>/init\_scripts/<cluster\_id>\_<container\_ip>`  .       If the cluster is configured to write logs to DBFS, you can view the logs using the  [File system utility (dbutils.fs)](../dev-tools/databricks-utils.html#dbutils-fs)  or the  [DBFS CLI](../dev-tools/cli/dbfs-cli.html)  . For example, if the cluster ID is  `1001-234039-abcde739`  :        ``` dbfs ls dbfs:/cluster-logs/1001-234039-abcde739/init_scripts  ```       ``` 1001-234039-abcde739\_10\_97\_225\_166 1001-234039-abcde739\_10\_97\_231\_88 1001-234039-abcde739\_10\_97\_244\_199  ```       ``` dbfs ls dbfs:/cluster-logs/1001-234039-abcde739/init_scripts/1001-234039-abcde739_10_97_225_166  ```       ``` <timestamp>\_<log-id>\_<init-script-name>.sh.stderr.log <timestamp>\_<log-id>\_<init-script-name>.sh.stdout.log  ```      When cluster log delivery is not configured, logs are written to  `/databricks/init\_scripts`  . You can use standard shell commands in a notebook to list and view the logs:        ``` %sh ls /databricks/init_scripts/ cat /databricks/init_scripts/<timestamp>_<log-id>_<init-script-name>.sh.stdout.log  ```          Every time a cluster launches, it writes a log to the init script log folder.        Important       Any user who creates a cluster and enables cluster log delivery can view the  `stderr`  and  `stdout`  output from global init scripts. You should ensure that your global init scripts do not output any sensitive information.            ###   Audit logs     Databricks audit logs capture global init script create, edit, and delete events under the event type  `globalInitScripts`  . See  [Configure audit logging](../administration-guide/account-settings/audit-logs.html)  .          Cluster-scoped init scripts -----------------------------     Cluster-scoped init scripts are init scripts defined in a cluster configuration. Cluster-scoped init scripts apply to both clusters you create and those created to run jobs.       You can configure cluster-scoped init scripts using the UI, the CLI, and by invoking the Clusters API. This section focuses on performing these tasks using the UI. For the other methods, see  [Databricks CLI setup & documentation](../dev-tools/cli/index.html)  and  [Clusters API 2.0](../dev-tools/api/latest/clusters.html)  .       You can add any number of scripts, and the scripts are executed sequentially in the order provided.       If a cluster-scoped init script returns a non-zero exit code, the cluster launch  *fails*   . You can troubleshoot cluster-scoped init scripts by configuring  [cluster log delivery](configure.html#cluster-log-delivery)  and examining the  [init script log](#init-script-log)  .      ###   Cluster-scoped init script locations     You can put init scripts in a cloud storage directory accessible by a cluster.          ####   Example: Use conda to install Python libraries     With Databricks Runtime 9.0 and above, you cannot use conda to install Python libraries. For instructions on how to install Python packages on a cluster, see  [Libraries](../libraries/index.html)  .        Important       Anaconda Inc. updated their  [terms of service](https://www.anaconda.com/terms-of-service)   for anaconda.org channels in September 2020. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See  [Anaconda Commercial Edition FAQ](https://www.anaconda.com/blog/anaconda-commercial-edition-faq)   for more information. Your use of any Anaconda channels is governed by their  [terms of service](https://www.anaconda.com/terms-of-service)   .       As a result of this change, Databricks has removed the default channel configuration for the Conda package manager. This is a breaking change. You must update the usage of conda commands in init-scripts to specify a channel using  `-c`  . If you do not specify a channel, conda commands will fail with  `PackagesNotFoundError`  .        In Databricks Runtime 8.4 ML and below, you use the  [Conda](https://conda.io/docs/)   package manager to install Python packages. To install a Python library at cluster initialization, you can use a script like the following:        ``` #!/bin/bash set -ex /databricks/python/bin/python -V . /dat"	To prevent init scripts, you can avoid storing init scripts in the reserved location `/databricks/init` to avoid unexpected behavior. Additionally, you can avoid using cluster-named and legacy global init scripts, which are deprecated and cannot be used in new workspaces starting February 21, 2023. Instead, you can use cluster-scoped init scripts, which are defined in a cluster configuration and apply to both clusters you create and those created to run jobs. You can configure cluster-scoped init scripts using the UI, the CLI, and by invoking the Clusters API. If you do not want to use init scripts, you can simply not define any init scripts.	0
no_reaction	['https://kb.databricks.com/notebooks/send-email']	How do i configure smtp email in databricks	"Title: How to send email or SMS messages from Databricks notebooks  Description: Learn how to send an email from your Databricks notebook.  Answer: You may need to send a notification to a set of recipients from a Databricks notebook. For example, you may want to send email based on matching business rules or based on a command’s success or failure. This article describes two approaches to sending email or SMS messages from a notebook. Both examples use Python notebooks: Send email or SMS messages from a notebook using Amazon SNS This approach requires that you have an Amazon Simple Notification Service (SNS) article to send the notification to. Recipients who are subscribed to the SNS article will receive the email or SMS notification. Info You may need to install boto3, a Python library for working with Amazon AWS: %python pip install boto3 Add the following Python commands to your notebook, replacing &lt;sample values&gt; with your own: %python # Import the boto3 client import boto3 # Set the AWS region name, retrieve the access key &amp; secret key from dbutils secrets. # For information about how to store the credentials in a secret, see # https://docs.databricks.com/user-guide/secrets/secrets.html AWS_REGION = ""&lt;region-name&gt;"" ACCESS_KEY = dbutils.secrets.get('&lt;scope-name&gt;','&lt;access-key&gt;') SECRET_KEY = dbutils.secrets.get('&lt;scope-name&gt;','&lt;secret-key&gt;') sender='&lt;sender@email-domain.com&gt;' #Create the boto3 client with the region name, access key and secret keys. client = boto3.client('sns',region_name=AWS_REGION, aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY) # Add email subscribers for email in list_of_emails:     client.subscribe(         articleArn=article_arn,         Protocol='email',         Endpoint=email  # &lt;-- email address who'll receive an email.     ) # Add phone subscribers for number in list_of_phone_numbers:     client.subscribe(         articleArn=article_arn,         Protocol='sms',         Endpoint=number  # &lt;-- phone numbers who'll receive SMS.     ) # Send message to the SNS article using publish method. response = client.publish( articleArn='&lt;article-arn-name&gt;', Message=""Hello From Databricks.."", Subject='Email Notification from Databricks..', MessageStructure='string' ) Send email from a notebook using Amazon SES This example notebook demonstrates how to send an email message that includes HTML content and a file attachment, using Amazon Simple Email Service (SES). Example Send email notebook Review the Send email notebook."	To configure SMTP email in Databricks, you can use either Amazon SNS or Amazon SES. For Amazon SNS, you need to have an SNS article to send the notification to, and you may need to install the boto3 Python library for working with Amazon AWS. Then, you can add email subscribers and phone subscribers, and send a message to the SNS article using the publish method. For Amazon SES, you can use the Send email notebook to send an email message that includes HTML content and a file attachment.	1
no_reaction	['https://docs.databricks.com/lakehouse/index.html']	"Q91- Which of the following is true of Delta Lake and the Lakehouse? a)	Delta Lake automatically collects statistics on the first 32 columns of each table which are leveraged Filters. b)	Primary and foreign key constraints can be leveraged to ensure duplicate values are never entered me c)	Because Parquet compresses data row by row, strings will only be compressed when a characterised d)	Views in the Lakehouse maintain a valid cache of the most recent versions of source tables at all times e)	Z-order can only be applied to numeric values stored in Delta Lake tables."	      What is Delta Lake? =====================     Delta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks Lakehouse Platform. Delta Lake is  [open source software](https://delta.io)   that extends Parquet data files with a file-based transaction log for  [ACID transactions](../lakehouse/acid.html)  and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.       Delta Lake is the default storage format for all operations on Databricks. Unless otherwise specified, all tables on Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project. Many of the optimizations and products in the Databricks Lakehouse Platform build upon the guarantees provided by Apache Spark and Delta Lake. For information on optimizations on Databricks, see  [Optimization recommendations on Databricks](../optimizations/index.html)  .       For reference information on Delta Lake SQL commands, see  [Delta Lake statements](../sql/language-manual/index.html#delta-lake-statements)  .       The Delta Lake transaction log has a well-defined open protocol that can be used by any system to read the log. See  [Delta Transaction Log Protocol](https://github.com/delta-io/delta/blob/master/PROTOCOL.md)   .            Getting started with Delta Lake ---------------------------------     All tables on Databricks are Delta tables by default. Whether you’re using Apache Spark  [DataFrames](../getting-started/dataframes-python.html)  or SQL, you get all the benefits of Delta Lake just by saving your data to the lakehouse with default settings.       For examples of basic Delta Lake operations such as creating tables, reading, writing, and updating data, see  [Tutorial: Delta Lake](tutorial.html)  .       Databricks has many recommendations for  [best practices for Delta Lake](best-practices.html)  .           Converting and ingesting data to Delta Lake ---------------------------------------------     Databricks provides a number of products to accelerate and simplify loading data to your lakehouse.     * [Delta Live Tables](../ingestion/auto-loader/dlt.html) * [COPY INTO](../ingestion/copy-into/index.html) * [Auto Loader](../ingestion/auto-loader/index.html) * [Add data UI](../ingestion/add-data/index.html) * [Incrementally convert Parquet or Iceberg data to Delta Lake](convert-to-delta.html) * [One-time conversion of Parquet or Iceberg data to Delta Lake](clone-parquet.html) * [Third-party partners](../integrations/index.html)     For a full list of ingestion options, see  [Load data into the Databricks Lakehouse](../ingestion/index.html)  .             Updating and modifying Delta Lake tables ------------------------------------------     Atomic transactions with Delta Lake provide many options for updating data and metadata. Databricks recommends you avoid interacting directly with data and transaction log files in Delta Lake file directories to avoid corrupting your tables.     * Delta Lake supports upserts using the  [merge operation](merge.html)  . * Delta Lake provides numerous options for  [selective overwrites](selective-overwrite.html)  based on filters and partitions. * You can manually or automatically  [update your table schema](update-schema.html)  without rewriting data. * [Column mapping](delta-column-mapping.html)  enables columns to be renamed or deleted without rewriting data.         Incremental and streaming workloads on Delta Lake ---------------------------------------------------     Delta Lake is optimized for Structured Streaming on Databricks.  [Delta Live Tables](../delta-live-tables/index.html)  extends native capabilities with simplified infrastructure deployment, enhanced scaling, and managed data dependencies.     * [Table streaming reads and writes](../structured-streaming/delta-lake.html) * [Use Delta Lake change data feed on Databricks](delta-change-data-feed.html) * [Enable idempotent writes across jobs](idempotent-writes.html)       Querying previous versions of a table ---------------------------------------     Each write to a Delta table creates a new table version. You can use the transaction log to review modifications to your table and query previous table versions. See  [Work with Delta Lake table history](history.html)  .           Delta Lake schema enhancements --------------------------------     Delta Lake validates schema on write, ensuring that all data written to a table matches the requirements you’ve set.     * [Delta Lake schema validation](schema-validation.html) * [Constraints on Databricks](../tables/constraints.html) * [Use Delta Lake generated columns](generated-columns.html) * [Enrich Delta Lake tables with custom metadata](custom-metadata.html)         Managing files and indexing data with Delta Lake --------------------------------------------------     Databricks sets many default parameters for Delta Lake that impact the size of data files and number of table versions that are retained in history. Delta Lake uses a combination of metadata parsing and physical data layout to reduce the number of files scanned to fulfill any query.     * [Data skipping with Z-order indexes for Delta Lake](data-skipping.html) * [Compact data files with optimize on Delta Lake](optimize.html) * [Remove unused data files with vacuum](vacuum.html) * [Configure Delta Lake to control data file size](tune-file-size.html)         Configuring and reviewing Delta Lake settings -----------------------------------------------     Databricks stores all data and metadata for Delta Lake tables in cloud object storage. Many configurations can be set at either the table level or within the Spark session. You can review the details of the Delta table to discover what options are configured.     * [Review Delta Lake table details with describe detail](table-details.html) * [Delta table properties reference](table-properties.html) * [Configure storage credentials for Delta Lake](configure-storage-credentials.html)         Data pipelines using Delta Lake and Delta Live Tables -------------------------------------------------------     Databricks encourages users to leverage a  [medallion architecture](../lakehouse/medallion.html)  to process data through a series of tables as data is cleaned and enriched.  [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL workloads through optimized execution and automated infrastructure deployment and scaling.           Troubleshooting Delta Lake features -------------------------------------     Not all Delta Lake features are in all versions of Databricks Runtime. You can find information about Delta Lake versioning and answers to frequent questions in the following articles:     * [How does Databricks manage Delta Lake feature compatibility?](feature-compatibility.html) * [Generate a manifest file](generate-manifest.html)           Delta Lake API documentation ------------------------------     For most read and write operations on Delta tables, you can use  [Spark SQL](../sql/language-manual/index.html)  or Apache Spark  [DataFrame](../getting-started/dataframes-python.html)  APIs.       For Delta Lake-spefic SQL statements, see  [Delta Lake statements](../sql/language-manual/index.html#delta-lake-statements)  .       Databricks ensures binary compatibility with Delta Lake APIs in Databricks Runtime. To view the Delta Lake API version packaged in each Databricks Runtime version, see the  **System environment**   section on the relevant article in the  [Databricks Runtime release notes](../release-notes/runtime/releases.html)  . Delta Lake APIs exist for Python, Scala, and Java:     * [Python API docs](https://docs.delta.io/latest/api/python/index.html) * [Scala API docs](https://docs.delta.io/latest/api/scala/io/delta/tables/index.html) * [Java API docs](https://docs.delta.io/latest/api/java/index.html)                                                      	Delta Lake is an optimized storage layer that provides the foundation for storing data and tables in the Databricks Lakehouse Platform. It extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale. Delta Lake is the default storage format for all operations on Databricks, and all tables on Databricks are Delta tables by default.	1
no_reaction	['https://docs.databricks.com/release-notes/delta-live-tables/2023/01/index.html']	Can I set the Databricks Runtime of a Delta Live Table?	"      Databricks Runtime 4.1 (Unsupported) ======================================     Databricks released this image in May 2018.        Important       This release was deprecated on January 17, 2019. For more information about the Databricks Runtime deprecation policy and schedule, see  [Databricks runtime support lifecycles](databricks-runtime-ver.html#runtime-support)  .        The following release notes provide information about Databricks Runtime 4.1, powered by Apache Spark.          Delta Lake ------------     Databricks Runtime version 4.1 adds major quality improvements and functionality to Delta Lake. Databricks highly recommends that all Delta Lake customers upgrade to the new runtime. This release remains in Private Preview, but it represents a candidate release in anticipation of the upcoming general availability (GA) release.      ###   Breaking changes   * Databricks Runtime 4.1 includes changes to the transaction protocol to enable new features, such as validation. Tables created with Databricks Runtime 4.1 automatically use the new version and cannot be written to by older versions of Databricks Runtime. You must upgrade existing tables in order to take advantage of these improvements. To upgrade an existing table, first upgrade all jobs that are writing to the table. Then run:        ``` com.databricks.delta.Delta.upgradeTableProtocol(""<path-to-table>"" or ""<tableName>"")  ```      See  [How does Databricks manage Delta Lake feature compatibility?](../../delta/feature-compatibility.html)  for more information. * Writes are now validated against the current schema of the table rather than, as before, automatically adding columns that are missing from the destination table. To enable the previous behavior, set the  `mergeSchema`  option to  `true`  . See  [Delta Lake schema validation](../../delta/schema-validation.html)  . * If you are running earlier versions of Databricks Delta, you must upgrade all jobs before you use Databricks Runtime 4.1. If you see either of these errors, upgrade to Databricks Runtime 4.1:        ``` java.lang.NumberFormatException: For input string: ""00000000000000....crc"" scala.MatchError  ``` * Tables can no longer have columns that differ only by case. * Delta-specific table configurations must now be prefixed with  `delta.`     ###   New features   * **Schema management**   - Databricks Delta now validates appends and overwrites to an existing table to ensure that the schema being written matches the schema.      + Databricks Delta continues to support automatic schema migration.  + Databricks Delta now supports the following DDL to modify schema explicitly:          - `ALTER           TABLE           ADD           COLUMN`    to add new columns to a table   - `ALTER           TABLE           CHANGE           COLUMNS`    to change column ordering   - `ALTER           TABLE           [SET|UNSET]           TBLPROPERTIES`  For details, see  [Delta Lake schema validation](../../delta/schema-validation.html)  . * **Enhanced DDL and table support**      + Full support for table DDL and   `saveAsTable()`   .   `save()`   and   `saveAsTable()`   now have identical semantics.  + All DDL and DML commands support both table name and   `delta.`<path-to-table>``   .  + `SHOW        PARTITIONS`  + `SHOW        COLUMNS`  + `DESC        TABLE`  + Detailed table information - You can see the current reader and writer versions of a table by running   `DESCRIBE        DETAIL`   . See   [How does Databricks manage Delta Lake feature compatibility?](../../delta/feature-compatibility.html)   .  + Table details - Provenance information is now available for each write to a table. The Data sidebar also shows detailed table information and history for Databricks Delta tables. See   [Review Delta Lake table details with describe detail](../../delta/table-details.html)   .  + Streaming tables - Streaming DataFrames can be created using   `spark.readStream.format(""delta"").table(""<table-name>"")`   .  + Append-only tables - Databricks Delta now supports basic data governance. You can block deletes and modifications to a table by setting the   [table property](../../delta/table-properties.html)  `delta.appendOnly=true`   .  + `MERGE        INTO`   source - Adds more comprehensive support to the source query specification of   `MERGE`   . For example, you can specify   `LIMIT`   ,   `ORDER        BY`   and   `INLINE        TABLE`   in the source.  + Full support for Table ACLs.     ###   Performance improvements   * Reduced stats collection overhead - The efficiency of stats collection has been improved and stats are now only collected for a configurable number of columns, set to 32 by default. Databricks Delta write performance has been improved by up to 2x due to the reduction in stats collection overhead. To configure the number of columns, set the  [table property](../../delta/table-properties.html) `delta.dataSkippingNumIndexedCols=<number-of-columns>`  . * Support for limit push down - Statistics are used to limit the number of files scanned for queries that have  `LIMIT`  and predicates over partition columns. This is applicable to queries in notebooks due to the implicit  `limit=1000`  in effect for all notebook commands. * Filter pushdown in the streaming source - Streaming queries now use partitioning when starting a new stream to skip irrelevant data. * Improved parallelism for  `OPTIMIZE`  -  `OPTIMIZE`  now runs as a single Spark task and will use all parallelism available on the cluster (previously was limited to 100 compacted files at a time). * Data Skipping in DML -  `UPDATE`  ,  `DELETE`  , and  `MERGE`  now use statistics when locating files that need to be rewritten. * Randomized S3 prefixes - To avoid hot spots in S3 metadata for large tables, you can now set the  [table property](../../delta/table-properties.html) `delta.randomizeFilePrefixes=true`  . * Decreased checkpoint retention - checkpoints are now retained for two days (history is still retained for 30) to decrease storage costs for the transaction log.     ###   API behavior   * The behavior of  `insertInto(<table-name>)`  in Databricks Delta is the same as other data sources.      + If no mode is specified or   `mode`   is   `ErrorIfExists`   ,   `Ignore`   , or   `Append`   , appends the data in the DataFrame to the Databricks Delta table.  + If   `mode`   is   `Overwrite`   , deletes all data in the existing table and inserts the data from the DataFrame into the Databricks Delta table. * If cached, the destination table of  `MERGE`  must be manually uncached.     ###   Usability improvements   * Workload migration validations - Common mistakes made when migrating workloads to Databricks Delta now throw an exception rather than failing:      + Using   `format(""parquet"")`   to read or write a table.  + Reading or writing directly to a partition (that is,   `/path/to/delta/part=1`   ).  + Vacuuming subdirectories of a table.  + `INSERT        OVERWRITE        DIRECTORY`   using Parquet on a table. * Case insensitive configuration - Options to the DataFrame Reader/Writer and table properties are now case insensitive (including both read path and write path). * Column names - Table column names can now include dots.     ###   Known issues      Warning       Writes to a single table must originate from a single cluster. There is experimental support for writes from multiple clusters in the same workspace. Contact Databricks support if you are interested in trying this feature.      * The inserts of multi-insert statements are in different units of work, rather than the same transaction.     ###   Bug fixes   * An infinite loop when starting a new stream on a fast updating table has been fixed.     ###   Deprecations     Structured Streaming does not handle input that is not an append and throws an exception if any modifications occur on the table being used as a source. Previously you could override this behavior using the  `ignoreFileDeletion`  flag, but it is now deprecated. Instead use  `ignoreDeletes`  or  `ignoreChanges`  . See  [Delta table as a source](../../structured-streaming/delta-lake.html#stream-source)  .          Other Changes and Improvements --------------------------------   * [Query Watchdog](../../clusters/query-watchdog.html)  is enabled for all all-purpose clusters created using the UI. * Improved driver-side performance for the DBIO cache * Improved performance for Parquet decoding via a new native Parquet decoder * Improved performance for common subexpression elimination * Improved data skipping performance for large table joining small tables (fact-dimension table joins) * `display()`  now renders columns containing image data types as rich HTML. * New s3select data source for leveraging  [S3 Select](../../external-data/amazon-s3-select.html) * Improvements to  [Log, load, register, and deploy MLflow models](../../mlflow/models.html)     + Upgraded dbml-local to latest release 0.4.1  + Fixed bug with models exported with   `threshold`   parameter specified  + Added support for exporting   `OneVsRestModel`   ,   `GBTClassificationModel` * Upgraded some installed Python libraries:      + pip: from 9.0.1 to 10.0.0b2  + setuptools: from 38.5.1 to 39.0.1  + tornado: 4.5.3 to 5.0.1  + wheel: 0.30.0 to 0.31.0 * Upgraded several installed R libraries. See  [Installed R Libraries](#installed-r-libraries)  . * Upgraded AWS Java SDK from 1.11.253 to 1.11.313. * Upgraded Azure Data Lake Store SDK from 2.0.11 to 2.2.8. * Upgraded CUDA to 9.0 from 8.0 and CUDNN to 7.0 from 6.0 for GPU clusters. * For GPU clusters, GPU metrics now appear in the Cluster metrics Ganglia UI.       Apache Spark --------------     Databricks Runtime 4.1 includes Apache Spark 2.3.0. This release includes all fixes and improvements included in  [Databricks Runtime 4.0 (Unsupported)](4.0.html)  , as well as the following additional bug fixes and improvements made to Spark:     * [[SPARK-24007]](https://issues.apache.org/jira/browse/SPARK-24007)   [SQL] EqualNullSafe for FloatType and DoubleType might generate a wrong result by codegen. * [[SPARK-23942]](https://issues.apache.org/jira/browse/SPARK-23942)   [PYTHON][SQL] Makes collect in PySpark as action for a query executor listener * [[SPARK-23815]](https://issues.apache.org/jira/browse/SPARK-23815)   [CORE] Spark writer dynamic partition overwrite mode may fail to write output on multi level partition * [[SPARK-23748]](https://issues.apache.org/jira/browse/SPARK-23748)   [SS] Fix SS continuous process doesn’t"	"Based on the context, it is possible to set the Databricks Runtime of a Delta Live Table by upgrading the existing table. To upgrade an existing table, first upgrade all jobs that are writing to the table. Then run: `com.databricks.delta.Delta.upgradeTableProtocol(""<path-to-table>"" or ""<tableName>"")`."	0
no_reaction	['https://docs.databricks.com/sql/admin/serverless.html']	Should I use DBSQL Serverless of Pro?	    Enable serverless SQL warehouses ==================================     With serverless compute, the compute layer exists in your Databricks account rather than your AWS account. Serverless compute is supported for use with Databricks SQL. Admins can create serverless SQL warehouses that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute resources in your Databricks account. Use serverless SQL warehouses with Databricks SQL queries the same way you would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.       If your workspace satisfies the serverless SQL warehouse requirements:     * New SQL warehouses are serverless by default when created from the UI, but you can also create new pro and classic SQL warehouses. * From the SQL Warehouses API or Terraform, Databricks recommends always explicitly set  `enable\_serverless\_compute`  to  `true`  and  `warehouse\_type`  to  `pro`  to create serverless SQL warehouses. For details about the defaults if omitted, see  [SQL Warehouses API](../api/sql-endpoints.html)  and  [Terraform](../../dev-tools/terraform/index.html)  . * You can  [create serverless SQL warehouses with the UI or API, or convert warehouses to serverless](#test)  .     This feature only affects Databricks SQL. It does not affect how Databricks Runtime clusters work with notebooks and jobs in the Data Science & Engineering or Databricks Machine Learning workspace environments.       Serverless SQL warehouses do  **not**   have public IP addresses. For more architectural information, see  [Serverless compute](../../serverless-compute/index.html)  .       Depending on when your Databricks account was created, to create  [serverless SQL warehouses](../../serverless-compute/index.html)  , your organization may need to perform several tasks:           |   Task   |   Who can do this step?   |   Where is this done?   | | --- | --- | --- | |   If your account needs updated  [terms of use](#accept-terms)  , workspace admins are prompted in the Databricks SQL UI.   |   Account owner or account administrator.   |   The account console’s settings page.   | |   If your workspace uses an instance profile for Databricks SQL, you may need to update its role to add a trust relationship.   |   Workspace administrator must confirm which instance profile your workspace uses for Databricks SQL. An AWS administrator with permissions to view and make changes to AWS IAM policies must check the role’s trust relationship policy or make any necessary changes.   |   The workspace’s Databricks SQL settings page and the AWS console.   |     This article describes how to perform these steps. If you are not able to perform all of these roles (for example, you are a workspace admin but not an account admin or you do not have access to your AWS IAM roles), you may need to contact others in your organization to perform some steps.       If acceptance of terms is required for your account to enable serverless compute, any workspace admin that uses Databricks SQL sees a banner on the top of each page to indicate that your account admin must  [accept the terms of use](#accept-terms)  . If the workspace admin is not an account admin, contact your account admin to do this step.        Note       Databricks changed the name from  *SQL endpoint*   to  *SQL warehouse*   because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it’s connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.           Requirements --------------     Account requirements:     * Your Databricks account must be on the E2 version of the platform. * Your Databricks workspace must be on the  [Premium or higher pricing tier](https://databricks.com/product/aws-pricing)   . * Your Databricks account must not be on a free trial. * Your Databricks account must not have the  [compliance security profile](../../security/privacy/security-profile.html)  enabled at the account level.     Workspace requirements:     * Your Databricks account must not have the  [compliance security profile](../../security/privacy/security-profile.html)  enabled at the workspace level for any workspaces that you intend to use with any Serverless compute features such as <ServerlessWarehouses>. * Your workspace must not use an  [external Hive legacy metastore](../../data/metastores/external-hive-metastore.html)  . * Your workspace must not use S3 access policies.     Other feature interactions:     * [Cluster policies, including spot instance policies](../../administration-guide/clusters/policies.html)  are unsupported. * [Customer-managed VPCs](../../administration-guide/cloud-configurations/aws/customer-managed-vpc.html)  are not applicable to compute resources for serverless SQL warehouses. * Although the Serverless data plane for serverless SQL warehouses does not use the  [customer-configurable AWS PrivateLink connectivity for the Classic data plane](../../administration-guide/cloud-configurations/aws/privatelink.html)  , it does use private connectivity to connect to the Databricks  [control plane](../../getting-started/overview.html)  . * Although the serverless data plane does not use the  [secure cluster connectivity relay for the classic data plane](../../security/network/secure-cluster-connectivity.html)  , serverless SQL warehouses do  **not**   have public IP addresses. * Serverless SQL warehouses do not use customer-managed keys for EBS storage encryption, which is an optional part of the  [customer-managed keys for workspace storage](../../security/keys/customer-managed-keys-storage-aws.html)  feature configuration. Disks for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. For example, when serverless SQL warehouses are stopped or scaled down, the VMs and their storage are destroyed. See  [Serverless compute and customer-managed keys](../../security/keys/customer-managed-keys.html#serverless)  .     For a list of regions that support serverless SQL warehouses, see  [Databricks clouds and regions](../../resources/supported-regions.html)  .       Also note that the Databricks  [documentation on cluster size instance types and CPU quotas](create-sql-warehouse.html#cluster-size)  applies only to pro and classic SQL warehouses, not to serverless SQL warehouses.           Serverless quotas -------------------         Serverless quotas are a safety measure for serverless compute. Serverless quotas restrict how many serverless compute resources a customer can have at any given time. The quota is enforced at the regional level for all workspaces in your account. Quotas are enforced only for  [serverless SQL warehouses](#)  . See  [Serverless quotas](../../serverless-compute/serverless-quotas.html)  .           Step 1: If prompted, accept updated account terms of use ----------------------------------------------------------      Important       If your account needs updated  [terms of use](#accept-terms)  , workspace admins are prompted in the Databricks SQL UI. If you are a workspace admin and you do not see a yellow notification when using Databricks SQL, you can skip this step.        If you are not an account owner or account administrator, you cannot perform this step. Contact the account owner or an account administrator before continuing to the next steps in this article.     1. As an account owner or account administrator, go to the  [feature enablement tab of the account console settings page](https://accounts.cloud.databricks.com/settings/feature-enablement)   . 2. Next to  **Enable use of serverless compute**   , click the blue button  **Enable**   .       If the blue button does not appear but there is text that says  **Enabled**   , this step is already complete. Continue to  [Step 2: Confirm or set up an AWS instance profile to use with your serverless SQL warehouses](#aws-instance-profile-setup)  . 3. A pop-up appears about agreeing to applicable terms of use. Click the link to open the applicable terms in a new browser tab. When complete, return to the original tab and click the  **Enable**   button in the pop-up.           Step 2: Confirm or set up an AWS instance profile to use with your serverless SQL warehouses ----------------------------------------------------------------------------------------------     An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. You can optionally  [configure an AWS instance profile](../../aws/iam/instance-profile-tutorial.html)  for Databricks SQL to connect to AWS S3 buckets other than your root bucket.       If you already use an instance profile with Databricks SQL, the role associated with the instance profile needs a Databricks Serverless compute trust relationship statement so that serverless SQL warehouses can use it.       Depending on how and when your instance profile was created, you might not need to modify the role because it may already have the trust relationship. If the instance profile was created in the following ways, it likely has the trust relationship statement:     * After June 24, 2022, your instance profile was created as part of creating a Databricks workspace by using AWS Quickstart. * After June 24, 2022, someone in your organization followed steps in the Databricks article to  [create the instance profile manually](../../aws/iam/instance-profile-tutorial.html)  .     This section describes how to  **confirm or update**   that the role associated with the instance profile has the trust relationship statement. That enables your serverless SQL warehouses to use the role to access your S3 buckets.        Important       To perform these steps, you must be a Databricks workspace admin to confirm which instance profile your workspace uses for Databricks SQL. You must also be an AWS account administrator to check the role’s trust relationship policy or make any necessary changes. If you are not both of these types of admin, contact the appropriate admins in your organization to complete these steps.      1. In the  [admin console](../../administration-guide/workspace/index.html#admin-console)  , click the  **SQL Warehouse Settings**   tab. 2. Look in the  **Data Security**   section for the  **Instance Profile**   field. Confirm whether your workspace is configured to use an AWS instance profile for Databricks SQL to connect to AWS S3 buckets other than your root bucket.      * If you are using an instance profile, its name is visible in the   **Instance Profile**    field. Make a note of it for the next step.  * If the field value is   **None**    , you are not using an   [instance profile](../../aws/iam/instance-profile-tutorial.html)   to access S3 buckets other than your workspace’s root bucket. Setup is complete. Skip to   [Step 3](#test)   . 3. Confirm whether your instance profile name matches the associated role name.      1. In the AWS console, go to the   [IAM service’s Roles tab](https://console.aws.amazon.com/iam/home#/roles)    . It lists all the IAM roles in your account.  2. Click the role with the name that matches the instance profile name in the Databricks SQL admin settings in the   **Data Security**    section for the   **Instance Profile**    field that you found earlier in this section.  3. In the summary area, find the   **Role ARN**    and   **Instance Profile ARNs**    fields.  4. Check if the last part of those two fields have matching names after the final slash. For example:           ![Does instance profile name and role arn name match](../../_images/serverless-compute-aws-console-instance-profile-names.png) 4. If you determined in the previous step that the role name (the text after the last slash in the role ARN) and the instance profile name (the text after the last slash in the instance profile ARN) do not match,  [edit your instance profile registration](../../aws/iam/instance-profile-tutorial.html#edit-instance-profile)  to specify your IAM role ARN.      1. To	Based on the context, if you want to use serverless SQL warehouses, you can create new serverless SQL warehouses with Databricks SQL. Databricks recommends always explicitly set `enable_serverless_compute` to `true` and `warehouse_type` to `pro` to create serverless SQL warehouses. However, the decision to use DBSQL Serverless or Pro depends on your specific use case and requirements.	1
no_reaction	['https://docs.databricks.com/clusters/preemption.html']	How does the spark scheduler work in databricks?	"   p    [org](../../../index.html)   .  [apache](../../index.html)   .  [spark](../index.html)    scheduler   [**](../../../../org/apache/spark/scheduler/index.html ""Permalink"") ==================================================================================  ####    package     scheduler   Spark's scheduling components. This includes the  `org.apache.spark.scheduler.DAGScheduler`   and lower level  `org.apache.spark.scheduler.TaskScheduler`   .     Source    [package.scala](https://github.com/apache/spark/tree/v3.4.0/core/src/main/scala/org/apache/spark/scheduler/package.scala)    Linear Supertypes    [AnyRef](../../../../scala/index.html#AnyRef=Object)   ,    Any    **   **    Ordering   1. Alphabetic 2. By Inheritance   Inherited      1. scheduler 2. AnyRef 3. Any  1. Hide All 2. Show All   Visibility   1. Public 2. All  ###   Type Members  1. [**](../../../../org/apache/spark/scheduler/AccumulableInfo.html ""Permalink"")   case class    [AccumulableInfo](AccumulableInfo.html "":: DeveloperApi :: Information about an org.apache.spark.util.AccumulatorV2 modified during a task or stage."")   extends    Product    with    Serializable     :: DeveloperApi :: Information about an  [org.apache.spark.util.AccumulatorV2](../util/AccumulatorV2.html)   modified during a task or stage.     :: DeveloperApi :: Information about an  [org.apache.spark.util.AccumulatorV2](../util/AccumulatorV2.html)   modified during a task or stage.     Annotations     @DeveloperApi     ()     Note     Once this is JSON serialized the types of  `update`   and  `value`   will be lost and be cast to strings. This is because the user can define an accumulator of any type and it will be difficult to preserve the type in consumers of the event log. This does not apply to internal accumulators that represent task level metrics. 2. [**](../../../../org/apache/spark/scheduler/InputFormatInfo.html ""Permalink"")   class    [InputFormatInfo](InputFormatInfo.html "":: DeveloperApi :: Parses and holds information about inputFormat (and files) specified as a parameter."")   extends    Logging     :: DeveloperApi :: Parses and holds information about inputFormat (and files) specified as a parameter.     :: DeveloperApi :: Parses and holds information about inputFormat (and files) specified as a parameter.     Annotations     @DeveloperApi     () 3. [**](../../../../org/apache/spark/scheduler/JobResult.html ""Permalink"")   sealed     trait    [JobResult](JobResult.html "":: DeveloperApi :: A result of a job in the DAGScheduler."")   extends  [AnyRef](../../../../scala/index.html#AnyRef=Object)    :: DeveloperApi :: A result of a job in the DAGScheduler.     :: DeveloperApi :: A result of a job in the DAGScheduler.     Annotations     @DeveloperApi     () 4. [**](../../../../org/apache/spark/scheduler/MiscellaneousProcessDetails.html ""Permalink"")   class    [MiscellaneousProcessDetails](MiscellaneousProcessDetails.html "":: DeveloperApi :: Stores information about an Miscellaneous Process to pass from the scheduler to SparkListeners."")   extends    Serializable     :: DeveloperApi :: Stores information about an Miscellaneous Process to pass from the scheduler to SparkListeners.     :: DeveloperApi :: Stores information about an Miscellaneous Process to pass from the scheduler to SparkListeners.     Annotations     @DeveloperApi     ()     @Since     (     ""3.2.0""     ) 5. [**](../../../../org/apache/spark/scheduler/SparkListener.html ""Permalink"")   abstract     class    [SparkListener](SparkListener.html "":: DeveloperApi :: A default implementation for SparkListenerInterface that has no-op implementations for all callbacks."")   extends    SparkListenerInterface     :: DeveloperApi :: A default implementation for  `SparkListenerInterface`   that has no-op implementations for all callbacks.     :: DeveloperApi :: A default implementation for  `SparkListenerInterface`   that has no-op implementations for all callbacks.     Note that this is an internal interface which might change in different Spark releases.     Annotations     @DeveloperApi     () 6. [**](../../../../org/apache/spark/scheduler/SparkListenerApplicationEnd.html ""Permalink"")   case class    [SparkListenerApplicationEnd](SparkListenerApplicationEnd.html)   (    time:    Long     )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 7. [**](../../../../org/apache/spark/scheduler/SparkListenerApplicationStart.html ""Permalink"")   case class    [SparkListenerApplicationStart](SparkListenerApplicationStart.html)   (    appName:    String     ,    appId:    Option    [    String    ]    ,    time:    Long     ,    sparkUser:    String     ,    appAttemptId:    Option    [    String    ]    ,    driverLogs:    Option    [    Map    [    String    ,    String    ]] =    [None](../../../../scala/index.html)    ,    driverAttributes:    Option    [    Map    [    String    ,    String    ]] =    [None](../../../../scala/index.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 8. [**](../../../../org/apache/spark/scheduler/SparkListenerBlockManagerAdded.html ""Permalink"")   case class    [SparkListenerBlockManagerAdded](SparkListenerBlockManagerAdded.html)   (    time:    Long     ,    blockManagerId:  [BlockManagerId](../storage/BlockManagerId.html)    ,    maxMem:    Long     ,    maxOnHeapMem:    Option    [    Long    ] =    [None](../../../../scala/index.html)    ,    maxOffHeapMem:    Option    [    Long    ] =    [None](../../../../scala/index.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 9. [**](../../../../org/apache/spark/scheduler/SparkListenerBlockManagerRemoved.html ""Permalink"")   case class    [SparkListenerBlockManagerRemoved](SparkListenerBlockManagerRemoved.html)   (    time:    Long     ,    blockManagerId:  [BlockManagerId](../storage/BlockManagerId.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 10. [**](../../../../org/apache/spark/scheduler/SparkListenerBlockUpdated.html ""Permalink"")   case class    [SparkListenerBlockUpdated](SparkListenerBlockUpdated.html)   (    blockUpdatedInfo:  [BlockUpdatedInfo](../storage/BlockUpdatedInfo.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 11. [**](../../../../org/apache/spark/scheduler/SparkListenerEnvironmentUpdate.html ""Permalink"")   case class    [SparkListenerEnvironmentUpdate](SparkListenerEnvironmentUpdate.html)   (    environmentDetails:    Map    [    String    ,    Seq    [(    String    ,    String    )]]    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 12. [**](../../../../org/apache/spark/scheduler/SparkListenerEvent.html ""Permalink"")   trait    [SparkListenerEvent](SparkListenerEvent.html)   extends  [AnyRef](../../../../scala/index.html#AnyRef=Object)    Annotations     @DeveloperApi     ()     @JsonTypeInfo     () 13. [**](../../../../org/apache/spark/scheduler/SparkListenerExecutorAdded.html ""Permalink"")   case class    [SparkListenerExecutorAdded](SparkListenerExecutorAdded.html)   (    time:    Long     ,    executorId:    String     ,    executorInfo:  [ExecutorInfo](cluster/ExecutorInfo.html)    )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     () 14. [**](../../../../org/apache/spark/scheduler/SparkListenerExecutorExcluded.html ""Permalink"")   case class    [SparkListenerExecutorExcluded](SparkListenerExecutorExcluded.html)   (    time:    Long     ,    executorId:    String     ,    taskFailures:    Int     )     extends  [SparkListenerEvent](SparkListenerEvent.html)   with    Product    with    Serializable     Annotations     @DeveloperApi     ()     @Since     (     ""3.1.0""     ) 15. [**](../../../../org/apache/spark/scheduler/SparkListenerExecutorExcludedForStage.html ""Permalink"")   case class    [SparkListenerExecutorExcludedForStage](SparkListenerExecutorExcludedForStage.html)   (    time:    Long     ,    executorId:    String     ,    taskFailures:    Int     ,    stageId:   "	The Spark scheduler in Databricks includes the `org.apache.spark.scheduler.DAGScheduler` and lower level `org.apache.spark.scheduler.TaskScheduler`. The DAGScheduler is responsible for scheduling stages of tasks based on their dependencies, while the TaskScheduler is responsible for assigning tasks to workers.	0
no_reaction	['https://docs.databricks.com/files/download-internet-files.html']	how can i load csv file to databricks?	"      CSV file ==========     This article provides examples for reading and writing to CSV files with Databricks using Python, Scala, R, and SQL.        Note       You can use SQL to read CSV data directly or by using a temporary view. Databricks recommends using a temporary view. Reading the CSV file directly has the following drawbacks:     * You can’t  [specify data source options](#options)  . * You can’t  [specify the schema](#specify-schema)  for the data.     See  [Examples](#examples)  .         Options ---------     You can configure several options for CSV file data sources. See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#csv(path:String):Unit) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html#pyspark.sql.DataFrameWriter.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#csv(path:String):Unit)         Work with malformed CSV records ---------------------------------     When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:     * `PERMISSIVE`  (default): nulls are inserted for fields that could not be parsed correctly * `DROPMALFORMED`  : drops lines that contain fields that could not be parsed * `FAILFAST`  : aborts the reading if any malformed data is found     To set the mode, use the  `mode`  option.        ``` diamonds\_df = (spark.read   .format(""csv"")   .option(""mode"", ""PERMISSIVE"")   .load(""/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv"") )  ```      In the  `PERMISSIVE`  mode it is possible to inspect the rows that could not be parsed correctly using one of the following methods:     * You can provide a custom path to the option  `badRecordsPath`  to record corrupt records to a file. * You can add the column  `\_corrupt\_record`  to the schema provided to the DataFrameReader to review corrupt records in the resultant DataFrame.      Note       The  `badRecordsPath`  option takes precedence over  `\_corrupt\_record`  , meaning that malformed rows written to the provided path do not appear in the resultant DataFrame.       Default behavior for malformed records changes when using the  [rescued data column](#rescued-data)  .         ###   Find malformed rows notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-corrupt-record.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                    Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.3 (Unsupported)](../release-notes/runtime/8.3.html)  and above.        When using the  `PERMISSIVE`  mode, you can enable the rescued data column to capture any data that wasn’t parsed because one or more fields in a record have one of the following issues:     * Absent from the provided schema. * Does not match the data type of the provided schema. * Has a case mismatch with the field names in the provided schema.     The rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set(""spark.databricks.sql.rescuedDataColumn.filePath.enabled"",     ""false"")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name when reading data, such as  `\_rescued\_data`  with  `spark.read.option(""rescuedDataColumn"",     ""\_rescued\_data"").format(""csv"").load(<path>)`  .       The CSV parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors.       When  `rescuedDataColumn`  is used in  `PERMISSIVE`  mode, the following rules apply to  [corrupt records](#corrupt-records)  :     * The first row of the file (either a header row or a data row) sets the expected row length. * A row with a different number of columns is considered incomplete. * Data type mismatches are not considered corrupt records. * Only incomplete and malformed CSV records are considered corrupt and recorded to the  `\_corrupt\_record`  column or  `badRecordsPath`  .       Examples ----------     These examples use the  [diamonds dataset](../dbfs/databricks-datasets.html)  . Specify the path to the dataset as well as any options that you would like.        In this section:     * [Read file in any language](#read-file-in-any-language) * [Specify schema](#specify-schema) * [Pitfalls of reading a subset of columns](#pitfalls-of-reading-a-subset-of-columns)     ###  [Read file in any language](#id1)     This notebook shows how to read a file, display sample data, and print the data schema using Scala, R, Python, and SQL.        ####   Read CSV files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Specify schema](#id2)     When the schema of the CSV file is known, you can specify the desired schema to the CSV reader with the  `schema`  option.        ####   Read CSV files with schema notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-schema.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Pitfalls of reading a subset of columns](#id3)     The behavior of the CSV parser depends on the set of columns that are read. If the specified schema is incorrect, the results might differ considerably depending on the subset of columns that is accessed. The following notebook presents the most common pitfalls.        ####   Caveats of reading a subset of columns of a CSV file notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-column-subset.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                  "	To load a CSV file to Databricks, you can use Python, Scala, R, or SQL. Databricks recommends using a temporary view to read CSV data directly. You can configure several options for CSV file data sources, such as specifying the schema and setting the mode for handling malformed CSV records. To load a CSV file with a specified schema, you can use the `schema` option.	1
no_reaction	['https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html']	Is there a private link option for databricks serverless sql ?	      Enable AWS PrivateLink ========================     This article explains how to use AWS PrivateLink to enable private connectivity between users and their Databricks workspaces and between clusters on the data plane and core services on the control plane within the Databricks workspace infrastructure.        Important       This article mentions the term  *data plane*   , which is the compute layer of the Databricks platform. In the context of this article, data plane refers to the Classic data plane in your AWS account. By contrast, the serverless data plane that supports  [serverless SQL warehouses (Public Preview)](../../../sql/admin/serverless.html)  runs in the Databricks AWS account. To learn more, see  [Serverless compute](../../../serverless-compute/index.html)  .         Overview ----------     AWS PrivateLink provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network. Databricks workspaces on the E2 version of the platform support PrivateLink connections for two connection types:     * **Front-end (user to workspace)**   : A front-end PrivateLink connection allows users to connect to the Databricks web application, REST API, and Databricks Connect API over a VPC interface endpoint. * **Back-end (data plane to control plane)**   : Databricks Runtime clusters in a customer-managed VPC (the  [data plane](../../../getting-started/overview.html)  ) connect to a Databricks workspace’s core services (the  [control plane](../../../getting-started/overview.html)  ) in the Databricks cloud account. Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay. This PrivateLink connection type involves two different VPC interface endpoints because of the two different destination services.     You can implement both front-end and back-end PrivateLink connections or just one of them. This article discusses how to configure either one or both PrivateLink connection types. If you implement PrivateLink for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means Databricks rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.       To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects.       To create configuration objects and create (or update) a workspace, this article describes how to  [use the account console](#account-console)  or  [use the Account API](#account-api)  .       The following table describes important terminology.          |   Terminology   |   Description   | | --- | --- | |   AWS PrivateLink   |   An AWS technology that provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network.   | |   Front-end PrivateLink   |   The PrivateLink connection for users to connect to the Databricks web application, REST API, and Databricks Connect API.   | |   Back-end PrivateLink   |   The PrivateLink connection for the  [data plane](../../../getting-started/overview.html)  in your AWS account to connect to the Databricks  [control plane](../../../getting-started/overview.html)  ).   | |   AWS VPC endpoint service   |   An AWS VPC endpoint service is a PrivateLink-powered service. Each Databricks control plane (typically one per region) publishes two AWS VPC endpoint services for PrivateLink. The workspace VPC endpoint service applies to both a Databricks front-end PrivateLink connection or the Databricks back-end PrivateLink connection for REST APIs. Databricks publishes another VPC endpoint service for its  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay.   | |   AWS VPC endpoint   |   An  [AWS VPC interface endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html)   enables private connections between your VPC and VPC endpoint services powered by AWS PrivateLink. You must create AWS VPC interface endpoints and then register them with Databricks. Registering a VPC endpoint creates a Databricks-specific object called a VPC endpoint registration that references the AWS VPC endpoint.   | |   Databricks network configuration   |   A Databricks object that describes the important information about a  [Customer-managed VPC](customer-managed-vpc.html)  . If you implement any PrivateLink connection (front-end or back-end), your workspace must use a customer-managed VPC. For PrivateLink back-end support only, your network configuration needs an extra property that identifies the VPC endpoints for the back-end connection.   | |   Databricks private access settings object   |   A Databricks object that describes a workspace’s PrivateLink connectivity. You must attach a private access settings object to the workspace during workspace creation, whether using front-end, back-end, or both. It expresses your intent to use AWS PrivateLink with your workspace. It controls your settings for the front-end use case of AWS PrivateLink for public network access. It controls which VPC endpoints are permitted to access your workspace.   | |   Databricks workspace configuration object   |   A Databricks object that describes a workspace. To enable PrivateLink, this object must reference Databricks private access settings object. For back-end PrivateLink, the workspace must also have a Databricks network configuration object with two extra fields that specify which VPC endpoint registrations to use, one for control plane’s secure cluster connectivity relay and the other connects to the workspace to access REST APIs.   |      ###   Updates of existing PrivateLink configuration objects     This article focuses on the main two use cases of creating a new workspace or enabling PrivateLink on a workspace. You also can make other configuration changes to related objects using the UI or API:     * You can upgrade a workspace’s PrivateLink support to add support for front-end, back-end, or both types of connectivity. Add a private access settings object (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ) or update a private access settings object (  [UI](../../account-settings-e2/private-access-settings.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/replace-private-access-settings)   ). To do so, create a new network configuration with new settings, for example for a new VPC or different PrivateLink support settings, and then update the workspace to use the new network configuration. Note that you cannot remove (downgrade) any existing front-end or back-end PrivateLink support on a workspace. * Add or update a workspace’s registered VPC endpoints by creating a new network configuration object with registered VPC endpoints and then update the workspace’s network configuration (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ). * For more information about what kinds of workspace fields can be changed on failed or running workspaces, see information about this task by using the  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   .     Note that not all related objects can be updated. Where update is not possible, create new objects and set their parent objects to reference the new objects. The following rules apply both to the account console UI and the Account API:           |   Object   |   Can be created   |   Can be updated   | | --- | --- | --- | |   Workspace configurations   |   Yes   |   Yes   | |   Private access settings   |   Yes   |   Yes   | |   Network configurations   |   Yes   |  **No**   | |   VPC endpoint registrations   |   Yes   |  **No**   |     To update CIDR ranges on an existing VPC, see  [Updating CIDRs](customer-managed-vpc.html#update-cidr)  .       ###   Network flow     The following diagram shows the network flow in a typical implementation.      ![PrivateLink network architecture](../../../_images/privatelink-network.png)             Requirements --------------    **Databricks account**     * Your Databricks account is on the E2 version of the platform. * Your Databricks account is on the  [Enterprise pricing tier](https://databricks.com/product/aws-pricing)   . * You have your Databricks account ID. Get your account ID from the  [account console](../../account-settings/index.html#account-id)  .    **Databricks workspace**     * Your workspace must be in an  [AWS region that supports the E2 version of the platform](../../../resources/supported-regions.html)  . However, the  `us-west-1`  region does not support PrivateLink even for workspaces on the E2 version of the platform. * Your Databricks workspace must use  [Customer-managed VPC](customer-managed-vpc.html)  to add any PrivateLink connection (even a front-end-only connection). Note that you cannot update an existing workspace with a Databricks-managed VPC and change it to use a customer-managed VPC. * If you implement the back-end PrivateLink connection, your Databricks workspace must use  [Secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  , which is the default for new workspaces on the E2 version of the platform. To add back-end PrivateLink to an older existing workspace that does not use secure cluster connectivity, contact your Databricks representative.    **AWS account permissions**     * If you are the user who sets up PrivateLink, you must have all necessary AWS permissions to provision a Databricks workspace and to provision new VPC endpoints for your workspace.    **Network architecture**     * To implement the front-end PrivateLink connection to access the workspace from your on-premises network, add private connectivity from the on-premises network to an AWS VPC using either Direct Connect or VPN. * For guidance for other network objects, see  [Step 1: Configure AWS network objects](#create-vpc)  .         Step 1: Configure AWS network objects ---------------------------------------         You can use the AWS Management Console to create these objects or automate the process with tools such as the  [Terraform provider for networks](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/mws_networks)   .       To configure a VPC, subnets, and security groups:     1. Set up a VPC for your workspace if you haven’t already done so. You may re-use a VPC from another workspace, but you must create separate subnets for each workspace. Every workspace requires at least two private subnets.      1. To create a VPC, see   [Customer-managed VPC](customer-managed-vpc.html)   . If you are updating a workspace for PrivateLink rather than creating a new workspace, note that the workspace must already be using a customer-managed VPC.  2. On your VPC, ensure that you enable both of the settings   **DNS Hostnames**    and   **DNS resolution**    .  3. Ensure that the network ACLs for the subnets have   **bidirectional**    (outbound and inbound) rules that allow TCP access to 0.0.0.0/0 for these ports:          * 443: for Databricks infrastructure, cloud data sources, and library repositories   * 3306: for the metastore   * 6666: for PrivateLink   * 2443: only for use with compliance security profile     Important            If your	Yes, there is a private link option for Databricks Serverless SQL. The article explains how to use AWS PrivateLink to enable private connectivity between users and their Databricks workspaces and between clusters on the data plane and core services on the control plane within the Databricks workspace infrastructure. Databricks workspaces on the E2 version of the platform support PrivateLink connections for two connection types: front-end (user to workspace) and back-end (data plane to control plane). To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects.	1
no_reaction	['https://docs.databricks.com/machine-learning/automl/train-ml-model-automl-ui.html']	automl regression evaluation metrics	    Train ML models with the Databricks AutoML UI ===============================================     This article demonstrates how to train a machine learning model using  [AutoML](index.html)  and the Databricks Machine Learning UI. The AutoML UI steps you through the process of training a classification, regression or forecasting model on a dataset.       To access the UI:     1. Select Machine Learning from the persona switcher at the top of the left sidebar. 2. In the sidebar, click Create > AutoML Experiment.       You can also create a new AutoML experiment from the  [Experiments page](../../mlflow/tracking.html#mlflow-experiments)  .       The  **Configure AutoML experiment page**   displays. On this page, you configure the AutoML process, specifying the dataset, problem type, target or label column to predict, metric to use to evaluate and score the experiment runs, and stopping conditions.      Requirements --------------     See  [Requirements](index.html#requirement)  for AutoML experiments.         Set up classification or regression problems ----------------------------------------------     You can set up a classification or regression problem using the AutoML UI with the following steps:     1. In the  **Compute**   field, select a cluster running Databricks Runtime ML. 2. From the  **ML problem type**   drop-down menu, select  **Regression**   or  **Classification**   . If you are trying to predict a continuous numeric value for each observation, such as annual income, select regression. If you are trying to assign each observation to one of a discrete set of classes, such as good credit risk or bad credit risk, select classification. 3. Under  **Dataset**   , select  **Browse**   . 4. Navigate to the table you want to use and click  **Select**   . The table schema appears.       For classification and regression problems only, you can specify which  [columns to include in training](#column-select)  and select  [custom imputation methods](#impute-missing-values)  . 5. Click in the  **Prediction target**   field. A drop-down appears listing the columns shown in the schema. Select the column you want the model to predict. 6. The  **Experiment name**   field shows the default name. To change it, type the new name in the field.     You can also:     * Specify  [additional configuration options](#advanced-config)  . * Use  [existing feature tables in Feature Store to augment the original input dataset](#feature-store)  .       Set up forecasting problems -----------------------------     You can set up a forecasting problem using the AutoML UI with the following steps:     1. In the  **Compute**   field, select a cluster running Databricks Runtime 10.0 ML or above. 2. From the  **ML problem type**   drop-down menu, select  **Forecasting**   . 3. Under  **Dataset**   , click  **Browse**   . Navigate to the table you want to use and click  **Select**   . The table schema appears. 4. Click in the  **Prediction target**   field. A dropdown menu appears listing the columns shown in the schema. Select the column you want the model to predict. 5. Click in the  **Time column**   field. A drop-down appears showing the dataset columns that are of type  `timestamp`  or  `date`  . Select the column containing the time periods for the time series. 6. For multi-series forecasting, select the column(s) that identify the individual time series from the  **Time series identifiers**   drop-down. AutoML groups the data by these columns as different time series and trains a model for each series independently. If you leave this field blank, AutoML assumes that the dataset contains a single time series. 7. In the  **Forecast horizon and frequency**   fields, specify the number of time periods into the future for which AutoML should calculate forecasted values. In the left box, enter the integer number of periods to forecast. In the right box, select the units. .. note :: To use Auto-ARIMA, the time series must have a regular frequency (that is, the interval between any two points must be the same throughout the time series). The frequency must match the frequency unit specified in the API call or in the AutoML UI. AutoML handles missing time steps by filling in those values with the previous value. 8. In Databricks Runtime 10.5 ML and above, you can save prediction results. To do so, specify a database in the  **Output Database**   field. Click  **Browse**   and select a database from the dialog. AutoML writes the prediction results to a table in this database. 9. The  **Experiment name**   field shows the default name. To change it, type the new name in the field.     You can also:     * Specify  [additional configuration options](#advanced-config)  . * Use  [existing feature tables in Feature Store to augment the original input dataset](#feature-store)  .      ###   Use existing feature tables from Databricks Feature Store     In Databricks Runtime 11.3 LTS ML and above, you can use feature tables in Databricks Feature Store to expand the input training dataset for your classification and regression problems.       In Databricks Runtime 12.2 LTS ML and above, you can use feature tables in Databricks Feature Store to expand the input training dataset for all of your AutoML problems: classification, regression, and forecasting.       To create a feature table, see  [Create a feature table in Databricks Feature Store](../feature-store/feature-tables.html#create-feature-table)  .       After you finish configuring your AutoML experiment, you can select a features table with the following steps:     1. Click  **Join features (optional)**   .      ![Select Join features button](../../_images/automl-join-features.png) 2. On the  **Join Additional Features**   page, select a feature table in the  **Feature Table**   field. 3. For each  **Feature table primary key**   , select the corresponding lookup key. The lookup key should be a column in the training dataset you provided for your AutoML experiment. 4. For  [time series feature tables](../feature-store/time-series.html)  , select the corresponding timestamp lookup key. Similarly, the timestamp lookup key should be a column in the training dataset you provided for your AutoML experiment.      ![Select primary key and lookup tables](../../_images/automl-feature-store-lookup-key.png) 5. To add more feature tables, click  **Add another Table**   and repeat the above steps.          Advanced configurations -------------------------     Open the  **Advanced Configuration (optional)**   section to access these parameters.     * The evaluation metric is the  [primary metric](train-ml-model-automl-api.html#classification-regression)  used to score the runs. * In Databricks Runtime 10.3 ML and above, you can exclude training frameworks from consideration. By default, AutoML trains models using frameworks listed under  [AutoML algorithms](how-automl-works.html#automl-algorithm)  . * You can edit the stopping conditions. Default stopping conditions are:      + For forecasting experiments, stop after 120 minutes.  + In Databricks Runtime 10.5 ML and below, for classification and regression experiments, stop after 60 minutes or after completing 200 trials, whichever happens sooner. For Databricks Runtime 11.0 ML and above, the number of trials is not used as a stopping condition.  + In Databricks Runtime 10.1 ML and above, for classification and regression experiments, AutoML incorporates early stopping; it stops training and tuning models if the validation metric is no longer improving. * In Databricks Runtime 10.1 ML and above, you can select a  [time column](how-automl-works.html#control-automl-split)  to split the data for training, validation, and testing in chronological order (applies only to classification and regression). * In the  **Data directory**   field, you can enter a  [DBFS](../../dbfs/index.html)  location where the training dataset is saved. If you leave the field blank, the training dataset is saved as an  [MLflow artifact](../../security/auth-authz/access-control/workspace-acl.html#mlflow-artifact-permissions)  .         Column selection ------------------      Note       This functionality is only available for classification and regression problems        In Databricks Runtime 10.3 ML and above, you can specify which columns AutoML should use for training. To exclude a column, uncheck it in the  **Include**   column.       You cannot drop the column selected as the prediction target or as the  [time column](how-automl-works.html#control-automl-split)  to split the data.       By default, all columns are included.           Imputation of missing values ------------------------------     In Databricks Runtime 10.4 LTS ML and above, you can specify how null values are imputed. In the UI, select a method from the drop-down in the  **Impute with**   column in the table schema.       By default, AutoML selects an imputation method based on the column type and content.        Note       If you specify a non-default imputation method, AutoML does not perform  [semantic type detection](how-automl-works.html#semantic-detection)  .          Run the experiment and monitor the results --------------------------------------------     To start the AutoML experiment, click  **Start AutoML**   . The experiment starts to run, and the AutoML training page appears. To refresh the runs table, click  ![Refresh button](../../_images/automl-refresh-button.png)  .       From this page, you can:     * Stop the experiment at any time. * Open the data exploration notebook. * Monitor runs. * Navigate to the run page for any run.     With Databricks Runtime 10.1 ML and above, AutoML displays warnings for potential issues with the dataset, such as unsupported column types or high cardinality columns.        Note       Databricks does its best to indicate potential errors or issues. However, this may not be comprehensive and may not capture issues or errors for which you may be searching. Please make sure to conduct your own reviews as well.        To see any warnings for the dataset, click the  **Warnings**   tab on the training page, or on the experiment page after the experiment has completed.      ![AutoML warnings](../../_images/automl-alerts.png)    When the experiment completes, you can:     * [Register and deploy](#register-deploy-automl-ui)  one of the models with MLflow. * Select  **View notebook for best model**   to review and edit the notebook that created the best model. * Select  **View data exploration notebook**   to open the data exploration notebook. * Search, filter, and sort the runs in the runs table. * See details for any run:      + To open the notebook containing source code for a trial run, click in the   **Source**    column.  + To view results of the run, click in the   **Models**    column or the   **Start Time**    column. The run page appears showing information about the trial run (such as parameters, metrics, and tags) and artifacts created by the run, including the model. This page also includes code snippets that you can use to make predictions with the model.     To return to this AutoML experiment later, find it in the table on the  [Experiments page](../../mlflow/tracking.html#mlflow-experiments)  . The results of each AutoML experiment, including the data exploration and training notebooks, are stored in a  `databricks\_automl`  folder in the  [home folder](../../workspace/workspace-objects.html#home-folder)  of the user who ran the experiment.           Register and deploy a model -----------------------------     You can register and deploy your model with the AutoML UI:     1. Select the link in the  **Models**   column for the model to register. When a run completes, the best model (based on the primary metric) is the top row.       The artifacts section of the  [run page](../../mlflow/tracking.html)	AutoML regression evaluation metrics depend on the primary metric used to score the runs. The primary metric is specified in the advanced configuration section of the AutoML experiment page. The default stopping conditions for regression experiments are to stop after 60 minutes or after completing 200 trials, whichever happens sooner.	1
no_reaction	['https://docs.databricks.com/error-messages/group-by-aggregate-error-class.html']	can you put the filter in the aggregate function	"  ![]()   Filter ========  `filter.Rd`    Filter the rows of a SparkDataFrame according to a given condition.     Usage -------  ``` filter(x, condition)  where(x, condition)  # S4 method for SparkDataFrame,characterOrColumn filter(x, condition)  # S4 method for SparkDataFrame,characterOrColumn where(x, condition) ```   Arguments -----------   x     A SparkDataFrame to be sorted.     condition     The condition to filter on. This may either be a Column expression or a string containing a SQL statement     Value -------   A SparkDataFrame containing only the rows that meet the condition.     Note ------   filter since 1.4.0     where since 1.4.0     See also ----------   Other SparkDataFrame functions:  `[SparkDataFrame-class](SparkDataFrame.html)`  ,  `[agg](summarize.html)   ()`   ,  `[alias](alias.html)   ()`   ,  `[arrange](arrange.html)   ()`   ,  `[as.data.frame](as.data.frame.html)   ()`   ,  `[attach,SparkDataFrame-method](attach.html)`  ,  `[broadcast](broadcast.html)   ()`   ,  `[cache](cache.html)   ()`   ,  `[checkpoint](checkpoint.html)   ()`   ,  `[coalesce](coalesce.html)   ()`   ,  `[collect](collect.html)   ()`   ,  `[colnames](columns.html)   ()`   ,  `[coltypes](coltypes.html)   ()`   ,  `[createOrReplaceTempView](createOrReplaceTempView.html)   ()`   ,  `[crossJoin](crossJoin.html)   ()`   ,  `[cube](cube.html)   ()`   ,  `[dapplyCollect](dapplyCollect.html)   ()`   ,  `[dapply](dapply.html)   ()`   ,  `[describe](describe.html)   ()`   ,  `[dim](dim.html)   ()`   ,  `[distinct](distinct.html)   ()`   ,  `[dropDuplicates](dropDuplicates.html)   ()`   ,  `[dropna](nafunctions.html)   ()`   ,  `[drop](drop.html)   ()`   ,  `[dtypes](dtypes.html)   ()`   ,  `[exceptAll](exceptAll.html)   ()`   ,  `[except](except.html)   ()`   ,  `[explain](explain.html)   ()`   ,  `[first](first.html)   ()`   ,  `[gapplyCollect](gapplyCollect.html)   ()`   ,  `[gapply](gapply.html)   ()`   ,  `[getNumPartitions](getNumPartitions.html)   ()`   ,  `[group\_by](groupBy.html)   ()`   ,  `[head](head.html)   ()`   ,  `[hint](hint.html)   ()`   ,  `[histogram](histogram.html)   ()`   ,  `[insertInto](insertInto.html)   ()`   ,  `[intersectAll](intersectAll.html)   ()`   ,  `[intersect](intersect.html)   ()`   ,  `[isLocal](isLocal.html)   ()`   ,  `[isStreaming](isStreaming.html)   ()`   ,  `[join](join.html)   ()`   ,  `[limit](limit.html)   ()`   ,  `[localCheckpoint](localCheckpoint.html)   ()`   ,  `[merge](merge.html)   ()`   ,  `[mutate](mutate.html)   ()`   ,  `[ncol](ncol.html)   ()`   ,  `[nrow](nrow.html)   ()`   ,  `[persist](persist.html)   ()`   ,  `[printSchema](printSchema.html)   ()`   ,  `[randomSplit](randomSplit.html)   ()`   ,  `[rbind](rbind.html)   ()`   ,  `[rename](rename.html)   ()`   ,  `[repartitionByRange](repartitionByRange.html)   ()`   ,  `[repartition](repartition.html)   ()`   ,  `[rollup](rollup.html)   ()`   ,  `[sample](sample.html)   ()`   ,  `[saveAsTable](saveAsTable.html)   ()`   ,  `[schema](schema.html)   ()`   ,  `[selectExpr](selectExpr.html)   ()`   ,  `[select](select.html)   ()`   ,  `[showDF](showDF.html)   ()`   ,  `[show](show.html)   ()`   ,  `[storageLevel](storageLevel.html)   ()`   ,  `[str](str.html)   ()`   ,  `[subset](subset.html)   ()`   ,  `[summary](summary.html)   ()`   ,  `[take](take.html)   ()`   ,  `[toJSON](toJSON.html)   ()`   ,  `[unionAll](unionAll.html)   ()`   ,  `[unionByName](unionByName.html)   ()`   ,  `[union](union.html)   ()`   ,  `[unpersist](unpersist.html)   ()`   ,  `[unpivot](unpivot.html)   ()`   ,  `[withColumn](withColumn.html)   ()`   ,  `[withWatermark](withWatermark.html)   ()`   ,  `[with](with.html)   ()`   ,  `[write.df](write.df.html)   ()`   ,  `[write.jdbc](write.jdbc.html)   ()`   ,  `[write.json](write.json.html)   ()`   ,  `[write.orc](write.orc.html)   ()`   ,  `[write.parquet](write.parquet.html)   ()`   ,  `[write.stream](write.stream.html)   ()`   ,  `[write.text](write.text.html)   ()`    Other subsetting functions:  `[select](select.html)   ()`   ,  `[subset](subset.html)   ()`    Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() path <- ""path/to/file.json"" df <- [read.json](read.json.html)(path) filter(df, ""col1 > 0"") filter(df, df$col2 != ""abcdefg"") }  ```  "	No, you cannot put the filter in the aggregate function.	0
no_reaction	['https://docs.databricks.com/delta/idempotent-writes.html']	how do I ensure the idempotency for streaming writes	"    Enable idempotent writes across jobs ======================================     Sometimes a job that writes data to a Delta table is restarted due to various reasons (for example, job encounters a failure). The failed job may or may not have written the data to Delta table before terminating. In the case where the data is written to the Delta table, the restarted job writes the same data to the Delta table which results in duplicate data.       To address this, Delta tables support the following  `DataFrameWriter`  options to make the writes idempotent:     * `txnAppId`  : A unique string that you can pass on each  `DataFrame`  write. For example, this can be the name of the job. * `txnVersion`  : A monotonically increasing number that acts as transaction version. This number needs to be unique for data that is being written to the Delta table(s). For example, this can be the epoch seconds of the instant when the query is attempted for the first time. Any subsequent restarts of the same job needs to have the same value for  `txnVersion`  .     The above combination of options needs to be unique for each new data that is being ingested into the Delta table and the  `txnVersion`  needs to be higher than the last data that was ingested into the Delta table. For example:     * Last successfully written data contains option values as  `dailyETL:23423`  (  `txnAppId:txnVersion`  ). * Next write of data should have  `txnAppId     =     dailyETL`  and  `txnVersion`  as at least  `23424`  (one more than the last written data  `txnVersion`  ). * Any attempt to write data with  `txnAppId     =     dailyETL`  and  `txnVersion`  as  `23422`  or less is ignored because the  `txnVersion`  is less than the last recorded  `txnVersion`  in the table. * Attempt to write data with  `txnAppId:txnVersion`  as  `anotherETL:23424`  is successful writing data to the table as it contains a different  `txnAppId`  compared to the same option value in last ingested data.      Warning       This solution assumes that the data being written to Delta table(s) in multiple retries of the job is the same. If a write attempt to a Delta table succeeds but due to some downstream failure there is a second write attempt with same txn options but different data, then that second write attempt will be ignored. This can cause unexpected results.        See the following code for an example:         ``` app\_id = ... # A unique string that is used as an application ID. version = ... # A monotonically increasing number that acts as transaction version.  dataFrame.write.option(""txnVersion"", version).option(""txnAppId"", app\_id).save(...)  ```       ``` val appId = ... // A unique string that is used as an application ID. version = ... // A monotonically increasing number that acts as transaction version.  dataFrame.write.option(""txnVersion"", version).option(""txnAppId"", appId).save(...)  ```                                                       "	To ensure idempotency for streaming writes, Delta tables support the `txnAppId` and `txnVersion` options in the `DataFrameWriter`. `txnAppId` is a unique string that you can pass on each `DataFrame` write, and `txnVersion` is a monotonically increasing number that acts as a transaction version. These options need to be unique for each new data that is being ingested into the Delta table, and the `txnVersion` needs to be higher than the last data that was ingested into the Delta table.	1
no_reaction	['https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-location.html']	How do I create an External Location?	    CREATE EXTERNAL LOCATION ==========================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Creates an external location with the specified name. If a location with the same name already exists, an exception is thrown.        Syntax --------      ``` CREATE EXTERNAL LOCATION [IF NOT EXISTS] location\_name     URL url     WITH (STORAGE CREDENTIAL credential\_name)     [COMMENT comment]  ```        Parameters ------------   * **[location\_name](sql-ref-names.html#location-name)**      The name of the location to be created. * **IF NOT EXISTS**       Creates a location with the given name if it does not exist. If a location with the same name already exists, nothing will happen. * **url**       A  `STRING`  literal with the location of the cloud storage described as an absolute URL. * **[credential\_name](sql-ref-names.html#credential-name)**      The named credential used to connect to this location. * **comment**       An optional description for the location, or  `NULL`  . The default is  `NULL`  .       Examples ----------      ``` -- Create a location accessed using the s3\_remote\_cred credential > CREATE EXTERNAL LOCATION s3\_remote URL 's3://us-east-1/location'  WITH (STORAGE CREDENTIAL s3\_remote\_cred)  COMMENT 'Default source for AWS exernal data';  ```        Related articles ------------------   * [ALTER EXTERNAL LOCATION](sql-ref-syntax-ddl-alter-location.html) * [DESCRIBE EXTERNAL LOCATION](sql-ref-syntax-aux-describe-location.html) * [DROP EXTERNAL LOCATION](sql-ref-syntax-ddl-drop-location.html) * [External locations and storage credentials](sql-ref-external-locations.html) * [SHOW EXTERNAL LOCATIONS](sql-ref-syntax-aux-show-locations.html)                                                      	To create an external location, you can use the `CREATE EXTERNAL LOCATION` command with the specified name, URL, and storage credential. If a location with the same name already exists, an exception is thrown. The syntax is as follows:  ``` CREATE EXTERNAL LOCATION [IF NOT EXISTS] location_name     URL url     WITH (STORAGE CREDENTIAL credential_name)     [COMMENT comment] ```  You can also provide an optional description for the location.	1
no_reaction	['https://docs.databricks.com/clusters/init-scripts.html']	how do i load maven packages into databricks clusters using init scripts?	Title: Init script fails to download Maven JAR  Description: Cluster init script fails to download a Maven JAR when trying to install a library.  Answer: Problem You have an init script that is attempting to install a library via Maven, but it fails when trying to download a JAR. https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/0.4.1/rapids-4-spark_2.12-0.4.1.jar%0D Resolving repo1.maven.org (repo1.maven.org)... 151.101.248.209 Connecting to repo1.maven.org (repo1.maven.org)|151.101.248.209|:443... connected. HTTP request sent, awaiting response... 404 Not Found 2021-07-30 01:31:11 ERROR 404: Not Found. Cause There is a carriage return (%0D) character at the end of one or more of the lines in the init script. This is usually caused by editing a file in Windows and then uploading it to your Databricks workspace without removing the excess carriage returns. Solution Remove the Windows carriage returns by running dos2unix on the file after you have uploaded it to the workspace. %sh sudo apt-get install dos2unix -y dos2unix file &lt;initscript.sh&gt; Once you have removed the Windows carriage returns from the file, you can configure the init script as normal.	To load Maven packages into Databricks clusters using init scripts, you can create an init script that installs the required libraries via Maven. However, if the init script fails to download a Maven JAR, it may be caused by a carriage return character at the end of one or more lines in the script. To fix this, you can remove the Windows carriage returns by running dos2unix on the file after uploading it to the workspace. Once the carriage returns are removed, you can configure the init script as normal.	0
