Reaction	Sources	Question	Document	Answer	Context_Relevance_Label	Answer_Faithfulness_Label	Answer_Relevance_Label
no_reaction	https://docs.databricks.com/delta-live-tables/tutorial-sql.html	Give a tutorial of declaring a delta live table?	      Delta Live Tables CLI =======================     You run Databricks Delta Live Tables CLI subcommands by appending them to  `databricks     pipelines`  . These subcommands call the  [Delta Live Tables API](../../delta-live-tables/api-guide.html)  .        ``` databricks pipelines -h  ```       ``` Usage: databricks pipelines [OPTIONS] COMMAND [ARGS]...    Utility to interact with Databricks Delta Live Tables Pipelines.  Options:   -v, --version   0.17.0   --debug         Debug Mode. Shows full stack trace on error.   --profile TEXT  CLI connection profile to use. The default profile is                   "DEFAULT".    -h, --help      Show this message and exit.  Commands:   create  Creates a pipeline.   delete  Deletes the pipeline and cancels any active updates.   deploy  [Deprecated] This command is deprecated, use create and edit           commands instead.  Creates or edits a pipeline specified by the           pipeline settings.    edit    Edits a pipeline.   get     Gets a pipeline's current settings and status.   list    Lists all pipelines and their statuses.   reset   [Deprecated] Use the "start --full-refresh" command instead. Resets           a pipeline so that data can be reprocessed from the beginning.    run     [Deprecated] Use the "start" command instead. Starts a pipeline           update.    start   Starts a pipeline update.   stop    Stops the pipeline by cancelling any active update.  ```       Create a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     create     --help`  .      ###   Usage      ``` databricks pipelines create --settings create-pipeline.json  ```     `create-pipeline.json`  :        ``` {  "name": "Wikipedia pipeline (SQL)",  "storage": "/Users/username/data",  "clusters": [  {  "label": "default",  "autoscale": {  "min\_workers": 1,  "max\_workers": 5  }  }  ],  "libraries": [  {  "notebook": {  "path": "/Users/username/Delta Live Tables quickstart (SQL)"  }  }  ],  "target": "wikipedia\_analysis",  "continuous": false }  ```       ``` Successfully created pipeline: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003 with ID: 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Edit a pipeline -----------------     To display usage documentation, run  `databricks     pipelines     edit     --help`  .        ###   Usage      ``` databricks pipelines edit --settings edit-pipeline.json  ```     `edit-pipeline.json`  :        ``` {  "id": "4ee6b5c2-7017-11ec-90d6-0242ac120003",  "name": "Wikipedia pipeline (SQL)",  "storage": "/Users/username/data",  "clusters": [  {  "label": "default",  "autoscale": {  "min\_workers": 1,  "max\_workers": 3  }  }  ],  "libraries": [  {  "notebook": {  "path": "/Users/username/Delta Live Tables quickstart (SQL)"  }  }  ],  "target": "wikipedia\_quickstart\_data",  "continuous": false }  ```       ``` Successfully edited pipeline settings: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Delete a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     delete     --help`  .        ###   Usage      ``` databricks pipelines delete --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003 deleted  ```         Get information about a pipeline ----------------------------------     To display usage documentation, run  `databricks     pipelines     get     --help`  .        ###   Usage      ``` databricks pipelines get --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` {  "pipeline\_id": "4ee6b5c2-7017-11ec-90d6-0242ac120003",  "spec": {  "id": "4ee6b5c2-7017-11ec-90d6-0242ac120003",  "name": "Wikipedia pipeline (SQL)",  "storage": "/Users/username/data",  "clusters": [  {  "label": "default",  "autoscale": {  "min\_workers": 1,  "max\_workers": 5  }  }  ],  "libraries": [  {  "notebook": {  "path": "/Users/username/Delta Live Tables quickstart (SQL)"  }  }  ],  "target": "wikipedia\_analysis",  "continuous": false  },  "state": "IDLE",  "name": "Wikipedia pipeline (SQL)",  "creator\_user\_name": "username@databricks.com",  "last\_modified": 1641600056265 }  ```         List all pipelines and information on their status ----------------------------------------------------     To display usage documentation, run  `databricks     pipelines     list     --help`  .        ###   Usage      ``` databricks pipelines list  ```       ``` [  {  "pipeline\_id": "8a93e1b4-7024-11ec-90d6-0242ac120003",  "state": "IDLE",  "name": "Retail sales pipeline (SQL)",  "latest\_updates": [  {  "update\_id": "9df03924-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-29T16:18:57.982Z"  },  {  "update\_id": "a57e601c-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-28T18:19:30.371Z"  }  ],  "creator\_user\_name": "user@databricks.com"  },  {  "pipeline\_id": "b46e2670-7024-11ec-90d6-0242ac120003",  "state": "IDLE",  "name": "DLT quickstart (Python)",  "latest\_updates": [  {  "update\_id": "bcd8fa2e-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-12-16T18:19:25.827Z"  },  {  "update\_id": "c2c7a2c8-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-29T22:22:32.586Z"  },  {  "update\_id": "c88d2818-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-29T22:16:18.258Z"  }  ],  "creator\_user\_name": "user@databricks.com"  } ]  ```         Start a pipeline update -------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Started an update c6462e2e-beb7-4c24-a9d3-962eaa19781b for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```        Start a pipeline update with full refresh -------------------------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ###   Usage      ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003 --full-refresh  ```       ``` Started an update ed885d3d-4fc0-44fd-b5dd-59471c789327 with full refresh for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Stop a pipeline update ------------------------     To display usage documentation, run  `databricks     pipelines     stop     --help`  .        ###   Usage      ``` databricks pipelines stop --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Stopped pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```                                                        	To declare a Delta live table, you need to use the Databricks Delta Live Tables CLI subcommands by appending them to `databricks pipelines`. The subcommands call the Delta Live Tables API. You can create a pipeline, edit a pipeline, delete a pipeline, get information about a pipeline, list all pipelines and information on their status, start a pipeline update, start a pipeline update with full refresh, and stop a pipeline update. To display usage documentation, you can run `databricks pipelines -h`.	1	1	1
no_reaction	https://docs.databricks.com/archive/admin-guide/account-console.html	how do I find out who the account owner is	Access the account console (legacy) =====================================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. To view current admin documentation, see  [Manage your Databricks account](../../administration-guide/account-settings/index.html)  .        The account console is where you administer your Databricks account-level configurations. Only the account owner who initially created the Databricks account can log in to the account console. To transfer account owner rights, contact your Databricks account representative.        Note       If your account is on the E2 version of the platform, instead see  [Manage your Databricks account](../../administration-guide/account-settings/index.html)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.        To access the account console when you are viewing a workspace, click  ![User Settings Icon](../../_images/user-settings-icon.png) **Settings**   at the lower left and select  **Manage Account**   .      ![Manage account](../../_images/manage-account.png)    You can also go directly to  <https://accounts.cloud.databricks.com>  .       Enter your account owner email address and password to log in. How you get your initial account owner login ID and password depends on the way you signed up for Databricks. In most cases, you will receive a Welcome email. Follow the link in the email to verify your email address and change your temporary password. When you’ve changed your password, you can open the account console.	To find out who the account owner is, you need to log in to the account console using the account owner's email address and password. Only the account owner who initially created the Databricks account can log in to the account console. If you need to transfer account owner rights, you should contact your Databricks account representative.	1	1	1
no_reaction	https://docs.databricks.com/ingestion/auto-loader/production.html	How to implement CDC using Autoloader using Databricks	    Configure Auto Loader for production workloads ================================================     Databricks recommends that you follow  [the streaming best practices](../../structured-streaming/production.html)  for running Auto Loader in production.       Databricks recommends using  [Auto Loader in Delta Live Tables](dlt.html)  for incremental data ingestion. Delta Live Tables extends functionality in Apache Spark Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:     * Autoscaling compute infrastructure for cost savings * Data quality checks with  [expectations](../../delta-live-tables/expectations.html) * Automatic  [schema evolution](schema.html)  handling * Monitoring via metrics in the  [event log](../../delta-live-tables/observability.html#event-log)      Monitoring Auto Loader ------------------------      ###   Querying files discovered by Auto Loader      Note       The  `cloud\_files\_state`  function is available in Databricks Runtime 10.5 and above.        Auto Loader provides a SQL API for inspecting the state of a stream. Using the  `cloud\_files\_state`  function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from  `cloud\_files\_state`  , providing the checkpoint location associated with an Auto Loader stream.        ``` SELECT \* FROM cloud\_files\_state('path/to/checkpoint');  ```      ###   Listen to stream updates     To further monitor Auto Loader streams, Databricks recommends using Apache Spark’s  [Streaming Query Listener interface](../../structured-streaming/stream-monitoring.html)  .       Auto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the  `numFilesOutstanding`  and  `numBytesOutstanding`  metrics under the  **Raw Data**   tab in the streaming query progress dashboard:        ``` {  "sources" : [  {  "description" : "CloudFilesSource[/path/to/source]",  "metrics" : {  "numFilesOutstanding" : "238",  "numBytesOutstanding" : "163939124006"  }  }  ] }  ```      In Databricks Runtime 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as  `approximateQueueSize`  for AWS and Azure.          Cost considerations ---------------------     When running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.       To reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using  `Trigger.AvailableNow`  instead of running it continuously as long as you don’t have low latency requirements. See  [Configure Structured Streaming trigger intervals](../../structured-streaming/triggers.html)  .       File discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:     * Providing a  `ProcessingTime`  trigger when running Auto Loader continuously in directory listing mode * Architecting file uploads to your storage account in lexical ordering to leverage  [Incremental Listing](directory-listing-mode.html#incremental-listing)  when possible * Using Databricks Runtime 9.0 or later in directory listing mode, especially for deeply nested directories * [Leveraging file notifications](file-notification-mode.html)  when incremental listing is not possible * Using  [resource tags](options.html#file-notification-options)  to tag resources created by Auto Loader to track your costs       Using Trigger.AvailableNow and rate limiting ----------------------------------------------      Note       Available in Databricks Runtime 10.1 for Scala only.       Available in Databricks Runtime 10.2 and above for Python and Scala.        Auto Loader can be scheduled to run in Databricks Jobs as a batch job by using  `Trigger.AvailableNow`  . The  `AvailableNow`  trigger will instruct Auto Loader to process all files that arrived  **before**   the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.       With  `Trigger.AvailableNow`  , file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure  `cloudFiles.maxFilesPerTrigger`  and  `cloudFiles.maxBytesPerTrigger`  to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided  `maxBytesPerTrigger`  . When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.           Event retention -----------------      Note       Available in Databricks Runtime 8.4 and above.        Auto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the  `cloudFiles.maxFileAge`  option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for  `cloudFiles.maxFileAge`  is  `"14     days"`  . Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.        Warning      `cloudFiles.maxFileAge`  is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning  `cloudFiles.maxFileAge`  incorrectly can lead to data quality issues. Therefore, Databricks doesn’t recommend tuning this parameter unless absolutely required.        Trying to tune the  `cloudFiles.maxFileAge`  option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a  `cloudFiles.maxFileAge`  :     * If your stream restarts after a long time, file notification events that are pulled from the queue that are older than  `cloudFiles.maxFileAge`  are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than  `cloudFiles.maxFileAge`  are ignored. * If you use directory listing mode and use  `cloudFiles.maxFileAge`  , for example set to  `"1     month"`  , you stop your stream and restart the stream with  `cloudFiles.maxFileAge`  set to  `"2     months"`  , all files that are older than 1 month, but more recent than 2 months are reprocessed.     The best approach to tuning  `cloudFiles.maxFileAge`  would be to start from a generous expiration, for example,  `"1     year"`  and working downwards to something like  `"9     months"`  . If you set this option the first time you start the stream, you will not ingest data older than  `cloudFiles.maxFileAge`  , therefore, if you want to ingest old data you should not set this option as you start your stream.                                                        	To implement CDC using Autoloader using Databricks, Databricks recommends using Auto Loader in Delta Live Tables for incremental data ingestion. Auto Loader provides a SQL API for inspecting the state of a stream. Using the cloud_files_state function, you can find metadata about files that have been discovered by an Auto Loader stream. Databricks also recommends using Apache Spark’s Streaming Query Listener interface to further monitor Auto Loader streams. To reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using Trigger.AvailableNow instead of running it continuously. File discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode.	0	1	1
no_reaction	https://docs.databricks.com/lakehouse/index.html	what is data lakehouse	What is the Databricks Lakehouse? ===================================     The Databricks Lakehouse combines the ACID transactions and data governance of enterprise data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data. The Databricks Lakehouse keeps your data in your massively scalable cloud object storage in open source data standards, allowing you to use your data however and wherever you want.      * [What are ACID guarantees on Databricks?](acid.html) * [What is the medallion lakehouse architecture?](medallion.html) * [What does it mean to build a single source of truth?](ssot.html) * [Data discovery and collaboration in the lakehouse](collaboration.html) * [Data objects in the Databricks Lakehouse](data-objects.html)       Components of the Databricks Lakehouse ----------------------------------------     The primary components of the Databricks Lakehouse are:     * **[Delta tables](#delta-tables)**  :      + [ACID transactions](acid.html)  + [Data versioning](../delta/history.html)  + [ETL](../ingestion/index.html)  + [Indexing](../delta/data-skipping.html) * **[Unity Catalog](../data-governance/unity-catalog/index.html)**  :      + [Data governance](../data-governance/index.html)  + [Data sharing](../data-sharing/index.html)  + [Data auditing](../data-governance/unity-catalog/audit.html)     By storing data with  [Delta Lake](../delta/index.html)  , you enable downstream data scientists, analysts, and machine learning engineers to leverage the same production data supporting your core ETL workloads as soon as data is processed.       Unity Catalog ensures that you have complete control over who gains access to which data and provides a centralized mechanism for managing all data governance and access controls without needing to replicate your data.         Delta tables --------------     Tables created on Databricks use the Delta Lake protocol by default. When you create a new Delta table:     * Metadata used to reference the table is added to the metastore in the declared schema or database. * Data and table metadata are saved to a directory in cloud object storage.     The metastore reference to a Delta table is technically optional; you can create Delta tables by directly interacting with directory paths using  [Spark APIs](../delta/tutorial.html)  . Some new features that build upon Delta Lake will store additional metadata in the table directory, but all Delta tables have:     * A directory containing table data in the Parquet file format. * A sub-directory  `/\_delta\_log`  that contains metadata about table versions in JSON and Parquet format.     Learn more about  [Data objects in the Databricks Lakehouse](data-objects.html)  .           Unity Catalog ---------------     Unity Catalog unifies data governance and discovery on Databricks. Available in notebooks, jobs, and Databricks SQL, Unity Catalog provides features and UIs that enable workloads and users designed for both data lakes and data warehouse.     * Account-level management of the Unity Catalog metastore means databases, data objects, and permissions can be shared across Databricks workspaces. * You can leverage three tier namespacing (  `<catalog>.<database>.<table>`  ) for organizing and granting access to data. * External locations and storage credentials are also securable objects with similar ACL setting to other data objects. * The  [Data Explorer](../data/index.html)  provides a graphical user interface to explore databases and manage permissions.       Data lakehouse vs. data warehouse vs. data lake -------------------------------------------------     Data warehouses have powered business intelligence (BI) decisions for about 30 years, having evolved as set of design guidelines for systems controlling the flow of data. Enterprise data warehouses optimize queries for BI reports, but can take minutes or even hours to generate results. Designed for data that is unlikely to change with high frequency, data warehouses seek to prevent conflicts between concurrently running queries. Many data warehouses rely on proprietary formats, which often limit support for machine learning. Data warehousing on Databricks leverages the capabilities of a Databricks Lakehouse and Databricks SQL. For more information, see  [What is data warehousing on Databricks?](../sql/index.html)  .       Powered by technological advances in data storage and driven by exponential increases in the types and volume of data, data lakes have come into widespread use over the last decade. Data lakes store and process data cheaply and efficiently. Data lakes are often defined in opposition to data warehouses: A data warehouse delivers clean, structured data for BI analytics, while a data lake permanently and cheaply stores data of any nature in any format. Many organizations use data lakes for data science and machine learning, but not for BI reporting due to its unvalidated nature.       The data lakehouse replaces the current dependency on data lakes and data warehouses for modern data companies that desire:     * Open, direct access to data stored in standard data formats. * Indexing protocols optimized for machine learning and data science. * Low query latency and high reliability for BI and advanced analytics.     By combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the data lakehouse allows data scientists and ML engineers to build models from the same data driving BI reports.	The Databricks Lakehouse combines the ACID transactions and data governance of enterprise data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data. It allows open, direct access to data stored in standard data formats, indexing protocols optimized for machine learning and data science, and low query latency and high reliability for BI and advanced analytics. By combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the data lakehouse allows data scientists and ML engineers to build models from the same data driving BI reports.	1	1	1
no_reaction	https://docs.databricks.com/resources/supported-regions.html	explain databricks on aws	What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf. Databricks is used for processing, storing, cleaning, sharing, analyzing, modeling, and monetizing datasets with solutions from BI to machine learning. Databricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-use-catalog.html	is it possible to set a default catalog and schema for a user in unity catalog?	USE CATALOG =============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Sets the current catalog. After the current catalog is set, partially and unqualified identifiers for tables, functions, and views that are referenced by SQLs are resolved from the current catalog.       Setting the catalog also resets the current schema to  `default`  .        Syntax --------      ``` { USE | SET } CATALOG [ catalog\_name | ' catalog\_name ' ]  ```        Parameter -----------   * **[catalog\_name](sql-ref-names.html#catalog-name)**      Name of the catalog to use. If the catalog does not exist, an exception is thrown.       Examples ----------      ``` -- Use the 'hive\_metastore' which exists. > USE CATALOG hive\_metastore;  > USE CATALOG 'hive\_metastore';  -- Use the 'some\_catalog' which doesn't exist > USE CATALOG `some\_catalog`;  Error: Catalog 'some\_catalog' not found;  -- Setting the catalog resets the datbase to `default` > USE CATALOG some\_cat; > SELECT current\_database(), current\_catalog();  some\_cat default  -- Setting the schema within the curret catalog > USE DATABASE some\_db; > SELECT current\_database(), current\_catalog();  some\_cat some\_db  -- Resetting both catalog and schema > USE DATABASE main.my\_db; > SELECT current\_database(), current\_catalog();  main my\_db  -- Setting the catalog resets the database to `default` again > USE CATALOG some\_cat; > SELECT current\_database(), current\_catalog();  some\_cat default  ```        Related articles ------------------   * [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html) * [DROP CATALOG](sql-ref-syntax-ddl-drop-catalog.html)	Based on the context, it is not possible to set a default schema for a user in Unity Catalog. However, setting the catalog resets the current schema to `default`.	1	1	0
no_reaction	https://docs.databricks.com/structured-streaming/tutorial.html	can u explain DLT streaming	What is Delta Live Tables? ============================     Delta Live Tables is a declarative framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.       Instead of defining your data pipelines using a series of separate Apache Spark tasks, you define streaming tables and materialized views that the system should create and keep up to date. Delta Live Tables manages how your data is transformed based on queries you define for each processing step. You can also enforce data quality with Delta Live Tables  *expectations*   , which allow you to define expected data quality and specify how to handle records that fail those expectations.            What are Delta Live Tables datasets? --------------------------------------     Delta Live Tables datasets are the streaming tables, materialized views, and views maintained as the results of declarative queries. The following table describes how each dataset is processed:          |   Dataset type   |   How are records processed through defined queries?   | | --- | --- | |   Streaming table   |   Each record is processed exactly once. This assumes an append-only source.   | |   Materialized views   |   Records are processed as required to return accurate results for the current data state. Materialized views should be used for data sources with updates, deletions, or aggregations, and for change data capture processing (CDC).   | |   Views   |   Records are processed each time the view is queried. Use views for intermediate transformations and data quality checks that should not be published to public datasets.   |    ###   Streaming table     A  *streaming table*   is a Delta table with extra support for streaming or incremental data processing. Streaming tables allow you to process a growing dataset, handling each row only once. Because most datasets grow continuously over time, streaming tables are good for most ingestion workloads. Streaming tables are optimal for pipelines that require data freshness and low latency. Streaming tables can also be useful for massive scale transformations, as results can be incrementally calculated as new data arrives, keeping results up to date without needing to fully recompute all source data with each update. Streaming tables are designed for data sources that are append-only.       ###   Materialized view     A  *materialized view*   (or  *live table*   ) is a view where the results have been precomputed. Materialized views are refreshed according to the update schedule of the pipeline in which they’re contained. Materialized views are powerful because they can handle any changes in the input. Each time the pipeline updates, query results are recalculated to reflect changes in upstream datasets that might have occurred because of compliance, corrections, aggregations, or general CDC. Delta Live Tables implements materialized views as Delta tables, but abstracts away complexities associated with efficient application of updates, allowing users to focus on writing queries.       ###   Views     All  *views*   in Databricks compute results from source datasets as they are queried, leveraging caching optimizations when available. Delta Live Tables does not publish views to the catalog, so views can be referenced only within the pipeline in which they are defined. Views are useful as intermediate queries that should not be exposed to end users or systems. Databricks recommends using views to enforce data quality constraints or transform and enrich datasets that drive multiple downstream queries.            Declare your first datasets in Delta Live Tables --------------------------------------------------     Delta Live Tables introduces new syntax for Python and SQL. To get started with Delta Live Tables syntax, use one of the following tutorials:     * [Tutorial: Declare a data pipeline with SQL in Delta Live Tables](tutorial-sql.html) * [Tutorial: Declare a data pipeline with Python in Delta Live Tables](tutorial-python.html)      Note       Delta Live Tables separates dataset definitions from update processing, and Delta Live Tables notebooks are not intended for interactive execution. See  [What is a Delta Live Tables pipeline?](#pipeline)  .              What is a Delta Live Tables pipeline? ---------------------------------------     A  *pipeline*   is the main unit used to configure and run data processing workflows with Delta Live Tables.       A pipeline contains materialized views and streaming tables declared in Python or SQL source files. Delta Live Tables infers the dependencies between these tables, ensuring updates occur in the right order. For each dataset, Delta Live Tables compares the current state with the desired state and proceeds to create or update datasets using efficient processing methods.       The settings of Delta Live Tables pipelines fall into two broad categories:     1. Configurations that define a collection of notebooks or files (known as  *source code*   or  *libraries*   ) that use Delta Live Tables syntax to declare datasets. 2. Configurations that control pipeline infrastructure, how updates are processed, and how tables are saved in the workspace.     Most configurations are optional, but some require careful attention, especially when configuring production pipelines. These include the following:     * To make data available outside the pipeline, you must declare a  **target schema**   . * Data access permissions are configured through the cluster used for execution. Make sure your cluster has appropriate permissions configured for data sources and the target  **storage location**   , if specified.     For details on using Python and SQL to write source code for pipelines, see  [Delta Live Tables SQL language reference](sql-ref.html)  and  [Delta Live Tables Python language reference](python-ref.html)  .       For more on pipeline settings and configurations, see  [Configure pipeline settings for Delta Live Tables](settings.html)  .         Deploy your first pipeline and trigger updates ------------------------------------------------     Before processing data with Delta Live Tables, you must configure a pipeline. Once a pipeline is configured, you can trigger an update to calculate results for each dataset in your pipeline. To get started using Delta Live Tables pipelines, see  [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html)  .           What is a pipeline update? ----------------------------     Pipelines deploy infrastructure and recompute data state when you start an  *update*   . An update does the following:     * Starts a cluster with the correct configuration. * Discovers all the tables and views defined, and checks for any analysis errors such as invalid column names, missing dependencies, and syntax errors. * Creates or updates tables and views with the most recent data available.     Pipelines can be run either continuously or on a schedule depending on the cost and latency requirements for your use case. See  [Run an update on a Delta Live Tables pipeline](updates.html)  .           Ingest data with Delta Live Tables ------------------------------------     Delta Live Tables supports all data sources available in Databricks.       Databricks recommends using streaming tables for most ingestion use cases. For files arriving in cloud object storage, Databricks recommends Auto Loader. You can directly ingest data with Delta Live Tables from most message buses.       For more information about configuring access to cloud storage, see  [Cloud storage configuration](settings.html#configure-cloud-storage)  .       For formats not supported by Auto Loader, you can use Python or SQL to query any format supported by Apache Spark. See  [Load data with Delta Live Tables](load.html)  .         Monitor and enforce data quality ----------------------------------     You can use  *expectations*   to specify data quality controls on the contents of a dataset. Unlike a  `CHECK`  constraint in a traditional database which prevents adding any records that fail the constraint, expectations provide flexibility when processing data that fails data quality requirements. This flexibility allows you to process and store data that you expect to be messy and data that must meet strict quality requirements. See  [Manage data quality with Delta Live Tables](expectations.html)  .           How are Delta Live Tables and Delta Lake related? ---------------------------------------------------     Delta Live Tables extends the functionality of Delta Lake. Tables created and managed by Delta Live Tables are Delta tables, and as such have the same guarantees and features provided by Delta Lake. See  [What is Delta Lake?](../delta/index.html)  .       Delta Live Tables adds several table properties in addition to the many table properties that can be set in Delta Lake. See  [Delta Live Tables properties reference](properties.html)  and  [Delta table properties reference](../delta/table-properties.html)  .           How tables are created and managed by Delta Live Tables ---------------------------------------------------------     Databricks automatically manages tables created with Delta Live Tables, determining how updates need to be processed to correctly compute the current state of a table and performing a number of maintenance and optimization tasks.       For most operations, you should allow Delta Live Tables to process all updates, inserts, and deletes to a target table. For details and limitations, see  [Retain manual deletes or updates](transform.html#manual-ddl)  .           Maintenance tasks performed by Delta Live Tables --------------------------------------------------     Delta Live Tables performs maintenance tasks within 24 hours of a table being updated. Maintenance can improve query performance and reduce cost by removing old versions of tables. By default, the system performs a full  [OPTIMIZE](../sql/language-manual/delta-optimize.html)  operation followed by  [VACUUM](../sql/language-manual/delta-vacuum.html)  . You can disable OPTIMIZE for a table by setting  `pipelines.autoOptimize.managed     =     false`  in the  [table properties](properties.html#table-properties)  for the table. Maintenance tasks are performed only if a pipeline update has run in the 24 hours before the maintenance tasks are scheduled.       To ensure the maintenance cluster has the required storage location access, you must apply security configurations required to access your storage locations to both the default cluster and the maintenance cluster. See  [Configure your compute settings](settings.html#cluster-config)  .         Limitations -------------     The following limitations apply:     * All tables created and updated by Delta Live Tables are Delta tables. * Delta Live Tables tables can only be defined once, meaning they can only be the target of a single operation in all Delta Live Tables pipelines. * Identity columns are not supported with tables that are the target of  `APPLY     CHANGES     INTO`  , and might be recomputed during updates for materialized views. For this reason, Databricks recommends only using identity columns with streaming tables in Delta Live Tables. See  [Use identity columns in Delta Lake](../delta/generated-columns.html#identity)  .       Additional resources ----------------------   * Delta Live Tables has full support in the Databricks REST API. See  [Delta Live Tables API guide](api-guide.html)  . * For pipeline and table settings, see  [Delta Live Tables properties reference](properties.html)  . * [Delta Live Tables SQL language reference](sql-ref.html)  . * [Delta Live Tables Python language reference](python-ref.html)  .	Delta Live Tables (DLT) is a declarative framework for building reliable, maintainable, and testable data processing pipelines. It allows you to define streaming tables and materialized views that the system should create and keep up to date. DLT manages how your data is transformed based on queries you define for each processing step. You can also enforce data quality with DLT expectations, which allow you to define expected data quality and specify how to handle records that fail those expectations. DLT datasets include streaming tables, materialized views, and views maintained as the results of declarative queries.	0	1	1
thumb_up	https://docs.databricks.com/workflows/jobs/jobs.html	How to create a job?	    Create, run, and manage Databricks Jobs =========================================     This article details how to create, edit, run, and monitor Databricks Jobs using the Jobs UI. To learn about using the Databricks CLI to create and run jobs, see  [Jobs CLI](../../dev-tools/cli/jobs-cli.html)  . To learn about using the Jobs API, see  [Jobs API 2.1](../../dev-tools/api/latest/jobs.html)  .       To create your first workflow with a Databricks job, see the  [quickstart](jobs-quickstart.html)  .        Important     * You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. * A workspace is limited to 1000 concurrent task runs. A  `429     Too     Many     Requests`  response is returned when you request a run that cannot start immediately. * The number of jobs a workspace can create in an hour is limited to 10000 (includes “runs submit”). This limit also affects jobs created by the REST API and notebook workflows.         Create a job --------------   1. Do one of the following:      * Click   ![Jobs Icon](../../_images/jobs-icon.png)  **Workflows**    in the sidebar and click   ![Create Job Button](../../_images/create-job.png)   .  * In the sidebar, click   ![New Icon](../../_images/create-icon.png)  **New**    and select   **Job**    .  The  **Tasks**   tab appears with the create task dialog.      ![Create task screen](../../_images/create-task.png) 2. Replace  **Add a name for your job…**   with your job name. 3. Enter a name for the task in the  **Task name**   field. 4. In the  **Type**   dropdown menu, select the type of task to run. See  [Task type options](#task-types)  . 5. Configure the cluster where the task runs. In the  **Cluster**   dropdown menu, select either  **New job cluster**   or  **Existing All-Purpose Clusters**   .      * **New Job Cluster**    : Click   **Edit**    in the   **Cluster**    dropdown menu and complete the   [cluster configuration](../../clusters/configure.html)   .  * **Existing All-Purpose Cluster**    : Select an existing cluster in the   **Cluster**    dropdown menu. To open the cluster in a new page, click the   ![External Link](../../_images/external-link.png)   icon to the right of the cluster name and description.  To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  . 6. To add dependent libraries, click  **+ Add**   next to  **Dependent libraries**   . See  [Dependent libraries](#task-config-dependent-libraries)  . 7. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters.      * **Notebook**    : Click   **Add**    and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the   [Run a job with different parameters](#job-run-with-different-params)   option. Parameters set the value of the   [notebook widget](../../notebooks/widgets.html)   specified by the key of the parameter. Use   [task parameter variables](#parameter-variables)   to pass a limited set of dynamic values as part of a parameter value.  * **JAR**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See   [Configure JAR job parameters](#configure-jar-job-parameters)   .  * **Spark Submit**    task: Parameters are specified as a JSON-formatted array of strings. Conforming to the   [Apache Spark spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html)    convention, parameters after the JAR path are passed to the main method of the main class.  * **Python script**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the   [argparse](https://docs.python.org/3/library/argparse.html)    module in Python.  * **Python Wheel**    : In the   **Parameters**    dropdown menu, select   **Positional arguments**    to enter parameters as a JSON-formatted array of strings, or select   **Keyword arguments > Add**    to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. 8. To optionally receive notifications for task start, success, or failure, click  **+ Add**   next to  **Emails**   . Failure notifications are sent on initial task failure and any subsequent retries. 9. To optionally configure a retry policy for the task, click  **+ Add**   next to  **Retries**   . See  [Retries](#retry-policies)  . 10. To optionally configure a timeout for the task, click  **+ Add**   next to  **Timeout in seconds**   . See  [Timeout](#timeout-setting)  . 11. Click  **Create**   .     After creating the first task, you can configure job-level settings such as notifications, job triggers, and permissions. See  [Edit a job](#job-edit)  .       To add another task, click  ![Add Task Button](../../_images/add-task.png)  in the DAG view. A shared cluster option is provided if you have configured a  **New Job Cluster**   for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  .           Task type options -------------------     The following are the task types you can add to your Databricks job and available options for the different task types:     * **Notebook**   : In the  **Source**   dropdown menu, select a location for the notebook; either  **Workspace**   for a notebook located in a Databricks workspace folder or  **Git provider**   for a notebook located in a remote Git repository.      **Workspace**   : Use the file browser to find the notebook, click the notebook name, and click  **Confirm**   .      **Git provider**   : Click  **Edit**   and enter the Git repository information. See  [Use version controlled notebooks in a Databricks job](how-to/use-repos.html)  . * **JAR**   : Specify the  **Main class**   . Use the fully qualified name of the class containing the main method, for example,  `org.apache.spark.examples.SparkPi`  . Then click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. One of these libraries must contain the main class.       To learn more about JAR tasks, see  [JAR jobs](#jar-jobs)  . * **Spark Submit**   : In the  **Parameters**   text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the  `DFSReadWriteTest`  from the Apache Spark examples:        ``` ["--class","org.apache.spark.examples.DFSReadWriteTest","dbfs:/FileStore/libraries/spark\_examples\_2\_12\_3\_1\_1.jar","/dbfs/databricks-datasets/README.md","/FileStore/examples/output/"]  ```       Important       There are several limitations for  **spark-submit**   tasks:       + You can run spark-submit tasks only on new clusters.  + Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see   [Cluster autoscaling](../../clusters/configure.html#autoscaling)   .  + Spark-submit does not support   [Databricks Utilities](../../dev-tools/databricks-utils.html)   . To use Databricks Utilities, use JAR tasks instead.  + If you are using a Unity Catalog-enabled cluster, spark-submit is supported only if the cluster uses Single User   [access mode](../../data-governance/unity-catalog/compute.html#access-mode)   . Shared access mode is not supported. * **Python script**   : In the  **Source**   drop-down, select a location for the Python script, either  **Workspace**   for a script in the local workspace, or  **DBFS / S3**   for a script located on DBFS or cloud storage. In the  **Path**   textbox, enter the path to the Python script:      **Workspace**   : In the  **Select Python File**   dialog, browse to the Python script and click  **Confirm**   . Your script must be in a  [Databricks repo](../../files/workspace.html)  .      **DBFS**   : Enter the URI of a Python script on DBFS or cloud storage; for example,  `dbfs:/FileStore/myscript.py`  . * **Delta Live Tables Pipeline**   : In the  **Pipeline**   dropdown menu, select an existing  [Delta Live Tables](../../delta-live-tables/index.html)  pipeline.        Important       You can use only triggered pipelines with the  **Pipeline**   task. Continuous pipelines are not supported as a job task. To learn more about triggered and continuous pipelines, see  [Continuous vs. triggered pipeline execution](../../delta-live-tables/updates.html#continuous-triggered)  . * **Python Wheel**   : In the  **Package name**   text box, enter the package to import, for example,  `myWheel-1.0-py2.py3-none-any.whl`  . In the  **Entry Point**   text box, enter the function to call when starting the wheel. Click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. * **SQL**   : In the  **SQL task**   dropdown menu, select  **Query**   ,  **Dashboard**   , or  **Alert**   .        Note       + The   **SQL**    task is in   [Public Preview](../../release-notes/release-types.html)   .  + The   **SQL**    task requires Databricks SQL and a   [serverless or pro SQL warehouse](../../sql/index.html#warehouse-types)   .   **Query**   : In the  **SQL query**   dropdown menu, select the query to execute when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Dashboard**   : In the  **SQL dashboard**   dropdown menu, select a dashboard to be updated when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Alert**   : In the  **SQL alert**   dropdown menu, select an alert to trigger for evaluation. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task. * **dbt**   : See  [Use dbt transformations in a Databricks job](how-to/use-dbt-in-workflows.html)  for a detailed example	To create a job in Databricks, you can use the Jobs UI. Here are the steps to create a job:  1. Click on the Jobs icon in the sidebar and then click on the Create Job button. 2. Replace "Add a name for your job..." with your job name. 3. Enter a name for the task in the Task name field. 4. In the Type dropdown menu, select the type of task to run. 5. Configure the cluster where the task runs. In the Cluster dropdown menu, select either New job cluster or Existing All-Purpose Clusters. 6. To add dependent libraries, click + Add next to Dependent libraries. 7. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. 8. To optionally receive notifications for task start, success, or failure, click + Add next to Emails. 9. To optionally configure a retry policy for the task, click + Add next to Retries. 10. To optionally configure a timeout for the task, click + Add next to Timeout in seconds. 11. Click Create.  After creating the first task, you can configure job-level settings such as notifications, job triggers, and permissions.	0	1	1
no_reaction	https://docs.databricks.com/external-data/parquet.html	How can I convert my files to parquet?	Parquet file ==============    [Apache Parquet](https://parquet.apache.org/)   is a columnar file format that provides optimizations to speed up queries. It is a far more efficient file format than  [CSV](csv.html)  or  [JSON](json.html)  .       For more information, see  [Parquet Files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#parquet(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#parquet(path:String):Unit)     The following notebook shows how to read and write data to Parquet files.        ###   Reading Parquet files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-parquet-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import	To convert files to Parquet format, you can use the supported read and write options in Apache Spark. For example, in Python, you can use the `pyspark.sql.DataFrameReader.parquet` method to read Parquet files and the `pyspark.sql.DataFrameWriter.parquet` method to write data to Parquet files. You can refer to the provided links for more information and examples.	1	1	1
no_reaction	https://docs.databricks.com/notebooks/testing.html	Q82- Incorporating unit tests into a PySpark application requires upfront attention to the design of your jobs, or a potentially significant refactoring of existing code.  Which statement describes a main benefit that offset this additional effort? a- Troubleshooting is easier since all steps are isolated and tested individually b- Yields faster deployment and execution times c- Improves the quality of your data d- Ensures that all steps interact correctly to achieve the desired end result e- Validates a complete use case of your application	    Unit testing for notebooks ============================     You can use  *unit testing*   to help improve the quality and consistency of your notebooks’ code. Unit testing is an approach to testing self-contained units of code, such as functions, early and often. This helps you find problems with your code faster, uncover mistaken assumptions about your code sooner, and streamline your overall coding efforts.       This article is an introduction to basic  [unit testing](https://en.wikipedia.org/wiki/Unit_testing)   with functions. Advanced concepts such as unit testing classes and interfaces, as well as the use of  [stubs](https://en.wikipedia.org/wiki/Method_stub)   ,  [mocks](https://en.wikipedia.org/wiki/Mock_object)   , and  [test harnesses](https://en.wikipedia.org/wiki/Test_harness)   , while also supported when unit testing for notebooks, are outside the scope of this article. This article also does not cover other kinds of testing methods, such as  [integration testing](https://en.wikipedia.org/wiki/Integration_testing)   ,  [system testing](https://en.wikipedia.org/wiki/System_testing)   ,  [acceptance testing](https://en.wikipedia.org/wiki/Acceptance_testing)   , or  [non-functional testing](https://en.wikipedia.org/wiki/Non-functional_testing)   methods such as  [performance testing](https://en.wikipedia.org/wiki/Software_performance_testing)   or  [usability testing](https://en.wikipedia.org/wiki/Usability_testing)   .       This article demonstrates the following:     * How to organize functions and their unit tests. * How to write functions in Python, R, Scala, as well as user-defined functions in SQL, that are well-designed to be unit tested. * How to call these functions from Python, R, Scala, and SQL notebooks. * How to write unit tests in Python, R, and Scala by using the popular test frameworks  [pytest](https://docs.pytest.org)   for Python,  [testthat](https://testthat.r-lib.org)   for R, and  [ScalaTest](https://docs.pytest.org)   for Scala. Also how to write SQL that unit tests SQL user-defined functions (SQL UDFs). * How to run these unit tests from Python, R, Scala, and SQL notebooks.      Organize functions and unit tests -----------------------------------       There are a few common approaches for organizing your functions and their unit tests with notebooks. Each approach has its benefits and challenges.       For Python, R, and Scala notebooks, common approaches include the following:     * [Store functions and their unit tests outside of notebooks.](test-notebooks.html#separate-test-code-from-the-notebook)  .      + Benefits: You can call these functions with and outside of notebooks. Test frameworks are better designed to run tests outside of notebooks.  + Challenges: This approach is not supported for Scala notebooks. This approach requires   [Databricks Repos](../repos/index.html)   . This approach also increases the number of files to track and maintain. * [Store functions in one notebook and their unit tests in a separate notebook.](test-notebooks.html#separate-test-code-from-the-notebook)  .      + Benefits: These functions are easier to reuse across notebooks.  + Challenges: The number of notebooks to track and maintain increases. These functions cannot be used outside of notebooks. These functions can also be more difficult to test outside of notebooks. * [Store functions and their unit tests within the same notebook.](test-notebooks.html)  .      + Benefits: Functions and their unit tests are stored within a single notebook for easier tracking and maintenance.  + Challenges: These functions can be more difficult to reuse across notebooks. These functions cannot be used outside of notebooks. These functions can also be more difficult to test outside of notebooks.     For Python and R notebooks, Databricks recommends storing functions and their unit tests outside of notebooks. For Scala notebooks, Databricks recommends including functions in one notebook and their unit tests in a separate notebook.       For SQL notebooks, Databricks recommends that you store functions as SQL user-defined functions (SQL UDFs) in your schemas (also known as databases). You can then call these SQL UDFs and their unit tests from SQL notebooks.         Write functions -----------------     This section describes a simple set of example functions that determine the following:     * Whether a table exists in a database. * Whether a column exists in a table. * How many rows exist in a column for a value within that column.     These functions are intended to be simple, so that you can focus on the unit testing details in this article rather than focus on the functions themselves.       To get the best unit testing results, a function should return a single predictable outcome and be of a single data type. For example, to check whether something exists, the function should return a boolean value of true or false. To return the number of rows that exist, the function should return a non-negative, whole number. It should not, in the first example, return either false if something does not exist or the thing itself if it does exist. Likewise, for the second example, it should not return either the number of rows that exist or false if no rows exist.       You can add these functions to an existing Databricks workspace as follows, in Python, R, Scala, or SQL.         The following code assumes you have  [Set up Databricks Repos](../repos/repos-setup.html)  ,  [added a repo](../repos/git-operations-with-repos.html)  , and have the repo open in your Databricks workspace.      [Create a file](../files/workspace-basics.html#create-a-new-file)  named  `myfunctions.py`  within the repo, and add the following contents to the file. Other examples in this article expect this file to be named  `myfunctions.py`  . You can use different names for your own files.        ``` import pyspark from pyspark.sql import SparkSession from pyspark.sql.functions import col  # Because this file is not a Databricks notebook, you # must create a Spark session. Databricks notebooks # create a Spark session for you by default. spark = SparkSession.builder \                     .appName('integrity-tests') \                     .getOrCreate()  # Does the specified table exist in the specified database? def tableExists(tableName, dbName):   return spark.catalog.tableExists(f"{dbName}.{tableName}")  # Does the specified column exist in the given DataFrame? def columnExists(dataFrame, columnName):   if columnName in dataFrame.columns:     return True   else:     return False  # How many rows are there for the specified value in the specified column # in the given DataFrame? def numRowsInColumnForValue(dataFrame, columnName, columnValue):   df = dataFrame.filter(col(columnName) == columnValue)    return df.count()  ```        The following code assumes you have  [Set up Databricks Repos](../repos/repos-setup.html)  ,  [added a repo](../repos/git-operations-with-repos.html)  , and have the repo open in your Databricks workspace.      [Create a file](../files/workspace-basics.html#create-a-new-file)  named  `myfunctions.r`  within the repo, and add the following contents to the file. Other examples in this article expect this file to be named  `myfunctions.r`  . You can use different names for your own files.        ``` library(SparkR)  # Does the specified table exist in the specified database? table\_exists <- function(table\_name, db\_name) {   tableExists(paste(db\_name, ".", table\_name, sep = "")) }  # Does the specified column exist in the given DataFrame? column\_exists <- function(dataframe, column\_name) {   column\_name %in% colnames(dataframe) }  # How many rows are there for the specified value in the specified column # in the given DataFrame? num\_rows\_in\_column\_for\_value <- function(dataframe, column\_name, column\_value) {   df = filter(dataframe, dataframe[[column\_name]] == column\_value)    count(df) }  ```        Create a  [Scala notebook](notebooks-manage.html#create-a-notebook)  named  `myfunctions`  with the following contents. Other examples in this article expect this notebook to be named  `myfunctions`  . You can use different names for your own notebooks.        ``` import org.apache.spark.sql.DataFrame import org.apache.spark.sql.functions.col  // Does the specified table exist in the specified database? def tableExists(tableName: String, dbName: String) : Boolean = {  return spark.catalog.tableExists(dbName + "." + tableName) }  // Does the specified column exist in the given DataFrame? def columnExists(dataFrame: DataFrame, columnName: String) : Boolean = {  val nameOfColumn = null   for(nameOfColumn <- dataFrame.columns) {  if (nameOfColumn == columnName) {  return true  }  }   return false }  // How many rows are there for the specified value in the specified column // in the given DataFrame? def numRowsInColumnForValue(dataFrame: DataFrame, columnName: String, columnValue: String) : Long = {  val df = dataFrame.filter(col(columnName) === columnValue)   return df.count() }  ```        The following code assumes you have the third-party sample dataset  [diamonds](../dbfs/databricks-datasets.html)  within a schema named  `default`  within a catalog named  `main`  that is accessible from your Databricks workspace. If the catalog or schema that you want to use has a different name, then change one or both of the following  `USE`  statements to match.       Create a  [SQL notebook](notebooks-manage.html#create-a-notebook)  and add the following contents to this new notebook. Then  [attach](notebook-ui.html#attach)  the notebook to a cluster and  [run](run-notebook.html)  the notebook to add the following SQL UDFs to the specified catalog and schema.        Note       The SQL UDFs  `table\_exists`  and  `column\_exists`  work only with Unity Catalog. SQL UDF support for Unity Catalog is in  [Public Preview](../release-notes/release-types.html)  .         ``` USE CATALOG main; USE SCHEMA default;  CREATE OR REPLACE FUNCTION table\_exists(catalog\_name STRING,  db\_name STRING,  table\_name STRING)  RETURNS BOOLEAN  RETURN if(  (SELECT count(\*) FROM system.information\_schema.tables  WHERE table\_catalog = table\_exists.catalog\_name  AND table\_schema = table\_exists.db\_name  AND table\_name = table\_exists.table\_name) > 0,  true,  false  );  CREATE OR REPLACE FUNCTION column\_exists(catalog\_name STRING,  db\_name STRING,  table\_name STRING,  column\_name STRING)  RETURNS BOOLEAN  RETURN if(  (SELECT count(\*) FROM system.information\_schema.columns  WHERE table\_catalog = column\_exists.catalog\_name  AND table\_schema = column\_exists.db\_name  AND table\_name = column\_exists.table\_name  AND column\_name = column\_exists.column\_name) >	The main benefit of incorporating unit tests into a PySpark application is that troubleshooting is easier since all steps are isolated and tested individually. Therefore, option a is the correct answer.	0	0	1
thumb_up	https://spark.apache.org/docs/latest/spark-connect-overview.html	What is Spark Connect?	   Spark Connect Overview ========================  **Building client-side Spark applications**    In Apache Spark 3.4, Spark Connect introduced a decoupled client-server architecture that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks and programming languages.     To get started, see  [Quickstart: Spark Connect](api/python/getting_started/quickstart_connect.html)   .    ![Spark Connect API Diagram](img/spark-connect-api.png "Spark Connect API")   How Spark Connect works =========================   The Spark Connect client library is designed to simplify Spark application development. It is a thin API that can be embedded everywhere: in application servers, IDEs, notebooks, and programming languages. The Spark Connect API builds on Spark’s DataFrame API using unresolved logical plans as a language-agnostic protocol between the client and the Spark driver.     The Spark Connect client translates DataFrame operations into unresolved logical query plans which are encoded using protocol buffers. These are sent to the server using the gRPC framework.     The Spark Connect endpoint embedded on the Spark Server receives and translates unresolved logical plans into Spark’s logical plan operators. This is similar to parsing a SQL query, where attributes and relations are parsed and an initial parse plan is built. From there, the standard Spark execution process kicks in, ensuring that Spark Connect leverages all of Spark’s optimizations and enhancements. Results are streamed back to the client through gRPC as Apache Arrow-encoded row batches.    ![Spark Connect communication](img/spark-connect-communication.png "Spark Connect communication")   Operational benefits of Spark Connect =======================================   With this new architecture, Spark Connect mitigates several multi-tenant operational issues:    **Stability**   : Applications that use too much memory will now only impact their own environment as they can run in their own processes. Users can define their own dependencies on the client and don’t need to worry about potential conflicts with the Spark driver.    **Upgradability**   : The Spark driver can now seamlessly be upgraded independently of applications, for example to benefit from performance improvements and security fixes. This means applications can be forward-compatible, as long as the server-side RPC definitions are designed to be backwards compatible.    **Debuggability and observability**   : Spark Connect enables interactive debugging during development directly from your favorite IDE. Similarly, applications can be monitored using the application’s framework native metrics and logging libraries.     How to use Spark Connect ==========================   Starting with Spark 3.4, Spark Connect is available and supports PySpark and Scala applications. We will walk through how to run an Apache Spark server with Spark Connect and connect to it from a client application using the Spark Connect client library.     Download and start Spark server with Spark Connect ----------------------------------------------------   First, download Spark from the  [Download Apache Spark](https://spark.apache.org/downloads.html)   page. Spark Connect was introduced in Apache Spark version 3.4 so make sure you choose 3.4.0 or newer in the release drop down at the top of the page. Then choose your package type, typically “Pre-built for Apache Hadoop 3.3 and later”, and click the link to download.     Now extract the Spark package you just downloaded on your computer, for example:    ``` tar -xvf spark-3.4.0-bin-hadoop3.tgz ```   In a terminal window, go to the  `spark`   folder in the location where you extracted Spark before and run the  `start-connect-server.sh`   script to start Spark server with Spark Connect, like in this example:    ``` ./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.4.0 ```   Note that we include a Spark Connect package (  `spark-connect_2.12:3.4.0`   ), when starting Spark server. This is required to use Spark Connect. Make sure to use the same version of the package as the Spark version you downloaded previously. In this example, Spark 3.4.0 with Scala 2.12.     Now Spark server is running and ready to accept Spark Connect sessions from client applications. In the next section we will walk through how to use Spark Connect when writing client applications.     Use Spark Connect in client applications ------------------------------------------   When creating a Spark session, you can specify that you want to use Spark Connect and there are a few ways to do that outlined as follows.     If you do not use one of the mechanisms outlined here, your Spark session will work just like before, without leveraging Spark Connect, and your application code will run on the Spark driver node.    ###   Set SPARK\_REMOTE environment variable   If you set the  `SPARK_REMOTE`   environment variable on the client machine where your Spark client application is running and create a new Spark Session as in the following example, the session will be a Spark Connect session. With this approach, there is no code change needed to start using Spark Connect.     In a terminal window, set the  `SPARK_REMOTE`   environment variable to point to the local Spark server you started previously on your computer:    ``` export SPARK\_REMOTE="sc://localhost" ```   And start the Spark shell as usual:    ``` ./bin/pyspark ```   The PySpark shell is now connected to Spark using Spark Connect as indicated in the welcome message:    ``` Client connected to the Spark Connect server at localhost ```   And if you write your own program, create a Spark session as shown in this example:    ``` from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() ```   This will create a Spark Connect session from your application by reading the  `SPARK_REMOTE`   environment variable we set previously.    ###   Specify Spark Connect when creating Spark session   You can also specify that you want to use Spark Connect explicitly when you create a Spark session.     For example, you can launch the PySpark shell with Spark Connect as illustrated here.     To launch the PySpark shell with Spark Connect, simply include the  `remote`   parameter and specify the location of your Spark server. We are using  `localhost`   in this example to connect to the local Spark server we started previously:    ``` ./bin/pyspark --remote "sc://localhost" ```   And you will notice that the PySpark shell welcome message tells you that you have connected to Spark using Spark Connect:    ``` Client connected to the Spark Connect server at localhost ```   You can also check the Spark session type. If it includes  `.connect.`   you are using Spark Connect as shown in this example:    ``` SparkSession available as 'spark'. >>> type(spark) <class 'pyspark.sql.connect.session.SparkSession'> ```   Now you can run PySpark code in the shell to see Spark Connect in action:    ``` >>> columns = ["id","name"] >>> data = [(1,"Sarah"),(2,"Maria")] >>> df = spark.createDataFrame(data).toDF(\*columns) >>> df.show() +---+-----+ | id| name| +---+-----+ |  1|Sarah| |  2|Maria| +---+-----+ ```   Or, when writing your own code, include the  `remote`   function with a reference to your Spark server when you create a Spark session, as in this example:    ``` from pyspark.sql import SparkSession spark = SparkSession.builder.remote("sc://localhost").getOrCreate() ```  ``` import org.apache.spark.sql.SparkSession val spark = SparkSession.builder().remote("sc://localhost").build() ```   Client application authentication ===================================   While Spark Connect does not have built-in authentication, it is designed to work seamlessly with your existing authentication infrastructure. Its gRPC HTTP/2 interface allows for the use of authenticating proxies, which makes it possible to secure Spark Connect without having to implement authentication logic in Spark directly.     What is supported in Spark 3.4 ================================  **PySpark**   : In Spark 3.4, Spark Connect supports most PySpark APIs, including  [DataFrame](api/python/reference/pyspark.sql/dataframe.html)   ,  [Functions](api/python/reference/pyspark.sql/functions.html)   , and  [Column](api/python/reference/pyspark.sql/column.html)   . However, some APIs such as  [SparkContext](api/python/reference/api/pyspark.SparkContext.html)   and  [RDD](api/python/reference/api/pyspark.RDD.html)   are not supported. You can check which APIs are currently supported in the  [API reference](api/python/reference/index.html)   documentation. Supported APIs are labeled “Supports Spark Connect” so you can check whether the APIs you are using are available before migrating existing code to Spark Connect.    **Scala**   : In Spark 3.4, Spark Connect supports most Scala APIs, including  [Dataset](api/scala/org/apache/spark/sql/Dataset.html)   ,  [functions](api/scala/org/apache/spark/sql/functions$.html)   , and  [Column](api/scala/org/apache/spark/sql/Column.html)   .     Support for more APIs is planned for upcoming Spark releases.    	Spark Connect is a decoupled client-server architecture introduced in Apache Spark 3.4 that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks, and programming languages. The Spark Connect client library is designed to simplify Spark application development. It is a thin API that can be embedded everywhere: in application servers, IDEs, notebooks, and programming languages. The Spark Connect API builds on Spark’s DataFrame API using unresolved logical plans as a language-agnostic protocol between the client and the Spark driver.	1	1	1
no_reaction	https://kb.databricks.com/scala/manage-size-delta-table	which DBR made it so saveastable defaulted to Delta tables	   pyspark.sql.DataFrameWriter.saveAsTable  [¶](#pyspark-sql-dataframewriter-saveastable "Permalink to this headline") =====================================================================================================================  `DataFrameWriter.`  `saveAsTable`    (   *name     :     str*  ,  *format     :     Optional    [    str    ]     =     None*  ,  *mode     :     Optional    [    str    ]     =     None*  ,  *partitionBy     :     Union[str, List[str], None]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → None  [[source]](../../../_modules/pyspark/sql/readwriter.html#DataFrameWriter.saveAsTable) [¶](#pyspark.sql.DataFrameWriter.saveAsTable "Permalink to this definition")    Saves the content of the  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")  as the specified table.     In the case the table already exists, behavior of this function depends on the save mode, specified by the    mode    function (default to throwing an exception). When    mode    is    Overwrite    , the schema of the  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")  does not need to be the same as that of the existing table.    * append    : Append contents of this  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")  to existing data. * overwrite    : Overwrite existing data. * error    or    errorifexists    : Throw an exception if data already exists. * ignore    : Silently ignore this operation if data already exists.   New in version 1.4.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **name**    str     the table name    **format**    str, optional     the format used to save    **mode**    str, optional     one of    append    ,    overwrite    ,    error    ,    errorifexists    ,    ignore    (default: error)    **partitionBy**    str or list     names of partitioning columns    **\*\*options**    dict     all other string options     Notes     When    mode    is    Append    , if there is an existing table, we will use the format and options of the existing table. The column order in the schema of the  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")  doesn’t need to be the same as that of the existing table. Unlike  [`DataFrameWriter.insertInto()`](pyspark.sql.DataFrameWriter.insertInto.html#pyspark.sql.DataFrameWriter.insertInto "pyspark.sql.DataFrameWriter.insertInto")  ,  [`DataFrameWriter.saveAsTable()`](#pyspark.sql.DataFrameWriter.saveAsTable "pyspark.sql.DataFrameWriter.saveAsTable")  will use the column names to find the correct column positions.     Examples     Creates a table from a DataFrame, and read it back.    ``` >>> \_ = spark.sql("DROP TABLE IF EXISTS tblA") >>> spark.createDataFrame([ ...     (100, "Hyukjin Kwon"), (120, "Hyukjin Kwon"), (140, "Haejoon Lee")], ...     schema=["age", "name"] ... ).write.saveAsTable("tblA") >>> spark.read.table("tblA").sort("age").show() +---+------------+ |age| name| +---+------------+ |100|Hyukjin Kwon| |120|Hyukjin Kwon| |140| Haejoon Lee| +---+------------+ >>> \_ = spark.sql("DROP TABLE tblA")  ```  [pyspark.sql.DataFrameWriter.save](pyspark.sql.DataFrameWriter.save.html "previous page")  [pyspark.sql.DataFrameWriter.sortBy](pyspark.sql.DataFrameWriter.sortBy.html "next page")   	The default behavior of `saveAsTable` function in PySpark's `DataFrameWriter` is to save the content of the DataFrame as a Delta table.	0	1	1
no_reaction	https://docs.databricks.com/repos/get-access-tokens-from-git-provider.html	how can i connect to a private git server	Git Credentials API 2.0 =========================	The context you provided is not relevant to the question. However, to connect to a private Git server, you can use SSH keys or HTTPS authentication. You will need to have the appropriate credentials and permissions to access the server. The specific steps may vary depending on the Git client and server you are using.	0	1	1
no_reaction	https://docs.databricks.com/delta/idempotent-writes.html	how do I ensure the idempotency for streaming writes	    Enable idempotent writes across jobs ======================================     Sometimes a job that writes data to a Delta table is restarted due to various reasons (for example, job encounters a failure). The failed job may or may not have written the data to Delta table before terminating. In the case where the data is written to the Delta table, the restarted job writes the same data to the Delta table which results in duplicate data.       To address this, Delta tables support the following  `DataFrameWriter`  options to make the writes idempotent:     * `txnAppId`  : A unique string that you can pass on each  `DataFrame`  write. For example, this can be the name of the job. * `txnVersion`  : A monotonically increasing number that acts as transaction version. This number needs to be unique for data that is being written to the Delta table(s). For example, this can be the epoch seconds of the instant when the query is attempted for the first time. Any subsequent restarts of the same job needs to have the same value for  `txnVersion`  .     The above combination of options needs to be unique for each new data that is being ingested into the Delta table and the  `txnVersion`  needs to be higher than the last data that was ingested into the Delta table. For example:     * Last successfully written data contains option values as  `dailyETL:23423`  (  `txnAppId:txnVersion`  ). * Next write of data should have  `txnAppId     =     dailyETL`  and  `txnVersion`  as at least  `23424`  (one more than the last written data  `txnVersion`  ). * Any attempt to write data with  `txnAppId     =     dailyETL`  and  `txnVersion`  as  `23422`  or less is ignored because the  `txnVersion`  is less than the last recorded  `txnVersion`  in the table. * Attempt to write data with  `txnAppId:txnVersion`  as  `anotherETL:23424`  is successful writing data to the table as it contains a different  `txnAppId`  compared to the same option value in last ingested data.      Warning       This solution assumes that the data being written to Delta table(s) in multiple retries of the job is the same. If a write attempt to a Delta table succeeds but due to some downstream failure there is a second write attempt with same txn options but different data, then that second write attempt will be ignored. This can cause unexpected results.        See the following code for an example:         ``` app\_id = ... # A unique string that is used as an application ID. version = ... # A monotonically increasing number that acts as transaction version.  dataFrame.write.option("txnVersion", version).option("txnAppId", app\_id).save(...)  ```       ``` val appId = ... // A unique string that is used as an application ID. version = ... // A monotonically increasing number that acts as transaction version.  dataFrame.write.option("txnVersion", version).option("txnAppId", appId).save(...)  ```                                                       	To ensure idempotency for streaming writes, Delta tables support the `txnAppId` and `txnVersion` options in the `DataFrameWriter`. `txnAppId` is a unique string that you can pass on each `DataFrame` write, and `txnVersion` is a monotonically increasing number that acts as a transaction version. These options need to be unique for each new data that is being ingested into the Delta table, and the `txnVersion` needs to be higher than the last data that was ingested into the Delta table.	1	1	1
no_reaction	https://docs.databricks.com/external-data/json.html	Give me the code to import a json file	      JSON file ===========     You can read JSON files in  [single-line](#single-line-mode)  or  [multi-line](#multi-line-mode)  mode. In single-line mode, a file can be split into many parts and read in parallel. In multi-line mode, a file is loaded as a whole entity and  *cannot*   be split.       For further information, see  [JSON Files](https://spark.apache.org/docs/latest/sql-data-sources-json.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html?highlight=json#pyspark.sql.DataFrameReader.json)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#json(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html?highlight=json#pyspark.sql.DataFrameWriter.json)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#json(path:String):Unit)       Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.2 (Unsupported)](../release-notes/runtime/8.2.html)  and above.        The rescued data column ensures that you never lose or miss out on data during ETL. The rescued data column contains any data that wasn’t parsed, either because it was missing from the given schema, or because there was a type mismatch, or because the casing of the column in the record or file didn’t match with that in the schema. The rescued data column is returned as a JSON blob containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set("spark.databricks.sql.rescuedDataColumn.filePath.enabled",     "false")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name, such as  `\_rescued\_data`  with  `spark.read.option("rescuedDataColumn",     "\_rescued\_data").format("json").load(<path>)`  .       The JSON parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed JSON—are dropped or throw errors. If you use the option  `badRecordsPath`  when parsing JSON, data type mismatches are not considered as bad records when using the  `rescuedDataColumn`  . Only incomplete and malformed JSON records are stored in  `badRecordsPath`  .         Examples ----------    ###   Single-line mode     In this example, there is one JSON object per line:        ``` {"string":"string1","int":1,"array":[1,2,3],"dict": {"key": "value1"}} {"string":"string2","int":2,"array":[2,4,6],"dict": {"key": "value2"}} {"string":"string3","int":3,"array":[3,6,9],"dict": {"key": "value3", "extra\_key": "extra\_value3"}}  ```      To read the JSON data, use:        ``` val df = spark.read.format("json").load("example.json")  ```      Spark infers the schema automatically.        ``` df.printSchema  ```       ``` root  |-- array: array (nullable = true)  |    |-- element: long (containsNull = true)  |-- dict: struct (nullable = true)  |    |-- extra\_key: string (nullable = true)  |    |-- key: string (nullable = true)  |-- int: long (nullable = true)  |-- string: string (nullable = true)  ```      ###   Multi-line mode     This JSON object occupies multiple lines:        ``` [  {"string":"string1","int":1,"array":[1,2,3],"dict": {"key": "value1"}},  {"string":"string2","int":2,"array":[2,4,6],"dict": {"key": "value2"}},  {  "string": "string3",  "int": 3,  "array": [  3,  6,  9  ],  "dict": {  "key": "value3",  "extra\_key": "extra\_value3"  }  } ]  ```      To read this object, enable multi-line mode:         ``` CREATE TEMPORARY VIEW multiLineJsonTable USING json OPTIONS (path="/tmp/multi-line.json",multiline=true)  ```       ``` val mdf = spark.read.option("multiline", "true").format("json").load("/tmp/multi-line.json") mdf.show(false)  ```         ###   Charset auto-detection     By default, the charset of input files is detected automatically. You can specify the charset explicitly using the  `charset`  option:        ``` spark.read.option("charset", "UTF-16BE").format("json").load("fileInUTF16.json")  ```      Some supported charsets include:  `UTF-8`  ,  `UTF-16BE`  ,  `UTF-16LE`  ,  `UTF-16`  ,  `UTF-32BE`  ,  `UTF-32LE`  ,  `UTF-32`  . For the full list of charsets supported by Oracle Java SE, see  [Supported Encodings](https://docs.oracle.com/javase/8/docs/technotes/guides/intl/encoding.doc.html)   .          Notebook ----------     The following notebook demonstrates single line and multi-line mode.        ###   Read JSON files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-json-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 	Here is the code to import a JSON file:  ``` val df = spark.read.format("json").load("path/to/json/file") ```  Replace "path/to/json/file" with the actual path to your JSON file.	0	1	1
no_reaction	https://docs.databricks.com/release-notes/unity-catalog/20220825.html	How do I add a catalog to Unity Catalog?	    Create and manage catalogs ============================     This article shows how to create and manage catalogs in Unity Catalog. A catalog contains  [schemas (databases)](create-schemas.html)  , and a schema contains tables and views.        Requirements --------------   * You must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore * Your Databricks account must be on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * You must have a Unity Catalog metastore  [linked to the workspace](create-metastore.html)  where you perform the catalog creation. * The  [compute resource](compute.html)  that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode.         Create a catalog ------------------     To create a catalog, you can use Data Explorer or a SQL command.       1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the  **Create Catalog**   button. 4. (Optional) Specify the location where data for  [managed tables](create-tables.html#managed-table)  in the catalog will be stored.       Specify a location here only if you do not want managed tables in this catalog to be stored in the default root storage location that was configured for the metastore. See  [Create a Unity Catalog metastore](create-metastore.html)  .       The path that you specify must be defined in an external location configuration, and you must have the  `CREATE     MANAGED     STORAGE`  privilege on that external location. You can also use a subpath of that path. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  . 5. Assign permissions for your catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  . 6. Click  **Save**   .     1. Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : Optional description or other comment.   ``` CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name>  [ MANAGED LOCATION '<location\_path>' ]  [ COMMENT <comment> ];  ```      For example, to create a catalog named  `example`  :        ``` CREATE CATALOG IF NOT EXISTS example;  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql("CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql("CREATE CATALOG IF NOT EXISTS example")  ```      For parameter descriptions, see  [CREATE CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-create-catalog.html)  . 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` library(SparkR)  sql("CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]")  ```      For example, to create a catalog named  `example`  :        ``` library(SparkR)  sql("CREATE CATALOG IF NOT EXISTS example")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql("CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql("CREATE CATALOG IF NOT EXISTS example")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .       When you create a catalog, two schemas (databases) are automatically created:  `default`  and  `information\_schema`  .       You can also create a catalog by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog)   . You can retrieve information about catalogs by using  [databricks\_catalogs](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/catalogs)   .      ###   Next steps     Now you can  [add schemas (databases) to your catalog](create-schemas.html)  .          Delete a catalog ------------------     To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.         You must delete all schemas in the catalog except  `information\_schema`  before you can delete a catalog. This includes the auto-created  `default`  schema.     1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. In the  **Data**   pane, on the left, click the catalog you want to delete. 4. In the detail pane, click the three-dot menu to the left of the  **Create database**   button and select  **Delete**   . 5. On the  **Delete catalog**   dialog, click  **Delete**   .       Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` DROP CATALOG [ IF EXISTS ] <catalog\_name> [ RESTRICT | CASCADE ]  ```      For example, to delete a catalog named  `vaccine`  and its schemas:        ``` DROP CATALOG vaccine CASCADE  ```        Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` spark.sql("DROP CATALOG [ IF EXISTS ] <catalog\_name>	To add a catalog to Unity Catalog, you must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore. Your Databricks account must be on the Premium plan and above. You must have a Unity Catalog metastore linked to the workspace where you perform the catalog creation. The compute resource that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode. To create a catalog, you can use Data Explorer or a SQL command. To create a catalog using SQL command, you can run the following SQL command in a notebook or Databricks SQL editor: CREATE CATALOG [ IF NOT EXISTS ] <catalog_name> [ MANAGED LOCATION '<location_path>' ] [ COMMENT <comment> ]. For example, to create a catalog named `example`, you can run the following SQL command: CREATE CATALOG IF NOT EXISTS example.	1	1	1
no_reaction	https://docs.databricks.com/release-notes/runtime/11.x-migration.html	how do I upgrade from dbr 10.4 to 11.3?	Databricks Runtime 11.x migration guide =========================================     This guide helps you migrate your Databricks workloads to the latest version of Databricks Runtime 11.x. To do so, Databricks recommends that you migrate your workloads in the following order:     * If your workloads are currently running on Databricks Runtime 10.4 LTS or above, you can migrate directly to the latest version of Databricks Runtime 11.x, as described later in this article. * If your workloads are currently running on Databricks Runtime 10.3 or below, do the following:      1. Migrate to Databricks Runtime 10.4 LTS first. See the   [Databricks Runtime 10.x migration guide](10.x-migration.html)   .  2. Follow the guidance in this article to migrate from Databricks Runtime 10.4 LTS to the latest version of Databricks Runtime 11.x.      Note      *LTS*   means this version is under  *long-term support*   . See  [Long-term support (LTS) lifecycle](databricks-runtime-ver.html#lts)  .             Apache Spark migration guidance ---------------------------------     The most recent version of the Databricks Runtime is 11.3 LTS, powered by Apache Spark 3.3.0. For Spark-specific migration information, click on one or more of the following links by Databricks Runtime version to view the corresponding Apache Spark Migration Guide. For reference, Databricks Runtime 10.4 LTS is powered by Apache Spark 3.2.1.          |   Databricks Runtime version   |   Apache Spark version   | | --- | --- | |   11.0 - 11.3   |  [3.3.0](https://spark.apache.org/docs/3.3.0/migration-guide.html)   | |   10.4 LTS - 10.5   |  [3.2.1](https://spark.apache.org/docs/3.2.1/migration-guide.html)   |           Databricks Runtime behavior changes -------------------------------------     The following notable behavior changes were introduced into each of the following DBR 11.x releases.         * [Databricks Runtime 11.3 LTS](11.3.html#behavior-changes) * [Databricks Runtime 11.1](11.1.html#behavior-changes) * [Databricks Runtime 11.0](11.0.html#behavior-changes)           Databricks Runtime system environment properties, features, and libraries ---------------------------------------------------------------------------     For information about system environment properties as well as new, changed, and deprecated features and libraries in Databricks Runtime releases from Databricks Runtime 10.4 LTS to the latest version of Databricks Runtime 11.x, see the following:     * [Databricks Runtime 11.3 LTS for Machine Learning](11.3ml.html) * [Databricks Runtime 11.3 LTS](11.3.html) * [Databricks Runtime 11.2 for Machine Learning](11.2ml.html) * [Databricks Runtime 11.2 (Unsupported)](11.2.html) * [Databricks Runtime 11.1 for Machine Learning](11.1ml.html) * [Databricks Runtime 11.1 (unsupported)](11.1.html) * [Databricks Runtime 11.0 for Machine Learning (Unsupported)](11.0ml.html) * [Databricks Runtime 11.0 (Unsupported)](11.0.html) * [Databricks Runtime 10.5 for Machine Learning (Unsupported)](10.5ml.html) * [Databricks Runtime 10.5 (Unsupported)](10.5.html) * [Databricks Runtime 10.4 LTS for Machine Learning](10.4ml.html) * [Databricks Runtime 10.4 LTS](10.4.html)     Post-release maintenance updates are listed in  [Databricks runtime maintenance updates](maintenance-updates.html)  .	To upgrade from DBR 10.4 to 11.3, you can migrate directly to the latest version of Databricks Runtime 11.x. The migration process is described in the Databricks Runtime 11.x migration guide.	0	1	1
no_reaction	https://docs.databricks.com/dbfs/file-browser.html	How do I store data in DBFS?	What is the Databricks File System (DBFS)? ============================================     The Databricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls.        Note       Databricks workspaces deploy with a  [DBFS root volume](#dbfs-root)  , accessible to all users by default. Databricks recommends against storing production data in this location.           What can you do with DBFS? ----------------------------     DBFS provides convenience by mapping cloud object storage URIs to relative paths.     * Allows you to  [interact with object storage](#interact-files)  using directory and file semantics instead of cloud-specific API commands. * Allows you to  [mount](#mount-storage)  cloud object storage locations so that you can map storage credentials to paths in the Databricks workspace. * Simplifies the process of persisting files to object storage, allowing virtual machines and attached volume storage to be safely deleted on cluster termination. * Provides a convenient location for storing init scripts, JARs, libraries, and configurations for cluster initialization. * Provides a convenient location for checkpoint files created during model training with OSS deep learning libraries.      Note       DBFS is the Databricks implementation for FUSE. See  [How to work with files on Databricks](../files/index.html)  .            Interact with files in cloud-based object storage ---------------------------------------------------     DBFS provides many options for interacting with files in cloud object storage:         * [How to work with files on Databricks](../files/index.html) * [List, move, copy, and delete files with Databricks Utilities](../dev-tools/databricks-utils.html#file-system-utility-dbutilsfs) * [Browse files in DBFS](file-browser.html) * [Upload files to DBFS with the UI](../archive/legacy/data-tab.html) * [Interact with DBFS files using the Databricks CLI](../dev-tools/cli/dbfs-cli.html) * [Interact with DBFS files using the Databricks REST API](../dev-tools/api/latest/dbfs.html)             Mount object storage ----------------------     Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage, so you do not need to specify these settings in code or during cluster configuration.       For more information, see  [Mounting cloud object storage on Databricks](mounts.html)  .           What is the DBFS root? ------------------------     The  *DBFS root*   is the default storage location for a Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Databricks workspace. For details on Databricks Filesystem root configuration and deployment, see  [Configure AWS storage](../administration-guide/cloud-configurations/aws/aws-storage.html)  . For best practices around securing data in the DBFS root, see  [Recommendations for working with DBFS root](dbfs-root.html)  .       Some users of Databricks may refer to the DBFS root as “DBFS” or “the DBFS”; it is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a cloud object storage location. You use DBFS to interact with the DBFS root, but they are distinct concepts, and DBFS has many applications beyond the DBFS root.       The DBFS root contains a number of special locations that serve as defaults for various actions performed by users in the workspace. For details, see  [What directories are in DBFS root by default?](root-locations.html)  .           How does DBFS work with Unity Catalog? ----------------------------------------     Unity Catalog adds the concepts of external locations and managed storage credentials to help organizations provide least privileges access to data in cloud object storage. Unity Catalog also provides a new default storage location for managed tables. Some security configurations provide direct access to both Unity Catalog-managed resources and DBFS. Databricks has compiled recommendations for  [using DBFS and Unity Catalog](unity-catalog.html)  .	To store data in DBFS, you can interact with object storage using directory and file semantics instead of cloud-specific API commands. You can also mount cloud object storage locations so that you can map storage credentials to paths in the Databricks workspace. DBFS simplifies the process of persisting files to object storage, allowing virtual machines and attached volume storage to be safely deleted on cluster termination. For more information on how to work with files on Databricks, you can refer to the links provided in the context.	0	1	1
no_reaction	https://docs.databricks.com/clusters/clusters-manage.html	how to terminate a cluster in a notebook	Title: Unexpected cluster termination  Description: Learn how to troubleshoot a Databricks cluster that stopped unexpectedly.  Answer: Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination. A cluster can be terminated for many reasons. Some terminations are initiated by Databricks and others are initiated by the cloud provider. This article describes termination reasons and steps for remediation. Databricks initiated request limit exceeded To defend against API abuses, ensure quality of service, and prevent you from accidentally creating too many large clusters, Databricks throttles all cluster up-sizing requests, including cluster creation, starting, and resizing. The throttling uses the token bucket algorithm to limit the total number of nodes that anyone can launch over a defined interval across your Databricks deployment, while allowing burst requests of certain sizes. Requests coming from both the web UI and the APIs are subject to rate limiting. When cluster requests exceed rate limits, the limit-exceeding request fails with a REQUEST_LIMIT_EXCEEDED error. Solution If you hit the limit for your legitimate workflow, Databricks recommends that you do the following: Retry your request a few minutes later. Spread out your recurring workflow evenly in the planned time frame. For example, instead of scheduling all of your jobs to run at an hourly boundary, try distributing them at different intervals within the hour. Consider using clusters with a larger node type and smaller number of nodes. Use autoscaling clusters. If these options don’t work for you, contact Databricks Support to request a limit increase for the core instance. For other Databricks initiated termination reasons, see Termination Code. Cloud provider initiated terminations This article lists common cloud provider related termination reasons and remediation steps. AWS Provider limit Databricks launches a cluster by requesting resources on behalf of your cloud account. Sometimes, these requests fail because they would exceed your cloud account’s resource limits. In AWS, common error codes include: InstanceLimitExceeded AWS limits the number of running instances for each node type. Possible solutions include: Request a cluster with fewer nodes. Request a cluster with a different node type. Ask AWS support to increase instance limits. Client.VolumeLimitExceeded The cluster creation request exceeded the EBS volume limit. AWS has two types of volume limits: a limit on the total number of EBS volumes, and a limit on the total storage size of EBS volumes. Potential remediation steps: Request a cluster with fewer nodes. Check which of the two limits was exceeded. (AWS trusted advisor shows service limits for free). If the request exceeded the total number of EBS volumes, try reducing the requested number of volumes per node. If the request exceeded the total EBS storage size, try reducing the requested storage size and/or the number of EBS volumes. Ask AWS support to increase EBS volume limits. RequestLimitExceeded AWS limits the rate of API requests made for an AWS account. Wait a while before retrying the request. Provider shutdown The Spark driver is a single point of failure because it holds all cluster state. If the instance hosting the driver node is shut down, Databricks terminates the cluster. In AWS, common error codes include: Client.UserInitiatedShutdown Instance was terminated by a direct request to AWS which did not originate from Databricks. Contact your AWS administrator for more details. Server.InsufficientInstanceCapacity AWS could not satisfy the instance request. Wait a while and retry the request. Contact AWS support if the problem persists. Server.SpotInstanceTermination Instance was terminated by AWS because the current spot price has exceeded the maximum bid made for this instance. Use an on-demand instance for the driver, choose a different availability zone, or specify a higher spot bid price. For other shutdown-related error codes, refer to AWS docs. Launch failure AWS In AWS, common error codes include: UnauthorizedOperation Databricks was not authorized to launch the requested instances. Possible reasons include: Your AWS administrator invalidated the AWS access key or IAM role used to launch instances. You are trying to launch a cluster using an IAM role that Databricks does not have permission to use. Contact the AWS administrator who set up the IAM role. For more information, see Secure Access to S3 Buckets Using IAM Roles. Unsupported with message “EBS-optimized instances are not supported for your requested configuration” The selected instance type is not available in the selected availability zone (AZ). It does not actually have anything to do with EBS-optimization being enabled. To remediate, you can choose a different instance type or AZ. AuthFailure.ServiceLinkedRoleCreationNotPermitted The provided credentials do not have permission to create the service-linked role for EC2 spot instances. The Databricks administrator needs to update the credentials used to launch instances in your account. Instructions and the updated policy can be found AWS Account. See Error Codes for a complete list of AWS error codes. Azure This termination reason occurs when Azure Databricks fails to acquire virtual machines. The error code and message from the API are propagated to help you troubleshoot the issue. OperationNotAllowed You have reached a quota limit, usually number of cores, that your subscription can launch. Request a limit increase in Azure portal. See Azure subscription and service limits, quotas, and constraints. PublicIPCountLimitReached You have reached the limit of the public IPs that you can have running. Request a limit increase in Azure Portal. SkuNotAvailable The resource SKU you have selected (such as VM size) is not available for the location you have selected. To resolve, see Resolve errors for SKU not available. ReadOnlyDisabledSubscription Your subscription was disabled. Follow the steps in Why is my Azure subscription disabled and how do I reactivate it? to reactivate your subscription. ResourceGroupBeingDeleted Can occur if someone cancels your Azure Databricks workspace in the Azure portal and you try to create a cluster at the same time. The cluster fails because the resource group is being deleted. SubscriptionRequestsThrottled Your subscription is hitting the Azure Resource Manager request limit (see Throttling Resource Manager requests). Typical cause is that another system outside Azure Databricks) making a lot of API calls to Azure. Contact Azure support to identify this system and then reduce the number of API calls. Communication lost Databricks was able to launch the cluster, but lost the connection to the instance hosting the Spark driver. AWS Caused by an incorrect networking configuration (for example, changing security group settings for Databricks workers) or a transient AWS networking issue. Azure Caused by the driver virtual machine going down or a networking issue.	To terminate a cluster in a notebook, you can go to the Clusters tab in the Databricks workspace, select the cluster you want to terminate, and click the "Terminate" button.	0	1	1
thumb_up	https://kb.databricks.com/clusters/cluster-spark-config-not-applied	how do I optimize my spark cluster?	Optimization recommendations on Databricks ============================================     Databricks provides many optimizations supporting a variety of workloads on the lakehouse, ranging from large-scale ETL processing to ad-hoc, interactive queries. Many of these optimizations take place automatically. You get their benefits simply by using Databricks. Additionally, most Databricks Runtime features require Delta Lake, the default storage layer used to create tables in Databricks.       Databricks configures default values that optimize most workloads. But, in some cases, changing configuration settings improves performance.            Databricks Runtime performance enhancements ---------------------------------------------      Note       Use the latest Databricks Runtime to leverage the newest performance enhancements. All behaviors documented here are enabled by default in Databricks Runtime 10.4 LTS and above.      * [Disk caching](disk-cache.html)  accelerates repeated reads against Parquet data files by loading data to disk volumes attached to compute clusters. * [Dynamic file pruning](dynamic-file-pruning.html)  improves query performance by skipping directories that do not contain data files that match query predicates. * [Low shuffle merge](low-shuffle-merge.html)  reduces the number of data files rewritten by  `MERGE`  operations and reduces the need to recaculate  `ZORDER`  clusters. * Apache Spark 3.0 introduced  [adaptive query execution](aqe.html)  , which provides enhanced performance for many operations.       Databricks recommendations for enhanced performance -----------------------------------------------------   * You can  [clone](../delta/clone.html)  tables on Databricks to make deep or shallow copies of source datasets. * The  [cost-based optimizer](cbo.html)  accelerates query performance by leveraging table statistics. * You can  [auto optimize](auto-optimize.html)  Delta tables using optimized writes and automatic file compaction; this is especially useful for long-running Structured Streaming jobs. * You can use Spark SQL to interact with  [semi-structured JSON data](semi-structured.html)  without parsing strings. * [Higher order functions](higher-order-lambda-functions.html)  provide built-in, optimized performance for many operations that do not have common Spark operators. Higher order functions provide a performance benefit over user defined functions. * Databricks provides a number of built-in operators and special syntax for working with  [complex data types](complex-types.html)  , including arrays, structs, and JSON strings. * You can manually tune settings for joins that include  [ranges](range-join.html)  or contain data with substanial  [skew](skew-join.html)  .       Opt-in behaviors ------------------   * Databricks provides a write serializable isolation guarantee by default; changing the  [isolation level](isolation-level.html)  to serializable can reduce throughput for concurrent operations, but might be necessary when read serializability is required. * You can use  [bloom filter indexes](bloom-filters.html)  to reduce the likelihood of scanning data files that don’t contain records matching a given condition.	To optimize your Spark cluster, you can use the default optimizations provided by Databricks, which are automatically applied when using Databricks. Additionally, you can manually change configuration settings to improve performance. Databricks also provides a number of recommendations for enhanced performance, such as using disk caching, dynamic file pruning, and adaptive query execution. You can also clone tables, use the cost-based optimizer, auto-optimize Delta tables, use Spark SQL to interact with semi-structured JSON data, use higher order functions, and work with complex data types. Finally, you can opt-in to behaviors such as changing the isolation level to serializable and using bloom filter indexes to reduce the likelihood of scanning data files that don’t contain records matching a given condition.	1	1	1
no_reaction	https://kb.databricks.com/metastore/hive-metastore-troubleshooting	what version of external hive are supported as a metastore	      External Apache Hive metastore ================================     This article describes how to set up Databricks clusters to connect to existing external Apache Hive metastores. It provides information about metastore deployment modes, recommended network setup, and cluster configuration requirements, followed by instructions for configuring clusters to connect to an external metastore. For Hive library versions included in Databricks Runtime, see the relevant Databricks Runtime version  [release notes](../../release-notes/runtime/index.html)  .        Important     * If you use Azure Database for MySQL as an external metastore, you must change the value of the  `lower\_case\_table\_names`  property from 1 (the default) to 2 in the server-side database configuration. For details, see  [Identifier Case Sensitivity](https://dev.mysql.com/doc/refman/5.6/en/identifier-case-sensitivity.html)   . * If you use a read-only metastore database, Databricks strongly recommends that you set  `spark.databricks.delta.catalog.update.enabled`  to  `false`  on your clusters for better performance.           Hive metastore deployment modes ---------------------------------     In a production environment, you can deploy a Hive metastore in two modes: local and remote.      **Local mode**       The metastore client running inside a cluster connects to the underlying metastore database directly via JDBC.      **Remote mode**       Instead of connecting to the underlying database directly, the metastore client connects to a separate metastore service via the Thrift protocol. The metastore service connects to the underlying database. When running a metastore in remote mode, DBFS is  *not supported*   .       For more details about these deployment modes, see the  [Hive documentation](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration)   .        Note       The examples in this document use MySQL as the underlying metastore database.              Network setup ---------------     Databricks clusters run inside a virtual private cloud (VPC). We recommend that you set up the external Hive metastore inside a new VPC and then peer these two VPCs to make clusters connect to the Hive metastore using a private IP address.  [VPC peering](../../administration-guide/cloud-configurations/aws/vpc-peering.html)  provides detailed instructions about how to peer the VPC used by Databricks clusters and the VPC where the metastore lives. After peering the VPCs, you can test network connectivity from a cluster to the metastore VPC by running the following command inside a notebook:            ``` %sh nc -vz <DNS name or private IP> <port>  ```      where     * `<DNS     name     or     private     IP>`  is the DNS name or the private IP address of the MySQL database (for local mode) or the metastore service (for remote mode). If you use a DNS name here, make sure that the resolved IP address is a private one. * `<port>`  is the port of the MySQL database or the port of the metastore service.       Cluster configurations ------------------------     You must set three sets of configuration options to connect a cluster to an external metastore:     * [Spark options](#spark-options)  configure Spark with the Hive metastore version and the JARs for the metastore client. * [Hive options](#hive-options)  configure the metastore client to connect to the external metastore. * An optional set of  [Hadoop options](#file-options)  configure file system options.          ###   Spark configuration options     Set  `spark.sql.hive.metastore.version`  to the version of your Hive metastore and  `spark.sql.hive.metastore.jars`  as follows:     * Hive 0.13: do not set  `spark.sql.hive.metastore.jars`  .        Note       Hive 1.2.0 and 1.2.1 are not the built-in metastore on Databricks Runtime 7.0 and above. If you want to use Hive 1.2.0 or 1.2.1 with Databricks Runtime 7.0 and above, follow the procedure described in  [Download the metastore jars and point to them](#download-the-metastore-jars-and-point-to-them)  . * Hive 2.3.7 (Databricks Runtime 7.0 - 9.x) or Hive 2.3.9 (Databricks Runtime 10.0 and above): set  `spark.sql.hive.metastore.jars`  to  `builtin`  . * For all other Hive versions, Databricks recommends that you download the metastore JARs and set the configuration  `spark.sql.hive.metastore.jars`  to point to the downloaded JARs using the procedure described in  [Download the metastore jars and point to them](#download-the-metastore-jars-and-point-to-them)  .    ####   Download the metastore jars and point to them   1. Create a cluster with  `spark.sql.hive.metastore.jars`  set to  `maven`  and  `spark.sql.hive.metastore.version`  to match the version of your metastore. 2. When the cluster is running, search the driver log and find a line like the following:        ``` 17/11/18 22:41:19 INFO IsolatedClientLoader: Downloaded metastore jars to <path>  ```      The directory  `<path>`  is the location of downloaded JARs in the driver node of the cluster.       Alternatively you can run the following code in a Scala notebook to print the location of the JARs:        ``` import com.typesafe.config.ConfigFactory val path = ConfigFactory.load().getString("java.io.tmpdir")  println(s"\nHive JARs are downloaded to the path: $path \n")  ``` 3. Run  `%sh     cp     -r     <path>     /dbfs/hive\_metastore\_jar`  (replacing  `<path>`  with your cluster’s info) to copy this directory to a directory in DBFS root called  `hive\_metastore\_jar`  through the DBFS client in the driver node. 4. Create an  [init script](../../clusters/init-scripts.html)  that copies  `/dbfs/hive\_metastore\_jar`  to the local filesystem of the node, making sure to make the init script sleep a few seconds before it accesses the DBFS client. This ensures that the client is ready. 5. Set  `spark.sql.hive.metastore.jars`  to use this directory. If your init script copies  `/dbfs/hive\_metastore\_jar`  to  `/databricks/hive\_metastore\_jars/`  , set  `spark.sql.hive.metastore.jars`  to  `/databricks/hive\_metastore\_jars/\*`  . The location must include the trailing  `/\*`  . 6. Restart the cluster.        ###   Hive configuration options     This section describes options specific to Hive.      ####   Configuration options for local mode     To connect to an external metastore using local mode, set the following Hive configuration options:        ``` # JDBC connect string for a JDBC metastore javax.jdo.option.ConnectionURL jdbc:mysql://<metastore-host>:<metastore-port>/<metastore-db>  # Username to use against metastore database javax.jdo.option.ConnectionUserName <mysql-username>  # Password to use against metastore database javax.jdo.option.ConnectionPassword <mysql-password>  # Driver class name for a JDBC metastore (Runtime 3.4 and later) javax.jdo.option.ConnectionDriverName org.mariadb.jdbc.Driver  # Driver class name for a JDBC metastore (prior to Runtime 3.4) # javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver  ```      where     * `<metastore-host>`  and  `<metastore-port>`  are the host and listening port of your MySQL instance. * `<metastore-db>`  is the name of the MySQL database that holds all of the metastore tables. * `<mysql-username>`  and  `<mysql-password>`  specify the username and password of your MySQL account that has read/write access to  `<metastore-db>`  .      Note     * Use the MariaDB driver to communicate with MySQL databases. * For production environments, we recommend that you set  `hive.metastore.schema.verification`  to  `true`  . This prevents Hive metastore client from implicitly modifying the metastore database schema when the metastore client version does not match the metastore database version. When enabling this setting for metastore client versions lower than Hive 1.2.0, make sure that the metastore client has the write permission to the metastore database (to prevent the issue described in  [HIVE-9749](https://issues.apache.org/jira/browse/HIVE-9749)   ).      + For Hive metastore 1.2.0 and higher, set   `hive.metastore.schema.verification.record.version`   to   `true`   to enable   `hive.metastore.schema.verification`   .  + For Hive metastore 2.1.1 and higher, set   `hive.metastore.schema.verification.record.version`   to   `true`   as it is set to   `false`   by default.      ####   Configuration options for remote mode     To connect to an external metastore using remote mode, set the following Hive configuration option:        ``` # Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore. hive.metastore.uris thrift://<metastore-host>:<metastore-port>  ```      where  `<metastore-host>`  and  `<metastore-port>`  are the listening host and port of your Hive metastore service.            ###   File system options     If you want to use an  [instance profile and set AssumeRole](../../archive/admin-guide/assume-role.html)  , you must set:     * `fs.s3a.credentialsType`  to  `AssumeRole` * `fs.s3a.stsAssumeRole.arn`  to the Amazon Resource Name (ARN) of the role to assume        Set up an external metastore using the UI -------------------------------------------     To set up an external metastore using the Databricks UI:     1. Click the  **Clusters**   button on the sidebar. 2. Click  **Create Cluster**   . 3. Enter the following  [Spark configuration options](../../clusters/configure.html#spark-configuration)  :      **Local mode**        ``` # Hive specific configuration options. # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client. spark.hadoop.javax.jdo.option.ConnectionURL jdbc:mysql://<mysql-host>:<mysql-port>/<metastore-db>  # Driver class name for a JDBC metastore (Runtime 3.4 and later) spark.hadoop.javax.jdo.option.ConnectionDriverName org.mariadb.jdbc.Driver  # Driver class name for a JDBC metastore (prior to Runtime 3.4) # spark.hadoop.javax.jdo.option	Based on the context, the following versions of external Hive metastore are supported:   - Hive 0.13: do not set `spark.sql.hive.metastore.jars`. - Hive 2.3.7 (Databricks Runtime 7.0 - 9.x) or Hive 2.3.9 (Databricks Runtime 10.0 and above): set `spark.sql.hive.metastore.jars` to `builtin`. - For all other Hive versions, Databricks recommends that you download the metastore JARs and set the configuration `spark.sql.hive.metastore.jars` to point to the downloaded JARs using the procedure described in the context.	0	1	1
thumb_up	https://docs.databricks.com/lakehouse/medallion.html	explain medalian architecture on databricks	What is the medallion lakehouse architecture? ===============================================     The medallion architecture describes a series of data layers that denote the quality of data stored in the lakehouse. Databricks recommends taking a multi-layered approach to building a single source of truth for enterprise data products. This architecture guarantees atomicity, consistency, isolation, and durability as data passes through multiple layers of validations and transformations before being stored in a layout optimized for efficient analytics. The terms  [bronze](#bronze)  (raw),  [silver](#silver)  (validated), and  [gold](#gold)  (enriched) describe the quality of the data in each of these layers.       It is important to note that this medallion architecture does not replace other dimensional modeling techniques. Schemas and tables within each layer can take on a variety of forms and degrees of normalization depending on the frequency and nature of data updates and the downstream use cases for the data.       Organizations can leverage the Databricks Lakehouse to create and maintain validated datasets accessible throughout the company. Adopting an organizational mindset focused on curating data-as-products is a key step in successfully building a data lakehouse.          Ingest raw data to the bronze layer -------------------------------------     The bronze layer contains unvalidated data. Data ingested in the bronze layer typically:     * Maintains the raw state of the data source. * Is appended incrementally and grows over time. * Can be any combination of streaming and batch transactions.     Retaining the full, unprocessed history of each dataset in an efficient storage format provides the ability to recreate any state of a given data system.       Additional metadata (such as source file names or recording the time data was processed) may be added to data on ingest for enhanced discoverability, description of the state of the source dataset, and optimized performance in downstream applications.           Validate and deduplicate data in the silver layer ---------------------------------------------------     Recall that while the bronze layer contains the entire data history in a nearly raw state, the silver layer represents a validated, enriched version of our data that can be trusted for downstream analytics.       While Databricks believes strongly in the lakehouse vision driven by bronze, silver, and gold tables, simply implementing a silver layer efficiently will immediately unlock many of the potential benefits of the lakehouse.       For any data pipeline, the silver layer may contain more than one table.           Power analytics with the gold layer -------------------------------------     This gold data is often highly refined and aggregated, containing data that powers analytics, machine learning, and production applications. While all tables in the lakehouse should serve an important purpose, gold tables represent data that has been transformed into knowledge, rather than just information.       Analysts largely rely on gold tables for their core responsibilities, and data shared with a customer would rarely be stored outside this level.       Updates to these tables are completed as part of regularly scheduled production workloads, which helps control costs and allows service level agreements (SLAs) for data freshness to be established.       While the lakehouse doesn’t have the same deadlock issues that you may encounter in a enterprise data warehouse, gold tables are often stored in a separate storage container to help avoid cloud limits on data requests.       In general, because aggregations, joins, and filtering are handled before data is written to the gold layer, users should see low latency query performance on data in gold tables.	The medallion architecture is a multi-layered approach to building a single source of truth for enterprise data products. It guarantees atomicity, consistency, isolation, and durability as data passes through multiple layers of validations and transformations before being stored in a layout optimized for efficient analytics. The terms bronze (raw), silver (validated), and gold (enriched) describe the quality of the data in each of these layers. The bronze layer contains unvalidated data, the silver layer represents a validated, enriched version of the data, and the gold layer contains highly refined and aggregated data that powers analytics, machine learning, and production applications.	1	1	1
thumb_up	https://kb.databricks.com/security/forbidden-access-to-s3-data	java.lang.IllegalArgumentException: Wrong FS s3 expected dbfs	Title: Forbidden error while accessing S3 data  Description:   Answer: Problem While trying to access S3 data using DBFS mount or directly in Spark APIs, the command fails with an exception similar to the following: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden; Request ID: XXXXXXXXXXXXX, Extended Request ID: XXXXXXXXXXXXXXXXXXX, Cloud Provider: AWS, Instance ID: XXXXXXXXXX (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: XXXXXXXXXXX; S3 Extended Request ID: Cause Below are the common causes: AWS keys are used in addition to the IAM role. Using global init scripts to set the AWS keys can cause this behavior. The IAM role has the required permission to access the S3 data, but AWS keys are set in the Spark configuration. For example, setting spark.hadoop.fs.s3a.secret.key can conflict with the IAM role. Setting AWS keys at environment level on the driver node from an interactive cluster through a notebook. DBFS mount points were created earlier with AWS keys and now trying to access using an IAM role. The files are written outside Databricks, and the bucket owner does not have read permission (see Step 7: Update cross-account S3 object ACLs). The IAM role is not attached to the cluster. The IAM role with read permission was attached, but you are trying to perform a write operation. That is, the IAM role does not have adequate permission for the operation you are trying to perform. Solution Below are the recommendations and best practices to avoid this issue: Use IAM roles instead of AWS keys. If you are trying to switch the configuration from AWS keys to IAM roles, unmount the DBFS mount points for S3 buckets created using AWS keys and remount using the IAM role. Avoid using global init script to set AWS keys. Always use a cluster-scoped init script if required. Avoid setting AWS keys in a notebook or cluster Spark configuration.	The error "java.lang.IllegalArgumentException: Wrong FS s3 expected DBFS (Databricks File Store)" is likely caused by trying to access S3 data using DBFS mount or directly in Spark APIs. This can happen if AWS keys are used in addition to the IAM role, or if the IAM role has the required permission to access the S3 data but AWS keys are set in the Spark configuration. The solution is to use IAM roles instead of AWS keys, unmount DBFS mount points for S3 buckets created using AWS keys and remount using the IAM role, avoid using global init script to set AWS keys, and avoid setting AWS keys in a notebook or cluster Spark configuration.	0	1	1
no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.Series.str.split.html	How do i split a string to columns based on the delimited ":" in a dataframe	   pyspark.pandas.Series.str.split  [¶](#pyspark-pandas-series-str-split "Permalink to this headline") =====================================================================================================  `str.`  `split`    (   *pat     :     Optional    [    str    ]     =     None*  ,  *n     :     int     =     - 1*  ,  *expand     :     bool     =     False*   )    → Union    [    pyspark.pandas.series.Series    ,    pyspark.pandas.frame.DataFrame    ]   [¶](#pyspark.pandas.Series.str.split "Permalink to this definition")    Split strings around given separator/delimiter.     Splits the string in the Series from the beginning, at the specified delimiter string. Equivalent to  [`str.split()`](#pyspark.pandas.Series.str.split "pyspark.pandas.Series.str.split")  .     Parameters    **pat**    str, optional     String or regular expression to split on. If not specified, split on whitespace.    **n**    int, default -1 (all)     Limit number of splits in output. None, 0 and -1 will be interpreted as return all splits.    **expand**    bool, default False     Expand the split strings into separate columns.    * If  `True`  ,    n    must be a positive integer, and return DataFrame expanding dimensionality. * If  `False`  , return Series, containing lists of strings.   Returns     Series, DataFrame     Type matches caller unless    expand=True    (see Notes).     See also    [`str.rsplit`](pyspark.pandas.Series.str.rsplit.html#pyspark.pandas.Series.str.rsplit "pyspark.pandas.Series.str.rsplit")   Splits string around given separator/delimiter, starting from the right.    [`str.join`](pyspark.pandas.Series.str.join.html#pyspark.pandas.Series.str.join "pyspark.pandas.Series.str.join")   Join lists contained as elements in the Series/Index with passed delimiter.     Notes     The handling of the    n    keyword depends on the number of found splits:    * If found splits >    n    , make first    n    splits only * If found splits <=    n    , make all splits * If for a certain row the number of found splits <    n    , append    None    for padding up to    n    if  `expand=True`   If using  `expand=True`  , Series callers return DataFrame objects with    n + 1    columns.     Note     Even if    n    is much larger than found splits, the number of columns does NOT shrink unlike pandas.     Examples    ``` >>> s = ps.Series(["this is a regular sentence", ...                "https://docs.python.org/3/tutorial/index.html", ...                np.nan])  ```   In the default setting, the string is split by whitespace.    ``` >>> s.str.split() 0 [this, is, a, regular, sentence] 1 [https://docs.python.org/3/tutorial/index.html] 2 None dtype: object  ```   Without the n parameter, the outputs of rsplit and split are identical.    ``` >>> s.str.rsplit() 0 [this, is, a, regular, sentence] 1 [https://docs.python.org/3/tutorial/index.html] 2 None dtype: object  ```   The n parameter can be used to limit the number of splits on the delimiter. The outputs of split and rsplit are different.    ``` >>> s.str.split(n=2) 0 [this, is, a regular sentence] 1 [https://docs.python.org/3/tutorial/index.html] 2 None dtype: object  ```  ``` >>> s.str.rsplit(n=2) 0 [this is a, regular, sentence] 1 [https://docs.python.org/3/tutorial/index.html] 2 None dtype: object  ```   The pat parameter can be used to split by other characters.    ``` >>> s.str.split(pat = "/") 0 [this is a regular sentence] 1 [https:, , docs.python.org, 3, tutorial, index... 2 None dtype: object  ```   When using  `expand=True`  , the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split.    ``` >>> s.str.split(n=4, expand=True)  0 1 2 3 4 0 this is a regular sentence 1 https://docs.python.org/3/tutorial/index.html None None None None 2 None None None None None  ```   For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used.    ``` >>> s.str.rsplit("/", n=1, expand=True)  0 1 0 this is a regular sentence None 1 https://docs.python.org/3/tutorial index.html 2 None None  ```   Remember to escape special characters when explicitly using regular expressions.    ``` >>> s = ps.Series(["1+1=2"]) >>> s.str.split(r"\+|=", n=2, expand=True)  0 1 2 0 1 1 2  ```  [pyspark.pandas.Series.str.slice\_replace](pyspark.pandas.Series.str.slice_replace.html "previous page")  [pyspark.pandas.Series.str.startswith](pyspark.pandas.Series.str.startswith.html "next page")   	To split a string to columns based on the delimited ":" in a dataframe, you can use the `str.split()` method in PySpark. Specifically, you can use the `str.split()` method on a PySpark DataFrame column and set the delimiter to ":". You can also set the `expand` parameter to `True` to expand the split strings into separate columns. Here is an example code snippet:  ``` from pyspark.sql.functions import split  df = df.withColumn('new_col', split(dfold_col, ':')) df = df.selectExpr('*', 'new_col[0] as col1', 'new_col[1] as col2') ```  This code splits the values in the 'old_col' column of the PySpark DataFrame 'df' based on the delimiter ":" and creates a new column 'new_col' with the resulting split values. Then, it selects the original columns and the new split columns 'col1' and 'col2' from the DataFrame.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/credential-passthrough/index.html	can I use credential pass through	Credential passthrough (legacy) =================================     Credential passthrough allows you to authenticate automatically to S3 buckets from Databricks clusters using the identity that you use to log in to Databricks.        Note       Credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to Unity Catalog. Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See  [What is Unity Catalog?](../unity-catalog/index.html)  .        This section covers:      * [Access S3 with IAM credential passthrough with SCIM (legacy)](iam-passthrough.html) * [Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)](iam-federation.html)	Yes, you can use credential passthrough to authenticate automatically to S3 buckets from Databricks clusters using the identity that you use to log in to Databricks. However, it is important to note that credential passthrough is a legacy data governance model and Databricks recommends upgrading to Unity Catalog for simplified security and governance of data access.	0	1	1
no_reaction	https://docs.databricks.com/introduction/index.html	What is databricks	What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used to process, store, clean, share, analyze, model, and monetize datasets with solutions from BI to machine learning. The Databricks platform provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks can be interacted with programmatically with REST API, CLI, and Terraform. It works with AWS by configuring a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control. Databricks is used for various use cases such as building an enterprise data lakehouse, ETL and data engineering, machine learning, AI, and data science, data warehousing, analytics, and BI, data governance and secure data sharing, DevOps, CI/CD, and task orchestration, and real-time and streaming analytics.	1	1	1
no_reaction	https://docs.databricks.com/workflows/jobs/jobs.html	Can I create a job?	    Create, run, and manage Databricks Jobs =========================================     This article details how to create, edit, run, and monitor Databricks Jobs using the Jobs UI. To learn about using the Databricks CLI to create and run jobs, see  [Jobs CLI](../../dev-tools/cli/jobs-cli.html)  . To learn about using the Jobs API, see  [Jobs API 2.1](../../dev-tools/api/latest/jobs.html)  .       To create your first workflow with a Databricks job, see the  [quickstart](jobs-quickstart.html)  .        Important     * You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. * A workspace is limited to 1000 concurrent task runs. A  `429     Too     Many     Requests`  response is returned when you request a run that cannot start immediately. * The number of jobs a workspace can create in an hour is limited to 10000 (includes “runs submit”). This limit also affects jobs created by the REST API and notebook workflows.         Create a job --------------   1. Do one of the following:      * Click   ![Jobs Icon](../../_images/jobs-icon.png)  **Workflows**    in the sidebar and click   ![Create Job Button](../../_images/create-job.png)   .  * In the sidebar, click   ![New Icon](../../_images/create-icon.png)  **New**    and select   **Job**    .  The  **Tasks**   tab appears with the create task dialog.      ![Create task screen](../../_images/create-task.png) 2. Replace  **Add a name for your job…**   with your job name. 3. Enter a name for the task in the  **Task name**   field. 4. In the  **Type**   dropdown menu, select the type of task to run. See  [Task type options](#task-types)  . 5. Configure the cluster where the task runs. In the  **Cluster**   dropdown menu, select either  **New job cluster**   or  **Existing All-Purpose Clusters**   .      * **New Job Cluster**    : Click   **Edit**    in the   **Cluster**    dropdown menu and complete the   [cluster configuration](../../clusters/configure.html)   .  * **Existing All-Purpose Cluster**    : Select an existing cluster in the   **Cluster**    dropdown menu. To open the cluster in a new page, click the   ![External Link](../../_images/external-link.png)   icon to the right of the cluster name and description.  To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  . 6. To add dependent libraries, click  **+ Add**   next to  **Dependent libraries**   . See  [Dependent libraries](#task-config-dependent-libraries)  . 7. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters.      * **Notebook**    : Click   **Add**    and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the   [Run a job with different parameters](#job-run-with-different-params)   option. Parameters set the value of the   [notebook widget](../../notebooks/widgets.html)   specified by the key of the parameter. Use   [task parameter variables](#parameter-variables)   to pass a limited set of dynamic values as part of a parameter value.  * **JAR**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See   [Configure JAR job parameters](#configure-jar-job-parameters)   .  * **Spark Submit**    task: Parameters are specified as a JSON-formatted array of strings. Conforming to the   [Apache Spark spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html)    convention, parameters after the JAR path are passed to the main method of the main class.  * **Python script**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the   [argparse](https://docs.python.org/3/library/argparse.html)    module in Python.  * **Python Wheel**    : In the   **Parameters**    dropdown menu, select   **Positional arguments**    to enter parameters as a JSON-formatted array of strings, or select   **Keyword arguments > Add**    to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. 8. To optionally receive notifications for task start, success, or failure, click  **+ Add**   next to  **Emails**   . Failure notifications are sent on initial task failure and any subsequent retries. 9. To optionally configure a retry policy for the task, click  **+ Add**   next to  **Retries**   . See  [Retries](#retry-policies)  . 10. To optionally configure a timeout for the task, click  **+ Add**   next to  **Timeout in seconds**   . See  [Timeout](#timeout-setting)  . 11. Click  **Create**   .     After creating the first task, you can configure job-level settings such as notifications, job triggers, and permissions. See  [Edit a job](#job-edit)  .       To add another task, click  ![Add Task Button](../../_images/add-task.png)  in the DAG view. A shared cluster option is provided if you have configured a  **New Job Cluster**   for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  .           Task type options -------------------     The following are the task types you can add to your Databricks job and available options for the different task types:     * **Notebook**   : In the  **Source**   dropdown menu, select a location for the notebook; either  **Workspace**   for a notebook located in a Databricks workspace folder or  **Git provider**   for a notebook located in a remote Git repository.      **Workspace**   : Use the file browser to find the notebook, click the notebook name, and click  **Confirm**   .      **Git provider**   : Click  **Edit**   and enter the Git repository information. See  [Use version controlled notebooks in a Databricks job](how-to/use-repos.html)  . * **JAR**   : Specify the  **Main class**   . Use the fully qualified name of the class containing the main method, for example,  `org.apache.spark.examples.SparkPi`  . Then click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. One of these libraries must contain the main class.       To learn more about JAR tasks, see  [JAR jobs](#jar-jobs)  . * **Spark Submit**   : In the  **Parameters**   text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the  `DFSReadWriteTest`  from the Apache Spark examples:        ``` ["--class","org.apache.spark.examples.DFSReadWriteTest","dbfs:/FileStore/libraries/spark\_examples\_2\_12\_3\_1\_1.jar","/dbfs/databricks-datasets/README.md","/FileStore/examples/output/"]  ```       Important       There are several limitations for  **spark-submit**   tasks:       + You can run spark-submit tasks only on new clusters.  + Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see   [Cluster autoscaling](../../clusters/configure.html#autoscaling)   .  + Spark-submit does not support   [Databricks Utilities](../../dev-tools/databricks-utils.html)   . To use Databricks Utilities, use JAR tasks instead.  + If you are using a Unity Catalog-enabled cluster, spark-submit is supported only if the cluster uses Single User   [access mode](../../data-governance/unity-catalog/compute.html#access-mode)   . Shared access mode is not supported. * **Python script**   : In the  **Source**   drop-down, select a location for the Python script, either  **Workspace**   for a script in the local workspace, or  **DBFS / S3**   for a script located on DBFS or cloud storage. In the  **Path**   textbox, enter the path to the Python script:      **Workspace**   : In the  **Select Python File**   dialog, browse to the Python script and click  **Confirm**   . Your script must be in a  [Databricks repo](../../files/workspace.html)  .      **DBFS**   : Enter the URI of a Python script on DBFS or cloud storage; for example,  `dbfs:/FileStore/myscript.py`  . * **Delta Live Tables Pipeline**   : In the  **Pipeline**   dropdown menu, select an existing  [Delta Live Tables](../../delta-live-tables/index.html)  pipeline.        Important       You can use only triggered pipelines with the  **Pipeline**   task. Continuous pipelines are not supported as a job task. To learn more about triggered and continuous pipelines, see  [Continuous vs. triggered pipeline execution](../../delta-live-tables/updates.html#continuous-triggered)  . * **Python Wheel**   : In the  **Package name**   text box, enter the package to import, for example,  `myWheel-1.0-py2.py3-none-any.whl`  . In the  **Entry Point**   text box, enter the function to call when starting the wheel. Click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. * **SQL**   : In the  **SQL task**   dropdown menu, select  **Query**   ,  **Dashboard**   , or  **Alert**   .        Note       + The   **SQL**    task is in   [Public Preview](../../release-notes/release-types.html)   .  + The   **SQL**    task requires Databricks SQL and a   [serverless or pro SQL warehouse](../../sql/index.html#warehouse-types)   .   **Query**   : In the  **SQL query**   dropdown menu, select the query to execute when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Dashboard**   : In the  **SQL dashboard**   dropdown menu, select a dashboard to be updated when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Alert**   : In the  **SQL alert**   dropdown menu, select an alert to trigger for evaluation. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task. * **dbt**   : See  [Use dbt transformations in a Databricks job](how-to/use-dbt-in-workflows.html)  for a detailed example	Yes, you can create a job in Databricks. To create a job, you can use the Jobs UI by clicking on the "Workflows" icon in the sidebar and then clicking on the "Create Job" button. Alternatively, you can click on "New" in the sidebar and select "Job". Then, you can follow the steps to configure the task, including selecting the task type, configuring the cluster, adding dependent libraries, passing parameters, and configuring notifications, retries, and timeouts. Finally, you can click on "Create" to create the job.	1	1	1
no_reaction	https://docs.databricks.com/external-data/json.html	How do i import a json file	      JSON file ===========     You can read JSON files in  [single-line](#single-line-mode)  or  [multi-line](#multi-line-mode)  mode. In single-line mode, a file can be split into many parts and read in parallel. In multi-line mode, a file is loaded as a whole entity and  *cannot*   be split.       For further information, see  [JSON Files](https://spark.apache.org/docs/latest/sql-data-sources-json.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html?highlight=json#pyspark.sql.DataFrameReader.json)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#json(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html?highlight=json#pyspark.sql.DataFrameWriter.json)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#json(path:String):Unit)       Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.2 (Unsupported)](../release-notes/runtime/8.2.html)  and above.        The rescued data column ensures that you never lose or miss out on data during ETL. The rescued data column contains any data that wasn’t parsed, either because it was missing from the given schema, or because there was a type mismatch, or because the casing of the column in the record or file didn’t match with that in the schema. The rescued data column is returned as a JSON blob containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set("spark.databricks.sql.rescuedDataColumn.filePath.enabled",     "false")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name, such as  `\_rescued\_data`  with  `spark.read.option("rescuedDataColumn",     "\_rescued\_data").format("json").load(<path>)`  .       The JSON parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed JSON—are dropped or throw errors. If you use the option  `badRecordsPath`  when parsing JSON, data type mismatches are not considered as bad records when using the  `rescuedDataColumn`  . Only incomplete and malformed JSON records are stored in  `badRecordsPath`  .         Examples ----------    ###   Single-line mode     In this example, there is one JSON object per line:        ``` {"string":"string1","int":1,"array":[1,2,3],"dict": {"key": "value1"}} {"string":"string2","int":2,"array":[2,4,6],"dict": {"key": "value2"}} {"string":"string3","int":3,"array":[3,6,9],"dict": {"key": "value3", "extra\_key": "extra\_value3"}}  ```      To read the JSON data, use:        ``` val df = spark.read.format("json").load("example.json")  ```      Spark infers the schema automatically.        ``` df.printSchema  ```       ``` root  |-- array: array (nullable = true)  |    |-- element: long (containsNull = true)  |-- dict: struct (nullable = true)  |    |-- extra\_key: string (nullable = true)  |    |-- key: string (nullable = true)  |-- int: long (nullable = true)  |-- string: string (nullable = true)  ```      ###   Multi-line mode     This JSON object occupies multiple lines:        ``` [  {"string":"string1","int":1,"array":[1,2,3],"dict": {"key": "value1"}},  {"string":"string2","int":2,"array":[2,4,6],"dict": {"key": "value2"}},  {  "string": "string3",  "int": 3,  "array": [  3,  6,  9  ],  "dict": {  "key": "value3",  "extra\_key": "extra\_value3"  }  } ]  ```      To read this object, enable multi-line mode:         ``` CREATE TEMPORARY VIEW multiLineJsonTable USING json OPTIONS (path="/tmp/multi-line.json",multiline=true)  ```       ``` val mdf = spark.read.option("multiline", "true").format("json").load("/tmp/multi-line.json") mdf.show(false)  ```         ###   Charset auto-detection     By default, the charset of input files is detected automatically. You can specify the charset explicitly using the  `charset`  option:        ``` spark.read.option("charset", "UTF-16BE").format("json").load("fileInUTF16.json")  ```      Some supported charsets include:  `UTF-8`  ,  `UTF-16BE`  ,  `UTF-16LE`  ,  `UTF-16`  ,  `UTF-32BE`  ,  `UTF-32LE`  ,  `UTF-32`  . For the full list of charsets supported by Oracle Java SE, see  [Supported Encodings](https://docs.oracle.com/javase/8/docs/technotes/guides/intl/encoding.doc.html)   .          Notebook ----------     The following notebook demonstrates single line and multi-line mode.        ###   Read JSON files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-json-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 	To import a JSON file, you can use the `spark.read.format("json").load(<path>)` command in Python or `spark.read.json(<path>)` command in Scala. The `<path>` parameter should be replaced with the path to the JSON file you want to import.	0	1	1
no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.to_delta.html	write dataframe to delta table in unity	   pyspark.pandas.DataFrame.to\_delta  [¶](#pyspark-pandas-dataframe-to-delta "Permalink to this headline") ==========================================================================================================  `DataFrame.`  `to_delta`    (   *path     :     str*  ,  *mode     :     str     =     'w'*  ,  *partition\_cols     :     Union[str, List[str], None]     =     None*  ,  *index\_col     :     Union[str, List[str], None]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → None  [[source]](../../../_modules/pyspark/pandas/frame.html#DataFrame.to_delta) [¶](#pyspark.pandas.DataFrame.to_delta "Permalink to this definition")    Write the DataFrame out as a Delta Lake table.     Parameters    **path**    str, required     Path to write to.    **mode**    str     Python write mode, default ‘w’.     Note     mode can accept the strings for Spark writing mode. Such as ‘append’, ‘overwrite’, ‘ignore’, ‘error’, ‘errorifexists’.    * ‘append’ (equivalent to ‘a’): Append the new data to existing data. * ‘overwrite’ (equivalent to ‘w’): Overwrite existing data. * ‘ignore’: Silently ignore this operation if data already exists. * ‘error’ or ‘errorifexists’: Throw an exception if data already exists.  **partition\_cols**    str or list of str, optional, default None     Names of partitioning columns    **index\_col: str or list of str, optional, default: None**    Column names to be used in Spark to represent pandas-on-Spark’s index. The index name in pandas-on-Spark is ignored. By default the index is always lost.    **options**    dict     All other options passed directly into Delta Lake.     See also    [`read\_delta`](pyspark.pandas.read_delta.html#pyspark.pandas.read_delta "pyspark.pandas.read_delta")  [`DataFrame.to\_parquet`](pyspark.pandas.DataFrame.to_parquet.html#pyspark.pandas.DataFrame.to_parquet "pyspark.pandas.DataFrame.to_parquet")  [`DataFrame.to\_table`](pyspark.pandas.DataFrame.to_table.html#pyspark.pandas.DataFrame.to_table "pyspark.pandas.DataFrame.to_table")  [`DataFrame.to\_spark\_io`](pyspark.pandas.DataFrame.to_spark_io.html#pyspark.pandas.DataFrame.to_spark_io "pyspark.pandas.DataFrame.to_spark_io")   Examples    ``` >>> df = ps.DataFrame(dict( ...    date=list(pd.date\_range('2012-1-1 12:00:00', periods=3, freq='M')), ...    country=KR', 'US', 'JP, ...    code=[1, 2 ,3]), columns=date', 'country', 'code) >>> df  date country code 0 2012-01-31 12:00:00 KR 1 1 2012-02-29 12:00:00 US 2 2 2012-03-31 12:00:00 JP 3  ```   Create a new Delta Lake table, partitioned by one column:    ``` >>> df.to\_delta('%s/to\_delta/foo' % path, partition\_cols='date')    ```   Partitioned by two columns:    ``` >>> df.to\_delta('%s/to\_delta/bar' % path, ...             partition\_cols=date', 'country)    ```   Overwrite an existing table’s partitions, using the ‘replaceWhere’ capability in Delta:    ``` >>> df.to\_delta('%s/to\_delta/bar' % path, ...             mode='overwrite', replaceWhere='date >= "2012-01-01"')    ```  [pyspark.pandas.read\_delta](pyspark.pandas.read_delta.html "previous page")  [pyspark.pandas.read\_parquet](pyspark.pandas.read_parquet.html "next page")   	The `DataFrame.to_delta` method in PySpark writes the DataFrame out as a Delta Lake table. The required parameter is the path to write to, and there are optional parameters for the write mode, partitioning columns, and index columns. The method also accepts other options passed directly into Delta Lake.	1	1	1
no_reaction	https://docs.databricks.com/administration-guide/account-settings-e2/private-access-settings.html	How to set up private link?	Manage private access settings ================================         This article describes a process that is available only for  [accounts on the E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.       This article discusses how to create private access settings objects, which are a required object as part of enabling  [AWS PrivateLink](https://aws.amazon.com/privatelink)   . This article does not contain all the information necessary to configure PrivateLink for your workspace. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see  [Enable AWS PrivateLink](../cloud-configurations/aws/privatelink.html)  .       The following related sections discuss updating existing network and configuration objects:     * [Update a running or failed workspace](../workspace/update-workspace.html)  . * [Updates of existing PrivateLink configuration objects](../cloud-configurations/aws/privatelink.html#update-related)  .      What is a private access settings object? -------------------------------------------     A private access settings object is a Databricks object that describes a workspace’s PrivateLink connectivity. Create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region.       This object serves several purposes:     * It expresses your intent to use AWS PrivateLink with your workspace. * It controls your settings for the front-end use case of AWS PrivateLink for public network access. * It controls which VPC endpoints are permitted to access your workspace.     Create a private access settings object using the account console or the  [Account API 2.0](../../dev-tools/api/latest/account.html)  . You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you  *must*   attach a private access settings object to the workspace during workspace creation.           Create a private access settings object -----------------------------------------      Note       These instructions show you how to create the private access object from the  **Cloud resources**   page in the account console before you create a new workspace. You can also create the private access settings in a similar way as part of the flow of creating a new workspace and choosing  **Add a new private access object**   from the picker instead of choosing an existing object. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .      1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. Click  **Add private access settings**   .      ![private access settings object](../../_images/privatelink-vpc-pas.png) 5. Enter a name for your new private access settings object. 6. For the region, be sure to match the region of your workspace as this is not validated immediately and workspace deployment will fail if it does not match. It is validated only during the actual creation of the workspace. 7. Set the  **Public access enabled**   field, which configures public access to the front-end connection (the web application and REST APIs) for your workspace.      * If set to   **False**    (the default), the front-end connection can be accessed only using PrivateLink connectivity and not from the public internet. When public access is disabled, the   [IP access lists for workspaces](../../security/network/ip-access-list-workspace.html)   feature is unsupported.  * If set to   **True**    , the front-end connection can be accessed either from PrivateLink connectivity or from the public internet. Any IP access lists only limit connections from the public internet but not traffic through the PrivateLink connection. 8. Set the  **Private Access Level**   field to the value that best represents which VPC endpoints to allow for your workspace.      * Set to   **Account**    to limit connections to those VPC endpoints that are registered in your Databricks account.  * Set to   **Endpoint**    to limit connections to an explicit set of VPC endpoints, which you can enter in a field that appears. It lets you select VPC endpoint registrations that you’ve already created. Be sure to include your   *front-end*    VPC endpoint registration if you created one. 9. Click  **Add**   .         Update a private access settings object -----------------------------------------     To update fields on a private access object:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Update**   . 5. Change any fields. For guidance on specific fields, see  [Create a private access settings object](#create)  .        Note       The private access access level  `ANY`  is deprecated. If the object previously had this value and you use the account console to update the private access settings for any fields, you must change the private access level to a non-deprecated value. To make changes to other fields without changing the  `ANY`  private access level at this time, use the  [Account API](../../dev-tools/api/latest/account.html)  . See  [AWS PrivateLink private access level ANY is deprecated](../../release-notes/product/2022/august.html#privatelink-private-access-level-any-deprecated)  . 6. Click  **Update private access setting**   .         Delete a private access settings object -----------------------------------------     Private access settings object cannot be edited after creation. If the configuration has incorrect data or if you no longer need it for any workspaces, delete it:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Delete**   . 5. In the confirmation dialog, click  **Confirm Delete**   .	To set up private link, you need to create a private access settings object, which is a required object as part of enabling AWS PrivateLink. This object controls your settings for the front-end use case of AWS PrivateLink for public network access and which VPC endpoints are permitted to access your workspace. You can create a private access settings object using the account console or the Account API 2.0.	1	1	1
no_reaction	https://docs.databricks.com/clusters/index.html	What is a cluster	Introduction to workspace assets ==================================     This article provides a high-level introduction to Databricks workspace assets. You can view and organize all workspace assets in the workspace browser across personas. To create a workspace asset, you must use the appropriate persona’s workspace browser.        Note       New Databricks SQL queries, dashboards, and alerts are visible in the workspace browser. To view and organize existing queries, dashboards, and articles in the workspace browser, users (or admins) must migrate them into the workspace browser. For information about migration, see  [Migrating existing queries, dashboards, and alerts](../sql/user/workspace-browser/index.html#migrating-existing-queries-dashboards-and-alerts)  .           Clusters ----------     Databricks Data Science & Engineering and Databricks Machine Learning clusters provide a unified platform for various use cases such as running production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning. A cluster is a type of Databricks  *compute resource*   . Other compute resource types include Databricks  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  .       For detailed information on managing and using clusters, see  [Clusters](../clusters/index.html)  .           Notebooks -----------     A notebook is a web-based interface to documents containing a series of runnable cells (commands) that operate on files and  [tables](../lakehouse/data-objects.html#table)  ,  [visualizations](../visualizations/index.html)  , and narrative text. Commands can be run in sequence, referring to the output of one or more previously run commands.       Notebooks are one mechanism for running code in Databricks. The other mechanism is  [jobs](../workflows/jobs/jobs.html)  .       For detailed information on managing and using notebooks, see  [Introduction to Databricks notebooks](../notebooks/index.html)  .           Jobs ------     Jobs are one mechanism for running code in Databricks. The other mechanism is  [notebooks](#ws-notebooks)  .       For detailed information on managing and using jobs, see  [Create, run, and manage Databricks Jobs](../workflows/jobs/jobs.html)  .           Libraries -----------     A library makes third-party or locally-built code available to notebooks and jobs running on your clusters.       For detailed information on managing and using libraries, see  [Libraries](../libraries/index.html)  .           Data ------     You can import data into a distributed file system mounted into a Databricks workspace and work with it in Databricks notebooks and clusters. You can also use a wide variety of Apache Spark data sources to access data.       For detailed information on loading data, see  [Load data into the Databricks Lakehouse](../ingestion/index.html)  .           Files -------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .        In Databricks Runtime 11.2 and above, you can create and use arbitrary files in the Databricks workspace. Files can be any file type. Common examples include:     * `.py`  files used in custom modules. * `.md`  files, such as  `README.md`  . * `.csv`  or other small data files. * `.txt`  files. * Log files.     For detailed information on using files, see  [How to work with files on Databricks](../files/index.html)  . For information about how to use files to modularize your code as you develop with Databricks notebooks, see  [Share code between Databricks notebooks](../notebooks/share-code.html)        Repos -------     Repos are Databricks folders whose contents are co-versioned together by syncing them to a remote Git repository. Using a Databricks repo, you can develop notebooks in Databricks and use a remote Git repository for collaboration and version control.       For detailed information on using repos, see  [Git integration with Databricks Repos](../repos/index.html)  .           Models --------    *Model*   refers to a model registered in MLflow Model Registry. Model Registry is a centralized model store that enables you to manage the full lifecycle of MLflow models. It provides chronological model lineage, model versioning, stage transitions, and model and model version annotations and descriptions.       For detailed information on managing and using models, see  [MLflow Model Registry on Databricks](../mlflow/model-registry.html)  .           Experiments -------------     An MLflow experiment is the primary unit of organization and access control for MLflow machine learning model training runs; all MLflow runs belong to an experiment. Each experiment lets you visualize, search, and compare runs, as well as download run artifacts or metadata for analysis in other tools.       For detailed information on managing and using experiments, see  [Organize training runs with MLflow experiments](../mlflow/tracking.html#mlflow-experiments)  .           Queries ---------     Queries are SQL statements that allow you to interact with your data. For more information, see  [Queries](../sql/user/queries/index.html)  .           Dashboards ------------     Dashboards are presentations of query visualizations and commentary. For more information, see  [Databricks SQL dashboards](../sql/user/dashboards/index.html)  .           Alerts --------     Alerts are notifications that a field returned by a query has reached a threshold. For more information, see  [Alerts](../sql/user/alerts/index.html)  .	A cluster is a type of Databricks compute resource that provides a unified platform for various use cases such as running production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning.	1	1	1
no_reaction	https://kb.databricks.com/notebooks/notebook-autosave	how to save a csv file from a notebook	   pyspark.sql.functions.to\_csv  [¶](#pyspark-sql-functions-to-csv "Permalink to this headline") ================================================================================================  `pyspark.sql.functions.`  `to_csv`    (   *col     :     ColumnOrName*  ,  *options     :     Optional    [    Dict    [    str    ,    str    ]     ]     =     None*   )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#to_csv) [¶](#pyspark.sql.functions.to_csv "Permalink to this definition")    Converts a column containing a  `StructType`  into a CSV string. Throws an exception, in the case of an unsupported type.     New in version 3.0.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **col**   [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")  or str     name of column containing a struct.    **options: dict, optional**    options to control converting. accepts the same options as the CSV datasource. See  [Data Source Option](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)   for the version you use.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")   a CSV string converted from given  `StructType`  .     Examples    ``` >>> from pyspark.sql import Row >>> data = [(1, Row(age=2, name='Alice'))] >>> df = spark.createDataFrame(data, ("key", "value")) >>> df.select(to\_csv(df.value).alias("csv")).collect() [Row(csv='2,Alice')]  ```  [pyspark.sql.functions.schema\_of\_csv](pyspark.sql.functions.schema_of_csv.html "previous page")  [pyspark.sql.functions.years](pyspark.sql.functions.years.html "next page")   	To save a CSV file from a notebook, you can use the `to_csv` function from the `pyspark.sql.functions` module. This function converts a column containing a `StructType` into a CSV string. You can pass the name of the column containing the struct and options to control converting. The function returns a `Column` object representing the CSV string.	0	1	1
thumb_up	https://spark.apache.org/docs/latest/sql-error-conditions-wrong-num-args-error-class.html	How do I handle errors in SQL Code?	###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) * [Error Conditions](sql-error-conditions.html) + [SQLSTATE Codes](sql-error-conditions-sqlstates.html) + [CONNECT error class](sql-error-conditions-connect-error-class.html) + [DATATYPE\_MISMATCH error class](sql-error-conditions-datatype-mismatch-error-class.html) + [INCOMPATIBLE\_DATA\_TO\_TABLE error class](sql-error-conditions-incompatible-data-to-table-error-class.html) + [INCOMPLETE\_TYPE\_DEFINITION error class](sql-error-conditions-incomplete-type-definition-error-class.html) + [INCONSISTENT\_BEHAVIOR\_CROSS\_VERSION error class](sql-error-conditions-inconsistent-behavior-cross-version-error-class.html) + [INVALID\_FORMAT error class](sql-error-conditions-invalid-format-error-class.html) + [INVALID\_OPTIONS error class](sql-error-conditions-invalid-options-error-class.html) + [INVALID\_PARAMETER\_VALUE error class](sql-error-conditions-invalid-parameter-value-error-class.html) + [INVALID\_SCHEMA error class](sql-error-conditions-invalid-schema-error-class.html) + [INVALID\_SUBQUERY\_EXPRESSION error class](sql-error-conditions-invalid-subquery-expression-error-class.html) + [NOT\_NULL\_CONSTRAINT\_VIOLATION error class](sql-error-conditions-not-null-constraint-violation-error-class.html) + [UNRESOLVED\_COLUMN error class](sql-error-conditions-unresolved-column-error-class.html) + [UNRESOLVED\_FIELD error class](sql-error-conditions-unresolved-field-error-class.html) + [UNRESOLVED\_MAP\_KEY error class](sql-error-conditions-unresolved-map-key-error-class.html) + [UNSUPPORTED\_DESERIALIZER error class](sql-error-conditions-unsupported-deserializer-error-class.html) + [UNSUPPORTED\_FEATURE error class](sql-error-conditions-unsupported-feature-error-class.html) + [UNSUPPORTED\_GENERATOR error class](sql-error-conditions-unsupported-generator-error-class.html) + [UNSUPPORTED\_SAVE\_MODE error class](sql-error-conditions-unsupported-save-mode-error-class.html) + [UNSUPPORTED\_SUBQUERY\_EXPRESSION\_CATEGORY error class](sql-error-conditions-unsupported-subquery-expression-category-error-class.html) + [WRONG\_NUM\_ARGS error class](sql-error-conditions-wrong-num-args-error-class.html)   Error Conditions ==================   This is a list of common, named error conditions returned by Spark SQL.     Also see  [SQLSTATE Codes](sql-error-conditions-sqlstates.html)   .    ###   AMBIGUOUS\_COLUMN\_OR\_FIELD  [SQLSTATE: 42702](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Column or field  `<name>`   is ambiguous and has  `<n>`   matches.    ###   AMBIGUOUS\_LATERAL\_COLUMN\_ALIAS  [SQLSTATE: 42702](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Lateral column alias  `<name>`   is ambiguous and has  `<n>`   matches.    ###   AMBIGUOUS\_REFERENCE  [SQLSTATE: 42704](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Reference  `<name>`   is ambiguous, could be:  `<referenceNames>`   .    ###   AMBIGUOUS\_REFERENCE\_TO\_FIELDS  [SQLSTATE: 42000](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Ambiguous reference to the field  `<field>`   . It appears  `<count>`   times in the schema.    ###   ARITHMETIC\_OVERFLOW  [SQLSTATE: 22003](sql-error-conditions-sqlstates.html#class-22-data-exception)   `<message>`   .  `<alternative>`   If necessary set  `<config>`   to “false” to bypass this error.    ###   CANNOT\_CAST\_DATATYPE  [SQLSTATE: 42846](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Cannot cast  `<sourceType>`   to  `<targetType>`   .    ###   CANNOT\_CONSTRUCT\_PROTOBUF\_DESCRIPTOR   SQLSTATE: none assigned     Error constructing FileDescriptor for  `<descFilePath>`   .    ###   CANNOT\_CONVERT\_PROTOBUF\_FIELD\_TYPE\_TO\_SQL\_TYPE   SQLSTATE: none assigned     Cannot convert Protobuf  `<protobufColumn>`   to SQL  `<sqlColumn>`   because schema is incompatible (protobufType =  `<protobufType>`   , sqlType =  `<sqlType>`   ).    ###   CANNOT\_CONVERT\_PROTOBUF\_MESSAGE\_TYPE\_TO\_SQL\_TYPE   SQLSTATE: none assigned     Unable to convert  `<protobufType>`   of Protobuf to SQL type  `<toType>`   .    ###   CANNOT\_CONVERT\_SQL\_TYPE\_TO\_PROTOBUF\_ENUM\_TYPE   SQLSTATE: none assigned     Cannot convert SQL  `<sqlColumn>`   to Protobuf  `<protobufColumn>`   because  `<data>`   cannot be written since it’s not defined in ENUM  `<enumString>`   .    ###   CANNOT\_CONVERT\_SQL\_TYPE\_TO\_PROTOBUF\_FIELD\_TYPE   SQLSTATE: none assigned     Cannot convert SQL  `<sqlColumn>`   to Protobuf  `<protobufColumn>`   because schema is incompatible (protobufType =  `<protobufType>`   , sqlType =  `<sqlType>`   ).    ###   CANNOT\_DECODE\_URL  [SQLSTATE: 22546](sql-error-conditions-sqlstates.html#class-22-data-exception)    Cannot decode url :  `<url>`   .    ###   CANNOT\_LOAD\_FUNCTION\_CLASS   SQLSTATE: none assigned     Cannot load class  `<className>`   when registering the function  `<functionName>`   , please make sure it is on the classpath.    ###   CANNOT\_LOAD\_PROTOBUF\_CLASS   SQLSTATE: none assigned     Could not load Protobuf class with name  `<protobufClassName>`   .  `<explanation>`   .    ###   CANNOT\_MERGE\_INCOMPATIBLE\_DATA\_TYPE  [SQLSTATE: 42825](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Failed to merge incompatible data types  `<left>`   and  `<right>`   .    ###   CANNOT\_MODIFY\_CONFIG  [SQLSTATE: 46110](sql-error-conditions-sqlstates.html#class-46-java-ddl-1)    Cannot modify the value of the Spark config:  `<key>`   .     See also ‘  `<docroot>`   /sql-migration-guide.html#ddl-statements’.    ###   CANNOT\_PARSE\_DECIMAL  [SQLSTATE: 22018](sql-error-conditions-sqlstates.html#class-22-data-exception)    Cannot parse decimal.    ###   CANNOT\_PARSE\_JSON\_FIELD  [SQLSTATE: 2203G](sql-error-conditions-sqlstates.html#class-22-data-exception)    Cannot parse the field name  `<fieldName>`   and the value  `<fieldValue>`   of the JSON token type  `<jsonType>`   to target Spark data type  `<dataType>`   .    ###   CANNOT\_PARSE\_PROTOBUF\_DESCRIPTOR   SQLSTATE: none assigned     Error parsing file  `<descFilePath>`   descriptor byte[] into Descriptor object.    ###   CANNOT\_PARSE\_TIMESTAMP  [SQLSTATE: 22007](sql-error-conditions-sqlstates.html#class-22-data-exception)   `<message>`   . If necessary set  `<ansiConfig>`   to “false” to bypass this error.    ###   CANNOT\_READ\_FILE\_FOOTER   SQLSTATE: none assigned     Could not read footer for file:  `<file>`   .    ###   CANNOT\_RECOGNIZE\_HIVE\_TYPE  [SQLSTATE: 429BB](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Cannot recognize hive type string:  `<fieldType>`   , column:  `<fieldName>`   .    ###   CANNOT\_RESTORE\_PERMISSIONS\_FOR\_PATH   SQLSTATE: none assigned     Failed to set permissions on created path  `<path>`   back to  `<permission>`   .    ###   CANNOT\_UP\_CAST\_DATATYPE   SQLSTATE: none assigned     Cannot up cast  `<expression>`   from  `<sourceType>`   to  `<targetType>`   .    `<details>`   ###   CAST\_INVALID\_INPUT  [SQLSTATE: 22018](sql-error-conditions-sqlstates.html#class-22-data-exception)    The value  `<expression>`   of the type  `<sourceType>`   cannot be cast to  `<targetType>`   because it is malformed. Correct the value as per the syntax, or change its target type. Use  `try_cast`	To handle errors in SQL code, Spark SQL provides a list of common, named error conditions returned by Spark SQL. These error conditions are listed in the "Error Conditions" section of the Spark SQL Guide. Some of the error conditions include "CANNOT_CAST_DATATYPE", "CANNOT_MODIFY_CONFIG", "CAST_INVALID_INPUT", and "ARITHMETIC_OVERFLOW". For each error condition, Spark SQL provides a SQLSTATE code and a description of the error.	1	1	1
no_reaction	https://docs.databricks.com/sql/index.html	Databricks SQL on Delta Lake	What is Delta Lake? =====================     Delta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks Lakehouse Platform. Delta Lake is  [open source software](https://delta.io)   that extends Parquet data files with a file-based transaction log for  [ACID transactions](../lakehouse/acid.html)  and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.       Delta Lake is the default storage format for all operations on Databricks. Unless otherwise specified, all tables on Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project. Many of the optimizations and products in the Databricks Lakehouse Platform build upon the guarantees provided by Apache Spark and Delta Lake. For information on optimizations on Databricks, see  [Optimization recommendations on Databricks](../optimizations/index.html)  .       For reference information on Delta Lake SQL commands, see  [Delta Lake statements](../sql/language-manual/index.html#delta-lake-statements)  .       The Delta Lake transaction log has a well-defined open protocol that can be used by any system to read the log. See  [Delta Transaction Log Protocol](https://github.com/delta-io/delta/blob/master/PROTOCOL.md)   .            Getting started with Delta Lake ---------------------------------     All tables on Databricks are Delta tables by default. Whether you’re using Apache Spark  [DataFrames](../getting-started/dataframes-python.html)  or SQL, you get all the benefits of Delta Lake just by saving your data to the lakehouse with default settings.       For examples of basic Delta Lake operations such as creating tables, reading, writing, and updating data, see  [Tutorial: Delta Lake](tutorial.html)  .       Databricks has many recommendations for  [best practices for Delta Lake](best-practices.html)  .           Converting and ingesting data to Delta Lake ---------------------------------------------     Databricks provides a number of products to accelerate and simplify loading data to your lakehouse.     * [Delta Live Tables](../ingestion/auto-loader/dlt.html) * [COPY INTO](../ingestion/copy-into/index.html) * [Auto Loader](../ingestion/auto-loader/index.html) * [Add data UI](../ingestion/add-data/index.html) * [Incrementally convert Parquet or Iceberg data to Delta Lake](convert-to-delta.html) * [One-time conversion of Parquet or Iceberg data to Delta Lake](clone-parquet.html) * [Third-party partners](../integrations/index.html)     For a full list of ingestion options, see  [Load data into the Databricks Lakehouse](../ingestion/index.html)  .             Updating and modifying Delta Lake tables ------------------------------------------     Atomic transactions with Delta Lake provide many options for updating data and metadata. Databricks recommends you avoid interacting directly with data and transaction log files in Delta Lake file directories to avoid corrupting your tables.     * Delta Lake supports upserts using the  [merge operation](merge.html)  . * Delta Lake provides numerous options for  [selective overwrites](selective-overwrite.html)  based on filters and partitions. * You can manually or automatically  [update your table schema](update-schema.html)  without rewriting data. * [Column mapping](delta-column-mapping.html)  enables columns to be renamed or deleted without rewriting data.         Incremental and streaming workloads on Delta Lake ---------------------------------------------------     Delta Lake is optimized for Structured Streaming on Databricks.  [Delta Live Tables](../delta-live-tables/index.html)  extends native capabilities with simplified infrastructure deployment, enhanced scaling, and managed data dependencies.     * [Table streaming reads and writes](../structured-streaming/delta-lake.html) * [Use Delta Lake change data feed on Databricks](delta-change-data-feed.html) * [Enable idempotent writes across jobs](idempotent-writes.html)       Querying previous versions of a table ---------------------------------------     Each write to a Delta table creates a new table version. You can use the transaction log to review modifications to your table and query previous table versions. See  [Work with Delta Lake table history](history.html)  .           Delta Lake schema enhancements --------------------------------     Delta Lake validates schema on write, ensuring that all data written to a table matches the requirements you’ve set.     * [Delta Lake schema validation](schema-validation.html) * [Constraints on Databricks](../tables/constraints.html) * [Use Delta Lake generated columns](generated-columns.html) * [Enrich Delta Lake tables with custom metadata](custom-metadata.html)         Managing files and indexing data with Delta Lake --------------------------------------------------     Databricks sets many default parameters for Delta Lake that impact the size of data files and number of table versions that are retained in history. Delta Lake uses a combination of metadata parsing and physical data layout to reduce the number of files scanned to fulfill any query.     * [Data skipping with Z-order indexes for Delta Lake](data-skipping.html) * [Compact data files with optimize on Delta Lake](optimize.html) * [Remove unused data files with vacuum](vacuum.html) * [Configure Delta Lake to control data file size](tune-file-size.html)         Configuring and reviewing Delta Lake settings -----------------------------------------------     Databricks stores all data and metadata for Delta Lake tables in cloud object storage. Many configurations can be set at either the table level or within the Spark session. You can review the details of the Delta table to discover what options are configured.     * [Review Delta Lake table details with describe detail](table-details.html) * [Delta table properties reference](table-properties.html) * [Configure storage credentials for Delta Lake](configure-storage-credentials.html)         Data pipelines using Delta Lake and Delta Live Tables -------------------------------------------------------     Databricks encourages users to leverage a  [medallion architecture](../lakehouse/medallion.html)  to process data through a series of tables as data is cleaned and enriched.  [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL workloads through optimized execution and automated infrastructure deployment and scaling.           Troubleshooting Delta Lake features -------------------------------------     Not all Delta Lake features are in all versions of Databricks Runtime. You can find information about Delta Lake versioning and answers to frequent questions in the following articles:     * [How does Databricks manage Delta Lake feature compatibility?](feature-compatibility.html) * [Generate a manifest file](generate-manifest.html)           Delta Lake API documentation ------------------------------     For most read and write operations on Delta tables, you can use  [Spark SQL](../sql/language-manual/index.html)  or Apache Spark  [DataFrame](../getting-started/dataframes-python.html)  APIs.       For Delta Lake-spefic SQL statements, see  [Delta Lake statements](../sql/language-manual/index.html#delta-lake-statements)  .       Databricks ensures binary compatibility with Delta Lake APIs in Databricks Runtime. To view the Delta Lake API version packaged in each Databricks Runtime version, see the  **System environment**   section on the relevant article in the  [Databricks Runtime release notes](../release-notes/runtime/releases.html)  . Delta Lake APIs exist for Python, Scala, and Java:     * [Python API docs](https://docs.delta.io/latest/api/python/index.html) * [Scala API docs](https://docs.delta.io/latest/api/scala/io/delta/tables/index.html) * [Java API docs](https://docs.delta.io/latest/api/java/index.html)	Delta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks Lakehouse Platform. It is open source software that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale. Delta Lake is the default storage format for all operations on Databricks.	0	1	1
no_reaction	https://kb.databricks.com/streaming/readstream-is-not-whitelisted	I get this error when I readStream with UC enabled: "iDisplayHelper.getStreamName() is not whitelisted on class". why?	Title: readStream() is not whitelisted error when running a query  Description: readStream() is not whitelisted error on clusters that have table access control enabled.  Answer: Problem You have table access control (AWS | Azure | GCP) enabled on your cluster. You are trying to run a structured streaming query and get and error message. py4j.security.Py4JSecurityException: Method public org.apache.spark.sql.streaming.DataStreamReader org.apache.spark.sql.SQLContext.readStream() is not whitelisted on class class org.apache.spark.sql.SQLContext Cause Streaming is not supported on clusters that have table access control enabled. Access control allows you to set permissions for data objects on a cluster. It requires user interaction to validate and refresh credentials. Because streaming queries run continuously, it is not supported on clusters with table access control. Solution You should use a cluster that does not have table access control enabled for streaming queries.	The error message "iDisplayHelper.getStreamName() is not whitelisted on class" occurs when you try to run a structured streaming query with table access control enabled on your cluster. Streaming is not supported on clusters with table access control enabled because it requires user interaction to validate and refresh credentials, which is not possible with continuously running streaming queries. The solution is to use a cluster that does not have table access control enabled for streaming queries.	1	1	1
no_reaction	https://docs.databricks.com/dbfs/root-locations.html	Two of the most common data locations on Databricks are the DBFS root storage and external object storage mounted with dbutils.fs.mount ()  Which of the following statements is correct? a- The DBFS root stores files in ephemeral block volumes attached to the driver while mounted directories will always persist saved data to external storage between sessions. b- By default, both the DBFS root and mounted data sources are only accessible to workspace administrators. c- The DBFS root is the most secure location to store data, because mounted storage volumes must have full public read and write permissions. d- DBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems. e- Neither the DBFS root nor mounted storage can be accessed when using %sh in a Databricks notebook.	What directories are in DBFS root by default? ===============================================     Each Databricks workspace has several directories configured in the DBFS root storage container by default. Some of these directories link to locations on the DBFS root, while others are virtual mounts. If you are unable to access data in any of these directories, contact your workspace administrator.     * `/FileStore` * `/databricks-datasets` * `/databricks-results` * `/databricks/init` * `/user/hive/warehouse`        What is stored in the  `/Filestore`  directory? -------------------------------------------------     Data and libraries uploaded through the Databricks UI go to the  `/Filestore`  location by default. Generated plots are also stored in this directory.           What is stored in the  `/databricks-datasets`  directory? -----------------------------------------------------------     Databricks provides a number of open source datasets in this directory. Many of the tutorials and demos provided by Databricks reference these datasets, but you can also use them to indepedently explore the functionality of Databricks.               What is stored in the  `/databricks-results`  directory? ----------------------------------------------------------    `/databricks-results`  stores files generated by downloading the  [full results](../notebooks/notebook-outputs.html#download-full-results)  of a query.         What is stored in the  `/databricks/init`  directory? -------------------------------------------------------     This directory contains global  [init scripts](../clusters/init-scripts.html)  .           What is stored in the  `/user/hive/warehouse`  directory? -----------------------------------------------------------     Databricks stores managed tables in the  `hive\_metastore`  here by default.	The correct statement regarding the DBFS (Databricks File Store) root storage and external object storage mounted with dbutils.fs.mount() is: "The DBFS (Databricks File Store) is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems." This statement is not one of the options provided in the question.	1	0	0
no_reaction	https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html	What is the recommended way of creating an MLOps pipeline?	MLOps workflow on Databricks ==============================     This article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.        What is MLOps? ----------------     MLOps is a set of processes and automated steps to manage code, data, and models. It combines DevOps, DataOps, and ModelOps.      ![MLOps lakehouse](../../_images/mlops-lakehouse.png)    ML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.         General recommendations for MLOps -----------------------------------     This section includes some general recommendations for MLOps on Databricks with links for more information.      ###   Create a separate environment for each stage     An execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.       Databricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:     * [Development](#development-stage) * [Staging](#staging-stage) * [Production](#production-stage)     Other configurations can also be used to meet the specific needs of your organization.       ###   Access control and versioning     Access control and versioning are key components of any software operations process. Databricks recommends the following:     * **Use Git for version control.**   Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use  [Databricks Repos](../../repos/index.html)  to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see  [Developer tools and guidance](../../dev-tools/index.html)  . * **Store data in a Lakehouse architecture using Delta tables.**   Data should be stored in a  [Lakehouse architecture](../../lakehouse/index.html)  in your cloud account. Both raw data and feature tables should be stored as  [Delta tables](../../delta/index.html)  with access controls to determine who can read and modify them. * **Manage models and model development with MLflow.**   You can use  [MLflow](../../mlflow/index.html)  to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the  [Model Registry](../../mlflow/model-registry.html)  to manage model versioning and deployment status. The Model Registry provides  [webhooks](../../mlflow/model-registry-webhooks.html)  and an API so you can integrate with CD systems, and also handles  [access control for models](../../security/auth-authz/access-control/workspace-acl.html#configure-mlflow-model-permissions)  .     ###   Deploy code, not models     In most situations, Databricks recommends that during the ML development process, you promote  *code*   , rather than  *models*   , from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see  [Model deployment patterns](deployment-patterns.html)  .                Recommended MLOps workflow ----------------------------     The following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.       This section uses the terms “data scientist” and “ML engineer” as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.           Development stage -------------------     The focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.      ![MLOps development stage diagram](../../_images/mlops-dev-diagram.png)    The numbered steps correspond to the numbers shown in the diagram.      ###   1. Data sources     Data scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.       ###   2. Exploratory data analysis (EDA)     Data scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and  [Databricks SQL](../../sql/index.html)  .       This ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.       ###   3. Code     All of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of Databricks and synced with the Databricks workspace using  [Databricks Repos](../../repos/index.html)  .       ###   4. Update feature tables     The model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the  [Feature Store](../feature-store/index.html)  . This pipeline includes 2 tasks:     * **Data preparation.**   Check for data quality issues. * **Create or update feature tables.**   Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.       Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.     ###   5. Train model     Data scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.       This pipeline includes 2 tasks:     * **Training and tuning.**   The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.       When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.       When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing. * **Evaluation.**   Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.       If your organization’s governance requirements include additional information about the model, you can save it using  [MLflow tracking](../../mlflow/tracking.html)  . Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.     ###   6. Commit code     After developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.            Staging stage ---------------     The focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.       ML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.      ![MLOps staging stage diagram](../../_images/mlops-staging-diagram.png)    The numbered steps correspond to the numbers shown in the diagram.       The staging environment can have its own storage area for testing feature tables and ML pipelines. This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.      ###   1. Merge request     The deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.       ###   2. Unit tests     The CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.       ###   3. Integration tests (CI)     The CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.       To reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.       After the integration tests pass on the staging branch, you can promote the code to production.       ###   4. Merge to staging branch     If the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.       You can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.       ###   5. Create a release branch     When the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.            Production stage ------------------     ML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.       Data scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.      ![MLOps production stage diagram](../../_images/mlops-prod-diagram.png)    The numbered steps correspond to the numbers shown in the diagram.      ###   1. Update feature tables     As new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.       ###   2. Train model     Train the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.       This pipeline includes 2 tasks:     * **Training and tuning.**   As in the development stage, autologging saves a record of the training process to the MLflow tracking server. This includes model metrics, parameters, tags, and the model itself.       During development, data scientists may test many algorithms and hyperparameters. In the production training code, it’s common to consider only the top-performing options. Limiting tuning in this way saves time and can reduce the variance from tuning in automated retraining. * **Evaluation.**   Model quality is evaluated by testing on held-out production data. The results of these tests are logged to the MLflow tracking server. This step uses the evaluation metrics specified by data scientists in the development stage. These metrics may include custom code.     When model training is complete, register the model artifact in the  [MLflow Model Registry](../../mlflow/model-	The recommended way of creating an MLOps pipeline is to create separate environments for each stage of ML code and model development with clearly defined transitions between stages. Databricks recommends using Git for version control, storing data in a Lakehouse architecture using Delta tables, and managing models and model development with MLflow. In most situations, Databricks recommends promoting code, rather than models, from one environment to the next. The MLOps workflow on Databricks includes three stages: development, staging, and production. Data scientists develop features and models and run experiments to optimize model performance in the development stage. The staging stage is for testing the ML pipeline code to ensure it is ready for production. The production stage is where ML pipelines are deployed, and ML engineers own the production environment.	0	1	1
thumb_down	https://docs.databricks.com/languages/python.html	install a python library to be available to all notebooks for all users that run on a Databricks Runtime for Machine Learning cluster	Databricks for Python developers ==================================     This section provides a guide to developing notebooks and jobs in Databricks using the Python language. The first subsection provides links to tutorials for common workflows and tasks. The second subsection provides links to APIs, libraries, and key tools.       A basic workflow for getting started is:     * [Import code](#manage-code-with-notebooks-and-databricks-repos)  : Either import your own code from files or Git repos or try a tutorial listed below. Databricks recommends learning using interactive Databricks Notebooks. * [Run your code on a cluster](#clusters-and-libraries)  : Either create a cluster of your own, or ensure you have permissions to use a shared cluster. Attach your notebook to the cluster, and run the notebook. * Beyond this, you can branch out into more specific topics:      + [Work with larger data sets](#python-apis)   using Apache Spark  + [Add visualizations](#visualizations)  + [Automate your workload](#jobs)   as a job  + [Use machine learning](#machine-learning)   to analyze your data  + [Develop in IDEs](#ides-developer-tools-and-apis)      Tutorials -----------     The below tutorials provide example code and notebooks to learn about common workflows. See  [Import a notebook](../notebooks/notebook-export-import.html#import-a-notebook)  for instructions on importing notebook examples into your workspace.      ###   Interactive data science and machine learning   * Getting started with Apache Spark DataFrames for data preparation and analytics:  [Tutorial: Work with PySpark DataFrames on Databricks](../getting-started/dataframes-python.html) * [Tutorial: End-to-end ML models on Databricks](../mlflow/end-to-end-example.html)  . For additional examples, see  [Tutorials: Get started with ML](../machine-learning/tutorial/index.html)  and the MLflow guide’s  [Quickstart Python](../mlflow/quick-start-python.html)  . * [Databricks AutoML](../machine-learning/automl/index.html)  lets you get started quickly with developing machine learning models on your own datasets. Its glass-box approach generates notebooks with the complete machine learning workflow, which you may clone, modify, and rerun.     ###   Data engineering   * [Tutorial: Work with PySpark DataFrames on Databricks](../getting-started/dataframes-python.html)  provides a walkthrough to help you learn about Apache Spark DataFrames for data preparation and analytics. * [Tutorial: Delta Lake](../delta/tutorial.html)  . * [Tutorial: Declare a data pipeline with Python in Delta Live Tables](../delta-live-tables/tutorial-python.html)  . * [Tutorial: Declare a data pipeline with SQL in Delta Live Tables](../delta-live-tables/tutorial-sql.html)  . * [Tutorial: Run your first Delta Live Tables pipeline](../delta-live-tables/tutorial-pipelines.html)  .     ###   Production machine learning and machine learning operations   * [MLflow Model Registry example](../mlflow/model-registry-example.html) * [Tutorial: End-to-end ML models on Databricks](../mlflow/end-to-end-example.html)       ###   Debug in Python notebooks     The example notebook illustrates how to use the Python debugger (pdb) in Databricks notebooks. To use the Python debugger, you must be running Databricks Runtime 11.2 or above.       With Databricks Runtime 12.1 and above, you can use  [variable explorer](../notebooks/notebooks-code.html#variable-explorer)  to track the current value of Python variables in the notebook UI. You can use variable explorer to observe the values of Python variables as you step through breakpoints.        ####   Python debugger example notebook     [Open notebook in new tab](/_extras/notebooks/source/python-debugger.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 Note      `breakpoint()`  is  [not supported in IPython](https://github.com/ipython/ipykernel/issues/897)   and thus does not work in Databricks notebooks. You can use  `import     pdb;     pdb.set\_trace()`  instead of  `breakpoint()`  .           Reference -----------     The below subsections list key features and tips to help you begin developing in Databricks with Python.      ###   Python APIs     Python code that runs outside of Databricks can generally run within Databricks, and vice versa. If you have existing code, just import it into Databricks to get started. See  [Manage code with notebooks and Databricks Repos](#manage-code-with-notebooks-and-databricks-repos)  below for details.       Databricks can run both single-machine and distributed Python workloads. For single-machine computing, you can use Python APIs and libraries as usual; for example, pandas and scikit-learn will “just work.” For distributed Python workloads, Databricks offers two popular APIs out of the box: the Pandas API on Spark and PySpark.      ####   Pandas API on Spark      Note       The  [Koalas open-source project](https://koalas.readthedocs.io/)   now recommends switching to the Pandas API on Spark. The Pandas API on Spark is available on clusters that run  [Databricks Runtime 10.0 (Unsupported)](../release-notes/runtime/10.0.html)  and above. For clusters that run  [Databricks Runtime 9.1 LTS](../release-notes/runtime/9.1.html)  and below, use  [Koalas](../archive/legacy/koalas.html)  instead.       [pandas](https://pandas.pydata.org)   is a Python package commonly used by data scientists for data analysis and manipulation. However, pandas does not scale out to big data.  [Pandas API on Spark](../pandas/pandas-on-spark.html)  fills this gap by providing pandas-equivalent APIs that work on Apache Spark. This  [open-source API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)   is an ideal choice for data scientists who are familiar with pandas but not Apache Spark.       ####   PySpark API     PySpark is the official Python API for Apache Spark. This API provides more flexibility than the Pandas API on Spark. These links provide an introduction to and reference for PySpark.     * [Introduction to DataFrames](../getting-started/dataframes-python.html) * [Introduction to Structured Streaming](../structured-streaming/examples.html) * [PySpark API reference](https://spark.apache.org/docs/latest/api/python/)      ###   Manage code with notebooks and Databricks Repos    [Databricks notebooks](../notebooks/index.html)  support Python. These notebooks provide functionality similar to that of Jupyter, but with additions such as built-in visualizations using big data, Apache Spark integrations for debugging and performance monitoring, and MLflow integrations for tracking machine learning experiments. Get started by  [importing a notebook](../notebooks/notebook-export-import.html#import-a-notebook)  . Once you have access to a cluster, you can  [attach a notebook](../notebooks/notebook-ui.html#attach)  to the cluster and  [run the notebook](../notebooks/run-notebook.html)  .        Tip       To completely reset the state of your notebook, it can be useful to restart the iPython kernel. For Jupyter users, the “restart kernel” option in Jupyter corresponds to detaching and re-attaching a notebook in Databricks. To restart the kernel in a Python notebook, click on the cluster dropdown in the upper-left and click  **Detach & Re-attach**   . This detaches the notebook from your cluster and reattaches it, which restarts the Python process.       [Databricks Repos](../repos/index.html)  allows users to synchronize notebooks and other files with Git repositories. Databricks Repos helps with code versioning and collaboration, and it can simplify importing a full repository of code into Databricks, viewing past notebook versions, and integrating with IDE development. Get started by  [cloning a remote Git repository](../repos/git-operations-with-repos.html)  . You can then open or create notebooks with the repository clone,  [attach the notebook](../notebooks/notebook-ui.html#attach)  to a cluster, and  [run the notebook](../notebooks/run-notebook.html)  .       ###   Clusters and libraries     Databricks  [Clusters](../clusters/index.html)  provide compute management for clusters of any size: from single node clusters up to large clusters. You can customize cluster hardware and libraries according to your needs. Data scientists will generally begin work either by  [creating a cluster](../clusters/configure.html)  or using an existing  [shared cluster](../clusters/clusters-manage.html#control-access-to-clusters)  . Once you have access to a cluster, you can  [attach a notebook](../notebooks/notebook-ui.html#attach)  to the cluster or  [run a job](../workflows/jobs/jobs.html#create-a-job)  on the cluster.     * For small workloads which only require single nodes, data scientists can use  [Single Node clusters](../clusters/single-node.html)  for cost savings. * For detailed tips, see  [Best practices: Cluster configuration](../clusters/cluster-config-best-practices.html) * Administrators can set up  [cluster policies](../administration-guide/clusters/policies.html)  to simplify and guide cluster creation.     Databricks clusters use a Databricks Runtime, which provides many popular libraries out-of-the-box, including Apache Spark, Delta Lake, pandas, and more. You can also install additional third-party or custom Python libraries to use with notebooks and jobs.     * Start with the default libraries in the  [Databricks Runtime](../runtime/dbr.html)  . Use the  [Introduction to Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  for machine learning workloads. For full lists of pre-installed libraries, see  [Databricks runtime releases](../release-notes/runtime/releases.html)  . * Customize your environment using  [Notebook-scoped Python libraries](../libraries/notebooks-python-libraries.html)  , which allow you to modify your notebook or job environment with libraries from PyPI or other repositories. The  `%pip     install     my\_library`  magic command installs  `my\_library`  to all nodes in your currently attached cluster, yet does not interfere with other workloads on shared clusters. * Install non-Python libraries as  [Cluster libraries](../libraries/cluster-libraries.html)  as needed. * For more details, see  [Libraries](../libraries/index.html)  .     ###   Visualizations     Databricks Python notebooks have built-in support for many types of  [visualizations](../visualizations/index.html)  . You can also use  [legacy visualizations](../visualizations/legacy-visualizations.html#visualizations-in-python)  .       You can also visualize data using third-party libraries; some are pre-installed in the Databricks Runtime, but you can install custom libraries as well. Popular options include:     * [Bokeh](../visualizations/bokeh.html) * [Matplotlib](../visualizations/matplotlib.html) * [Plotly](../visualizations/plotly.html)     ###   Jobs     You can automate Python workloads as scheduled or triggered  [Create, run, and manage Databricks Jobs](../workflows/jobs/jobs.html)  in Databricks	To install a Python library to be available to all notebooks for all users that run on a Databricks Runtime for Machine Learning cluster, you can customize your environment using Notebook-scoped Python libraries, which allow you to modify your notebook or job environment with libraries from PyPI or other repositories. The `%pip install my_library` magic command installs `my_library` to all nodes in your currently attached cluster, yet does not interfere with other workloads on shared clusters. You can also install non-Python libraries as Cluster libraries as needed. For more details, see Libraries.	1	1	1
no_reaction	https://docs.databricks.com/administration-guide/account-settings/audit-logs.html	to setup the audit logs, however, it seems a given set of settings are required to be applied to every cluster created, instead of only to the workspace. Is this understanding correct?	      Configure audit logging =========================      Note       This feature is available on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   .        Databricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns.       There are two types of logs:     * Workspace-level audit logs with workspace-level events. * Account-level audit logs with account-level events.     For a list of each of these types of events and the associated services, see  [Audit events](#events)  .          Configure verbose audit logs ------------------------------     In addition to the default  [events](#events)  , you can configure a workspace to generate additional events by enabling  *verbose audit logs*   .       To enable verbose audit logs, your account and workspace must be on the E2 version of the platform. To confirm the version of the platform you are using, contact your Databricks representative.      ###   Enable or disable verbose audit logs   1. As an admin, go to the Databricks  [admin console](../workspace/index.html#admin-console)  . 2. Click  **Workspace settings**   . 3. Next to  **Verbose Audit Logs**   , enable or disable the feature.     When you enable or disable verbose logging, an auditable event is emitted in the category  `workspace`  with action  `workspaceConfKeys`  . The  `workspaceConfKeys`  request parameter is  `enableVerboseAuditLogs`  . The request parameter  `workspaceConfValues`  is  `true`  (feature enabled) or  `false`  (feature disabled).       ###   Additional verbose notebook action     Additional verbose action in audit log category  `notebook`  :           |   Action   |   Description   |   Request Parameters   | | --- | --- | --- | |  `runCommand`  |   Emitted after Databricks runs a command in a notebook. A command corresponds to a cell in a notebook.   |   [“notebookId”, “executionTime”, “status”, “commandId”, “commandText”]   |       ###   Additional verbose Databricks SQL actions     Additional actions in audit log category  `databrickssql`  :           |   Action   |   Description   |   Request Parameters   | | --- | --- | --- | |  `commandSubmit`  |   Runs when a command is submitted to Databricks SQL.   |   [“commandText”, “warehouseId”, “commandId”]   | |  `commandFinish`  |   Runs when a command completes or a command is cancelled.   |   [“warehouseId”, “commandId”]   |     Check the  `response`  field for additional information related to the command result:     * `statusCode`  - The HTTP response code. This will be error 400 if it is a general error.      + `errorMessage`   - Error message. In some cases for certain long-running commands, the   `errorMessage`   field may not populate on failure.  + `result`   :   *This field is empty*    .          Configure audit log delivery ------------------------------     As a Databricks account admin, you can configure low-latency delivery of audit logs in JSON file format to an AWS S3 storage bucket, where you can make the data available for  [usage analysis](#analyze)  . Databricks delivers a separate JSON file for each workspace in your account and a separate file for account-level events.       After initial setup or other log delivery configuration changes, expect a delay of up to one hour until changes take effect. After logging delivery begins, auditable events are typically logged within 15 minutes. For the file naming, delivery rules, and schema, see  [Audit delivery details and format](#details)  .       The API to configure low-latency delivery of audit logs is  [Account API 2.0](../../dev-tools/api/latest/account.html)  , which is the same API used to configure  [billable usage log delivery](billable-usage-delivery.html)  .       You can optionally deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery. This allows flexibility, for example setting up workspaces from multiple AWS accounts to deliver to the same S3 bucket. This option requires that you configure an S3 bucket policy that references a cross-account IAM role. Instructions and a policy template are provided in this article.       Access to the logs depends on how you set up the S3 bucket. Databricks delivers logs to your S3 bucket with AWS’s built-in  [BucketOwnerFullControl Canned ACL](https://docs.aws.amazon.com/AmazonS3/latest/dev/about-object-ownership.html)   so that account owners and designees can download the logs directly. To support bucket ownership for newly-created objects, you must set your bucket’s  **S3 Object Ownership**   setting to the value  **Bucket owner preferred**   .        Important       If instead you set your bucket’s  **S3 Object Ownership**   setting to  **Object writer**   , new objects such as your logs remain owned by the uploading account, which is by default the IAM role you created and specified to access your bucket. This can make it difficult to access the logs, because you cannot access them from the AWS console or automation tools that you authenticated with as the bucket owner.        Databricks recommends that you review  [Security Best Practices for S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html)   for guidance around protecting the data in your bucket from unwanted access.      ###   Configuration options     To configure audit log delivery, you have the following options.     * If you have one workspace in your Databricks account, follow the instructions in the sections that follow, creating a single configuration object with a common configuration for your workspace. * If you have multiple workspaces in the same Databricks account, you can do any of the following:      + Share the same configuration (log delivery S3 bucket and IAM role) for all workspaces in the account. This is the only configuration option that also delivers account-level audit logs. It is the default option.  + Use separate configurations for each workspace in the account.  + Use separate configurations for different groups of workspaces, each sharing a configuration. * If you have multiple workspaces, each associated with a separate Databricks account, you must create unique storage and credential configuration objects for each account, but you can reuse an S3 bucket or IAM role between these configuration objects.      Note       Even though you use the Account API to configure log delivery, you can configure log delivery for any workspace, including workspaces that were not created using the Account API.        ###   High-level flow     The high-level flow of audit log delivery:     1. [Configure storage](#configure-storage)  : In AWS, create a new AWS S3 bucket. Using Databricks APIs, call the Account API to create a storage configuration object that uses the bucket name.        Note       To deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery, you need to add an S3 bucket policy. You do not add the policy in this step. 2. [Configure credentials](#configure-credentials)  : In AWS, create the appropriate AWS IAM role. Using Databricks APIs, call the Account API to create a credentials configuration object that uses the IAM role’s ARN. The role policy can specify a path prefix for log delivery within your S3 bucket. You can choose to define an IAM role to include multiple path prefixes if you want log delivery configurations for different workspaces that share the S3 bucket but use different path prefixes. 3. [Optional cross-account support](#cross-account-policy)  To deliver logs to an AWS account other than the account of the IAM role that you create for log delivery, add an S3 bucket policy. This policy references IDs for the cross-account IAM role that you created in the previous step. 4. [Call the log delivery API](#log-delivery-api)  : Call the Account API to create a log delivery configuration that uses the credential and storage configuration objects from previous steps. This step lets you specify if you want to associate the log delivery configuration for all workspaces in your account (current and future workspaces) or for a specific set of workspaces. For a list of account-level events, see  [Audit events](#events)  .     After you complete these steps, you can access the JSON files. The delivery location is:        ``` bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs\_<internal-id>.json  ```      If you configure audit log delivery for the entire account, account-level audit events that are not associated with any single workspace are delivered to the  `workspaceId=0`  partition.       New JSON files are delivered every few minutes, potentially overwriting existing files for each workspace. When you initially set up audit log delivery, it can take up to one hour for log delivery to begin. After audit log delivery begins, auditable events are typically logged within 15 minutes. Additional configuration changes typically take an hour to take effect.       For more information about accessing these files and analyzing them using Databricks, see  [Analyze audit logs](#analyze)  .        Important       There is a limit on the number of log delivery configurations available per account (each limit applies separately to each log type including billable usage and audit logs). You can create a maximum of two enabled account-level delivery configurations (configurations without a workspace filter) per type. Additionally, you can create and enable two workspace level delivery configurations per workspace for each log type, meaning the same workspace ID can occur in the workspace filter for no more than two delivery configurations per log type. You cannot delete a log delivery configuration, but you can disable it. You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.        ###   Requirements   * Account admin email address and password to authenticate with the APIs. The email address and password are both case sensitive. * Account ID. Get your account ID from  [the account console](index.html#account-id)  .     ###   How to authenticate to the APIs     The APIs described in this article are published on the  `accounts.cloud.databricks.com`  base endpoint for all AWS regional deployments.       Use the following base URL for API requests:  `https://accounts.cloud.databricks.com/api/2.0/`      This REST API requires HTTP basic authentication, which involves setting the HTTP header  `Authorization`  . In this article,  *username*   refers to your account admin email address. The email address is case sensitive. There are several ways to provide your credentials to tools such as curl.     * Pass your username and account password separately in the headers of each request in  `<username>:<password>`  syntax.       For example:        ``` curl -X GET -u `<username>:<password>` -H "Content-Type: application/json" \  'https://accounts.cloud.databricks.com/api/2.0/accounts/<account-id>/<endpoint>'  ``` * Apply base64 encoding to your  `<username>:<password>`  string and provide it directly in the HTTP header:        ``` curl -X GET -H "Content-Type: application/json" \  -H 'Authorization: Basic <base64-username-pw>'  'https://accounts.cloud.databricks.com/api/2.0/accounts/<account-id>/<endpoint>'  ``` * Create a  [.netrc](https://everything.curl.dev/usingcurl/netrc)   file with  `machine`  ,  `login`  , and  `password`  properties:        ``` machine accounts.cloud.databricks.com login <username>	The context explains how to configure audit logging in Databricks. It provides information on the types of logs, workspace-level audit logs, and account-level audit logs. It also explains how to enable verbose audit logs and additional verbose actions in audit log categories. The context also explains how to configure audit log delivery, including the high-level flow of audit log delivery, configuration options, and requirements. However, it does not provide information on whether a given set of settings is required to be applied to every cluster created, instead of only to the workspace.	0	0	0
no_reaction	https://docs.databricks.com/delta/tune-file-size.html	what size files should I set in Delta tables?	      Configure Delta Lake to control data file size ================================================     Delta Lake provides options for manually or automatically configuring the target file size for writes and for  `OPTIMIZE`  operations.        Limit rows written in a data file -----------------------------------     You can use the SQL session configuration  `spark.sql.files.maxRecordsPerFile`  to specify the maximum number of records to write to a single file for a Delta Lake table. Specifying a value of zero or a negative value represents no limit.       In Databricks Runtime 10.5 and above, you can also use the DataFrameWriter option  `maxRecordsPerFile`  when using the DataFrame APIs to write to a Delta Lake table. When  `maxRecordsPerFile`  is specified, the value of the SQL session configuration  `spark.sql.files.maxRecordsPerFile`  is ignored.         ``` (df.write   .mode("append")   .option("maxRecordsPerFile", "10000")   .save("/tmp/delta/people10m") )  ```       ``` df.write  .mode("append")  .option("maxRecordsPerFile", "10000")  .save("/tmp/delta/people10m")  ```           Set a target file size ------------------------      Note       Available in Databricks Runtime 8.2 and above.        If you want to tune the size of files in your Delta table, set the  [table property](table-properties.html) `delta.targetFileSize`  to the desired size. If this property is set, all data layout optimization operations will make a best-effort attempt to generate files of the specified size. Examples here include  [optimize](optimize.html)  or  [Z-order](data-skipping.html)  ,  [Auto Compaction](../optimizations/auto-optimize.html#how-auto-compaction-works)  , and  [Optimized Writes](../optimizations/auto-optimize.html#how-optimized-writes-work)  .         |   Table property   | | --- | |  **delta.targetFileSize**     Type: Size in bytes or higher units.     The target file size. For example,  `104857600`  (bytes) or  `100mb`  .     Default value: None   |     For existing tables, you can set and unset properties using the SQL command  [ALTER TABLE SET TBL PROPERTIES](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html)  . You can also set these properties automatically when creating new tables using Spark session configurations. See  [Delta table properties reference](table-properties.html)  for details.         Autotune file size based on workload --------------------------------------      Note       Available in Databricks Runtime 8.2 and above.        To minimize the need for manual tuning, Databricks can automatically tune the file size of Delta tables, based on workloads operating on the table. Databricks can automatically detect if a Delta table has frequent  `MERGE`  operations that rewrite files and may choose to reduce the size of rewritten files in anticipation of further file rewrites in the future. For example, when executing a  `MERGE`  operation, if 9 out of last 10 previous operations on the table were also MERGEs, then  [Optimized Writes](../optimizations/auto-optimize.html#delta-optimized-writes)  and  [Auto Compaction](../optimizations/auto-optimize.html#how-auto-compaction-works)  used by  `MERGE`  (if enabled) will generate smaller file sizes than it would otherwise. This helps in reducing the duration of future  `MERGE`  operations.       Autotune is activated after a few rewrite operations have occurred. However, if you anticipate a Delta table will experience frequent  `MERGE`  ,  `UPDATE`  , or  `DELETE`  operations and want this tuning immediately, you can explicitly tune file sizes for rewrites by setting the table property  `delta.tuneFileSizesForRewrites`  . Set this property to  `true`  to always use lower file sizes for all data layout optimization operations on the table. Set it to  `false`  to never tune to lower file sizes, that is, prevent auto-detection from being activated.         |   Table property   | | --- | |  **delta.tuneFileSizesForRewrites**     Type:  `Boolean`    Whether to tune file sizes for data layout optimization.     Default value: None   |     For existing tables, you can set and unset properties using the SQL command  [ALTER TABLE SET TBL PROPERTIES](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html)  . You can also set these properties automatically when creating new tables using Spark session configurations. See  [Delta table properties reference](table-properties.html)  for details.         Autotune file size based on table size ----------------------------------------      Note       Available in Databricks Runtime 8.4 and above.        To minimize the need for manual tuning, Databricks automatically tunes the file size of Delta tables based on the size of the table. Databricks will use smaller file sizes for smaller tables and larger file sizes for larger tables so that the number of files in the table does not grow too large. Databricks does not autotune tables that you have tuned with a  [specific target size](#set-target-size)  or based on a workload with frequent rewrites.       The target file size is based on the current size of the Delta table. For tables smaller than 2.56 TB, the autotuned target file size is 256 MB. For tables with a size between 2.56 TB and 10 TB, the target size will grow linearly from 256 MB to 1 GB. For tables larger than 10 TB, the target file size is 1 GB.        Note       When the target file size for a table grows, existing files are not re-optimized into larger files by the  `OPTIMIZE`  command. A large table can therefore always have some files that are smaller than the target size. If it is required to optimize those smaller files into larger files as well, you can configure a fixed target file size for the table using the  `delta.targetFileSize`  table property.        When a table is written incrementally, the target file sizes and file counts will be close to the following numbers, based on table size. The file counts in this table are only an example. The actual results will be different depending on many factors.           |   Table size   |   Target file size   |   Approximate number of files in table   | | --- | --- | --- | |   10 GB   |   256 MB   |   40   | |   1 TB   |   256 MB   |   4096   | |   2.56 TB   |   256 MB   |   10240   | |   3 TB   |   307 MB   |   12108   | |   5 TB   |   512 MB   |   17339   | |   7 TB   |   716 MB   |   20784   | |   10 TB   |   1 GB   |   24437   | |   20 TB   |   1 GB   |   34437   | |   50 TB   |   1 GB   |   64437   | |   100 TB   |   1 GB   |   114437   |                                                      	To configure the size of files in Delta tables, you can set the `delta.targetFileSize` table property to the desired size. For example, `104857600` (bytes) or `100mb`. If this property is set, all data layout optimization operations will make a best-effort attempt to generate files of the specified size. Additionally, you can use the SQL session configuration `spark.sql.files.maxRecordsPerFile` to specify the maximum number of records to write to a single file for a Delta Lake table. Specifying a value of zero or a negative value represents no limit. In Databricks Runtime 10.5 and above, you can also use the DataFrameWriter option `maxRecordsPerFile` when using the DataFrame APIs to write to a Delta Lake table.	1	1	1
no_reaction	https://docs.databricks.com/administration-guide/clusters/index.html	how do i set a cluster policy?	Configure clusters ====================      Note       These are instructions for the legacy create cluster UI, and are included only for historical accuracy. All customers should be using the  [updated create cluster UI](../../clusters/configure.html)  .        This article explains the configuration options available when you create and edit Databricks clusters. It focuses on creating and editing clusters using the UI. For other methods, see  [Clusters CLI](../../dev-tools/cli/clusters-cli.html)  ,  [Clusters API 2.0](../../dev-tools/api/latest/clusters.html)  , and  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  .       For help deciding what combination of configuration options suits your needs best, see  [cluster configuration best practices](../../clusters/cluster-config-best-practices.html)  .      ![Create cluster](../../_images/create-dialog-aws.png)       Cluster policy ----------------     A  [cluster policy](../../administration-guide/clusters/policies.html)  limits the ability to configure clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster.       To configure a cluster policy, select the cluster policy in the  **Policy**   drop-down.      ![Select cluster policy](../../_images/policy.png)     Note       If no policies have been  [created in the workspace](../../administration-guide/clusters/policies.html#create-a-cluster-policy)  , the  **Policy**   drop-down does not display.        If you have:     * [Cluster create permission](../../administration-guide/access-control/cluster-acl.html#cluster-create-permission)  , you can select the  **Unrestricted**   policy and create fully-configurable clusters. The  **Unrestricted**   policy does not limit any cluster attributes or attribute values. * Both cluster create permission and access to cluster policies, you can select the  **Unrestricted**   policy and the policies you have access to. * Access to cluster policies only, you can select the policies you have access to.       Cluster mode --------------      Note       This article describes the legacy clusters UI. For information about the new clusters UI (in preview), see  [Create a cluster](../../clusters/configure.html)  . This includes some terminology changes for cluster access types and modes. For a comparison of the new and legacy cluster types, see  [Clusters UI changes and cluster access modes](cluster-ui-preview.html)  . In the preview UI:     * *Standard mode clusters*   are now called  *No Isolation Shared access mode clusters*   . * *High Concurrency with Tables ACLs*   are now called  *Shared access mode clusters*   .      Databricks supports three cluster modes: Standard, High Concurrency, and  [Single Node](../../clusters/single-node.html)  . The default cluster mode is Standard.        Important     * If your workspace is assigned to a  [Unity Catalog](../../data-governance/unity-catalog/index.html)  metastore, High Concurrency clusters are not available. Instead, you use  [access mode](../../data-governance/unity-catalog/compute.html#access-mode)  to ensure the integrity of access controls and enforce strong isolation guarantees. See also  [Create a cluster that can access Unity Catalog](../../data-governance/unity-catalog/compute.html#create-cluster)  . * You cannot change the cluster mode after a cluster is created. If you want a different cluster mode, you must create a new cluster.      The cluster configuration includes an  [auto terminate](../../clusters/clusters-manage.html#automatic-termination)  setting whose  *default value*   depends on cluster mode:     * Standard and Single Node clusters terminate automatically after 120 minutes by default. * High Concurrency clusters  *do not*   terminate automatically by default.      ###   Standard clusters          Warning       Standard mode clusters (sometimes called No Isolation Shared clusters) can be shared by multiple users, with no isolation between users. If you use the High Concurrency cluster mode  *without additional security settings such as Table ACLs or Credential Passthrough*   , the same settings are used as Standard mode clusters. Account admins can  [prevent internal credentials from being automatically generated for Databricks workspace admins](../../administration-guide/account-settings/no-isolation-shared.html)  on these types of cluster. For more secure options, Databricks recommends alternatives such as high concurrency clusters with Table ACLs.        A Standard cluster is recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.         ###   High Concurrency clusters     A High Concurrency cluster is a managed cloud resource. The key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies.       High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.       In addition, only High Concurrency clusters support  [table access control](../../data-governance/table-acls/index.html)  .       To create a High Concurrency cluster, set  **Cluster Mode**   to  **High Concurrency**   .      ![High Concurrency cluster mode](../../_images/high-concurrency.png)        For an example of how to create a High Concurrency cluster using the Clusters API, see  [High Concurrency cluster example](../../dev-tools/api/latest/examples.html#high-concurrency-example)  .         ###   Single Node clusters     A Single Node cluster has no workers and runs Spark jobs on the driver node.       In contrast, a Standard cluster requires  *at least one*   Spark worker node in addition to the driver node to execute Spark jobs.       To create a Single Node cluster, set  **Cluster Mode**   to  **Single Node**   .      ![Single Node cluster mode](../../_images/single-node.png)    To learn more about working with Single Node clusters, see  [Single Node clusters](../../clusters/single-node.html)  .              Pools -------     To reduce cluster start time, you can attach a cluster to a predefined  [pool](../../clusters/pools.html)  of idle instances, for the driver and worker nodes. The cluster is created using instances in the pools. If a pool does not have sufficient idle resources to create the requested driver or worker nodes, the pool expands by allocating new instances from the instance provider. When an attached cluster is terminated, the instances it used are returned to the pools and can be reused by a different cluster.       If you select a pool for worker nodes but not for the driver node, the driver node inherit the pool from the worker node configuration.        Important       If you attempt to select a pool for the driver node but not for worker nodes, an error occurs and your cluster isn’t created. This requirement prevents a situation where the driver node has to wait for worker nodes to be created, or vice versa.        See  [Create a pool](../../clusters/pools.html)  to learn more about working with pools in Databricks.             Databricks Runtime --------------------     Databricks runtimes are the set of core components that run on your  [clusters](../../clusters/index.html)  . All Databricks runtimes include Apache Spark and add components and updates that improve usability, performance, and security. For details, see  [Databricks runtimes](../../runtime/index.html)  .       Databricks offers several types of runtimes and several versions of those runtime types in the  **Databricks Runtime Version**   drop-down when you create or edit a cluster.      ![Select Runtime version](../../_images/runtime-version.png)     ###   Photon acceleration    [Photon](../../runtime/photon.html)  is available for clusters running  [Databricks Runtime 9.1 LTS](../../release-notes/runtime/9.1.html)  and above.       To enable Photon acceleration, select the  **Use Photon Acceleration**   checkbox.       If desired, you can specify the instance type in the Worker Type and Driver Type drop-down.       You can view Photon activity in the  [Spark UI](../../clusters/clusters-manage.html#spark-ui)  . The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with “Photon”, for example,  `PhotonGroupingAgg`  . Second, in the DAG, Photon operators and stages are colored peach, while the non-Photon ones are blue.      ![Photon DAG](../../_images/photon-dag.png)        ###   Docker images     For some Databricks Runtime versions, you can specify a Docker image when you create a cluster. Example use cases include library customization, a golden container environment that doesn’t change, and Docker CI/CD integration.       You can also use Docker images to create custom deep learning environments on clusters with GPU devices.       For instructions, see  [Customize containers with Databricks Container Services](../../clusters/custom-containers.html)  and  [Databricks Container Services on GPU clusters](../../clusters/gpu.html#databricks-container-services-on-gpu)  .              Cluster node type -------------------     A cluster consists of one driver node and zero or more worker nodes.       You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads.      * [Driver node](#driver-node) * [Worker node](#worker-node) * [GPU instance types](#gpu-instance-types) * [AWS Graviton instance types](#aws-graviton-instance-types)     ###  [Driver node](#id1)     The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext and interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors.       The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to  `collect()`  a lot of data from Spark workers and analyze them in the notebook.        Tip       Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node.          ###  [Worker node](#id2)     Databricks worker nodes run the Spark executors and other services required for the proper functioning of the clusters. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Databricks runs one executor per worker node; therefore the terms  *executor*   and  *worker*   are used interchangeably in the context of the Databricks architecture.        Tip       To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail.         Note       Databricks launches worker nodes with two private IP addresses each. The node’s primary private IP address is used to host Databricks internal traffic. The secondary private IP address is used by the Spark container for intra-cluster communication. This model allows Databricks to provide isolation between multiple clusters in the same workspace.        ###  [GPU instance types](#id3)     For computationally challenging tasks that demand high performance, like those associated with deep learning, Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see  [GPU-enabled	To set a cluster policy in Databricks, you can select the cluster policy in the Policy drop-down. If no policies have been created in the workspace, the Policy drop-down does not display. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster. If you have access to cluster policies only, you can select the policies you have access to.	1	1	0
no_reaction	https://kb.databricks.com/streaming/sqs-stream-partition-values	how do I stream data in from kafka into an s3 bucket	    Stream processing with Apache Kafka and Databricks ====================================================     This article describes how you can use Apache Kafka as either a source or a sink when running Structured Streaming workloads on Databricks.       For more Kafka, see the  [Kafka documentation](https://kafka.apache.org/documentation/)   .        Read data from Kafka ----------------------     The following is an example for reading data from Kafka:        ``` df = (spark.readStream   .format("kafka")   .option("kafka.bootstrap.servers", "<server:ip>")   .option("subscribe", "<topic>")   .option("startingOffsets", "latest")   .load() )  ```        Write data to Kafka ---------------------     The following is an example for writing data to Kafka:        ``` (df.writeStream   .format("kafka")   .option("kafka.bootstrap.servers", "<server:ip>")   .option("topic", "<topic>")   .option("checkpointLocation", "<checkpoint\_path>")   .start() )  ```            Configure Kafka for Apache Spark on Databricks ------------------------------------------------     Databricks provides the  `kafka`  keyword as a data format to configure connections to Kafka 0.10+.       The following are the most common configurations for Kafka:       There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:           |   Option   |   Value   |   Description   | | --- | --- | --- | |   subscribe   |   A comma-separated list of topics.   |   The topic list to subscribe to.   | |   subscribePattern   |   Java regex string.   |   The pattern used to subscribe to topic(s).   | |   assign   |   JSON string  `{"topicA":[0,1],"topic":[2,4]}`  .   |   Specific topicPartitions to consume.   |     Other notable configurations:            |   Option   |   Value   |   Default Value   |   Description   | | --- | --- | --- | --- | |   kafka.bootstrap.servers   |   Comma-separated list of host:port.   |   empty   |   [Required] The Kafka  `bootstrap.servers`  configuration. If you find there is no data from Kafka, check the broker address list first. If the broker address list is incorrect, there might not be any errors. This is because Kafka client assumes the brokers will become available eventually and in the event of network errors retry forever.   | |   failOnDataLoss   |  `true`  or  `false`  .   |  `true`  |   [Optional] Whether to fail the query when it’s possible that data was lost. Queries can permanently fail to read data from Kafka due to many scenarios such as deleted topics, topic truncation before processing, and so on. We try to estimate conservatively whether data was possibly lost or not. Sometimes this can cause false alarms. Set this option to  `false`  if it does not work as expected, or you want the query to continue processing despite data loss.   | |   minPartitions   |   Integer >= 0, 0 = disabled.   |   0 (disabled)   |   [Optional] Minimum number of partitions to read from Kafka. You can configure Spark to use an arbitrary minimum of partitions to read from Kafka using the  `minPartitions`  option. Normally Spark has a 1-1 mapping of Kafka topicPartitions to Spark partitions consuming from Kafka. If you set the  `minPartitions`  option to a value greater than your Kafka topicPartitions, Spark will divvy up large Kafka partitions to smaller pieces. This option can be set at times of peak loads, data skew, and as your stream is falling behind to increase processing rate. It comes at a cost of initializing Kafka consumers at each trigger, which may impact performance if you use SSL when connecting to Kafka.   | |   kafka.group.id   |   A Kafka consumer group ID.   |   not set   |   [Optional] Group ID to use while reading from Kafka. Use this with caution. By default, each query generates a unique group ID for reading data. This ensures that each query has its own consumer group that does not face interference from any other consumer, and therefore can read all of the partitions of its subscribed topics. In some scenarios (for example, Kafka group-based authorization), you may want to use specific authorized group IDs to read data. You can optionally set the group ID. However, do this with extreme caution as it can cause unexpected behavior.  * Concurrently running queries (both, batch and streaming) with the same group ID are likely interfere with each other causing each query to read only part of the data. * This may also occur when queries are started/restarted in quick succession. To minimize such issues, set the Kafka consumer configuration  `session.timeout.ms`  to be very small.  | |   startingOffsets   |   earliest , latest   |   latest   |   [Optional] The start point when a query is started, either “earliest” which is from the earliest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.   |     See  [Structured Streaming Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)   for other optional configurations.         Schema for Kafka records --------------------------     The schema of Kafka records is:          |   Column   |   Type   | | --- | --- | |   key   |   binary   | |   value   |   binary   | |   topic   |   string   | |   partition   |   int   | |   offset   |   long   | |   timestamp   |   long   | |   timestampType   |   int   |     The  `key`  and the  `value`  are always deserialized as byte arrays with the  `ByteArrayDeserializer`  . Use DataFrame operations (such as  `cast("string")`  ) to explicitly deserialize the keys and values.           Retrieve Kafka metrics ------------------------      Note       Available in Databricks Runtime 8.1 and above.        You can get the average, min, and max of the number of offsets that the streaming query is behind the latest available offset among all the subscribed topics with the  `avgOffsetsBehindLatest`  ,  `maxOffsetsBehindLatest`  , and  `minOffsetsBehindLatest`  metrics. See  [Reading Metrics Interactively](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reading-metrics-interactively)   .        Note       Available in Databricks Runtime 9.1 and above.        Get the estimated total number of bytes that the query process has not consumed from the subscribed topics by examining the value of  `estimatedTotalBytesBehindLatest`  . This estimate is based on the batches that were processed in the last 300 seconds. The timeframe that the estimate is based on can be changed by setting the option  `bytesEstimateWindowLength`  to a different value. For example, to set it to 10 minutes:        ``` df = (spark.readStream   .format("kafka")   .option("bytesEstimateWindowLength", "10m") # m for minutes, you can also use "600s" for 600 seconds )  ```      If you are running the stream in a notebook, you can see these metrics under the  **Raw Data**   tab in the streaming query progress dashboard:        ``` {  "sources" : [ {  "description" : "KafkaV2[Subscribe[topic]]",  "metrics" : {  "avgOffsetsBehindLatest" : "4.0",  "maxOffsetsBehindLatest" : "4",  "minOffsetsBehindLatest" : "4",  "estimatedTotalBytesBehindLatest" : "80.0"  },  } ] }  ```        Use SSL to connect Databricks to Kafka ----------------------------------------     To enable SSL connections to Kafka, follow the instructions in the Confluent documentation  [Encryption and Authentication with SSL](https://docs.confluent.io/current/kafka/authentication_ssl.html#clients)   . You can provide the configurations described there, prefixed with  `kafka.`  , as options. For example, you specify the trust store location in the property  `kafka.ssl.truststore.location`  .       Databricks recommends that you:     * Store your certificates in cloud object storage. You can restrict access to the certificates only to clusters that can access Kafka. See  [Data governance guide](../data-governance/index.html)  . * Store your certificate passwords as  [secrets](../security/secrets/secrets.html)  in a  [secret scope](../security/secrets/secret-scopes.html)  .     The following example uses object storage locations and Databricks secrets to enable an SSL connection:        ``` df = (spark.readStream   .format("kafka")   .option("kafka.bootstrap.servers", ...)   .option("kafka.security.protocol", "SASL\_SSL")   .option("kafka.ssl.truststore.location", <truststore-location>)   .option("kafka.ssl.keystore.location", <keystore-location>)   .option("kafka.ssl.keystore.password", dbutils.secrets.get(scope=<certificate-scope-name>,key=<keystore-password-key-name>))   .option("kafka.ssl.truststore.password", dbutils.secrets.get(scope=<certificate-scope-name>,key=<truststore-password-key-name>)) )  ```                                                       	To stream data in from Kafka into an S3 bucket, you can use the `writeStream` method to write data to Kafka and then use the `kafka` format to read data from Kafka. Here is an example for reading data from Kafka:  ``` df = (spark.readStream   .format("kafka")   .option("kafka.bootstrap.servers", "<server:ip>")   .option("subscribe", "<topic>")   .option("startingOffsets", "latest")   .load() ) ```  And here is an example for writing data to Kafka:  ``` (df.writeStream   .format("kafka")   .option("kafka.bootstrap.servers", "<server:ip>")   .option("topic", "<topic>")   .option("checkpointLocation", "<checkpoint\_path>")   .start() ) ```  Once you have the data in a DataFrame, you can use the `writeStream` method to write the data to an S3 bucket.	1	1	1
no_reaction	https://docs.databricks.com/administration-guide/account-settings/index.html	how do i get the account id in databricks sql	Manage your Databricks account ================================     The Databricks account configurations are managed by  [account admins](../index.html)  . The articles in this section cover tasks performed by account admins.       As an account admin, you can manage your Databricks account using the  [account console](#account-console)  , the  [Account API](../workspace/create-workspace-api.html)  , or  [Automation templates](../workspace/templates.html)  .            Manage your account using the Databricks account console ----------------------------------------------------------     The account console is where account admins manage Databricks account-level configurations.       Tasks performed through the account console include  [workspace creation](../workspace/create-workspace.html)  ,  [user management](../users-groups/users.html)  ,  [subscription management](account.html)  , and configuring cloud resources (such as  [credentials](../account-settings-e2/credentials.html)  ,  [storage](../account-settings-e2/storage.html)  , and  [networks](../account-settings-e2/networks.html)  ).      ###   Access the account console     Users can access the account console at  <https://accounts.cloud.databricks.com>  . Account admins can also access the account console from within a workspace:     1. Click your email address at the top of the Databricks workspace UI. 2. Select  **Manage Account**   .     ###   Change the account console language settings     The account console is available in multiple languages. To change the account console language, select  **Settings**   then go to the  **Language settings**   tab.            Locate your account ID ------------------------     To retrieve your account ID, go to the  [account console](#account-console)  and click the down arrow next to your username in the upper right corner. In the dropdown menu you can view and copy your  **Account ID**   .           Manage email preferences --------------------------     Databricks can occasionally send emails with personalized product and feature recommendations based on your use of Databricks. These messages may include information to help users get started with Databricks or learn about new features and previews.       You can manage whether you receive these emails in the account console:     1. Log in to the  [account console](#account-console)  and click the  **Settings**   icon in the sidebar. 2. In the  **My preferences**   section, click the  **Instructional product and feature emails**   toggle.     You can also manage your promotional email communications by clicking  **Manage**   under  **Promotional email communications**   or by going to the  [Marketing preference center](https://preferences.databricks.com/)   . Non-admin users can update this setting by clicking the  **My preferences**   link next to their workspace in the account console.	To retrieve your account ID in Databricks SQL, you can go to the account console and click the down arrow next to your username in the upper right corner. In the dropdown menu, you can view and copy your Account ID.	1	1	1
no_reaction	https://docs.databricks.com/query-federation/snowflake.html	What can Databricks do with Snowflake?	    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format("snowflake")   .option("dbtable", table\_name)   .option("sfUrl", database\_host\_url)   .option("sfUser", username)   .option("sfPassword", password)   .option("sfDatabase", database\_name)   .option("sfSchema", schema\_name)   .option("sfWarehouse", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format("snowflake")  .option("dbtable", table\_name)  .option("sfUrl", database\_host\_url)  .option("sfUser", username)  .option("sfPassword", password)  .option("sfDatabase", database\_name)  .option("sfSchema", schema\_name)  .option("sfWarehouse", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         	Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake. You can configure a connection to Snowflake and then query data. Databricks can also store machine learning training results in Snowflake.	1	1	1
no_reaction	https://docs.databricks.com/delta/table-details.html	How do I get the schema of a delta table	Explore tables ================     In Data Explorer you can view table schema details, preview sample data, table type, table location, table history, frequent queries and users, and other details.       For information about using Data Explorer to set table ownership and permissions, see  [Manage Unity Catalog object ownership in Data Explorer](manage-ownership.html)  and  [Manage Unity Catalog permissions in Data Explorer](manage-permissions.html)  .        Select a table ----------------   1. [Display schemas](explore-schemas.html#display-schemas)  . 2. Select a schema. 3. Click the Filter tables field. Optionally type a string to filter the tables.      ![Filter tables](../_images/filter-tables.png) 4. Click a table. The table comment, owner, and size information displays and the selected Schema tab shows the table schema.       Hover over the icon next to a column name to see its data type. If the table is partitioned,  ![Search Icon](../_images/search-icon.png)  (magnifying glass) appears next to the partition column.      ![Table schema](../_images/table-schema.png)       View sample data ------------------     Click the  **Sample Data**   tab to view sample data.      ![Sample data](../_images/sample-data.png)      View table details --------------------     Click the  **Details**   tab to view the location of the table files, the type of table, and table properties.      ![Table details](../_images/table-details.png)      View table history --------------------     If the table is a Delta table, you can click the  **History**   tab to view a timeline of changes to the table, including who made the change, the compute resource that was used, and where the change originated, such as a job or notebook.      ![Table history](../_images/table-history.png)      View frequent queries and users ---------------------------------     If the table is registered in Unity Catalog, you can view the most frequent queries made on the table and users who accessed the table in the past 30 days. See  [View frequent queries and users of a table](table-insights.html)  .         Create a basic query ----------------------     Click  **Create > Query**   to create a query that selects all columns and returns the first 1000 rows of results. You can modify the query after creation.         Create a quick dashboard --------------------------     Click  **Create > Quick Dashboard**   to open a configuration page where you can select columns of interest and create a dashboard and supporting queries that provide some basic information using those columns and showcase dashboard-level parameters and other capabilities. You can modify the dashboard after creation, and you can share it with other users and configured  [notification destinations](../sql/user/alerts/index.html)  .      ![Animation showing how to create a quick dashboard](../_images/create-quick-dashboard-animation.gif)	To get the schema of a delta table, you can select the table in Data Explorer, click on the Schema tab, and view the schema details.	0	1	1
no_reaction	https://docs.databricks.com/ingestion/index.html	how can iload a json file into databricks?	  ![]()   Create a SparkDataFrame from a JSON file. ===========================================  `read.json.Rd`    Loads a JSON file, returning the result as a SparkDataFrame By default, (  [JSON Lines text format or newline-delimited JSON](https://jsonlines.org/)   ) is supported. For JSON (one record per file), set a named property  `multiLine`   to  `TRUE`   . It goes through the entire dataset once to determine the schema.     Usage -------  ``` read.json(path, ...) ```   Arguments -----------   path     Path of file to read. A vector of multiple paths is allowed.     ...     additional external data source specific named properties. You can find the JSON-specific options for reading JSON files in  <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>  Data Source Option in the version you use.     Value -------   SparkDataFrame     Note ------   read.json since 1.6.0     Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() path <- "path/to/file.json" df <- read.json(path) df <- read.json(path, multiLine = TRUE) }  ```  	To load a JSON file into Databricks, you can use the `read.json()` function. This function loads a JSON file and returns the result as a SparkDataFrame. You can specify the path of the file to read as an argument to the function. Additionally, you can use external data source specific named properties to specify JSON-specific options for reading JSON files.	1	1	1
thumb_up	https://docs.databricks.com/files/download-internet-files.html	how can I download file from http source using GET method and save it in dbfs using python?	    Download data from the internet =================================     You can use Databricks notebooks to download data from public URLs to volume storage attached to the driver of your cluster. If you are accessing data from cloud object storage,  [accessing data directly](../storage/index.html#configure-object)  with Apache Spark provides better results.       Databricks clusters provide general compute, allowing you to run arbitrary code in addtion to Apache Spark commands. Because arbitrary commands execute against the root directory for the cluster rather than the DBFS root, you must move downloaded data to a new location before reading it with Apache Spark.        Note       Some workspace configurations might prevent access to the public internet. Consult your workspace administrator if you need expanded network access.         Download a file with Bash, Python, or Scala ---------------------------------------------     Databricks does not provide any native tools for downloading data from the internet, but you can use open source tools in supported languages. The following examples use packages for Bash, Python, and Scala to download the same file.         ``` %sh curl https://data.cityofnewyork.us/api/views/kk4q-3rt2/rows.csv --output /tmp/curl-subway.csv  ```       ``` import urllib urllib.request.urlretrieve("https://data.cityofnewyork.us/api/views/kk4q-3rt2/rows.csv", "/tmp/python-subway.csv")  ```       ``` import java.net.URL import java.io.File import org.apache.commons.io.FileUtils  FileUtils.copyURLToFile(new URL("https://data.cityofnewyork.us/api/views/kk4q-3rt2/rows.csv"), new File("/tmp/scala-subway.csv"))  ```       Because these files downloaded to the volume storage attached to the driver, use  `%sh`  to see these files, as in the following example:        ``` %sh ls /tmp/  ```      You can use Bash commands to preview the contents of files download this way, as in the following example:        ``` %sh head /tmp/curl-subway.csv  ```        Moving data with dbutils --------------------------     To access data with Apache Spark, move it from its current location. The current location for this data is in ephemeral volume storage that is only visible to the driver. Databricks loads data from file sources in parallel, and so files must be visible to all nodes in the compute environment. While Databricks supports a wide range of  [external data sources](../external-data/index.html)  , file-based data access generally assumes access to cloud object storage.       The  [Databricks Utilities](../dev-tools/databricks-utils.html)  (  `dbutils`  ) allow you to move files from volume storage attached to the driver to other locations accessible with the DBFS, including external object storage locations you’ve configured access to. The following example moves data to a directory in the  [DBFS root](../dbfs/dbfs-root.html)  , a cloud object storage location configured during initial workspace deployment.        ``` dbutils.fs.mv("file:/tmp/curl-subway.csv", "dbfs:/tmp/subway.csv")  ```        Reading downloaded data -------------------------     After you move the data to cloud object storage, you can read the data as normal. The following code reads in the CSV data moved to the DBFS root.        ``` df = spark.read.format("csv").option("header", True).load("/tmp/subway.csv") display(df)  ```                                                       	To download a file from an HTTP source using the GET method and save it in DBFS using Python, you can use the `urllib` package in Python to download the file and then use `dbutils.fs.cp()` to copy the file to DBFS. Here is an example code snippet:  ``` import urllib  # Download the file using urllib url = "http://example.com/file.csv" urllib.request.urlretrieve(url, "/tmp/file.csv")  # Copy the file to DBFS dbutils.fs.cp("file:/tmp/file.csv", "dbfs:/path/to/file.csv") ```  Note that you will need to replace the URL and the DBFS path with the appropriate values for your use case.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/delta-clone.html	CREATE TABLE SHALLOW CLONE is not supported in Unity Catalog ?	CREATE TABLE CLONE ====================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Clones a source Delta table to a target destination at a specific version. A clone can be either deep or shallow: deep clones copy over the data from the source and shallow clones do not. You can also clone source Parquet and Iceberg tables. See  [Incrementally clone Parquet and Iceberg tables to Delta Lake](../../delta/clone-parquet.html)  .        Important       There are important differences between shallow and deep clones that can determine how best to use them. See  [Clone a table on Databricks](../../delta/clone.html)  .         Syntax --------      ``` CREATE TABLE [IF NOT EXISTS] table\_name    [SHALLOW | DEEP] CLONE source\_table\_name [TBLPROPERTIES clause] [LOCATION path]  ```       ``` [CREATE OR] REPLACE TABLE table\_name    [SHALLOW | DEEP] CLONE source\_table\_name [TBLPROPERTIES clause] [LOCATION path]  ```        Parameters ------------   * **IF NOT EXISTS**       If specified, the statement is ignored if  `table\_name`  already exists. * **[CREATE OR] REPLACE**       If  `CREATE     OR`  is specified the table is replaced if it exists and newly created if it does not. Without  `CREATE     OR`  the  `table\_name`  must exist. * **[table\_name](sql-ref-names.html#table-name)**      The name of the Delta Lake table to be created. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the name is not qualified the table is created in the current schema.  `table\_name`  must not exist already unless  `REPLACE`  or  `IF     NOT     EXISTS`  has been specified. * **SHALLOW CLONE**   or  **DEEP CLONE**       If you specify  `SHALLOW     CLONE`  Databricks will make a copy of the source table’s definition, but refer to the source table’s files. When you specify  `DEEP     CLONE`  (default) Databricks will make a complete, independent copy of the source table. * **[source\_table\_name](sql-ref-names.html#table-name)**      The name of the Delta Lake table to be cloned. The name may include a  [temporal specification](sql-ref-names.html#table-name)  . * **[TBLPROPERTIES](sql-ref-syntax-ddl-tblproperties.html#tblproperties)**      Optionally sets one or more user-defined properties. * **LOCATION path**       Optionally creates an external table, with the provided location as the path where the data is stored. If  `table\_name`  itself a path instead of a table identifier, the operation will fail.  `path`  must be a STRING literal.       Examples ----------     You can use  `CREATE     TABLE     CLONE`  for complex operations like data migration, data archiving, machine learning flow reproduction, short-term experiments, and data sharing. See  [Clone a table on Databricks](../../delta/clone.html)  .	The syntax `SHALLOW CLONE` is not supported in Unity Catalog.	0	1	1
no_reaction	https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Normalizer.html	Explain this error: TypeError: 'Column' object is not callable --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <command-4134732480957838> in <module> 1 AccessJson = spark.read.format("json").load("file:/Workspace/Repos/Ivan.Santos@ADP.com/shared-workspace-security-config-dbs/942420684378/hro-data-governance/prod.json",multiLine=True) ----> 2 pd.json_normalize(AccessJson) /databricks/python/lib/python3.8/site-packages/pandas/io/json/_normalize.py in _json_normalize(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level) 268 269 if record_path is None: --> 270 if any([isinstance(x, dict) for x in y.values()] for y in data): 271 # naive normalization, this is idempotent for flat records 272 # and potentially will inflate the data considerably for /databricks/python/lib/python3.8/site-packages/pandas/io/json/_normalize.py in <genexpr>(.0) 268 269 if record_path is None: --> 270 if any([isinstance(x, dict) for x in y.values()] for y in data): 271 # naive normalization, this is idempotent for flat records 272 # and potentially will inflate the data considerably for TypeError: 'Column' object is not callable	   Normalizer  [¶](#normalizer "Permalink to this headline") ===========================================================  *class*  `pyspark.ml.feature.`  `Normalizer`    (   *\**  ,  *p     :     float     =     2.0*  ,  *inputCol     :     Optional    [    str    ]     =     None*  ,  *outputCol     :     Optional    [    str    ]     =     None*   )   [[source]](../../_modules/pyspark/ml/feature.html#Normalizer) [¶](#pyspark.ml.feature.Normalizer "Permalink to this definition")   >  >  >  >  Normalize a vector to have unit norm using the given p-norm. >   >  >  >  >    New in version 1.4.0.     Examples    ``` >>> from pyspark.ml.linalg import Vectors >>> svec = Vectors.sparse(4, {1: 4.0, 3: 3.0}) >>> df = spark.createDataFrame([(Vectors.dense([3.0, -4.0]), svec)], ["dense", "sparse"]) >>> normalizer = Normalizer(p=2.0) >>> normalizer.setInputCol("dense") Normalizer... >>> normalizer.setOutputCol("features") Normalizer... >>> normalizer.transform(df).head().features DenseVector([0.6, -0.8]) >>> normalizer.setParams(inputCol="sparse", outputCol="freqs").transform(df).head().freqs SparseVector(4, {1: 0.8, 3: 0.6}) >>> params = {normalizer.p: 1.0, normalizer.inputCol: "dense", normalizer.outputCol: "vector"} >>> normalizer.transform(df, params).head().vector DenseVector([0.4286, -0.5714]) >>> normalizerPath = temp\_path + "/normalizer" >>> normalizer.save(normalizerPath) >>> loadedNormalizer = Normalizer.load(normalizerPath) >>> loadedNormalizer.getP() == normalizer.getP() True >>> loadedNormalizer.transform(df).take(1) == normalizer.transform(df).take(1) True  ```   Methods    |  [`clear`](#pyspark.ml.feature.Normalizer.clear "pyspark.ml.feature.Normalizer.clear")  (param)   |   Clears a param from the param map if it has been explicitly set.   | |  [`copy`](#pyspark.ml.feature.Normalizer.copy "pyspark.ml.feature.Normalizer.copy")  ([extra])   |   Creates a copy of this instance with the same uid and some extra params.   | |  [`explainParam`](#pyspark.ml.feature.Normalizer.explainParam "pyspark.ml.feature.Normalizer.explainParam")  (param)   |   Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string.   | |  [`explainParams`](#pyspark.ml.feature.Normalizer.explainParams "pyspark.ml.feature.Normalizer.explainParams")  ()   |   Returns the documentation of all params with their optionally default values and user-supplied values.   | |  [`extractParamMap`](#pyspark.ml.feature.Normalizer.extractParamMap "pyspark.ml.feature.Normalizer.extractParamMap")  ([extra])   |   Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra.   | |  [`getInputCol`](#pyspark.ml.feature.Normalizer.getInputCol "pyspark.ml.feature.Normalizer.getInputCol")  ()   |   Gets the value of inputCol or its default value.   | |  [`getOrDefault`](#pyspark.ml.feature.Normalizer.getOrDefault "pyspark.ml.feature.Normalizer.getOrDefault")  (param)   |   Gets the value of a param in the user-supplied param map or its default value.   | |  [`getOutputCol`](#pyspark.ml.feature.Normalizer.getOutputCol "pyspark.ml.feature.Normalizer.getOutputCol")  ()   |   Gets the value of outputCol or its default value.   | |  [`getP`](#pyspark.ml.feature.Normalizer.getP "pyspark.ml.feature.Normalizer.getP")  ()   |   Gets the value of p or its default value.   | |  [`getParam`](#pyspark.ml.feature.Normalizer.getParam "pyspark.ml.feature.Normalizer.getParam")  (paramName)   |   Gets a param by its name.   | |  [`hasDefault`](#pyspark.ml.feature.Normalizer.hasDefault "pyspark.ml.feature.Normalizer.hasDefault")  (param)   |   Checks whether a param has a default value.   | |  [`hasParam`](#pyspark.ml.feature.Normalizer.hasParam "pyspark.ml.feature.Normalizer.hasParam")  (paramName)   |   Tests whether this instance contains a param with a given (string) name.   | |  [`isDefined`](#pyspark.ml.feature.Normalizer.isDefined "pyspark.ml.feature.Normalizer.isDefined")  (param)   |   Checks whether a param is explicitly set by user or has a default value.   | |  [`isSet`](#pyspark.ml.feature.Normalizer.isSet "pyspark.ml.feature.Normalizer.isSet")  (param)   |   Checks whether a param is explicitly set by user.   | |  [`load`](#pyspark.ml.feature.Normalizer.load "pyspark.ml.feature.Normalizer.load")  (path)   |   Reads an ML instance from the input path, a shortcut of    read().load(path)    .   | |  [`read`](#pyspark.ml.feature.Normalizer.read "pyspark.ml.feature.Normalizer.read")  ()   |   Returns an MLReader instance for this class.   | |  [`save`](#pyspark.ml.feature.Normalizer.save "pyspark.ml.feature.Normalizer.save")  (path)   |   Save this ML instance to the given path, a shortcut of ‘write().save(path)’.   | |  [`set`](#pyspark.ml.feature.Normalizer.set "pyspark.ml.feature.Normalizer.set")  (param, value)   |   Sets a parameter in the embedded param map.   | |  [`setInputCol`](#pyspark.ml.feature.Normalizer.setInputCol "pyspark.ml.feature.Normalizer.setInputCol")  (value)   |   Sets the value of  [`inputCol`](#pyspark.ml.feature.Normalizer.inputCol "pyspark.ml.feature.Normalizer.inputCol")  .   | |  [`setOutputCol`](#pyspark.ml.feature.Normalizer.setOutputCol "pyspark.ml.feature.Normalizer.setOutputCol")  (value)   |   Sets the value of  [`outputCol`](#pyspark.ml.feature.Normalizer.outputCol "pyspark.ml.feature.Normalizer.outputCol")  .   | |  [`setP`](#pyspark.ml.feature.Normalizer.setP "pyspark.ml.feature.Normalizer.setP")  (value)   |   Sets the value of  [`p`](#pyspark.ml.feature.Normalizer.p "pyspark.ml.feature.Normalizer.p")  .   | |  [`setParams`](#pyspark.ml.feature.Normalizer.setParams "pyspark.ml.feature.Normalizer.setParams")  (self, \\*[, p, inputCol, outputCol])   |   Sets params for this Normalizer.   | |  [`transform`](#pyspark.ml.feature.Normalizer.transform "pyspark.ml.feature.Normalizer.transform")  (dataset[, params])   |   Transforms the input dataset with optional parameters.   | |  [`write`](#pyspark.ml.feature.Normalizer.write "pyspark.ml.feature.Normalizer.write")  ()   |   Returns an MLWriter instance for this ML instance.   |   Attributes    |  [`inputCol`](#pyspark.ml.feature.Normalizer.inputCol "pyspark.ml.feature.Normalizer.inputCol")  |   | |  [`outputCol`](#pyspark.ml.feature.Normalizer.outputCol "pyspark.ml.feature.Normalizer.outputCol")  |   | |  [`p`](#pyspark.ml.feature.Normalizer.p "pyspark.ml.feature.Normalizer.p")  |   | |  [`params`](#pyspark.ml.feature.Normalizer.params "pyspark.ml.feature.Normalizer.params")  |   Returns all params ordered by name.   |   Methods Documentation    `clear`    (   *param     :    [pyspark.ml.param.Param](pyspark.ml.param.Param.html#pyspark.ml.param.Param "pyspark.ml.param.Param")*   )    → None  [¶](#pyspark.ml.feature.Normalizer.clear "Permalink to this definition")    Clears a param from the param map if it has been explicitly set.    `copy`    (   *extra     :     Optional    [    ParamMap    ]     =     None*   )    → JP  [¶](#pyspark.ml.feature.Normalizer.copy "Permalink to this definition")    Creates a copy of this instance with the same uid and some extra params. This implementation first calls Params.copy and then make a copy of the companion Java pipeline component with extra params. So both the Python wrapper and the Java pipeline component get copied.     Parameters    **extra**    dict, optional     Extra parameters to copy to the new instance     Returns    `JavaParams`   Copy of this instance    `explainParam`    (   *param     :     Union    [    str    ,   [pyspark.ml.param.Param](pyspark.ml.param.Param.html#pyspark.ml.param.Param "pyspark.ml.param.Param")    ]*   )    → str  [¶](#pyspark.ml.feature.Normalizer.explainParam "Permalink to this definition")    Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string.    `explainParams`    (     )    → str  [¶](#pyspark.ml.feature.	The error message "TypeError: 'Column' object is not callable" is not related to the `Normalizer` class. It is likely caused by the line of code `pd.json_normalize(AccessJson)` which is trying to call a `Column` object as a function. This error can occur when a `Column` object is mistakenly used as a function call, for example, if parentheses are used instead of square brackets to access a column in a DataFrame.	0	0	1
thumb_down	https://docs.databricks.com/notebooks/schedule-notebook-jobs.html	how do I run a job in a notebook	Create and manage scheduled notebook jobs ===========================================     You can create and manage notebook jobs directly in the notebook UI. If a notebook is already assigned to one or more jobs, you can create and manage schedules for those jobs. If a notebook is not assigned to a job, you can create a job and a schedule to run the notebook.        Schedule a notebook job -------------------------     To schedule a notebook job to run periodically:     1. In the notebook, click  ![Notebook schedule button](../_images/schedule-button.png)  at the top right. If no jobs exist for this notebook, the Schedule dialog appears.      ![Schedule notebook dialog](../_images/schedule-dialog.png)    If jobs already exist for the notebook, the Jobs List dialog appears. To display the Schedule dialog, click  **Add a schedule**   .      ![Job list dialog](../_images/job-list-dialog.png) 2. In the Schedule dialog, optionally enter a name for the job. The default name is the name of the notebook. 3. Select  **Manual**   to run your job only when manually triggered, or  **Scheduled**   to define a schedule for running the job. If you select  **Scheduled**   , use the drop-downs to specify the frequency, time, and time zone. 4. In the  **Cluster**   drop-down, select the cluster to run the task.       If you have  **Allow Cluster Creation**   permissions, by default the job runs on a  [new job cluster](../workflows/jobs/jobs.html#choose-the-correct-cluster-type-for-your-job)  . To edit the configuration of the default job cluster, click  **Edit**   at the right of the field to display the  [cluster configuration dialog](../clusters/configure.html)  .       If you do not have  **Allow Cluster Creation**   permissions, by default the job runs on the cluster that the notebook is attached to. If the notebook is not attached to a cluster, you must select a cluster from the  **Cluster**   drop-down. 5. Optionally, enter any  **Parameters**   to pass to the job. Click  **Add**   and specify the key and value of each parameter. Parameters set the value of the  [notebook widget](widgets.html)  specified by the key of the parameter. Use  [Task parameter variables](../workflows/jobs/jobs.html#task-parameter-variables)  to pass a limited set of dynamic values as part of a parameter value. 6. Optionally, specify email addresses to receive  **Alerts**   on job events. See  [Add email and system notifications for job events](../workflows/jobs/job-notifications.html)  . 7. Click  **Submit**   .       Run a notebook job --------------------     To manually run a notebook job:     1. In the notebook, click  ![Notebook schedule button](../_images/schedule-button.png)  at the top right. 2. Click  **Run now**   . 3. To view the  [job run details](../workflows/jobs/jobs.html#job-run-details)  , click  ![New Tab Icon](../_images/open-in-new-tab.png)  .       Manage scheduled notebook jobs --------------------------------     To display jobs associated with this notebook, click the  **Schedule**   button. The jobs list dialog appears, showing all jobs currently defined for this notebook. To manage jobs, click  ![Jobs Vertical Ellipsis](../_images/jobs-vertical-ellipsis.png)  at the right of a job in the list.      ![Job list menu](../_images/job-list-menu.png)    From this menu, you can edit the schedule,  [clone](../workflows/jobs/jobs.html#clone-job)  the job, view  [job run details](../workflows/jobs/jobs.html#view-job-run-list)  , pause the job, resume the job, or delete a scheduled job.       When you clone a scheduled job, a new job is created with the same parameters as the original. The new job appears in the list with the name  `Clone     of     <initial     job     name>`  .       How you edit a job depends on the complexity of the job’s schedule. Either the Schedule dialog or the  [Job details panel](../workflows/jobs/jobs.html#job-edit)  displays, allowing you to edit the schedule, cluster, parameters, and so on.	To run a job in a notebook, you can create and manage notebook jobs directly in the notebook UI. To schedule a notebook job to run periodically, click the "Notebook schedule button" at the top right of the notebook and select "Scheduled" to define a schedule for running the job. Then, select the cluster to run the task and optionally enter any parameters to pass to the job. Finally, click "Submit" to create the job. To manually run a notebook job, click the "Notebook schedule button" and select "Run now". To manage scheduled notebook jobs, click the "Schedule" button to display jobs associated with the notebook.	1	1	1
no_reaction	https://kb.databricks.com/sql/inner-join-drops-records-in-result	How do I perform an outer join between two tables using dataframes in python?	   pyspark.sql.DataFrame.join  [¶](#pyspark-sql-dataframe-join "Permalink to this headline") ===========================================================================================  `DataFrame.`  `join`    (   *other     :     pyspark.sql.dataframe.DataFrame*  ,  *on     :     Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], None]     =     None*  ,  *how     :     Optional    [    str    ]     =     None*   )    → pyspark.sql.dataframe.DataFrame  [[source]](../../../_modules/pyspark/sql/dataframe.html#DataFrame.join) [¶](#pyspark.sql.DataFrame.join "Permalink to this definition")    Joins with another  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")  , using the given join expression.     New in version 1.3.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **other**   [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")   Right side of the join    **on**    str, list or  [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")  , optional     a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If    on    is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.    **how**    str, optional     default  `inner`  . Must be one of:  `inner`  ,  `cross`  ,  `outer`  ,  `full`  ,  `fullouter`  ,  `full\_outer`  ,  `left`  ,  `leftouter`  ,  `left\_outer`  ,  `right`  ,  `rightouter`  ,  `right\_outer`  ,  `semi`  ,  `leftsemi`  ,  `left\_semi`  ,  `anti`  ,  `leftanti`  and  `left\_anti`  .     Returns    [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")   Joined DataFrame.     Examples     The following performs a full outer join between  `df1`  and  `df2`  .    ``` >>> from pyspark.sql import Row >>> from pyspark.sql.functions import desc >>> df = spark.createDataFrame([(2, "Alice"), (5, "Bob")]).toDF("age", "name") >>> df2 = spark.createDataFrame([Row(height=80, name="Tom"), Row(height=85, name="Bob")]) >>> df3 = spark.createDataFrame([Row(age=2, name="Alice"), Row(age=5, name="Bob")]) >>> df4 = spark.createDataFrame([ ...     Row(age=10, height=80, name="Alice"), ...     Row(age=5, height=None, name="Bob"), ...     Row(age=None, height=None, name="Tom"), ...     Row(age=None, height=None, name=None), ... ])  ```   Inner join on columns (default)    ``` >>> df.join(df2, 'name').select(df.name, df2.height).show() +----+------+ |name|height| +----+------+ | Bob| 85| +----+------+ >>> df.join(df4, name', 'age).select(df.name, df.age).show() +----+---+ |name|age| +----+---+ | Bob| 5| +----+---+  ```   Outer join for both DataFrames on the ‘name’ column.    ``` >>> df.join(df2, df.name == df2.name, 'outer').select( ...     df.name, df2.height).sort(desc("name")).show() +-----+------+ | name|height| +-----+------+ | Bob| 85| |Alice| null| | null| 80| +-----+------+ >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc("name")).show() +-----+------+ | name|height| +-----+------+ | Tom| 80| | Bob| 85| |Alice| null| +-----+------+  ```   Outer join for both DataFrams with multiple columns.    ``` >>> df.join( ...     df3, ...     [df.name == df3.name, df.age == df3.age], ...     'outer' ... ).select(df.name, df3.age).show() +-----+---+ | name|age| +-----+---+ |Alice| 2| | Bob| 5| +-----+---+  ```  [pyspark.sql.DataFrame.isStreaming](pyspark.sql.DataFrame.isStreaming.html "previous page")  [pyspark.sql.DataFrame.limit](pyspark.sql.DataFrame.limit.html "next page")   	To perform an outer join between two tables using dataframes in Python, you can use the `join` method of the `DataFrame` class in PySpark. The `join` method takes two parameters: `other` and `how`. `other` is the right side of the join, which is another `DataFrame`. `how` specifies the type of join to perform, and can be one of `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, and `left_anti`. To perform an outer join, you can set `how` to `outer`.	1	1	1
no_reaction	https://kb.databricks.com/delta/id-duplicate-on-append	How do I insert into a delta table and remove any duplicates	    INSERT ========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Inserts new rows into a table and optionally truncates the table or partitions. You specify the inserted rows by value expressions or the result of a query.       Databricks SQL supports this statement only for Delta Lake tables.        Syntax --------      ``` INSERT { OVERWRITE | INTO } [ TABLE ] table\_name     [ PARTITION clause ]     [ ( column\_name [, ...] ) ]     query  INSERT INTO [ TABLE ] table\_name     REPLACE WHERE predicate     query  ```       Note       When you  `INSERT     INTO`  a Delta table, schema enforcement and evolution is supported. If a column’s data type cannot be safely cast to a Delta table’s data type, a runtime exception is thrown. If  [schema evolution](../../delta/update-schema.html)  is enabled, new columns can exist as the last columns of your schema (or nested columns) for the schema to evolve.          Parameters ------------   * **`INTO`  or  `OVERWRITE`**      If you specify  `OVERWRITE`  the following applies:      + Without a   `partition\_spec`   the table is truncated before inserting the first row.  + Otherwise, all partitions matching the   `partition\_spec`   are truncated before inserting the first row.  If you specify  `INTO`  all rows inserted are additive to the existing rows. * **[table\_name](sql-ref-names.html#table-name)**      Identifies the table to be inserted to. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the table cannot be found Databricks raises a  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error. * **[PARTITION clause](sql-ref-partition.html#partition)**      An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition.       When specifying a static partition  `column     =     value`  this column must not be repeated in the insert column list. * **(  [column\_name](sql-ref-identifiers.html)  [, …] )**       An optional list of columns in the table. The insert command may specify any particular column from the table at most once.      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  [SQL warehouse version 2022.35](../release-notes/index.html)  or higher  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.2 and above      + If this command omits a column, Databricks SQL assigns the corresponding default value instead.  + If the target table schema does not define any default value for the inserted column, Databricks SQL assigns   `NULL`   if the column is nullable. Otherwise, Databricks SQL raises an error.  Providing no column list is equivalent to specifying all columns, except for those with assigned values in the  `PARTITION`  clause, in the order defined in the table. * **REPLACE WHERE  [boolean\_expression](sql-ref-expression.html)**     **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 12.0 and above       If  `table\_name`  is a Delta Lake table, delete rows matching  `boolean\_expression`  before inserting any rows matching  `boolean-expression`  specified in  `query`  . Rows in  `query`  which do not match  `boolean\_expression`  are ignored.      `boolean\_expression`  can be any expression that evaluates to a result type  `BOOLEAN`  .       See  [Arbitrary selective overwrite with replaceWhere](../../delta/selective-overwrite.html#replace-where)  . * **[query](sql-ref-syntax-qry-query.html)**      A query that produces the rows to be inserted.       You must match the number of columns returned by the query with the specified or implied insert column list.       If a data type cannot be safely cast to the matching column data type, a runtime exception is thrown.      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  [SQL warehouse version 2022.35](../release-notes/index.html)  or higher  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.2 and above      + If   `query`   consists of a   [VALUES](sql-ref-syntax-qry-select-values.html)   clause the   `expression`   can be   `DEFAULT`   .  + If   `query`   consists of a   [SELECT](sql-ref-syntax-qry-select.html)   clause the   `named\_expression`   can be   `DEFAULT`   .  + `DEFAULT`   will insert the explicitly defined   `DEFAULT`   expression of the corresponding column in   `table\_name`   , or   `NULL`   if none is defined.  If  [schema evolution](../../delta/update-schema.html)  is enabled, new columns can exist as the last columns of your schema (or nested columns) for the schema to evolve.       Examples ----------      In this section:     * [INSERT INTO](#insert-into) * [Insert with a column list](#insert-with-a-column-list) * [Insert with both a partition spec and a column list](#insert-with-both-a-partition-spec-and-a-column-list) * [REPLACE WHERE](#replace-where) * [INSERT OVERWRITE](#insert-overwrite)     ###  [INSERT INTO](#id3)    ####   INSERT using VALUES      ``` > CREATE TABLE students (name VARCHAR(64), address VARCHAR(64) DEFAULT 'unknown', student\_id INT)  PARTITIONED BY (student\_id);  -- Single row insert using a `VALUES` clause specifying all columns. > INSERT INTO students VALUES  ('Amy Smith', '123 Park Ave, San Jose', 111111);  -- Single row insert using an implicit default for address > INSERT INTO students(name, student\_id) VALUES('Grayson Miller', 222222);  -- Single row insert using an explicit DEFAULT keyword for address > INSERT INTO students VALUES('Youna Kim', DEFAULT, 333333);  -- Multi-row insert using a `VALUES` clause > INSERT INTO students VALUES  ('Bob Brown', '456 Taylor St, Cupertino', 444444),  ('Cathy Johnson', '789 Race Ave, Palo Alto', 555555);  -- Multi-row insert using a mix of DEFAULT and literals > INSERT INTO students VALUES  ('Gwyneth Zhao', '120 Main St, Rockport', 666666),  ('Jackson Peterson', DEFAULT, 777777);  > SELECT \* FROM students;  name address student\_id  ---------------- ------------------------ ----------  Amy Smith 123 Park Ave, San Jose 111111  Grayson Miller unknown 222222  Youna Kim unknown 333333  Bob Brown 456 Taylor St, Cupertino 444444  Cathy Johnson 789 Race Ave, Palo Alto 555555  Gwyneth Zhao 120 Main St, Rockport 666666  Jackson Peterson unknown 777777  ```      ####   Insert using a subquery      ``` -- Assuming the persons table has already been created and populated. > SELECT \* FROM persons;  name address ssn  ------------- ------------------------- ---------  Dora Williams 134 Forest Ave, Melo Park 123456789  Eddie Davis 245 Market St, Milpitas 345678901  > INSERT INTO students PARTITION (student\_id = 444444)  SELECT name, address FROM persons WHERE name = "Dora Williams";  > SELECT \* FROM students;  name address student\_id  ------------- ------------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111  Bob Brown 456 Taylor St, Cupertino 222222  Cathy Johnson 789 Race Ave, Palo Alto 333333  Dora Williams 134 Forest Ave, Melo Park 444444  ```      ####   Insert using a  `TABLE`  clause      ``` -- Assuming the visiting\_students table has already been created and populated. > SELECT \* FROM visiting\_students;  name address student\_id  ------------- --------------------- ----------  Fleur Laurent 345 Copper St, London 777777  Gordon Martin 779 Lake Ave, Oxford 888888  > INSERT INTO students TABLE visiting\_students;  > SELECT \* FROM students;  name address student\_id  ------------- ------------------------- ----------  Amy Smith 123 Park Ave,San Jose 111111  Bob Brown 456 Taylor St, Cupertino 222222  Cathy Johnson 789 Race Ave, Palo Alto 333333  Dora Williams 134 Forest Ave, Melo Park 444444  Fleur Laurent 345 Copper St, London 777777  Gordon Martin 779 Lake Ave, Oxford 888888  ```      ####   Insert into a directory      ``` > CREATE TABLE students (name VARCHAR(64), address VARCHAR(64), student\_id INT)  PARTITIONED BY (student\_id)  LOCATION "/mnt/user1/students";  > INSERT INTO delta.`/mnt/user1/students` VALUES  ('Amy Smith', '123 Park Ave, San Jose', 111111); > SELECT \* FROM students;  name address student\_id  ------------- ------------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111  ```       ###  [Insert with a column list](#id4)      ``` > INSERT INTO students (address, name, student\_id) VALUES  ('Hangzhou, China', 'Kent Yao', 11215016); > SELECT \* FROM students WHERE name = 'Kent Yao';  name address student\_id  --------- ---------------------- ----------  Kent Yao Hangzhou, China 11215016  ```      ###  [Insert with both a partition spec and a column list](#id5)      ``` > INSERT INTO students PARTITION (student\_id = 11215017) (address, name) VALUES  ('Hangzhou, China', 'Kent Yao Jr.'); > SELECT \* FROM students WHERE student\_id = 11215017;  name address student\_id  ------------ ---------------------- ----------  Kent Yao Jr. Hangzhou, China 11215017  ```      ###  [REPLACE WHERE](#id6)      ``` > CREATE TABLE sales(tx\_date DATE, amount INTEGER); > INSERT INTO sales VALUES  (DATE'2022-10-01', 1234),  (DATE'2022-10-02', 2345),  (DATE'2022-10-03', 3456),  (DATE'2022-11-01', 3214);  -- Replace any rows with a transaction date in October 2022. > INSERT INTO sales REPLACE WHERE tx\_date BETWEEN '2022-10-01' AND '2022-10-31'  VALUES (DATE'2022-10-01', 1237),  (DATE'2022-10-02', 2378),  (DATE'2022-10-04', 2456),  (DATE'2022-10-05', 6328); > SELECT \* FROM sales ORDER BY tx\_date;  tx\_date amount  ---------- ------  2022-10-01 1237  2022-10-02 2378  2022-10-04 2456  2022-10-05 6328  2022-	To insert into a delta table and remove any duplicates, you can use the `INSERT INTO` statement with the `MERGE` clause. Here is an example:  ``` INSERT INTO delta_table SELECT DISTINCT * FROM source_table MERGE INTO delta_table ON delta_table.key_column = source_table.key_column WHEN NOT MATCHED THEN   INSERT * ```  In this example, `delta_table` is the name of the Delta table you want to insert into, and `source_table` is the name of the table you want to insert from. The `MERGE` clause is used to remove any duplicates based on the `key_column` in both tables. The `WHEN NOT MATCHED THEN INSERT *` clause is used to insert the non-duplicate rows into the Delta table.	0	0	0
no_reaction	https://docs.databricks.com/security/network/secure-cluster-connectivity.html	how do I enable secure cluster connectivity	Secure cluster connectivity =============================      Important       This feature is available only if your account is on the  [E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . Secure cluster connectivity is enabled for accounts on the E2 platform by default as of September 1, 2020.        With secure cluster connectivity enabled, customer VPCs have no open ports and Databricks Runtime cluster nodes have no public IP addresses.        Important       This article mentions the term  *data plane*   , which is the compute layer of the Databricks platform. In the context of this article, data plane refers to the Classic data plane in your AWS account. By contrast, the serverless data plane that supports  [serverless SQL warehouses (Public Preview)](../../sql/admin/serverless.html)  runs in the Databricks AWS account. To learn more, see  [Serverless compute](../../serverless-compute/index.html)  .      * At a network level, each cluster initiates a connection to the control plane secure cluster connectivity relay during cluster creation. The cluster establishes this connection using port 443 (HTTPS) and uses a different IP address than is used for the Web application and REST API. * When the control plane logically starts new Databricks Runtime jobs or performs other cluster administration tasks, these requests are sent to the cluster through this tunnel. * The data plane (the VPC) has no open ports, and Databricks Runtime cluster nodes have no public IP addresses.     Benefits:     * Easy network administration, with no need to configure ports on security groups or to configure network peering. * With enhanced security and simple network administration, information security teams can expedite approval of Databricks as a PaaS provider.    ![Secure cluster connectivity](../../_images/secure-cluster-connectivity-aws.png)     Use secure cluster connectivity ---------------------------------     To use secure cluster connectivity for a workspace, create a new workspace. You cannot add secure cluster connectivity to an existing workspace.	To enable secure cluster connectivity, you need to have an account on the E2 version of the Databricks platform. If you have an account on the E2 platform, secure cluster connectivity is enabled by default as of September 1, 2020. With secure cluster connectivity enabled, customer VPCs have no open ports and Databricks Runtime cluster nodes have no public IP addresses. At a network level, each cluster initiates a connection to the control plane secure cluster connectivity relay during cluster creation. The cluster establishes this connection using port 443 (HTTPS) and uses a different IP address than is used for the Web application and REST API. When the control plane logically starts new Databricks Runtime jobs or performs other cluster administration tasks, these requests are sent to the cluster through this tunnel. The data plane (the VPC) has no open ports, and Databricks Runtime cluster nodes have no public IP addresses. To use secure cluster connectivity for a workspace, create a new workspace. You cannot add secure cluster connectivity to an existing workspace.	1	1	1
no_reaction	https://docs.databricks.com/delta/clone.html	shallow clone table performance	    Clone a table on Databricks =============================     You can create a copy of an existing Delta Lake table on Databricks at a specific version using the  `clone`  command. Clones can be either deep or shallow.       Clone is a Databricks-exclusive feature enabled in the Databricks Runtime by default.       Databricks also supports cloning Parquet and Iceberg tables. See  [Incrementally clone Parquet and Iceberg tables to Delta Lake](clone-parquet.html)  .          Clone types -------------   * A  *deep clone*   is a clone that copies the source table data to the clone target in addition to the metadata of the existing table. Additionally, stream metadata is also cloned such that a stream that writes to the Delta table can be stopped on a source table and continued on the target of a clone from where it left off. * A  *shallow clone*   is a clone that does not copy the data files to the clone target. The table metadata is equivalent to the source. These clones are cheaper to create.     Any changes made to either deep or shallow clones affect only the clones themselves and not the source table.       The metadata that is cloned includes: schema, partitioning information, invariants, nullability. For deep clones only, stream and  [COPY INTO](../sql/language-manual/delta-copy-into.html)  metadata are also cloned. Metadata not cloned are the table description and  [user-defined commit metadata](custom-metadata.html)  .        Important     * Shallow clones reference data files in the source directory. If you run  `vacuum`  on the source table clients will no longer be able to read the referenced data files and a  `FileNotFoundException`  will be thrown. In this case, running clone with replace over the shallow clone will repair the clone. If this occurs often, consider using a deep clone instead which does not depend on the source table. * Deep clones do not depend on the source from which they were cloned, but are expensive to create because a deep clone copies the data as well as the metadata. * Cloning with  `replace`  to a target that already has a table at that path creates a Delta log if one does not exist at that path. You can clean up any existing data by running  `vacuum`  . * If an existing Delta table exists, a new commit is created that includes the new metadata and new data from the source table. This new commit is incremental, meaning that only new changes since the last clone are committed to the table. * Cloning a table is not the same as  `Create     Table     As     Select`  or  `CTAS`  . A clone copies the metadata of the source table in addition to the data. Cloning also has simpler syntax: you don’t need to specify partitioning, format, invariants, nullability and so on as they are taken from the source table. * A cloned table has an independent history from its source table. Time travel queries on a cloned table will not work with the same inputs as they work on its source table.        ``` CREATE TABLE delta.`/data/target/` CLONE delta.`/data/source/` -- Create a deep clone of /data/source at /data/target  CREATE OR REPLACE TABLE db.target\_table CLONE db.source\_table -- Replace the target  CREATE TABLE IF NOT EXISTS delta.`/data/target/` CLONE db.source\_table -- No-op if the target table exists  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source`  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` VERSION AS OF version  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` TIMESTAMP AS OF timestamp\_expression -- timestamp can be like “2019-01-01” or like date\_sub(current\_date(), 1)  ```       ``` from delta.tables import \*  deltaTable = DeltaTable.forPath(spark, pathToTable)  # path-based tables, or deltaTable = DeltaTable.forName(spark, tableName)    # Hive metastore-based tables  deltaTable.clone(target, isShallow, replace) # clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) # clone the source at a specific version  # clone the source at a specific timestamp such as timestamp=“2019-01-01” deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace)  ```       ``` import io.delta.tables.\_  val deltaTable = DeltaTable.forPath(spark, pathToTable) val deltaTable = DeltaTable.forName(spark, tableName)  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       ``` import io.delta.tables.\*;  DeltaTable deltaTable = DeltaTable.forPath(spark, pathToTable); DeltaTable deltaTable = DeltaTable.forName(spark, tableName);  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       For syntax details, see  [CREATE TABLE CLONE](../sql/language-manual/delta-clone.html)  .         Clone metrics ---------------      Note       Available in Databricks Runtime 8.2 and above.       `CLONE`  reports the following metrics as a single row DataFrame once the operation is complete:     * `source\_table\_size`  : Size of the source table that’s being cloned in bytes. * `source\_num\_of\_files`  : The number of files in the source table. * `num\_removed\_files`  : If the table is being replaced, how many files are removed from the current table. * `num\_copied\_files`  : Number of files that were copied from the source (0 for shallow clones). * `removed\_files\_size`  : Size in bytes of the files that are being removed from the current table. * `copied\_files\_size`  : Size in bytes of the files copied to the table.    ![Clone metrics example](../_images/clone-metrics.png)      Permissions -------------     You must configure permissions for Databricks table access control and your cloud provider.      ###   Table access control     The following permissions are required for both deep and shallow clones:     * `SELECT`  permission on the source table. * If you are using  `CLONE`  to create a new table,  `CREATE`  permission on the database in which you are creating the table. * If you are using  `CLONE`  to replace a table, you must have  `MODIFY`  permission on the table.     ###   Cloud provider permissions     If you have created a deep clone, any user that reads the deep clone must have read access to the clone’s directory. To make changes to the clone, users must have write access to the clone’s directory.       If you have created a shallow clone, any user that reads the shallow clone needs permission to read the files in the original table, since the data files remain in the source table with shallow clones, as well as the clone’s directory. To make changes to the clone, users will need write access to the clone’s directory.          Use clone for data archiving ------------------------------     Data may need to be kept for longer than is feasible with time travel or for disaster recovery. In these cases, you can create a deep clone to preserve the state of a table at a certain point in time for archival. Incremental archiving is also possible to keep a continually updating state of a source table for disaster recovery.        ``` -- Every month run CREATE OR REPLACE TABLE delta.`/some/archive/path` CLONE my\_prod\_table  ```        Use clone for ML model reproduction -------------------------------------     When doing machine learning, you may want to archive a certain version of a table on which you trained an ML model. Future models can be tested using this archived data set.        ``` -- Trained model on version 15 of Delta table CREATE TABLE delta.`/model/dataset` CLONE entire\_dataset VERSION AS OF 15  ```        Use clone for short-term experiments on a production table ------------------------------------------------------------     To test a workflow on a production table without corrupting the table, you can easily create a shallow clone. This allows you to run arbitrary workflows on the cloned table that contains all the production data but does not affect any production workloads.        ``` -- Perform shallow clone CREATE OR REPLACE TABLE my\_test SHALLOW CLONE my\_prod\_table;  UPDATE my\_test WHERE user\_id is null SET invalid=true; -- Run a bunch of validations. Once happy:  -- This should leverage the update information in the clone to prune to only -- changed files in the clone if possible MERGE INTO my\_prod\_table USING my\_test ON my\_test.user\_id <=> my\_prod\_table.user\_id WHEN MATCHED AND my\_test.user\_id is null THEN UPDATE \*;  DROP TABLE my\_test;  ```        Use clone for data sharing ----------------------------     Other business units within a single organization may want to access the same data but may not require the latest updates. Instead of giving access to the source table directly, you can provide clones with different permissions for different business units. The performance of the clone can exceed that of a simple view.        ``` -- Perform deep clone CREATE OR REPLACE TABLE shared\_table CLONE my\_prod\_table;  -- Grant other users access to the shared table GRANT SELECT ON shared\_table TO `<user-name>@<user-domain>.com`;  ```        Use clone to override table properties ----------------------------------------      Note       Available in Databricks Runtime 7.5 and above.        Table property overrides are particularly useful for:     * Annotating tables with owner or user information when sharing data with different business units. * Archiving Delta tables and time travel is required. You can specify the log retention period independently for the archive table. For example:       ``` CREATE OR REPLACE TABLE archive.my\_table CLONE prod.my\_table TBLPROPERTIES ( delta.logRetentionDuration = '3650 days', delta.deletedFileRetentionDuration = '3650 days' ) LOCATION 'xx://archive/my\_table'  ```       ``` dt = DeltaTable.forName(spark, "prod.my\_table") tblProps = { "delta.logRetentionDuration": "3650 days", "delta.deletedFileRetentionDuration": "3650 days" } dt.clone('xx://archive/my\_table', isShallow=False, replace=True, tblProps)  ```       ``` val dt = DeltaTable.forName(spark, "prod.my\_table") val tblProps = Map( "delta.logRetentionDuration" -> "3650 days", "delta.deletedFileRetentionDuration" -> "3650 days" ) dt.clone("xx://archive/my\_table", isShallow = false, replace = true, properties = tblProps)  ```                                                        	Shallow clone table performance.	0	0	0
no_reaction	https://docs.databricks.com/mlflow/models.html	Can I containerize models with mlflow?	      Log, load, register, and deploy MLflow models ===============================================     An MLflow  [Model](https://mlflow.org/docs/latest/models.html)   is a standard format for packaging machine learning models that can be used in a variety of downstream tools—for example, batch inference on Apache Spark or real-time serving through a REST API. The format defines a convention that lets you save a model in different  [flavors](https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors)   (python-function, pytorch, sklearn, and so on), that can be understood by different model  [serving and inference platforms](https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools)   .          Log and load models ---------------------     When you log a model, MLflow automatically logs  `requirements.txt`  and  `conda.yaml`  files. You can use these files to recreate the model development environment and reinstall dependencies using  `virtualenv`  (recommended) or  `conda`  .        Important       Anaconda Inc. updated their  [terms of service](https://www.anaconda.com/terms-of-service)   for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See  [Anaconda Commercial Edition FAQ](https://www.anaconda.com/blog/anaconda-commercial-edition-faq)   for more information. Your use of any Anaconda channels is governed by their terms of service.       MLflow models logged before  [v1.18](https://mlflow.org/news/2021/06/18/1.18.0-release/index.html)   (Databricks Runtime 8.3 ML or earlier) were by default logged with the conda  `defaults`  channel (  <https://repo.anaconda.com/pkgs/>  ) as a dependency. Because of this license change, Databricks has stopped the use of the  `defaults`  channel for models logged using MLflow v1.18 and above. The default channel logged is now  `conda-forge`  , which points at the community managed  <https://conda-forge.org/>  .       If you logged a model before MLflow v1.18 without excluding the  `defaults`  channel from the conda environment for the model, that model may have a dependency on the  `defaults`  channel that you may not have intended. To manually confirm whether a model has this dependency, you can examine  `channel`  value in the  `conda.yaml`  file that is packaged with the logged model. For example, a model’s  `conda.yaml`  with a  `defaults`  channel dependency may look like this:        ``` channels: - defaults dependencies: - python=3.8.8 - pip - pip:  - mlflow  - scikit-learn==0.23.2  - cloudpickle==1.6.0  name: mlflow-env  ```      Because Databricks can not determine whether your use of the Anaconda repository to interact with your models is permitted under your relationship with Anaconda, Databricks is not forcing its customers to make any changes. If your use of the Anaconda.com repo through the use of Databricks is permitted under Anaconda’s terms, you do not need to take any action.       If you would like to change the channel used in a model’s environment, you can re-register the model to the model registry with a new  `conda.yaml`  . You can do this by specifying the channel in the  `conda\_env`  parameter of  `log\_model()`  .       For more information on the  `log\_model()`  API, see the MLflow documentation for the model flavor you are working with, for example,  [log\_model for scikit-learn](https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)   .       For more information on  `conda.yaml`  files, see the  [MLflow documentation](https://www.mlflow.org/docs/latest/models.html#additional-logged-files)   .       ###   API commands     To log a model to the MLflow  [tracking server](tracking.html)  , use  `mlflow.<model-type>.log\_model(model,     ...)`  .       To load a previously logged model for inference or further development, use  `mlflow.<model-type>.load\_model(modelpath)`  , where  `modelpath`  is one of the following:     * a run-relative path (such as  `runs:/{run\_id}/{model-path}`  ) * a DBFS path * a  [registered model](model-registry.html)  path (such as  `models:/{model\_name}/{model\_stage}`  ).     For a complete list of options for loading MLflow models, see  [Referencing Artifacts in the MLflow documentation](https://www.mlflow.org/docs/latest/concepts.html#artifact-locations)   .       For Python MLflow models, an additional option is to use  `mlflow.pyfunc.load\_model()`  to load the model as a generic Python function. You can use the following code snippet to load the model and score data points.        ``` model = mlflow.pyfunc.load\_model(model\_path) model.predict(model\_input)  ```      As an alternative, you can export the model as an Apache Spark UDF to use for scoring on a Spark cluster, either as a batch job or as a real-time  [Spark Streaming](../structured-streaming/index.html)  job.        ``` # load input data table as a Spark DataFrame input\_data = spark.table(input\_table\_name) model\_udf = mlflow.pyfunc.spark\_udf(model\_path) df = input\_data.withColumn("prediction", model\_udf())  ```      ###   Log model dependencies     To accurately load a model, you should make sure the model dependencies are loaded with the correct versions into the notebook environment. In Databricks Runtime 10.5 ML and above, MLflow warns you if a mismatch is detected between the current environment and the model’s dependencies.       Additional functionality to simplify restoring model dependencies is included in Databricks Runtime 11.0 ML and above. In Databricks Runtime 11.0 ML and above, for  `pyfunc`  flavor models, you can call  `mlflow.pyfunc.get\_model\_dependencies`  to retrieve and download the model dependencies. This function returns a path to the dependencies file which you can then install by using  `%pip     install     <file-path>`  . When you load a model as a PySpark UDF, specify  `env\_manager="virtualenv"`  in the  `mlflow.pyfunc.spark\_udf`  call. This restores model dependencies in the context of the PySpark UDF and does not affect the outside environment.       You can also use this functionality in Databricks Runtime 10.5 or below by manually installing  [MLflow version 1.25.0 or above](https://www.mlflow.org/docs/latest/index.html)   :        ``` %pip install "mlflow>=1.25.0"  ```      For additional information on how to log model dependencies (Python and non-Python) and artifacts, see  [Log model dependencies](log-model-dependencies.html)  .       Learn how to log model dependencies and custom artifacts for model serving:     * [Deploy models with dependencies](log-model-dependencies.html#deploy-dependencies) * [Use custom Python libraries with Model Serving](../machine-learning/model-serving/private-libraries-model-serving.html) * [Package custom artifacts for Model Serving](../machine-learning/model-serving/model-serving-custom-artifacts.html)    * [Log model dependencies](log-model-dependencies.html)      ###   Automatically generated code snippets in the MLflow UI     When you log a model in a Databricks notebook, Databricks automatically generates code snippets that you can copy and use to load and run the model. To view these code snippets:     1. Navigate to the Runs screen for the run that generated the model. (See  [View notebook experiment](tracking.html#view-notebook-experiment)  for how to display the Runs screen.) 2. Scroll to the  **Artifacts**   section. 3. Click the name of the logged model. A panel opens to the right showing code you can use to load the logged model and make predictions on Spark or pandas DataFrames.    ![Artifact panel code snippets](../_images/code-snippets.png)    ###   Examples     For examples of logging models, see the examples in  [Track machine learning training runs examples](tracking.html#tracking-examples)  . For an example of loading a logged model for inference, see the following example.      * [Model inference example](model-example.html)         Register models in the Model Registry ---------------------------------------     You can register models in the  [MLflow Model Registry](model-registry.html)  , a centralized model store that provides a UI and set of APIs to manage the full lifecycle of MLflow Models. For general information about the Model Registry, see  [MLflow Model Registry on Databricks](model-registry.html)  . For instructions on how to use the Model Registry to manage models in Databricks, see  [Manage model lifecycle](../machine-learning/manage-model-lifecycle/index.html)  .       To register a model using the API, use  `mlflow.register\_model("runs:/{run\_id}/{model-path}",     "{registered-model-name}")`  .         Save models to DBFS ---------------------     To save a model locally, use  `mlflow.<model-type>.save\_model(model,     modelpath)`  .  `modelpath`  must be a  [DBFS](../dbfs/index.html)  path. For example, if you use a DBFS location  `dbfs:/my\_project\_models`  to store your project work, you must use the model path  `/dbfs/my\_project\_models`  :        ```   modelpath = "/dbfs/my\_project\_models/model-%f-%f" % (alpha, l1\_ratio)   mlflow.sklearn.save\_model(lr, modelpath)  ```      For MLlib models, use  [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines)   .         Download model artifacts --------------------------     You can download the logged model artifacts (such as model files, plots, and metrics) for a registered model with various APIs.      [Python API](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_model_version_download_uri)   example:        ``` from mlflow.store.artifact.models\_artifact\_repo import ModelsArtifactRepository  model\_uri = MlflowClient.get\_model\_version\_download\_uri(model\_name, model\_version) ModelsArtifactRepository(model\_uri).download\_artifacts(artifact\_path="")  ```     [Java API](https://mlflow.org/docs/latest/java_api/org/mlflow/tracking/MlflowClient.html#downloadModelVersion-java.lang	Yes, you can containerize models with MLflow. MLflow provides a standard format for packaging machine learning models that can be used in a variety of downstream tools, including containerization. The format defines a convention that lets you save a model in different flavors (python-function, pytorch, sklearn, and so on), that can be understood by different model serving and inference platforms. You can log a model to the MLflow tracking server and then save it locally to a DBFS path, which can be used to containerize the model. For more information on how to log and load models with MLflow, see the MLflow documentation.	0	1	1
no_reaction	https://docs.databricks.com/sql/index.html	why use databricks sql	###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) * [Error Conditions](sql-error-conditions.html)   Spark SQL, DataFrames and Datasets Guide ==========================================   Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.     All of the examples on this page use sample data included in the Spark distribution and can be run in the  `spark-shell`   ,  `pyspark`   shell, or  `sparkR`   shell.     SQL -----   One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the  [Hive Tables](sql-data-sources-hive-tables.html)   section. When running SQL from within another programming language the results will be returned as a  [Dataset/DataFrame](#datasets-and-dataframes)   . You can also interact with the SQL interface using the  [command-line](sql-distributed-sql-engine.html#running-the-spark-sql-cli)   or over  [JDBC/ODBC](sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server)   .     Datasets and DataFrames -------------------------   A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be  [constructed](sql-getting-started.html#creating-datasets)   from JVM objects and then manipulated using functional transformations (  `map`   ,  `flatMap`   ,  `filter`   , etc.). The Dataset API is available in  [Scala](api/scala/org/apache/spark/sql/Dataset.html)   and  [Java](api/java/index.html?org/apache/spark/sql/Dataset.html)   . Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally  `row.columnName`   ). The case for R is similar.     A DataFrame is a  *Dataset*   organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of  [sources](sql-data-sources.html)   such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java,  [Python](api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)   , and  [R](api/R/index.html)   . In Scala and Java, a DataFrame is represented by a Dataset of  `Row`   s. In  [the Scala API](api/scala/org/apache/spark/sql/Dataset.html)   ,  `DataFrame`   is simply a type alias of  `Dataset[Row]`   . While, in  [Java API](api/java/index.html?org/apache/spark/sql/Dataset.html)   , users need to use  `Dataset<Row>`   to represent a  `DataFrame`   .     Throughout this document, we will often refer to Scala/Java Datasets of  `Row`   s as DataFrames.	Databricks SQL is used for structured data processing. It provides interfaces for Spark SQL, DataFrames, and Datasets. Spark SQL uses extra information to perform extra optimizations. One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. A Dataset is a distributed collection of data. A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.	1	1	1
no_reaction	https://docs.databricks.com/libraries/cluster-libraries.html	How to install Maven library in cluster	Cluster libraries ===================     Cluster libraries can be used by all notebooks running on a cluster. You can install a cluster library directly from a public repository such as PyPI or Maven, using a previously installed workspace library, or using an init script.          Install a library on a cluster --------------------------------     There are two primary ways to install a library on a cluster:     * Install a  [workspace library](workspace-libraries.html)  that has been already been uploaded to the workspace. * Install a library for use with a specific cluster only.     In addition, if your library requires custom configuration, you may not be able to install it using the methods listed above. Instead, you can install the library using an  [init script](#init-script)  that runs at cluster creation time.        Note       When you install a library on a cluster, a notebook already attached to that cluster will not immediately see the new library. You must first  [detach](../notebooks/notebook-ui.html#detach)  and then  [reattach](../notebooks/notebook-ui.html#attach)  the notebook to the cluster.        In addition to the approaches in this article, you can also install a library on a cluster by using the  [Databricks Terraform provider](../dev-tools/terraform/index.html)  and  [databricks\_library](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/library)   .        In this section:     * [Workspace library](#workspace-library) * [Cluster-installed library](#cluster-installed-library) * [Init script](#init-script)       ###  [Workspace library](#id3)      Note       Databricks processes all workspace libraries in the order that they were installed on the cluster. You might need to pay attention to the order of installation on the cluster if there are dependencies between libraries.        To install a library that already exists in the workspace, you can start from the cluster UI or the library UI:      ####   Cluster   1. Click  ![compute icon](../_images/clusters-icon.png) **Compute**   in the sidebar. 2. Click a cluster name. 3. Click the  **Libraries**   tab. 4. Click  **Install New**   . 5. In the Library Source button list, select  **Workspace**   . 6. Select a workspace library. 7. Click  **Install**   . 8. To configure the library to be installed on all clusters:      1. Click the library.  2. Select the   **Install automatically on all clusters**    checkbox.  3. Click   **Confirm**    .     ####   Library   1. Go to the folder containing the library. 2. Click the library name. 3. Do one of the following:      * To configure the library to be installed on all clusters, select the   **Install automatically on all clusters**    checkbox and click   **Confirm**    .              Important            This option does not install the library on clusters running Databricks Runtime 7.0 and above.  * Select the checkbox next to the cluster that you want to install the library on and click   **Install**    .     The library is installed on the cluster.        ###  [Cluster-installed library](#id4)     You can install a library on a specific cluster without making it available as a workspace library.       To install a library on a cluster:     1. Click  ![compute icon](../_images/clusters-icon.png) **Compute**   in the sidebar. 2. Click a cluster name. 3. Click the  **Libraries**   tab. 4. Click  **Install New**   . 5. Follow one of the methods for creating a  [workspace library](workspace-libraries.html)  . After you click  **Create**   , the library is installed on the cluster.       ###  [Init script](#id5)     If your library requires custom configuration, you may not be able to install it using the workspace or cluster library interface. Instead, you can install the library using an  [init script](../clusters/init-scripts.html)  .       Here is an example of an init script that uses pip to install Python libraries on a Databricks Runtime cluster at cluster initialization.        ``` #!/bin/bash  /databricks/python/bin/pip install astropy  ```           Uninstall a library from a cluster ------------------------------------      Note       When you uninstall a library from a cluster, the library is removed only when you restart the cluster. Until you restart the cluster, the status of the uninstalled library appears as  **Uninstall pending restart**   .        To uninstall a library you can start from a cluster or a library:        ###   Cluster   1. Click  ![compute icon](../_images/clusters-icon.png) **Compute**   in the sidebar. 2. Click a cluster name. 3. Click the  **Libraries**   tab. 4. Select the checkbox next to the cluster you want to uninstall the library from, click  **Uninstall**   , then  **Confirm**   . The Status changes to  **Uninstall pending restart**   .       ###   Library   1. Go to the folder containing the library. 2. Click the library name. 3. Select the checkbox next to the cluster you want to uninstall the library from, click  **Uninstall**   , then  **Confirm**   . The Status changes to  **Uninstall pending restart**   . 4. Click the cluster name to go to the cluster detail page.     Click  **Restart**   and  **Confirm**   to uninstall the library. The library is removed from the cluster’s Libraries tab.          View the libraries installed on a cluster -------------------------------------------   1. Click  ![compute icon](../_images/clusters-icon.png) **Compute**   in the sidebar. 2. Click the cluster name. 3. Click the  **Libraries**   tab. For each library, the tab displays the name and version, type,  [install status](../dev-tools/api/latest/libraries.html#managedlibrarieslibraryinstallstatus)  , and, if uploaded, the source file.       Update a cluster-installed library ------------------------------------     To update a cluster-installed library, uninstall the old version of the library and install a new version.	To install a Maven library in a cluster, you can use the "Install New" option in the Libraries tab of the cluster UI and select "Maven" as the Library Source. Then, you can enter the Maven coordinates for the library you want to install and click "Install".	0	1	0
no_reaction	https://docs.databricks.com/archive/spark-sql-2.x-language-manual/vacuum.html	what is vacuum?	Vacuum ========      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [VACUUM](../../sql/language-manual/delta-vacuum.html)  .        Clean up files associated with a table.       This command works differently depending on whether you’re working on a Delta or Apache Spark table.          Vacuum a Delta table (Delta Lake on Databricks) -------------------------------------------------      ``` VACUUM [ [db\_name.]table\_name | path] [RETAIN num HOURS] [DRY RUN]  ```      Recursively vacuum directories associated with the Delta table and remove data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. Files are deleted according to the time they have been logically removed from Delta’s transaction log + retention hours, not their modification timestamps on the storage system. The default threshold is 7 days.       On Delta tables, Databricks  *does not*   automatically trigger  `VACUUM`  operations. See  [Remove unused data files with vacuum](../../delta/vacuum.html)  .       If you run  `VACUUM`  on a Delta table, you lose the ability to  [time travel](../../delta/history.html)  back to a version older than the specified data retention period.      **`RETAIN     num     HOURS`**      The retention threshold.      **`DRY     RUN`**      Return a list of files to be deleted.           Vacuum a Spark table (Apache Spark) -------------------------------------      ``` VACUUM [ [db\_name.]table\_name | path] [RETAIN num HOURS]  ```     **`RETAIN     num     HOURS`**      The retention threshold.       Recursively vacuum directories associated with the Spark table and remove uncommitted files older than a retention threshold. The default threshold is 7 days.       On Spark tables, Databricks automatically triggers  `VACUUM`  operations as data is written.	Vacuum is a command used to clean up files associated with a table. It works differently depending on whether you’re working on a Delta or Apache Spark table. When vacuuming a Delta table, it recursively vacuums directories associated with the table and removes data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. When vacuuming a Spark table, it recursively vacuums directories associated with the table and removes uncommitted files older than a retention threshold.	1	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/get-started.html	How to use unity data catalog?	Data discovery and collaboration in the lakehouse ===================================================     Databricks designed Unity Catalog to help organizations reduce time to insights by empowering a broader set of data users to discover and analyze data at scale. Data stewards can securely grant access to data assets for diverse teams of end users in Unity Catalog. These users can then use a variety of languages and tools, including SQL and Python, to create derivative datasets, models, and dashboards that can be shared across teams.        Manage permissions at scale -----------------------------     Unity Catalog provides administrators a unified location to assign permissions for catalogs, databases, tables, and views to groups of users. Privileges and metastores are shared across workspaces, allowing administrators to set secure permissions once against groups synced from identity providers and know that end users only have access to the proper data in any Databricks workspace they enter.       Unity Catalog also allows administrators to define storage credentials, a secure way to store and share permissions on cloud storage infrastructure. You can grant privileges on these securables to power users within the organization so they can define external locations against cloud object storage locations, allowing data engineers to self-service for new workloads without needing to provide elevated permissions in cloud account consoles.         Discover data on Databricks -----------------------------     Users can browse available data objects in Unity Catalog using the  [Data Explorer](../data/index.html)  . Data Explorer uses the privileges configured by Unity Catalog administrators to ensure that users are only able to see catalogs, databases, tables, and views that they have permissions to query. Once users find a dataset of interest, they can review field names and types, read comments on tables and individual fields, and preview a sample of the data. Users can also review the full history of the table to understand when and how data has changed, and the lineage feature allows users to track how certain datasets are derived from upstream jobs and used in downstream jobs.       Storage credentials and external locations are also displayed in Data Explorer, allowing each user to fully grasp the privileges they have to read and write data across available locations and resources.         Accelerate time to production with the lakehouse --------------------------------------------------     Databricks supports workloads in SQL, Python, Scala, and R, allowing users with diverse skill sets and technical backgrounds to leverage their knowledge to derive analytic insights. You can use all languages supported by Databricks to define production jobs, and notebooks can leverage a combination of languages. This means that you can promote queries written by SQL analysts for last mile ETL into production data engineering code with almost no effort. Queries and workloads defined by personas across the organization leverage the same datasets, so there’s no need to reconcile field names or make sure dashboards are up to date before sharing code and results with other teams. You can securely share code, notebooks, queries, and dashboards, all powered by the same scalable cloud infrastructure and defined against the same curated data sources.	To use Unity Catalog, data stewards can securely grant access to data assets for diverse teams of end users in Unity Catalog. Users can browse available data objects in Unity Catalog using the Data Explorer. Unity Catalog provides administrators a unified location to assign permissions for catalogs, databases, tables, and views to groups of users. Privileges and metastores are shared across workspaces, allowing administrators to set secure permissions once against groups synced from identity providers and know that end users only have access to the proper data in any Databricks workspace they enter. Unity Catalog also allows administrators to define storage credentials, a secure way to store and share permissions on cloud storage infrastructure.	1	1	1
no_reaction	https://docs.databricks.com/machine-learning/feature-store/time-series.html	Does FeatureStoreClient().score_batch support multidimentional predictions? I have a pyfunc model that I can use to get predictions. It takes time series data with context information at each date, and produces a string of predictions. For example:     The data is set up like below (temp/pressure/output are different than my input data columns)  date,sales,temperature,pressure,output 01-09-2020,100,101,5000,10 01-10-2020,120,91,4000,24 01-11-2020,50,110,6000,30 let's say the model is trained using a window size of 60 and prediction inverval of 14. then in the model, you provide provide 60 records, and the prediction will get 14 predictions starting the last date+1 in your prediction dataset.     the return is just of the form  date,prediction 01-12-2022,81 01-13-2022,60 01-14-2022,111 ... with N records (in our example 14). it works brilliantly if I augment the data myself and work with the predict function.     Does working with the feature store support this? score batch doesn't seem to be able to return arbitrary/different shaped data. I could try making the data wide, but then that would defeat the purpose of trying to use the feature store.     don't know if I'm making sense.	   pyspark.ml.functions.predict\_batch\_udf  [¶](#pyspark-ml-functions-predict-batch-udf "Permalink to this headline") =====================================================================================================================  `pyspark.ml.functions.`  `predict_batch_udf`    (   *make\_predict\_fn     :     Callable    [     ]     ,    PredictBatchFunction    ]*  ,  *\**  ,  *return\_type     :     DataType*  ,  *batch\_size     :     int*  ,  *input\_tensor\_shapes     :     Optional    [    Union    [    List    [    Optional    [    List    [    int    ]     ]     ]     ,    Mapping    [    int    ,    List    [    int    ]     ]     ]     ]     =     None*   )    → UserDefinedFunctionLike  [[source]](../../_modules/pyspark/ml/functions.html#predict_batch_udf) [¶](#pyspark.ml.functions.predict_batch_udf "Permalink to this definition")    Given a function which loads a model and returns a    predict    function for inference over a batch of numpy inputs, returns a Pandas UDF wrapper for inference over a Spark DataFrame.     The returned Pandas UDF does the following on each DataFrame partition:    * calls the    make\_predict\_fn    to load the model and cache its    predict    function. * batches the input records as numpy arrays and invokes    predict    on each batch.   Note: this assumes that the    make\_predict\_fn    encapsulates all of the necessary dependencies for running the model, or the Spark executor environment already satisfies all runtime requirements.     For the conversion of the Spark DataFrame to numpy arrays, there is a one-to-one mapping between the input arguments of the    predict    function (returned by the    make\_predict\_fn    ) and the input columns sent to the Pandas UDF (returned by the    predict\_batch\_udf    ) at runtime. Each input column will be converted as follows:    * scalar column -> 1-dim np.ndarray * tensor column + tensor shape -> N-dim np.ndarray   Note that any tensor columns in the Spark DataFrame must be represented as a flattened one-dimensional array, and multiple scalar columns can be combined into a single tensor column using the standard  [`pyspark.sql.functions.array()`](../pyspark.sql/api/pyspark.sql.functions.array.html#pyspark.sql.functions.array "pyspark.sql.functions.array")  function.     New in version 3.4.0.     Parameters    **make\_predict\_fn**    callable     Function which is responsible for loading a model and returning a  `PredictBatchFunction`  which takes one or more numpy arrays as input and returns one of the following:    * a numpy array (for a single output) * a dictionary of named numpy arrays (for multiple outputs) * a row-oriented list of dictionaries (for multiple outputs).   For a dictionary of named numpy arrays, the arrays can only be one or two dimensional, since higher dimensional arrays are not supported. For a row-oriented list of dictionaries, each element in the dictionary must be either a scalar or one-dimensional array.    **return\_type**   [`pyspark.sql.types.DataType`](../pyspark.sql/api/pyspark.sql.types.DataType.html#pyspark.sql.types.DataType "pyspark.sql.types.DataType")  or str.     Spark SQL datatype for the expected output:    * Scalar (e.g. IntegerType, FloatType) –> 1-dim numpy array. * ArrayType –> 2-dim numpy array. * StructType –> dict with keys matching struct fields. * StructType –> list of dict with keys matching struct fields, for models like the  [Huggingface pipeline for sentiment analysis](https://huggingface.co/docs/transformers/quicktour#pipeline-usage)   .  **batch\_size**    int     Batch size to use for inference. This is typically a limitation of the model and/or available hardware resources and is usually smaller than the Spark partition size.    **input\_tensor\_shapes**    list, dict, optional.     A list of ints or a dictionary of ints (key) and list of ints (value). Input tensor shapes for models with tensor inputs. This can be a list of shapes, where each shape is a list of integers or None (for scalar inputs). Alternatively, this can be represented by a “sparse” dictionary, where the keys are the integer indices of the inputs, and the values are the shapes. Each tensor input value in the Spark DataFrame must be represented as a single column containing a flattened 1-D array. The provided    input\_tensor\_shapes    will be used to reshape the flattened array into the expected tensor shape. For the list form, the order of the tensor shapes must match the order of the selected DataFrame columns. The batch dimension (typically -1 or None in the first dimension) should not be included, since it will be determined by the batch\_size argument. Tabular datasets with scalar-valued columns should not provide this argument.     Returns    `UserDefinedFunctionLike`   A Pandas UDF for model inference on a Spark DataFrame.     Examples     For a pre-trained TensorFlow MNIST model with two-dimensional input images represented as a flattened tensor value stored in a single Spark DataFrame column of type    array<float>    .    ``` from pyspark.ml.functions import predict\_batch\_udf  def make\_mnist\_fn():     # load/init happens once per python worker     import tensorflow as tf     model = tf.keras.models.load\_model('/path/to/mnist\_model')      # predict on batches of tasks/partitions, using cached model     def predict(inputs: np.ndarray) -> np.ndarray:         # inputs.shape = [batch\_size, 784], see input\_tensor\_shapes         # outputs.shape = [batch\_size, 10], see return\_type         return model.predict(inputs)      return predict  mnist\_udf = predict\_batch\_udf(make\_mnist\_fn,                               return\_type=ArrayType(FloatType()),                               batch\_size=100,                               input\_tensor\_shapes=[[784]])  df = spark.read.parquet("/path/to/mnist\_data") df.show(5) # +--------------------+ # | data| # +--------------------+ # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # +--------------------+  df.withColumn("preds", mnist\_udf("data")).show(5) # +--------------------+--------------------+ # | data| preds| # +--------------------+--------------------+ # |[0.0, 0.0, 0.0, 0...|[-13.511008, 8.84...| # |[0.0, 0.0, 0.0, 0...|[-5.3957458, -2.2...| # |[0.0, 0.0, 0.0, 0...|[-7.2014456, -8.8...| # |[0.0, 0.0, 0.0, 0...|[-19.466187, -13....| # |[0.0, 0.0, 0.0, 0...|[-5.7757926, -7.8...| # +--------------------+--------------------+  ```   To demonstrate usage with different combinations of input and output types, the following examples just use simple mathematical transforms as the models.    * Single scalar column     Input DataFrame has a single scalar column, which will be passed to the    predict    function as a 1-D numpy array.    ``` >>> import numpy as np >>> import pandas as pd >>> from pyspark.ml.functions import predict\_batch\_udf >>> from pyspark.sql.types import FloatType >>> >>> df = spark.createDataFrame(pd.DataFrame(np.arange(100))) >>> df.show(5) +---+ | 0| +---+ | 0| | 1| | 2| | 3| | 4| +---+ only showing top 5 rows  ```  ``` >>> def make\_times\_two\_fn(): ...     def predict(inputs: np.ndarray) -> np.ndarray: ...         # inputs.shape = [batch\_size] ...         # outputs.shape = [batch\_size] ...         return inputs \* 2 ...     return predict >>> >>> times\_two\_udf = predict\_batch\_udf(make\_times\_two\_fn, ...                                   return\_type=FloatType(), ...                                   batch\_size=10) >>> >>> df = spark.createDataFrame(pd.DataFrame(np.arange(100))) >>> df.withColumn("x2", times\_two\_udf("0")).show(5) +---+---+ | 0| x2| +---+---+ | 0|0.0| | 1|2.0| | 2|4.0| | 3|6.0| | 4|8.0| +---+---+ only showing top 5 rows  ``` * Multiple scalar columns     Input DataFrame has muliple columns of scalar values. If the user-provided    predict    function expects a single input, then the user must combine the multiple columns into a single tensor using    pyspark.sql.functions.array    .    ``` >>> import numpy as np >>> import pandas as pd >>> from pyspark.ml.functions import predict\_batch\_udf >>> from pyspark.sql.functions import array >>> >>> data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4) >>> pdf = pd.DataFrame(data, columns=a','b','c','d) >>> df = spark.create	Based on the context, it seems that `score_batch` does not support multidimensional predictions. The user has a pyfunc model that produces a string of predictions for time series data with context information at each date. The model is trained using a window size of 60 and prediction interval of 14. The return is of the form date, prediction with N records (in the example 14). The user is wondering if working with the feature store supports this, but `score_batch` doesn't seem to be able to return arbitrary/different shaped data. The user could try making the data wide, but then that would defeat the purpose of trying to use the feature store.	0	0	0
no_reaction	https://docs.databricks.com/data/manage-permissions.html	in unity catalog, how do I grant permissions on a view?	    Create views ==============     This article shows how to create views in Unity Catalog.       A view is a read-only object composed from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs.      [Dynamic views](#dynamic-view)  can be used to provide row- and column-level access control, in addition to data masking.       The owner of a view must have the ability to read the tables and views referenced in the view. A reader of a view does not need the ability to read the tables and views referenced in the view, unless they are using a cluster with single user access mode.        Note       To read from a view from a cluster with  **Single User**  [access mode](index.html#cluster-security-mode)  , you must have  `SELECT`  on all referenced tables and views. Dynamic views are not supported with  **Single User**   access mode.        Example syntax for creating a view:        ``` CREATE VIEW main.default.experienced\_employee  (id COMMENT 'Unique identification number', Name)  COMMENT 'View for experienced employees' AS SELECT id, name  FROM all\_employee  WHERE working\_years > 5;  ```       Requirements --------------   * You must have the  `USE     CATALOG`  permission on the parent catalog and the  `USE     SCHEMA`  and  `CREATE     TABLE`  permissions on the parent schema. A metastore admin or the catalog owner can grant you all of these privileges. A schema owner can grant you  `USE     SCHEMA`  and  `CREATE     TABLE`  privileges on the schema. * You must have the ability to read the tables and views referenced in the view (  `SELECT`  on the table or view, as well as  `USE     CATALOG`  on the catalog and  `USE     SCHEMA`  on the schema).     If a view references tables in the workspace-local Hive metastore, the view can be accessed only from the workspace that contains the workspace-local tables. For this reason, Databricks recommends creating views only from tables or views that are in the Unity Catalog metastore.         Create a view ---------------     To create a view, run the following SQL command. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. * `<schema\_name>`  : The name of the schema. * `<view\_name>`  : A name for the view. * `<query>`  : The query, columns, and tables and views used to compose the view.       ``` CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS SELECT <query>;  ```       ``` spark.sql("CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS "   "SELECT <query>")  ```       ``` library(SparkR)  sql(paste("CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS ",  "SELECT <query>",  sep = ""))  ```       ``` spark.sql("CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS " +  "SELECT <query>")  ```       For example, to create a view named  `sales\_redacted`  from columns in the  `sales\_raw`  table:         ``` CREATE VIEW sales\_metastore.sales.sales\_redacted AS SELECT  user\_id,  email,  country,  product,  total FROM sales\_metastore.sales.sales\_raw;  ```       ``` spark.sql("CREATE VIEW sales\_metastore.sales.sales\_redacted AS "   "SELECT "   " user\_id, "   " email, "   " country, "   " product, "   " total "   "FROM sales\_metastore.sales.sales\_raw")  ```       ``` library(SparkR)  sql(paste("CREATE VIEW sales\_metastore.sales.sales\_redacted AS ",  "SELECT ",  " user\_id, ",  " email, ",  " country, ",  " product, ",  " total ",  "FROM sales\_metastore.sales.sales\_raw",  sep = ""))  ```       ``` spark.sql("CREATE VIEW sales\_metastore.sales.sales\_redacted AS " +  "SELECT " +  " user\_id, " +  " email, " +  " country, " +  " product, " +  " total " +  "FROM sales\_metastore.sales.sales\_raw")  ```       You can also create a view by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of view full names by using  [databricks\_views](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/views)   .           Create a dynamic view -----------------------     In Unity Catalog, you can use dynamic views to configure fine-grained access control, including:     * Security at the level of columns or rows. * Data masking.      Note       Fine-grained access control using dynamic views is not available on clusters with  **Single User**  [access mode](compute.html#access-mode)  .        Unity Catalog introduces the following functions, which allow you to dynamically limit which users can access a row, column, or record in a view:     * `current\_user()`  : Returns the current user’s email address. * `is\_account\_group\_member()`  : Returns  `TRUE`  if the current user is a member of a specific account-level group. Recommended for use in dynamic views against Unity Catalog data. * `is\_member()`  : Returns  `TRUE`  if the current user is a member of a specific workspace-level group. This function is provided for compatibility with the existing Hive metastore. Avoid using it with views against Unity Catalog data, because it does not evaluate account-level group membership.     Databricks recommends that you do not grant users the ability to read the tables and views referenced in the view.       The following examples illustrate how to create dynamic views in Unity Catalog.      ###   Column-level permissions     With a dynamic view, you can limit the columns a specific user or group can access. In the following example, only members of the  `auditors`  group can access email addresses from the  `sales\_raw`  table. During query analysis, Apache Spark replaces the  `CASE`  statement with either the literal string  `REDACTED`  or the actual contents of the email address column. Other columns are returned as normal. This strategy has no negative impact on the query performance.         ``` -- Alias the field 'email' to itself (as 'email') to prevent the -- permission logic from showing up directly in the column name results. CREATE VIEW sales\_redacted AS SELECT  user\_id,  CASE WHEN  is\_account\_group\_member('auditors') THEN email  ELSE 'REDACTED'  END AS email,  country,  product,  total FROM sales\_raw  ```       ``` # Alias the field 'email' to itself (as 'email') to prevent the # permission logic from showing up directly in the column name results. spark.sql("CREATE VIEW sales\_redacted AS "   "SELECT "   " user\_id, "   " CASE WHEN "   " is\_account\_group\_member('auditors') THEN email "   " ELSE 'REDACTED' "   " END AS email, "   " country, "   " product, "   " total "   "FROM sales\_raw")  ```       ``` library(SparkR)  # Alias the field 'email' to itself (as 'email') to prevent the # permission logic from showing up directly in the column name results. sql(paste("CREATE VIEW sales\_redacted AS ",  "SELECT ",  " user\_id, ",  " CASE WHEN ",  " is\_account\_group\_member('auditors') THEN email ",  " ELSE 'REDACTED' ",  " END AS email, ",  " country, ",  " product, ",  " total ",  "FROM sales\_raw",  sep = ""))  ```       ``` // Alias the field 'email' to itself (as 'email') to prevent the // permission logic from showing up directly in the column name results. spark.sql("CREATE VIEW sales\_redacted AS " +  "SELECT " +  " user\_id, " +  " CASE WHEN " +  " is\_account\_group\_member('auditors') THEN email " +  " ELSE 'REDACTED' " +  " END AS email, " +  " country, " +  " product, " +  " total " +  "FROM sales\_raw")  ```       ###   Row-level permissions     With a dynamic view, you can specify permissions down to the row or field level. In the following example, only members of the  `managers`  group can view transaction amounts when they exceed $1,000,000. Matching results are filtered out for other users.         ```  CREATE VIEW sales\_redacted AS  SELECT  user\_id,  country,  product,  total  FROM sales\_raw  WHERE  CASE  WHEN is\_account\_group\_member('managers') THEN TRUE  ELSE total <= 1000000  END;  ```       ```  spark.sql("CREATE VIEW sales\_redacted AS "    "SELECT "    " user\_id, "    " country, "    " product, "    " total "    "FROM sales\_raw "    "WHERE "    "CASE "    " WHEN is\_account\_group\_member('managers') THEN TRUE "    " ELSE total <= 1000000 "    "END")  ```       ```  library(SparkR)   sql(paste("CREATE VIEW sales\_redacted AS ",  "SELECT ",  " user\_id, ",  " country, ",  " product, ",  " total ",  "FROM sales\_raw ",  "WHERE ",  "CASE ",  " WHEN is\_account\_group\_member('managers') THEN TRUE ",  " ELSE total <= 1000000 ",  "END",  sep = ""))  ```       ```  spark.sql("CREATE VIEW sales\_redacted AS " +  "SELECT " +  " user\_id, " +  " country, " +  " product, " +  " total " +  "FROM sales\_raw " +  "WHERE " +  "CASE " +  " WHEN is\_account\_group\_member('managers') THEN TRUE " +  " ELSE total <= 1000000 " +  "END")  ```       ###   Data masking     Because views in Unity Catalog use Spark SQL, you can implement advanced data masking by using more complex SQL expressions and regular expressions. In the following example, all users can analyze email domains, but only members of the  `auditors`  group can view a user’s entire email address.         ``` -- The regexp\_extract function takes an email address such as -- user.x.lastname@example.com and	To grant permissions on a view in Unity Catalog, you must have the `USE CATALOG` permission on the parent catalog and the `USE SCHEMA` and `CREATE TABLE` permissions on the parent schema. The owner of a view must have the ability to read the tables and views referenced in the view. A reader of a view does not need the ability to read the tables and views referenced in the view, unless they are using a cluster with single-user access mode. To create a view, you can use the `CREATE VIEW` command followed by the name of the view and the query that defines the view.	1	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/index.html	What is the first step to setting up unity catalog?	What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	The first step to setting up Unity Catalog is to configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account.	0	1	1
no_reaction	https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html	how do i deploy a databricks workspace with private link in aws	Enable AWS PrivateLink ========================     This article explains how to use AWS PrivateLink to enable private connectivity between users and their Databricks workspaces and between clusters on the data plane and core services on the control plane within the Databricks workspace infrastructure.        Important       This article mentions the term  *data plane*   , which is the compute layer of the Databricks platform. In the context of this article, data plane refers to the Classic data plane in your AWS account. By contrast, the serverless data plane that supports  [serverless SQL warehouses (Public Preview)](../../../sql/admin/serverless.html)  runs in the Databricks AWS account. To learn more, see  [Serverless compute](../../../serverless-compute/index.html)  .         Overview ----------     AWS PrivateLink provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network. Databricks workspaces on the E2 version of the platform support PrivateLink connections for two connection types:     * **Front-end (user to workspace)**   : A front-end PrivateLink connection allows users to connect to the Databricks web application, REST API, and Databricks Connect API over a VPC interface endpoint. * **Back-end (data plane to control plane)**   : Databricks Runtime clusters in a customer-managed VPC (the  [data plane](../../../getting-started/overview.html)  ) connect to a Databricks workspace’s core services (the  [control plane](../../../getting-started/overview.html)  ) in the Databricks cloud account. Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay. This PrivateLink connection type involves two different VPC interface endpoints because of the two different destination services.     You can implement both front-end and back-end PrivateLink connections or just one of them. This article discusses how to configure either one or both PrivateLink connection types. If you implement PrivateLink for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means Databricks rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.       To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects.       To create configuration objects and create (or update) a workspace, this article describes how to  [use the account console](#account-console)  or  [use the Account API](#account-api)  .       The following table describes important terminology.          |   Terminology   |   Description   | | --- | --- | |   AWS PrivateLink   |   An AWS technology that provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network.   | |   Front-end PrivateLink   |   The PrivateLink connection for users to connect to the Databricks web application, REST API, and Databricks Connect API.   | |   Back-end PrivateLink   |   The PrivateLink connection for the  [data plane](../../../getting-started/overview.html)  in your AWS account to connect to the Databricks  [control plane](../../../getting-started/overview.html)  ).   | |   AWS VPC endpoint service   |   An AWS VPC endpoint service is a PrivateLink-powered service. Each Databricks control plane (typically one per region) publishes two AWS VPC endpoint services for PrivateLink. The workspace VPC endpoint service applies to both a Databricks front-end PrivateLink connection or the Databricks back-end PrivateLink connection for REST APIs. Databricks publishes another VPC endpoint service for its  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay.   | |   AWS VPC endpoint   |   An  [AWS VPC interface endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html)   enables private connections between your VPC and VPC endpoint services powered by AWS PrivateLink. You must create AWS VPC interface endpoints and then register them with Databricks. Registering a VPC endpoint creates a Databricks-specific object called a VPC endpoint registration that references the AWS VPC endpoint.   | |   Databricks network configuration   |   A Databricks object that describes the important information about a  [Customer-managed VPC](customer-managed-vpc.html)  . If you implement any PrivateLink connection (front-end or back-end), your workspace must use a customer-managed VPC. For PrivateLink back-end support only, your network configuration needs an extra property that identifies the VPC endpoints for the back-end connection.   | |   Databricks private access settings object   |   A Databricks object that describes a workspace’s PrivateLink connectivity. You must attach a private access settings object to the workspace during workspace creation, whether using front-end, back-end, or both. It expresses your intent to use AWS PrivateLink with your workspace. It controls your settings for the front-end use case of AWS PrivateLink for public network access. It controls which VPC endpoints are permitted to access your workspace.   | |   Databricks workspace configuration object   |   A Databricks object that describes a workspace. To enable PrivateLink, this object must reference Databricks private access settings object. For back-end PrivateLink, the workspace must also have a Databricks network configuration object with two extra fields that specify which VPC endpoint registrations to use, one for control plane’s secure cluster connectivity relay and the other connects to the workspace to access REST APIs.   |      ###   Updates of existing PrivateLink configuration objects     This article focuses on the main two use cases of creating a new workspace or enabling PrivateLink on a workspace. You also can make other configuration changes to related objects using the UI or API:     * You can upgrade a workspace’s PrivateLink support to add support for front-end, back-end, or both types of connectivity. Add a private access settings object (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ) or update a private access settings object (  [UI](../../account-settings-e2/private-access-settings.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/replace-private-access-settings)   ). To do so, create a new network configuration with new settings, for example for a new VPC or different PrivateLink support settings, and then update the workspace to use the new network configuration. Note that you cannot remove (downgrade) any existing front-end or back-end PrivateLink support on a workspace. * Add or update a workspace’s registered VPC endpoints by creating a new network configuration object with registered VPC endpoints and then update the workspace’s network configuration (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ). * For more information about what kinds of workspace fields can be changed on failed or running workspaces, see information about this task by using the  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   .     Note that not all related objects can be updated. Where update is not possible, create new objects and set their parent objects to reference the new objects. The following rules apply both to the account console UI and the Account API:           |   Object   |   Can be created   |   Can be updated   | | --- | --- | --- | |   Workspace configurations   |   Yes   |   Yes   | |   Private access settings   |   Yes   |   Yes   | |   Network configurations   |   Yes   |  **No**   | |   VPC endpoint registrations   |   Yes   |  **No**   |     To update CIDR ranges on an existing VPC, see  [Updating CIDRs](customer-managed-vpc.html#update-cidr)  .       ###   Network flow     The following diagram shows the network flow in a typical implementation.      ![PrivateLink network architecture](../../../_images/privatelink-network.png)             Requirements --------------    **Databricks account**     * Your Databricks account is on the E2 version of the platform. * Your Databricks account is on the  [Enterprise pricing tier](https://databricks.com/product/aws-pricing)   . * You have your Databricks account ID. Get your account ID from the  [account console](../../account-settings/index.html#account-id)  .    **Databricks workspace**     * Your workspace must be in an  [AWS region that supports the E2 version of the platform](../../../resources/supported-regions.html)  . However, the  `us-west-1`  region does not support PrivateLink even for workspaces on the E2 version of the platform. * Your Databricks workspace must use  [Customer-managed VPC](customer-managed-vpc.html)  to add any PrivateLink connection (even a front-end-only connection). Note that you cannot update an existing workspace with a Databricks-managed VPC and change it to use a customer-managed VPC. * If you implement the back-end PrivateLink connection, your Databricks workspace must use  [Secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  , which is the default for new workspaces on the E2 version of the platform. To add back-end PrivateLink to an older existing workspace that does not use secure cluster connectivity, contact your Databricks representative.    **AWS account permissions**     * If you are the user who sets up PrivateLink, you must have all necessary AWS permissions to provision a Databricks workspace and to provision new VPC endpoints for your workspace.    **Network architecture**     * To implement the front-end PrivateLink connection to access the workspace from your on-premises network, add private connectivity from the on-premises network to an AWS VPC using either Direct Connect or VPN. * For guidance for other network objects, see  [Step 1: Configure AWS network objects](#create-vpc)  .         Step 1: Configure AWS network objects ---------------------------------------         You can use the AWS Management Console to create these objects or automate the process with tools such as the  [Terraform provider for networks](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/mws_networks)   .       To configure a VPC, subnets, and security groups:     1. Set up a VPC for your workspace if you haven’t already done so. You may re-use a VPC from another workspace, but you must create separate subnets for each workspace. Every workspace requires at least two private subnets.      1. To create a VPC, see   [Customer-managed VPC](customer-managed-vpc.html)   . If you are updating a workspace for PrivateLink rather than creating a new workspace, note that the workspace must already be using a customer-managed VPC.  2. On your VPC, ensure that you enable both of the settings   **DNS Hostnames**    and   **DNS resolution**    .  3. Ensure that the network ACLs for the subnets have   **bidirectional**    (outbound and inbound) rules that allow TCP access to 0.0.0.0/0 for these ports:          * 443: for Databricks infrastructure, cloud data sources, and library repositories   * 3306: for the metastore   * 6666: for PrivateLink   * 2443: only for use with compliance security profile     Important            If your	To deploy a Databricks workspace with PrivateLink in AWS, you need to create Databricks configuration objects and add new fields to existing configuration objects. You can implement both front-end and back-end PrivateLink connections or just one of them. To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects. To create configuration objects and create (or update) a workspace, you can use the account console or use the Account API. Additionally, you need to configure a VPC, subnets, and security groups in AWS. For more detailed information, please refer to the context provided.	0	1	1
no_reaction	https://docs.databricks.com/administration-guide/account-settings-e2/private-access-settings.html	how to set up private link	Manage private access settings ================================         This article describes a process that is available only for  [accounts on the E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.       This article discusses how to create private access settings objects, which are a required object as part of enabling  [AWS PrivateLink](https://aws.amazon.com/privatelink)   . This article does not contain all the information necessary to configure PrivateLink for your workspace. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see  [Enable AWS PrivateLink](../cloud-configurations/aws/privatelink.html)  .       The following related sections discuss updating existing network and configuration objects:     * [Update a running or failed workspace](../workspace/update-workspace.html)  . * [Updates of existing PrivateLink configuration objects](../cloud-configurations/aws/privatelink.html#update-related)  .      What is a private access settings object? -------------------------------------------     A private access settings object is a Databricks object that describes a workspace’s PrivateLink connectivity. Create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region.       This object serves several purposes:     * It expresses your intent to use AWS PrivateLink with your workspace. * It controls your settings for the front-end use case of AWS PrivateLink for public network access. * It controls which VPC endpoints are permitted to access your workspace.     Create a private access settings object using the account console or the  [Account API 2.0](../../dev-tools/api/latest/account.html)  . You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you  *must*   attach a private access settings object to the workspace during workspace creation.           Create a private access settings object -----------------------------------------      Note       These instructions show you how to create the private access object from the  **Cloud resources**   page in the account console before you create a new workspace. You can also create the private access settings in a similar way as part of the flow of creating a new workspace and choosing  **Add a new private access object**   from the picker instead of choosing an existing object. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .      1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. Click  **Add private access settings**   .      ![private access settings object](../../_images/privatelink-vpc-pas.png) 5. Enter a name for your new private access settings object. 6. For the region, be sure to match the region of your workspace as this is not validated immediately and workspace deployment will fail if it does not match. It is validated only during the actual creation of the workspace. 7. Set the  **Public access enabled**   field, which configures public access to the front-end connection (the web application and REST APIs) for your workspace.      * If set to   **False**    (the default), the front-end connection can be accessed only using PrivateLink connectivity and not from the public internet. When public access is disabled, the   [IP access lists for workspaces](../../security/network/ip-access-list-workspace.html)   feature is unsupported.  * If set to   **True**    , the front-end connection can be accessed either from PrivateLink connectivity or from the public internet. Any IP access lists only limit connections from the public internet but not traffic through the PrivateLink connection. 8. Set the  **Private Access Level**   field to the value that best represents which VPC endpoints to allow for your workspace.      * Set to   **Account**    to limit connections to those VPC endpoints that are registered in your Databricks account.  * Set to   **Endpoint**    to limit connections to an explicit set of VPC endpoints, which you can enter in a field that appears. It lets you select VPC endpoint registrations that you’ve already created. Be sure to include your   *front-end*    VPC endpoint registration if you created one. 9. Click  **Add**   .         Update a private access settings object -----------------------------------------     To update fields on a private access object:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Update**   . 5. Change any fields. For guidance on specific fields, see  [Create a private access settings object](#create)  .        Note       The private access access level  `ANY`  is deprecated. If the object previously had this value and you use the account console to update the private access settings for any fields, you must change the private access level to a non-deprecated value. To make changes to other fields without changing the  `ANY`  private access level at this time, use the  [Account API](../../dev-tools/api/latest/account.html)  . See  [AWS PrivateLink private access level ANY is deprecated](../../release-notes/product/2022/august.html#privatelink-private-access-level-any-deprecated)  . 6. Click  **Update private access setting**   .         Delete a private access settings object -----------------------------------------     Private access settings object cannot be edited after creation. If the configuration has incorrect data or if you no longer need it for any workspaces, delete it:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Delete**   . 5. In the confirmation dialog, click  **Confirm Delete**   .	To set up PrivateLink in Databricks, you need to create a private access settings object, which is a Databricks object that describes a workspace's PrivateLink connectivity. You can create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region. You can create a private access settings object using the account console or the Account API 2.0. You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you must attach a private access settings object to the workspace during workspace creation.	1	1	1
no_reaction	https://kb.databricks.com/scala/jobs-fails-with-a-timeoutexception-error	I have a timeout issue when you're ingesting data, what should I do?	Title: Jobs fails with a TimeoutException error  Description: This error is usually caused by a Broadcast join that takes excessively long to complete.  Answer: Problem You are running Apache Spark SQL queries that perform join operations DataFrames, but the queries keep failing with a TimeoutException error message. Example stack trace Caused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]               at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)               at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)               at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)               at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) Cause This problem usually stems from Spark trying to perform Broadcast join and when the fetching of blocks from different executors consumes an excessive amount of time. Spark performs Broadcast join using the BitTorrent protocol. The driver splits the data to be broadcasted into small chunks and stores the chunks in the block manager of the driver. The driver also sends the chunks of data to the executors. Each executors keeps copies of the chunks of data in its own block manager.  When a specific executor is not able to fetch the chunks of data from its local block manager (say the executor died and re-launched) that executor tries to fetch the broadcast data from the driver as well as other executors. This avoids driver being the bottleneck in serving the remote requests.  Even with this distributed approach, there are some scenarios where the broadcast can take an excessive amount of time, resulting in a TimeoutException error. Busy driver or busy executor: If the Spark driver and executors are extremely busy, it can introduce delay in the broadcast process. If the broadcast process exceeds the threshold limits, it can result in a broadcast timeout. Large broadcast data size: Trying to broadcast a large amount of data can also result in a broadcast timeout. Spark has a default limit of 8GB for broadcast data.  Solution You need to identify the query that is causing resource bottleneck on the cluster. Open the Spark UI (AWS | Azure | GCP) and review any failed stages to locate the SQL query causing the failure. Review the Spark SQL plan to see if it uses BroadcastNestedLoopJoin. If the Spark SQL plan uses BroadcastNestedLoopJoin, you need to follow the instructions in the Disable broadcast when query plan has BroadcastNestedLoopJoin article. If the Spark SQL plan does not use BroadcastNestedLoopJoin, you can disable the Broadcast join by setting Spark config values right before the problematic query. You can then revert these changes after the problematic query. Making the change query specific allows other queries, which can benefit from the Broadcast join, to still leverage the benefits. SET spark.sql.autoBroadcastJoinThreshold=-1  This disables Broadcast join. SET spark.databricks.adaptive.autoBroadcastJoinThreshold=-1 This particular configuration disables adaptive Broadcast join. Another option is to increase spark.sql.broadcastTimeout to a value above 300 seconds, which is the default value. Increasing spark.sql.broadcastTimeout allows more time for the broadcasting process to finish before it generates a failure. The downside to this approach, is that it may result in longer query times. For example, setting the value to 600 doubles the amount of time for the Broadcast join to complete. SET spark.sql.broadcastTimeout=600 This value can be set at the cluster level or the notebook level.	If you are experiencing a timeout issue when ingesting data, it may be caused by a Broadcast join that takes excessively long to complete. You can try to identify the query causing the resource bottleneck on the cluster and review the Spark SQL plan to see if it uses BroadcastNestedLoopJoin. If it does, you can disable the Broadcast join by setting Spark config values right before the problematic query. Alternatively, you can increase the spark.sql.broadcastTimeout to a value above 300 seconds, which is the default value.	1	1	1
no_reaction	https://docs.databricks.com/data-sharing/grant-access.html	How do I grant access to my Delta share	      Grant and manage access to Delta Sharing data shares ======================================================     This article explains how to grant a data recipient access to a Delta Sharing share. It also explains how to view, update, and revoke access.        Requirements --------------     To share data with recipients:     * You must be a metastore admin, have the  `CREATE\_SHARE`  privilege for the Unity Catalog metastore where the data you want to share is registered, or be the owner of both the share and recipient objects. * You must use a Databricks workspace that has a Unity Catalog metastore attached. * You must use a SQL warehouse or cluster that uses a Unity-Catalog-capable cluster access mode. * Shares and recipients must already be defined.       Grant recipient access to share ---------------------------------     To grant share access to recipients, you can use Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or share object owner. The share object owner must also be the owner of any recipient objects being added.         To add recipients to a share (starting at the share):     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. Click  **Add recipient**   . 5. On the  **Add recipient**   dialog, start typing the recipient name or click the drop-down menu to select the recipients you want to add to the share. 6. Click  **Add**   .     To grant share access to a recipient (starting at the recipient):     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. Click  **Grant share**   . 5. On the  **Grant share**   dialog, start typing the share name or click the drop-down menu to select the shares you want to grant. 6. Click  **Grant**   .       Run the following command in a notebook or the Databricks SQL query editor.        ``` GRANT SELECT ON SHARE <share\_name> TO RECIPIENT <recipient\_name>;  ```     `SELECT`  is the only privilege that you can grant a recipient on a share.         Create a JSON file that includes the following, replacing  `recipient-name`  .  `SELECT`  is the only privilege that you can grant on a share.        ``` {  "privilege\_assignments": [{  "principal": "recipient-name",  "privileges": [  "SELECT"  ]  }] }  ```      Then run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  . Replace  `<share\_name>`  with the name of the share you want to grant to the recipient, and replace  `update-share-perms.json`  with the filename of the JSON file.        ``` databricks unity-catalog shares update-permissions --name <share_name> \  --json-file update-share-perms.json  ```            Revoke recipient access to a share ------------------------------------     To revoke a recipient’s access to a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `REVOKE     ON     SHARE`  SQL command in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or share object owner.         To revoke a recipient’s access to a share, starting at the share:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. On the  **Recipients**   tab, find the recipient. 5. Click the  ![Kebab menu](../_images/kebab-menu.png)  kebab menu (also known as the three-dot menu) and select  **Revoke**   . 6. On the confirmation dialog, click  **Revoke**   .     To revoke a recipient’s access to a share, starting at the recipient:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. On the  **Shares**   tab, find the share. 5. Click the  ![Kebab menu](../_images/kebab-menu.png)  kebab menu (also known as the three-dot menu) on the share row and select  **Revoke**   . 6. On the confirmation dialog, click  **Revoke**   .       Run the following command in a notebook or the Databricks SQL query editor.        ``` REVOKE SELECT ON SHARE <share\_name> FROM RECIPIENT <recipient\_name>;  ```        Create a JSON file that includes the following, replacing  `recipient-name`  :        ``` {  "changes": [{  "principal": "recipient-name",  "remove": ["SELECT"]  }] }  ```      Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  . Replace  `<share\_name>`  with the name of the share you want to grant to the recipient, and replace  `update-share-perms.json`  with the filename of the JSON file.        ``` databricks unity-catalog shares update-permissions --name <share_name> --json-file update-share-perms.json  ```          View grants on a share or grants possessed by a recipient -----------------------------------------------------------     To view the current grants on a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `SHOW     GRANTS     ON     SHARE`  SQL command in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : If you are viewing recipients granted access to a share, you must be a metastore admin or the share object owner. If you are viewing shares granted to a recipient, you must be a metastore admin or the recipient object owner.         To view recipients with access to a share:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. Go to the  **Recipients**   tab to view all recipients who have access to the share.       Run the following command in a notebook or the Databricks SQL query editor.        ``` SHOW GRANT ON SHARE <share\_name>;  ```        Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog shares list-permissions --name <share_name>  ```        To view the current share grants possessed by a recipient, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `SHOW     GRANTS     TO     RECIPIENT`  SQL command in a Databricks notebook or the Databricks SQL query editor.         To view shares granted to a recipient:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. Go to the  **Shares**   tab to view all shares that the recipient has access to.       Run the following command in a notebook or the Databricks SQL query editor.        ``` SHOW GRANTS TO RECIPIENT <recipient\_name>;  ```        Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog recipients list-permissions --name <recipient_name>  ```                                                         	To grant access to a Delta share, you can use Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor. To add recipients to a share, you need to be a metastore admin, have the CREATE_SHARE privilege for the Unity Catalog metastore where the data you want to share is registered, or be the owner of both the share and recipient objects. You also need to use a Databricks workspace that has a Unity Catalog metastore attached and use a SQL warehouse or cluster that uses a Unity-Catalog-capable cluster access mode. Once you have met these requirements, you can follow the steps outlined in the context to grant access to your Delta share.	1	1	1
no_reaction	https://docs.databricks.com/machine-learning/feature-store/workflow-overview-and-notebook.html	can you write me an example of how to use the feature store?	Databricks Feature Store ==========================     This page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.       The Databricks Feature Store library is available only on Databricks Runtime for Machine Learning and is accessible through Databricks notebooks and workflows.        Note       At this time, Feature Store does not support writing to a Unity Catalog metastore. In Unity Catalog-enabled workspaces, you can write feature tables only to the default Hive metastore.         What is a feature store? --------------------------     A feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.       Machine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.       Developing features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.         Why use Databricks Feature Store? -----------------------------------     Databricks Feature Store is fully integrated with other components of Databricks.     * Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features. * Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature. * Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier. * Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.       Start using Feature Store ---------------------------     See the following articles to get started with Feature Store:     * Try one of the  [example notebooks](example-notebooks.html)  that illustrate feature store capabilities. * See the reference material for the  [Feature Store Python API](python-api.html)  . * Learn about  [training models with Feature Store](train-models-with-feature-store.html)  . * Learn about  [working with feature tables](feature-tables.html)  . * Use  [time series feature tables and point-in-time lookups](time-series.html)  to retrieve the latest feature values as of a particular time for training or scoring a model. * Learn about  [publishing features to online feature stores](online-feature-stores.html)  for real-time serving and automatic feature lookup. * [Share feature tables across workspaces](multiple-workspaces.html)  . * [Control access to feature tables](access-control.html)  .       More information ------------------     For more information on best practices for using Feature Store, download  [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores)   .	To use the Databricks Feature Store, you can start by trying one of the example notebooks that illustrate feature store capabilities, or you can see the reference material for the Feature Store Python API. You can also learn about training models with Feature Store, working with feature tables, using time series feature tables and point-in-time lookups, publishing features to online feature stores for real-time serving and automatic feature lookup, sharing feature tables across workspaces, and controlling access to feature tables. Databricks Feature Store is fully integrated with other components of Databricks, providing discoverability, lineage, integration with model scoring and serving, and point-in-time lookups.	0	1	1
no_reaction	https://docs.databricks.com/release-notes/runtime/11.x-migration.html	Is there a best practice for upgrading databricks runtime versions?	Databricks Runtime ====================     Databricks Runtime includes Apache Spark but also adds a number of components and updates that substantially improve the usability, performance, and security of big data analytics:     * [Delta Lake](../delta/index.html)  , a next-generation storage layer built on top of Apache Spark that provides ACID transactions, optimized layouts and indexes, and execution engine improvements for building data pipelines. * Installed Java, Scala, Python, and R libraries * Ubuntu and its accompanying system libraries * GPU libraries for GPU-enabled clusters * Databricks services that integrate with other components of the platform, such as notebooks, jobs, and cluster manager     For information about the contents of each runtime version, see the  [release notes](../release-notes/runtime/releases.html)  .        Runtime versioning --------------------     Databricks Runtime versions are released on a regular basis:     * **Major**   versions are represented by an increment to the version number that precedes the decimal point (the jump from 3.5 to 4.0, for example). They are released when there are major changes, some of which may not be backwards-compatible. * **Feature**   versions are represented by an increment to the version number that follows the decimal point (the jump from 3.4 to 3.5, for example). Each major release includes multiple feature releases. Feature releases are always backwards compatible with previous releases within their major release. * **Long Term Support**   versions are represented by an  **LTS**   qualifier (for example,  **3.5 LTS**   ). For each major release, we declare a “canonical” feature version, for which we provide two full years of support. See  [Databricks runtime support lifecycles](../release-notes/runtime/databricks-runtime-ver.html#runtime-support)  for more information.	Yes, there is a best practice for upgrading Databricks runtime versions. It is recommended to test the new runtime version in a non-production environment before upgrading the production environment. Additionally, it is important to review the release notes for the new runtime version to understand any changes or new features. It is also recommended to upgrade to the latest LTS (Long Term Support) version for the best support and stability.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/index.html	Does unity catalog work with Informatica?	What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	Based on the context, there is no clear information on whether Unity Catalog works with Informatica.	0	0	0
no_reaction	https://docs.databricks.com/migration/index.html	How do I migrate from Snowflake to Databricks?	Migrate data applications to Databricks =========================================     This article provides an introduction to migrating existing data applications to Databricks. Databricks provides a unified approach that lets you work with data from many source systems on a single platform.       For an overview of platform capabilities, see  [What is Databricks?](../introduction/index.html)  .          Migrate ETL jobs to Databricks --------------------------------     You can migrate Apache Spark jobs used to extract, transform, and load data from on-premises or cloud-native implementations to Databricks with just a few steps. See  [Adapt your exisiting Apache Spark code for Databricks](spark.html)  .       Databricks extends the functionality of Spark SQL with pre-configured open source integrations, partner integrations, and enterprise product offerings. If your ETL workloads are written in SQL or Hive, you can migrate to Databricks with minimal refactoring. Learn more about Databricks SQL offerings:     * [What is data warehousing on Databricks?](../sql/index.html) * [What is Delta Live Tables?](../delta-live-tables/index.html) * [What is Databricks Partner Connect?](../partner-connect/index.html)     For specific instructions on migrating from various source systems to Databricks, see  [Migrate ETL pipelines to Databricks](etl.html)  .         Replace your enterprise data warehouse with a lakehouse ---------------------------------------------------------     Databricks provides optimal value and performance when workloads align around data stored in the lakehouse. Many enterprise data stacks include both a data lake and an enterprise data warehouse, and organizations create complex ETL workflows to try to keep these systems and data in sync. The lakehouse allows you to use the same data, stored in the data lake, across queries and systems that usually rely on a separate data warehouse. For more on the lakehouse, see  [What is the Databricks Lakehouse?](../lakehouse/index.html)  . For more on data warehousing on Databricks, see  [What is data warehousing on Databricks?](../sql/index.html)  .       Migrating from an enterprise data warehouse to the lakehouse generally involves reducing the complexity of your data architecture and workflows, but there are some caveats and best practices to keep in mind while completing this work. See  [Migrate your data warehouse to the Databricks Lakehouse](warehouse-to-lakehouse.html)  .         Unify your ML, data science, and analytics workloads ------------------------------------------------------     Because the lakehouse provides optimized access to cloud-based data files through table queries or file paths, you can do ML, data science, and analytics on a single copy of your data. Databricks makes it easy to move workloads from both open source and proprietary tools, and maintains updated versions of many of open source libraries used by analysts and data scientists.       Pandas workloads in Jupyter notebooks can be synced and run using  [Databricks Repos](../repos/index.html)  . Databricks provides  [native support for pandas](../pandas/index.html)  in all Databricks Runtime versions, and configures many popular ML and deep learning libraries in the  [Databricks ML Runtime](../runtime/mlruntime.html)  . If you sync your local workloads using Git and  [Files in Repos](../files/workspace.html)  , you can use the same relative paths for data and custom libaries present in your local environment.        Note       By default, Databricks maintains  `.ipynb`  extensions for Jupyter notebooks synced with Databricks Repos, but automatically converts Jupyter notebooks to Databricks notebooks when  [imported with the UI](../notebooks/notebook-export-import.html#import-notebook)  . Databricks notebooks save with a  `.py`  extension, and so can live side-by-side with Jupyter notebooks in a Git repository.	To migrate from Snowflake to Databricks, you can adapt your existing Apache Spark code for Databricks. Databricks extends the functionality of Spark SQL with pre-configured open source integrations, partner integrations, and enterprise product offerings. If your ETL workloads are written in SQL or Hive, you can migrate to Databricks with minimal refactoring. For specific instructions on migrating from various source systems to Databricks, see "Migrate ETL pipelines to Databricks" in the provided context.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/information-schema/metastores.html	where are stored metastore metadata on the cloud storage ?	METASTORES ============    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime 10.2 and above       INFORMATION\_SCHEMA.METASTORES describes the current metastore.       Information is displayed only for the current metastore for all users.       This is an extension to the SQL Standard Information Schema.        Definition ------------     The  `METASTORES`  relation contains the following columns:            |   Name   |   Data type   |   Nullable   |   Description   | | --- | --- | --- | --- | |  `METASTORE\_ID`  |  `STRING`  |   No   |   ID of the metastore.   | |  `METASTORE\_NAME`  |  `STRING`  |   No   |   Name of the metastore.   | |  `METASTORE\_OWNER`  |  `STRING`  |   No   |   Owner of the metastore.   | |  `STORAGE\_ROOT`  |  `STRING`  |   No   |   Storage root URL of the metastore.   | |  `STORAGE\_ROOT\_CREDENTIAL\_ID`  |  `STRING`  |   Yes   |   ID of the  [storage credential](storage_credentials.html)  to access storage root.   | |  `STORAGE\_ROOT\_CREDENTIAL\_NAME`  |  `STRING`  |   Yes   |   Name of the storage credential to access storage root.   | |  `DELTA\_SHARING\_SCOPE`  |  `STRING`  |   No   |   Delta Sharing scope for the metastore.   | |  `DELTA\_SHARING\_RECIPIENT\_TOKEN\_LIFETIME`  |  `LONG`  |   Yes   |   The lifetime of Delta Sharing recipient tokens in seconds.   | |  `DELTA\_SHARING\_ORGANIZATION\_NAME`  |  `STRING`  |   No   |   The organization name of a Delta Sharing entity. The name will be used in Databricks-to-Databricks Delta Sharing as the official name.   | |  `PRIVILEGE\_MODEL\_VERSION`  |  `STRING`  |   No   |   Privilege model version.   | |  `CLOUD`  |  `STRING`  |   No   |   Cloud vendor of the metastore.   | |  `REGION`  |  `STRING`  |   No   |   Dataplane region that the metastore serves.   | |  `GLOBAL\_METASTORE\_ID`  |  `STRING`  |   No   |   Globally unique metastore ID across clouds and regions. Used as Delta Sharing sharing identifier.   | |  `CREATED`  |  `TIMESTAMP`  |   No   |   Timestamp when the metastore was created.   | |  `CREATED\_BY`  |  `STRING`  |   No   |  [Principal](../sql-ref-principal.html)  who created the metastore.   | |  `LAST\_ALTERED`  |  `TIMESTAMP`  |   No   |   Timestamp when the metastore was last altered in any way.   | |  `LAST\_ALTERED\_BY`  |  `STRING`  |   No   |   Principal who last altered the metastore.   |       Constraints -------------     The following constraints apply to the  `METASTORES`  relation:            |   Class   |   Name   |   Column List   |   Description   | | --- | --- | --- | --- | |   Primary key   |  `METASTORES\_PK`  |  `METASTORE\_ID`  ,  `METASTORE\_NAME`  |   Unique identifier for the metastore.   | |   Foreign key   |  `METASTORES\_CREDS\_FK`  |  `STORAGE\_ROOT\_CREDENTIAL\_ID`  ,  `STORAGE\_ROOT\_CREDENTIAL\_NAME`  |   References  [STORAGE\_CREDENTIALS](storage_credentials.html)  |       Examples ----------      ``` > SELECT metastore\_owner  FROM information\_schema.metastores  ```        Related ---------   * [Information schema](../sql-ref-information-schema.html) * [INFORMATION\_SCHEMA.METASTORE\_PRIVILEGES](metastore_privileges.html)	The storage root URL of the metastore is where the metastore metadata is stored on cloud storage.	0	1	1
no_reaction	https://kb.databricks.com/clusters/calculate-number-of-cores	how to get the number of total cores in a databricks cluster?	Title: How to calculate the number of cores in a cluster  Description: Learn how to calculate the number of cores in a Databricks cluster.  Answer: You can view the number of cores in a Databricks cluster in the Workspace UI using the Metrics tab on the cluster details page. Note Azure Databricks cluster nodes must have a metrics service installed. If the driver and executors are of the same node type, you can also determine the number of cores available in a cluster programmatically, using Scala utility code: Use sc.statusTracker.getExecutorInfos.length to get the total number of nodes. The result includes the driver node, so subtract 1. Use java.lang.Runtime.getRuntime.availableProcessors to get the number of cores per node. Multiply both results (subtracting 1 from the total number of nodes) to get the total number of cores available. Scala example code: java.lang.Runtime.getRuntime.availableProcessors * (sc.statusTracker.getExecutorInfos.length -1)	To get the number of total cores in a Databricks cluster, you can view the Metrics tab on the cluster details page in the Workspace UI. If the driver and executors are of the same node type, you can also determine the number of cores available in a cluster programmatically using Scala utility code. You can use `sc.statusTracker.getExecutorInfos.length` to get the total number of nodes and `java.lang.Runtime.getRuntime.availableProcessors` to get the number of cores per node. Then, you can multiply both results (subtracting 1 from the total number of nodes) to get the total number of cores available. An example of Scala code to calculate this is `java.lang.Runtime.getRuntime.availableProcessors * (sc.statusTracker.getExecutorInfos.length -1)`.	1	1	1
no_reaction	https://docs.databricks.com/archive/spark-sql-2.x-language-manual/clone.html	What's the different between shadow clone and deep clone	    Clone a table on Databricks =============================     You can create a copy of an existing Delta Lake table on Databricks at a specific version using the  `clone`  command. Clones can be either deep or shallow.       Clone is a Databricks-exclusive feature enabled in the Databricks Runtime by default.       Databricks also supports cloning Parquet and Iceberg tables. See  [Incrementally clone Parquet and Iceberg tables to Delta Lake](clone-parquet.html)  .          Clone types -------------   * A  *deep clone*   is a clone that copies the source table data to the clone target in addition to the metadata of the existing table. Additionally, stream metadata is also cloned such that a stream that writes to the Delta table can be stopped on a source table and continued on the target of a clone from where it left off. * A  *shallow clone*   is a clone that does not copy the data files to the clone target. The table metadata is equivalent to the source. These clones are cheaper to create.     Any changes made to either deep or shallow clones affect only the clones themselves and not the source table.       The metadata that is cloned includes: schema, partitioning information, invariants, nullability. For deep clones only, stream and  [COPY INTO](../sql/language-manual/delta-copy-into.html)  metadata are also cloned. Metadata not cloned are the table description and  [user-defined commit metadata](custom-metadata.html)  .        Important     * Shallow clones reference data files in the source directory. If you run  `vacuum`  on the source table clients will no longer be able to read the referenced data files and a  `FileNotFoundException`  will be thrown. In this case, running clone with replace over the shallow clone will repair the clone. If this occurs often, consider using a deep clone instead which does not depend on the source table. * Deep clones do not depend on the source from which they were cloned, but are expensive to create because a deep clone copies the data as well as the metadata. * Cloning with  `replace`  to a target that already has a table at that path creates a Delta log if one does not exist at that path. You can clean up any existing data by running  `vacuum`  . * If an existing Delta table exists, a new commit is created that includes the new metadata and new data from the source table. This new commit is incremental, meaning that only new changes since the last clone are committed to the table. * Cloning a table is not the same as  `Create     Table     As     Select`  or  `CTAS`  . A clone copies the metadata of the source table in addition to the data. Cloning also has simpler syntax: you don’t need to specify partitioning, format, invariants, nullability and so on as they are taken from the source table. * A cloned table has an independent history from its source table. Time travel queries on a cloned table will not work with the same inputs as they work on its source table.        ``` CREATE TABLE delta.`/data/target/` CLONE delta.`/data/source/` -- Create a deep clone of /data/source at /data/target  CREATE OR REPLACE TABLE db.target\_table CLONE db.source\_table -- Replace the target  CREATE TABLE IF NOT EXISTS delta.`/data/target/` CLONE db.source\_table -- No-op if the target table exists  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source`  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` VERSION AS OF version  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` TIMESTAMP AS OF timestamp\_expression -- timestamp can be like “2019-01-01” or like date\_sub(current\_date(), 1)  ```       ``` from delta.tables import \*  deltaTable = DeltaTable.forPath(spark, pathToTable)  # path-based tables, or deltaTable = DeltaTable.forName(spark, tableName)    # Hive metastore-based tables  deltaTable.clone(target, isShallow, replace) # clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) # clone the source at a specific version  # clone the source at a specific timestamp such as timestamp=“2019-01-01” deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace)  ```       ``` import io.delta.tables.\_  val deltaTable = DeltaTable.forPath(spark, pathToTable) val deltaTable = DeltaTable.forName(spark, tableName)  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       ``` import io.delta.tables.\*;  DeltaTable deltaTable = DeltaTable.forPath(spark, pathToTable); DeltaTable deltaTable = DeltaTable.forName(spark, tableName);  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       For syntax details, see  [CREATE TABLE CLONE](../sql/language-manual/delta-clone.html)  .         Clone metrics ---------------      Note       Available in Databricks Runtime 8.2 and above.       `CLONE`  reports the following metrics as a single row DataFrame once the operation is complete:     * `source\_table\_size`  : Size of the source table that’s being cloned in bytes. * `source\_num\_of\_files`  : The number of files in the source table. * `num\_removed\_files`  : If the table is being replaced, how many files are removed from the current table. * `num\_copied\_files`  : Number of files that were copied from the source (0 for shallow clones). * `removed\_files\_size`  : Size in bytes of the files that are being removed from the current table. * `copied\_files\_size`  : Size in bytes of the files copied to the table.    ![Clone metrics example](../_images/clone-metrics.png)      Permissions -------------     You must configure permissions for Databricks table access control and your cloud provider.      ###   Table access control     The following permissions are required for both deep and shallow clones:     * `SELECT`  permission on the source table. * If you are using  `CLONE`  to create a new table,  `CREATE`  permission on the database in which you are creating the table. * If you are using  `CLONE`  to replace a table, you must have  `MODIFY`  permission on the table.     ###   Cloud provider permissions     If you have created a deep clone, any user that reads the deep clone must have read access to the clone’s directory. To make changes to the clone, users must have write access to the clone’s directory.       If you have created a shallow clone, any user that reads the shallow clone needs permission to read the files in the original table, since the data files remain in the source table with shallow clones, as well as the clone’s directory. To make changes to the clone, users will need write access to the clone’s directory.          Use clone for data archiving ------------------------------     Data may need to be kept for longer than is feasible with time travel or for disaster recovery. In these cases, you can create a deep clone to preserve the state of a table at a certain point in time for archival. Incremental archiving is also possible to keep a continually updating state of a source table for disaster recovery.        ``` -- Every month run CREATE OR REPLACE TABLE delta.`/some/archive/path` CLONE my\_prod\_table  ```        Use clone for ML model reproduction -------------------------------------     When doing machine learning, you may want to archive a certain version of a table on which you trained an ML model. Future models can be tested using this archived data set.        ``` -- Trained model on version 15 of Delta table CREATE TABLE delta.`/model/dataset` CLONE entire\_dataset VERSION AS OF 15  ```        Use clone for short-term experiments on a production table ------------------------------------------------------------     To test a workflow on a production table without corrupting the table, you can easily create a shallow clone. This allows you to run arbitrary workflows on the cloned table that contains all the production data but does not affect any production workloads.        ``` -- Perform shallow clone CREATE OR REPLACE TABLE my\_test SHALLOW CLONE my\_prod\_table;  UPDATE my\_test WHERE user\_id is null SET invalid=true; -- Run a bunch of validations. Once happy:  -- This should leverage the update information in the clone to prune to only -- changed files in the clone if possible MERGE INTO my\_prod\_table USING my\_test ON my\_test.user\_id <=> my\_prod\_table.user\_id WHEN MATCHED AND my\_test.user\_id is null THEN UPDATE \*;  DROP TABLE my\_test;  ```        Use clone for data sharing ----------------------------     Other business units within a single organization may want to access the same data but may not require the latest updates. Instead of giving access to the source table directly, you can provide clones with different permissions for different business units. The performance of the clone can exceed that of a simple view.        ``` -- Perform deep clone CREATE OR REPLACE TABLE shared\_table CLONE my\_prod\_table;  -- Grant other users access to the shared table GRANT SELECT ON shared\_table TO `<user-name>@<user-domain>.com`;  ```        Use clone to override table properties ----------------------------------------      Note       Available in Databricks Runtime 7.5 and above.        Table property overrides are particularly useful for:     * Annotating tables with owner or user information when sharing data with different business units. * Archiving Delta tables and time travel is required. You can specify the log retention period independently for the archive table. For example:       ``` CREATE OR REPLACE TABLE archive.my\_table CLONE prod.my\_table TBLPROPERTIES ( delta.logRetentionDuration = '3650 days', delta.deletedFileRetentionDuration = '3650 days' ) LOCATION 'xx://archive/my\_table'  ```       ``` dt = DeltaTable.forName(spark, "prod.my\_table") tblProps = { "delta.logRetentionDuration": "3650 days", "delta.deletedFileRetentionDuration": "3650 days" } dt.clone('xx://archive/my\_table', isShallow=False, replace=True, tblProps)  ```       ``` val dt = DeltaTable.forName(spark, "prod.my\_table") val tblProps = Map( "delta.logRetentionDuration" -> "3650 days", "delta.deletedFileRetentionDuration" -> "3650 days" ) dt.clone("xx://archive/my\_table", isShallow = false, replace = true, properties = tblProps)  ```                                                        	A deep clone is a clone that copies the source table data to the clone target in addition to the metadata of the existing table. Additionally, stream metadata is also cloned such that a stream that writes to the Delta table can be stopped on a source table and continued on the target of a clone from where it left off. On the other hand, a shallow clone is a clone that does not copy the data files to the clone target. The table metadata is equivalent to the source. These clones are cheaper to create.	1	1	1
no_reaction	https://docs.databricks.com/structured-streaming/synapse.html	how to apply deletes in structured streaming, bronze and silver layers?	DELETE FROM =============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Deletes the rows that match a predicate. When no predicate is provided, deletes all rows.       This statement is only supported for Delta Lake tables.        Syntax --------      ``` DELETE FROM table\_name [table\_alias] [WHERE predicate]  ```        Parameters ------------   * [table\_name](sql-ref-names.html#table-name)      Identifies an existing table. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . * [table\_alias](sql-ref-names.html#table-alias)      Define an alias for the table. The alias must not include a column list. * **[WHERE](sql-ref-syntax-qry-select-where.html)**      Filter rows by predicate.       The  `WHERE`  predicate supports subqueries, including  `IN`  ,  `NOT     IN`  ,  `EXISTS`  ,  `NOT     EXISTS`  , and scalar subqueries. The following types of subqueries are not supported:      + Nested subqueries, that is, an subquery inside another subquery  + `NOT        IN`   subquery inside an   `OR`   , for example,   `a        =        3        OR        b        NOT        IN        (SELECT        c        from        t)`  In most cases, you can rewrite  `NOT     IN`  subqueries using  `NOT     EXISTS`  . We recommend using  `NOT     EXISTS`  whenever possible, as  `DELETE`  with  `NOT     IN`  subqueries can be slow.       Examples ----------      ``` > DELETE FROM events WHERE date < '2017-01-01'  > DELETE FROM all\_events  WHERE session\_time < (SELECT min(session\_time) FROM good\_events)  > DELETE FROM orders AS t1  WHERE EXISTS (SELECT oid FROM returned\_orders WHERE t1.oid = oid)  > DELETE FROM events  WHERE category NOT IN (SELECT category FROM events2 WHERE date > '2001-01-01')  ```    * [COPY](delta-copy-into.html) * [INSERT](sql-ref-syntax-dml-insert-into.html) * [MERGE](delta-merge-into.html) * [PARTITION](sql-ref-partition.html#partition) * [query](sql-ref-syntax-qry-query.html) * [UPDATE](delta-update.html)	To apply deletes in structured streaming, bronze and silver layers, you can use the `DELETE FROM` statement in Databricks SQL. However, this statement is only supported for Delta Lake tables. The syntax for the statement is `DELETE FROM table_name [table_alias] [WHERE predicate]`. The `WHERE` predicate can be used to filter rows by a condition.	1	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-metastore.html	Is the Unity Catalog metastore in the data plane?	Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .	Based on the context, it is not explicitly stated whether the Unity Catalog metastore is in the data plane or not.	1	0	0
no_reaction	https://docs.databricks.com/sql/language-manual/functions/current_date.html	databricks sql get current date	   pyspark.sql.functions.current\_date  [¶](#pyspark-sql-functions-current-date "Permalink to this headline") ============================================================================================================  `pyspark.sql.functions.`  `current_date`    (     )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#current_date) [¶](#pyspark.sql.functions.current_date "Permalink to this definition")    Returns the current date at the start of query evaluation as a  `DateType`  column. All calls of current\_date within the same query return the same value.     New in version 1.5.0.     Changed in version 3.4.0:    Supports Spark Connect.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")   current date.     Examples    ``` >>> df = spark.range(1) >>> df.select(current\_date()).show()  +--------------+ |current\_date()| +--------------+ | 2022-08-26| +--------------+  ```  [pyspark.sql.functions.add\_months](pyspark.sql.functions.add_months.html "previous page")  [pyspark.sql.functions.current\_timestamp](pyspark.sql.functions.current_timestamp.html "next page")   	To get the current date in Databricks SQL, you can use the `current_date()` function from the `pyspark.sql.functions` module. It returns the current date at the start of query evaluation as a `DateType` column.	1	1	1
no_reaction	https://docs.databricks.com/delta/index.html	What's the difference between delta lake and dbfs	What is the Databricks File System (DBFS)? ============================================     The Databricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls.        Note       Databricks workspaces deploy with a  [DBFS root volume](#dbfs-root)  , accessible to all users by default. Databricks recommends against storing production data in this location.           What can you do with DBFS? ----------------------------     DBFS provides convenience by mapping cloud object storage URIs to relative paths.     * Allows you to  [interact with object storage](#interact-files)  using directory and file semantics instead of cloud-specific API commands. * Allows you to  [mount](#mount-storage)  cloud object storage locations so that you can map storage credentials to paths in the Databricks workspace. * Simplifies the process of persisting files to object storage, allowing virtual machines and attached volume storage to be safely deleted on cluster termination. * Provides a convenient location for storing init scripts, JARs, libraries, and configurations for cluster initialization. * Provides a convenient location for checkpoint files created during model training with OSS deep learning libraries.      Note       DBFS is the Databricks implementation for FUSE. See  [How to work with files on Databricks](../files/index.html)  .            Interact with files in cloud-based object storage ---------------------------------------------------     DBFS provides many options for interacting with files in cloud object storage:         * [How to work with files on Databricks](../files/index.html) * [List, move, copy, and delete files with Databricks Utilities](../dev-tools/databricks-utils.html#file-system-utility-dbutilsfs) * [Browse files in DBFS](file-browser.html) * [Upload files to DBFS with the UI](../archive/legacy/data-tab.html) * [Interact with DBFS files using the Databricks CLI](../dev-tools/cli/dbfs-cli.html) * [Interact with DBFS files using the Databricks REST API](../dev-tools/api/latest/dbfs.html)             Mount object storage ----------------------     Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage, so you do not need to specify these settings in code or during cluster configuration.       For more information, see  [Mounting cloud object storage on Databricks](mounts.html)  .           What is the DBFS root? ------------------------     The  *DBFS root*   is the default storage location for a Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Databricks workspace. For details on Databricks Filesystem root configuration and deployment, see  [Configure AWS storage](../administration-guide/cloud-configurations/aws/aws-storage.html)  . For best practices around securing data in the DBFS root, see  [Recommendations for working with DBFS root](dbfs-root.html)  .       Some users of Databricks may refer to the DBFS root as “DBFS” or “the DBFS”; it is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a cloud object storage location. You use DBFS to interact with the DBFS root, but they are distinct concepts, and DBFS has many applications beyond the DBFS root.       The DBFS root contains a number of special locations that serve as defaults for various actions performed by users in the workspace. For details, see  [What directories are in DBFS root by default?](root-locations.html)  .           How does DBFS work with Unity Catalog? ----------------------------------------     Unity Catalog adds the concepts of external locations and managed storage credentials to help organizations provide least privileges access to data in cloud object storage. Unity Catalog also provides a new default storage location for managed tables. Some security configurations provide direct access to both Unity Catalog-managed resources and DBFS. Databricks has compiled recommendations for  [using DBFS and Unity Catalog](unity-catalog.html)  .	Delta Lake is a storage layer that provides ACID transactions and other features on top of data stored in cloud object storage. DBFS, on the other hand, is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls. In summary, Delta Lake is a storage layer while DBFS is a distributed file system.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/parameters/timezone.html	how to change the time zone ?	TIMEZONE ==========    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL       The  `TIMEZONE`  configuration parameter controls the local timezone used for timestamp operations within a session.       You can set this parameter at the session level using the  [SET statement](../sql-ref-syntax-aux-conf-mgmt-set.html)  and at the global level using  [SQL configuration parameters](../../admin/sql-configuration-parameters.html)  or  [Global SQL Warehouses API](../../api/sql-endpoints.html#global-sql-warehouses-api)  .       An alternative way to set the session timezone is using the  [SET TIME ZONE statement](../sql-ref-syntax-aux-conf-mgmt-set-timezone.html)  .        Setting ---------     The parameter may be set to one of these values:     * **Region base zone ID**       Region IDs must have the form ‘area/city’, such as  `America/Los\_Angeles`  . * **Zone offset**       Zone offsets must be in the format ‘  `(+|-)HH`  ’,  `(+|-)HH:mm`  or  `(+|-)HH:mm:ss`  , e.g  `-08`  ,  `+01:00`  or  `-13:33:33`  . Also, ‘UTC’ is supported as an alias for  `+00:00`  .     Other short names are not recommended to use because they can be ambiguous.         System default ----------------     The system default is  `UTC`  .         Examples ----------      ``` -- Set time zone to the region-based zone ID. > SET timezone = America/Los\_Angeles; > SELECT current\_timezone();  America/Los\_Angeles  -- Set time zone to the Zone offset. > SET timezone = +08:00; > SELECT current\_timezone();  +08:00  ```        Related ---------   * [RESET](../sql-ref-syntax-aux-conf-mgmt-reset.html) * [SET statement](../sql-ref-syntax-aux-conf-mgmt-set.html) * [SET TIME ZONE statement](../sql-ref-syntax-aux-conf-mgmt-set-timezone.html) * [SQL configuration parameters](../../admin/sql-configuration-parameters.html) * [Global SQL Warehouses API](../../api/sql-endpoints.html#global-sql-warehouses-api)	To change the local timezone used for timestamp operations within a session, you can set the `TIMEZONE` configuration parameter at the session level using the `SET statement` and at the global level using SQL configuration parameters or Global SQL Warehouses API. An alternative way to set the session timezone is using the `SET TIME ZONE statement`. The parameter may be set to one of these values: Region base zone ID or Zone offset. The system default is UTC.	0	1	1
no_reaction	https://docs.databricks.com/lakehouse/index.html	Give me a short, accurate definition of a lakehouse.  How is it it defined?  What is required to be categorized as a lakehouse vs other product categories?	What is the Databricks Lakehouse? ===================================     The Databricks Lakehouse combines the ACID transactions and data governance of enterprise data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data. The Databricks Lakehouse keeps your data in your massively scalable cloud object storage in open source data standards, allowing you to use your data however and wherever you want.      * [What are ACID guarantees on Databricks?](acid.html) * [What is the medallion lakehouse architecture?](medallion.html) * [What does it mean to build a single source of truth?](ssot.html) * [Data discovery and collaboration in the lakehouse](collaboration.html) * [Data objects in the Databricks Lakehouse](data-objects.html)       Components of the Databricks Lakehouse ----------------------------------------     The primary components of the Databricks Lakehouse are:     * **[Delta tables](#delta-tables)**  :      + [ACID transactions](acid.html)  + [Data versioning](../delta/history.html)  + [ETL](../ingestion/index.html)  + [Indexing](../delta/data-skipping.html) * **[Unity Catalog](../data-governance/unity-catalog/index.html)**  :      + [Data governance](../data-governance/index.html)  + [Data sharing](../data-sharing/index.html)  + [Data auditing](../data-governance/unity-catalog/audit.html)     By storing data with  [Delta Lake](../delta/index.html)  , you enable downstream data scientists, analysts, and machine learning engineers to leverage the same production data supporting your core ETL workloads as soon as data is processed.       Unity Catalog ensures that you have complete control over who gains access to which data and provides a centralized mechanism for managing all data governance and access controls without needing to replicate your data.         Delta tables --------------     Tables created on Databricks use the Delta Lake protocol by default. When you create a new Delta table:     * Metadata used to reference the table is added to the metastore in the declared schema or database. * Data and table metadata are saved to a directory in cloud object storage.     The metastore reference to a Delta table is technically optional; you can create Delta tables by directly interacting with directory paths using  [Spark APIs](../delta/tutorial.html)  . Some new features that build upon Delta Lake will store additional metadata in the table directory, but all Delta tables have:     * A directory containing table data in the Parquet file format. * A sub-directory  `/\_delta\_log`  that contains metadata about table versions in JSON and Parquet format.     Learn more about  [Data objects in the Databricks Lakehouse](data-objects.html)  .           Unity Catalog ---------------     Unity Catalog unifies data governance and discovery on Databricks. Available in notebooks, jobs, and Databricks SQL, Unity Catalog provides features and UIs that enable workloads and users designed for both data lakes and data warehouse.     * Account-level management of the Unity Catalog metastore means databases, data objects, and permissions can be shared across Databricks workspaces. * You can leverage three tier namespacing (  `<catalog>.<database>.<table>`  ) for organizing and granting access to data. * External locations and storage credentials are also securable objects with similar ACL setting to other data objects. * The  [Data Explorer](../data/index.html)  provides a graphical user interface to explore databases and manage permissions.       Data lakehouse vs. data warehouse vs. data lake -------------------------------------------------     Data warehouses have powered business intelligence (BI) decisions for about 30 years, having evolved as set of design guidelines for systems controlling the flow of data. Enterprise data warehouses optimize queries for BI reports, but can take minutes or even hours to generate results. Designed for data that is unlikely to change with high frequency, data warehouses seek to prevent conflicts between concurrently running queries. Many data warehouses rely on proprietary formats, which often limit support for machine learning. Data warehousing on Databricks leverages the capabilities of a Databricks Lakehouse and Databricks SQL. For more information, see  [What is data warehousing on Databricks?](../sql/index.html)  .       Powered by technological advances in data storage and driven by exponential increases in the types and volume of data, data lakes have come into widespread use over the last decade. Data lakes store and process data cheaply and efficiently. Data lakes are often defined in opposition to data warehouses: A data warehouse delivers clean, structured data for BI analytics, while a data lake permanently and cheaply stores data of any nature in any format. Many organizations use data lakes for data science and machine learning, but not for BI reporting due to its unvalidated nature.       The data lakehouse replaces the current dependency on data lakes and data warehouses for modern data companies that desire:     * Open, direct access to data stored in standard data formats. * Indexing protocols optimized for machine learning and data science. * Low query latency and high reliability for BI and advanced analytics.     By combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the data lakehouse allows data scientists and ML engineers to build models from the same data driving BI reports.	A lakehouse is a combination of the ACID transactions and data governance of enterprise data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data. It keeps data in massively scalable cloud object storage in open source data standards, allowing users to use their data however and wherever they want. To be categorized as a lakehouse, a product must combine the features of data warehouses and data lakes, including ACID transactions, data governance, flexibility, and cost-efficiency.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/information-schema/views.html	What is the difference between a table and a view in databricks sql?	VIEWS =======    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime 10.2 and above  ![check marked yes](../../../_images/check.png)  Unity Catalog only       INFORMATION\_SCHEMA.VIEWS describes view specific information about views in the catalog.       The rows returned are limited to the views the user is privileged to interact with.        Definition ------------     The  `VIEWS`  relation contains the following columns:             |   Name   |   Data type   |   Nullable   |   Standard   |   Description   | | --- | --- | --- | --- | --- | |  `TABLE\_CATALOG`  |  `STRING`  |   No   |   Yes   |   Catalog containing the view.   | |  `TABLE\_SCHEMA`  |  `STRING`  |   No   |   Yes   |   Schema containing the view.   | |  `TABLE\_NAME`  |  `STRING`  |   No   |   Yes   |   Name of the relation.   | |  `VIEW\_DEFINITION`  |  `STRING`  |   Yes   |   Yes   |   The view text if the user owns the view,  `NULL`  otherwise.   | |  `CHECK\_OPTION`  |  `STRING`  |   No   |   Yes   |   Always  `'NONE'`  . Reserved for future use.   | |  `IS\_UPDATABLE`  |  `STRING`  |   No   |   Yes   |   Always  `NO`  . Reserved for future use   | |  `IS\_INSERTABLE\_INTO`  |  `STRING`  |   No   |   Yes   |   Always  `NO`  . Reserved for future use.   | |  `SQL\_PATH`  |  `STRING`  |   Yes   |   Yes   |   Always  `NULL`  . Reserved for future use.   |       Constraints -------------     The following constraints apply to the  `VIEWS`  relation:            |   Class   |   Name   |   Column List   |   Description   | | --- | --- | --- | --- | |   Primary key   |  `VIEWS\_PK`  |  `TABLE\_CATALOG`  ,  `TABLE\_SCHEMA`  ,  `TABLE\_NAME`  |   Unique identifier for the view.   | |   Foreign key   |  `VIEWS\_TABLES\_FK`  |  `TABLE\_CATALOG`  ,  `TABLE\_SCHEMA`  ,  `TABLE\_NAME`  |   References  [TABLES](tables.html)  .   |       Examples ----------      ``` > SELECT is\_intertable\_into  FROM information\_schema.views  WHERE table\_schema = 'information\_schema'  AND table\_name = 'columns'  NO  ```        Related ---------   * [DESCRIBE TABLE](../sql-ref-syntax-aux-describe-table.html) * [Information schema](../sql-ref-information-schema.html) * [INFORMATION\_SCHEMA.COLUMNS](columns.html) * [INFORMATION\_SCHEMA.SCHEMATA](schemata.html) * [INFORMATION\_SCHEMA.TABLES](tables.html) * [INFORMATION\_SCHEMA.TABLE\_PRIVILEGES](table_privileges.html) * [SHOW CREATE TABLE](../sql-ref-syntax-aux-show-create-table.html) * [SHOW VIEWS](../sql-ref-syntax-aux-show-views.html)	A table is a collection of structured data organized in rows and columns, while a view is a virtual table that does not store data on its own but is based on the result set of an SQL statement. In Databricks SQL, the `VIEWS` relation in the `INFORMATION_SCHEMA` describes view-specific information about views in the catalog.	0	1	1
no_reaction	https://docs.databricks.com/runtime/dbr.html	runtime versioning	Databricks Runtime ====================     Databricks Runtime includes Apache Spark but also adds a number of components and updates that substantially improve the usability, performance, and security of big data analytics:     * [Delta Lake](../delta/index.html)  , a next-generation storage layer built on top of Apache Spark that provides ACID transactions, optimized layouts and indexes, and execution engine improvements for building data pipelines. * Installed Java, Scala, Python, and R libraries * Ubuntu and its accompanying system libraries * GPU libraries for GPU-enabled clusters * Databricks services that integrate with other components of the platform, such as notebooks, jobs, and cluster manager     For information about the contents of each runtime version, see the  [release notes](../release-notes/runtime/releases.html)  .        Runtime versioning --------------------     Databricks Runtime versions are released on a regular basis:     * **Major**   versions are represented by an increment to the version number that precedes the decimal point (the jump from 3.5 to 4.0, for example). They are released when there are major changes, some of which may not be backwards-compatible. * **Feature**   versions are represented by an increment to the version number that follows the decimal point (the jump from 3.4 to 3.5, for example). Each major release includes multiple feature releases. Feature releases are always backwards compatible with previous releases within their major release. * **Long Term Support**   versions are represented by an  **LTS**   qualifier (for example,  **3.5 LTS**   ). For each major release, we declare a “canonical” feature version, for which we provide two full years of support. See  [Databricks runtime support lifecycles](../release-notes/runtime/databricks-runtime-ver.html#runtime-support)  for more information.	The context is about Databricks Runtime, which includes Apache Spark and additional components and updates to improve the usability, performance, and security of big data analytics. Runtime versions are released regularly, with major versions representing major changes that may not be backwards-compatible, feature versions being backwards-compatible with previous releases within their major release, and Long Term Support versions providing two full years of support for a canonical feature version within each major release.	1	1	1
no_reaction	https://docs.databricks.com/clusters/graviton.html	Not all metrics are available when using Graviton instances	   p    [org](../../../index.html)   .  [apache](../../index.html)   .  [spark](../index.html)    metrics   [**](../../../../org/apache/spark/metrics/index.html "Permalink") ==============================================================================  ####    package     metrics  **   **    Ordering   1. Alphabetic   Visibility   1. Public 2. All  ###   Type Members  1. [**](../../../../org/apache/spark/metrics/ExecutorMetricType.html "Permalink")   sealed     trait    [ExecutorMetricType](ExecutorMetricType.html "Executor metric types for executor-level metrics stored in ExecutorMetrics.")   extends  [AnyRef](../../../../scala/index.html#AnyRef=Object)    Executor metric types for executor-level metrics stored in ExecutorMetrics. 2. [**](../../../../org/apache/spark/metrics/SingleValueExecutorMetricType.html "Permalink")   sealed     trait    [SingleValueExecutorMetricType](SingleValueExecutorMetricType.html)   extends  [ExecutorMetricType](ExecutorMetricType.html)  ###   Value Members  1. [**](../../../../org/apache/spark/metrics/DirectPoolMemory$.html "Permalink")   object    [DirectPoolMemory](DirectPoolMemory$.html)   extends    MBeanExecutorMetricType    with    Product    with    Serializable 2. [**](../../../../org/apache/spark/metrics/GarbageCollectionMetrics$.html "Permalink")   object    [GarbageCollectionMetrics](GarbageCollectionMetrics$.html)   extends  [ExecutorMetricType](ExecutorMetricType.html)   with    Logging    with    Product    with    Serializable 3. [**](../../../../org/apache/spark/metrics/JVMHeapMemory$.html "Permalink")   object    [JVMHeapMemory](JVMHeapMemory$.html)   extends  [SingleValueExecutorMetricType](SingleValueExecutorMetricType.html)   with    Product    with    Serializable 4. [**](../../../../org/apache/spark/metrics/JVMOffHeapMemory$.html "Permalink")   object    [JVMOffHeapMemory](JVMOffHeapMemory$.html)   extends  [SingleValueExecutorMetricType](SingleValueExecutorMetricType.html)   with    Product    with    Serializable 5. [**](../../../../org/apache/spark/metrics/MappedPoolMemory$.html "Permalink")   object    [MappedPoolMemory](MappedPoolMemory$.html)   extends    MBeanExecutorMetricType    with    Product    with    Serializable 6. [**](../../../../org/apache/spark/metrics/OffHeapExecutionMemory$.html "Permalink")   object    [OffHeapExecutionMemory](OffHeapExecutionMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 7. [**](../../../../org/apache/spark/metrics/OffHeapStorageMemory$.html "Permalink")   object    [OffHeapStorageMemory](OffHeapStorageMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 8. [**](../../../../org/apache/spark/metrics/OffHeapUnifiedMemory$.html "Permalink")   object    [OffHeapUnifiedMemory](OffHeapUnifiedMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 9. [**](../../../../org/apache/spark/metrics/OnHeapExecutionMemory$.html "Permalink")   object    [OnHeapExecutionMemory](OnHeapExecutionMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 10. [**](../../../../org/apache/spark/metrics/OnHeapStorageMemory$.html "Permalink")   object    [OnHeapStorageMemory](OnHeapStorageMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 11. [**](../../../../org/apache/spark/metrics/OnHeapUnifiedMemory$.html "Permalink")   object    [OnHeapUnifiedMemory](OnHeapUnifiedMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 12. [**](../../../../org/apache/spark/metrics/ProcessTreeMetrics$.html "Permalink")   object    [ProcessTreeMetrics](ProcessTreeMetrics$.html)   extends  [ExecutorMetricType](ExecutorMetricType.html)   with    Product    with    Serializable  ###   Ungrouped  	Based on the context provided, there is no direct answer to the question "Not all metrics are available when using Graviton instances". The context seems to be a documentation page for the Apache Spark Metrics package, which lists various types of metrics and their corresponding objects. It does not provide any information about Graviton instances or their compatibility with metrics.	0	0	0
thumb_down	https://kb.databricks.com/sql/to_json-results-in-cannot-use-null-as-map-key-error	Hi Assitant. how do I make to_json SQL function to preserve null fields from passed struct?	Title: to_json() results in Cannot use null as map key error  Description: You must filter or replace null values in your input data before using to_json().  Answer: Problem You are using to_json() to convert data to JSON and you get a Cannot use null as map key error: RuntimeException: Cannot use null as map key. Cause to_json() function does not support using null values as the input map keys. This example code causes the Cannot use null as map key error when run, because of the null value used as a map key in the fourth line. %sql select   to_json(     map(       1, 'Databricks',       2, 'Map',       3, 'Error',       null, 'Data'     )   ) as json; Solution  You should filter out any null values present in the input data before running to_json(), or use nvl() to replace all of the null values with non-null values. Filter null values Consider this example DataFrame: +---+----------+-------+ | Id|     Value|address| +---+----------+-------+ |  1|Databricks|   null| |  2|       Map|   null| |  3|     Error|    xyz| +---+----------+-------+ There are two null values in the example. Attempting to use to_json() on this DataFrame will return an error. We can filter the null data by showing only the rows that have non-null values. For example, filtering with df.filter("address is not null").show() returns: +---+-----+-------+ | Id|Value|address| +---+-----+-------+ |  3|Error|    xyz| +---+-----+-------+ This filtered DataFrame does not contain any null values, so it can now be used as an input with to_json(). Replace null values with replacements If you cannot filter out the null values, you can use nvl() to replace the null values with non-null values.  The sample code originally had a null value as the map key for the fourth line. Since that results in an error, nvl() is used in this updated example to substitute 4 for the null value. %sql select   to_json(     map(       1, 'Databricks',       2, 'Map',       3, 'Error', nvl(null, 4), 'Data'     ) ) as JSON;	To preserve null fields from a passed struct in the to_json SQL function, you should filter out any null values present in the input data before running to_json(), or use nvl() to replace all of the null values with non-null values. If you cannot filter out the null values, you can use nvl() to replace the null values with non-null values.	1	1	1
no_reaction	https://docs.databricks.com/getting-started/admin-get-started.html	how do I get started using databricks	Get started articles, tutorials, and best practices =====================================================     Databricks documentation includes many tutorials, Get started articles, and best practices guides.        Get started articles vs. tutorials ------------------------------------     Get started articles provide a shortcut to understanding Databricks features or typical tasks you can perform in Databricks. Most of our Get started articles are intended for new users trying out Databricks.       Tutorials are slightly more complex, prescriptive steps for typical workflows in Databricks that you can use as examples for your projects.         Video tours -------------   * [Video: Databricks Data Science and Engineering workspace](https://www.youtube.com/watch?v=YGq150jtc58&index=5) * [Video: Databricks SQL workspace](https://www.youtube.com/watch?v=zDOVr-Wxyd0&list=PLTPXxbhUt-YWPEK3NYh433T8TwrpVbyce&index=6) * [Video: Databricks Machine Learning workspace](https://www.youtube.com/watch?v=JvXg1iycvQc&list=PLTPXxbhUt-YWPEK3NYh433T8TwrpVbyce&index=7) * [Video: Notebook basics](https://youtu.be/zhUtu0J8LWg)         Get started with Databricks Data Science & Engineering --------------------------------------------------------   * [Get started: Query data from a notebook](../quick-start.html) * [Get started: Build a basic ETL pipeline](../etl-quick-start.html) * [Tutorial: Run an end-to-end lakehouse analytics pipeline](/getting-started/lakehouse-e2e.html) * [Tutorial: Build an end-to-end data pipeline](../data-pipeline-get-started.html)         Get started with Databricks Machine Learning ----------------------------------------------   * [Get started: Databricks Machine Learning in-product quickstart](../../machine-learning/tutorial/ml-quickstart.html) * [10-min tutorials: ML notebooks](../../machine-learning/tutorial/index.html) * [Get started: MLflow quickstart notebooks](../../mlflow/quick-start.html)         Get started with Databricks SQL ---------------------------------   * [Databricks SQL user quickstart: Import and explore sample dashboards](../../sql/get-started/sample-dashboards.html) * [Databricks SQL user quickstart: Run and visualize a query](../../sql/get-started/user-quickstart.html)       Best practices for Databricks -------------------------------     The Databricks documentation includes a number of best practices articles to help you get the best performance at the lowest cost when using and administering Databricks.      ###   Data science and engineering best practices   * [Delta Lake](../../delta/best-practices.html) * [Hyperparameter tuning with Hyperopt](../../machine-learning/automl-hyperparam-tuning/hyperopt-best-practices.html) * [Deep learning in Databricks](../../machine-learning/train-model/dl-best-practices.html) * [Delta Lake Structured Streaming with Amazon Kinesis](../../structured-streaming/kinesis-best-practices.html) * [CI/CD](../../dev-tools/index-ci-cd.html#dev-tools-ci-cd) * [MLOps worflows](../../machine-learning/mlops/mlops-workflow.html)     ###   Best practices for Databricks admins   * [Cluster configuration](../../clusters/cluster-config-best-practices.html) * [Pools](../../clusters/pool-best-practices.html) * [Cluster policies](../../administration-guide/clusters/policies-best-practices.html) * [Data governance](../../data-governance/best-practices.html) * [GDPR and CCPA compliance using Delta Lake](../../security/privacy/gdpr-delta.html)	To get started using Databricks, you can refer to the Get started articles, tutorials, and best practices guides available in the Databricks documentation. The Get started articles provide a shortcut to understanding Databricks features or typical tasks you can perform in Databricks, while tutorials are slightly more complex, prescriptive steps for typical workflows in Databricks that you can use as examples for your projects. Additionally, there are video tours available for Databricks Data Science and Engineering workspace, Databricks SQL workspace, Databricks Machine Learning workspace, and Notebook basics.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/functions/char.html	What is the difference between CHAR and VARCHAR2 datatype in SQL?	Title: Unable to cast string to varchar  Description: Use varchar type in Databricks Runtime 8.0 and above. It can only be used in table schema. It cannot be used in functions or operators.  Answer: Problem You are trying to cast a string type column to varchar but it isn’t working. Info The varchar data type (AWS | Azure | GCP) is available in Databricks Runtime 8.0 and above. Create a simple Delta table, with one column as type string.%sql CREATE OR REPLACE TABLE delta_table1 (`col1` string) USING DELTA; Use SHOW TABLE on the newly created table and it reports a string type.%sql SHOW CREATE TABLE delta_table1; Create a second Delta table, based on the first, and convert the string type column into varchar.%sql CREATE OR REPLACE TABLE delta_varchar_table1 USING DELTA AS SELECT cast(col1 AS VARCHAR(1000)) FROM delta_table1; Use SHOW TABLE on the newly created table and it reports that the table got created, but the column is string type.%sql SHOW CREATE TABLE delta_varchar_table1; Cause The varchar type can only be used in table schema. It cannot be used in functions or operators. Please review the Spark supported data types documentation for more information. Solution You cannot cast string to varchar, but you can create a varchar Delta table.%sql CREATE OR REPLACE TABLE delta_varchar_table2 (`col1` VARCHAR(1000)) USING DELTA; Use SHOW TABLE on the newly created table and it reports a varchartype.%sql SHOW CREATE TABLE delta_varchar_table2; You can now create another varchar Delta table, based on the first, and it keeps the varchartype.%sql CREATE OR REPLACE TABLE delta_varchar_table3 USING DELTA AS SELECT * FROM delta_varchar_table2; Use SHOW TABLE on the newly created table and it reports a varchartype.%sql SHOW CREATE TABLE delta_varchar_table3;	The main difference between CHAR and VARCHAR2 datatype in SQL is that CHAR datatype is a fixed-length character data type, while VARCHAR2 datatype is a variable-length character data type. CHAR datatype always takes up the same amount of space, even if the actual data is shorter, while VARCHAR2 datatype only takes up as much space as the actual data. In Databricks Runtime 8.0 and above, the VARCHAR datatype is available and can only be used in table schema, not in functions or operators.	0	1	1
no_reaction	https://docs.databricks.com/delta-live-tables/import-workspace-files.html	dlt with python	Python programming for Delta Live Tables ==========================================     The articles linked here provide guidance for using Python to perform programming tasks with Delta Live Tables. These recommendations assume familiarity with concepts and patterns in the following articles:     * [Tutorial: Declare a data pipeline with Python in Delta Live Tables](tutorial-python.html) * [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html) * [How to develop and test Delta Live Tables pipelines](testing.html)      Note       Patterns shown in this article cannot be easily completed with only SQL. Because Python datasets can be defined against any query that returns a DataFrame, you can use  `spark.sql()`  as necessary to leverage SQL syntax within Python functions.       You can use Python user-defined functions (UDFs) in your SQL queries, but you must define these UDFs in Python files in the same pipeline before calling them in SQL source files. See  [User-defined scalar functions - Python](../udf/python.html)  .       * [Programmatically create tables](create-multiple-tables.html) * [Import modules from workspace files](import-workspace-files.html)	The context is about using Python to perform programming tasks with Delta Live Tables. It provides guidance for using Python to declare a data pipeline, run a Delta Live Tables pipeline, and develop and test Delta Live Tables pipelines. The article also mentions that Python datasets can be defined against any query that returns a DataFrame, and that you can use `spark.sql()` as necessary to leverage SQL syntax within Python functions. Additionally, the article mentions that you can use Python user-defined functions (UDFs) in your SQL queries, but you must define these UDFs in Python files in the same pipeline before calling them in SQL source files. Finally, the article provides links to other articles that cover topics such as programmatically creating tables and importing modules from workspace files.	1	0	1
no_reaction	https://kb.databricks.com/delta/delta-cache-autoscaling	does spark scale down nodes that have RDD cache?	  ![]()   Cache =======  `cache.Rd`    Persist with the default storage level (MEMORY\_ONLY).     Usage -------  ``` cache(x)  # S4 method for SparkDataFrame cache(x) ```   Arguments -----------   x     A SparkDataFrame     Note ------   cache since 1.4.0     See also ----------   Other SparkDataFrame functions:  `[SparkDataFrame-class](SparkDataFrame.html)`  ,  `[agg](summarize.html)   ()`   ,  `[alias](alias.html)   ()`   ,  `[arrange](arrange.html)   ()`   ,  `[as.data.frame](as.data.frame.html)   ()`   ,  `[attach,SparkDataFrame-method](attach.html)`  ,  `[broadcast](broadcast.html)   ()`   ,  `[checkpoint](checkpoint.html)   ()`   ,  `[coalesce](coalesce.html)   ()`   ,  `[collect](collect.html)   ()`   ,  `[colnames](columns.html)   ()`   ,  `[coltypes](coltypes.html)   ()`   ,  `[createOrReplaceTempView](createOrReplaceTempView.html)   ()`   ,  `[crossJoin](crossJoin.html)   ()`   ,  `[cube](cube.html)   ()`   ,  `[dapplyCollect](dapplyCollect.html)   ()`   ,  `[dapply](dapply.html)   ()`   ,  `[describe](describe.html)   ()`   ,  `[dim](dim.html)   ()`   ,  `[distinct](distinct.html)   ()`   ,  `[dropDuplicates](dropDuplicates.html)   ()`   ,  `[dropna](nafunctions.html)   ()`   ,  `[drop](drop.html)   ()`   ,  `[dtypes](dtypes.html)   ()`   ,  `[exceptAll](exceptAll.html)   ()`   ,  `[except](except.html)   ()`   ,  `[explain](explain.html)   ()`   ,  `[filter](filter.html)   ()`   ,  `[first](first.html)   ()`   ,  `[gapplyCollect](gapplyCollect.html)   ()`   ,  `[gapply](gapply.html)   ()`   ,  `[getNumPartitions](getNumPartitions.html)   ()`   ,  `[group\_by](groupBy.html)   ()`   ,  `[head](head.html)   ()`   ,  `[hint](hint.html)   ()`   ,  `[histogram](histogram.html)   ()`   ,  `[insertInto](insertInto.html)   ()`   ,  `[intersectAll](intersectAll.html)   ()`   ,  `[intersect](intersect.html)   ()`   ,  `[isLocal](isLocal.html)   ()`   ,  `[isStreaming](isStreaming.html)   ()`   ,  `[join](join.html)   ()`   ,  `[limit](limit.html)   ()`   ,  `[localCheckpoint](localCheckpoint.html)   ()`   ,  `[merge](merge.html)   ()`   ,  `[mutate](mutate.html)   ()`   ,  `[ncol](ncol.html)   ()`   ,  `[nrow](nrow.html)   ()`   ,  `[persist](persist.html)   ()`   ,  `[printSchema](printSchema.html)   ()`   ,  `[randomSplit](randomSplit.html)   ()`   ,  `[rbind](rbind.html)   ()`   ,  `[rename](rename.html)   ()`   ,  `[repartitionByRange](repartitionByRange.html)   ()`   ,  `[repartition](repartition.html)   ()`   ,  `[rollup](rollup.html)   ()`   ,  `[sample](sample.html)   ()`   ,  `[saveAsTable](saveAsTable.html)   ()`   ,  `[schema](schema.html)   ()`   ,  `[selectExpr](selectExpr.html)   ()`   ,  `[select](select.html)   ()`   ,  `[showDF](showDF.html)   ()`   ,  `[show](show.html)   ()`   ,  `[storageLevel](storageLevel.html)   ()`   ,  `[str](str.html)   ()`   ,  `[subset](subset.html)   ()`   ,  `[summary](summary.html)   ()`   ,  `[take](take.html)   ()`   ,  `[toJSON](toJSON.html)   ()`   ,  `[unionAll](unionAll.html)   ()`   ,  `[unionByName](unionByName.html)   ()`   ,  `[union](union.html)   ()`   ,  `[unpersist](unpersist.html)   ()`   ,  `[unpivot](unpivot.html)   ()`   ,  `[withColumn](withColumn.html)   ()`   ,  `[withWatermark](withWatermark.html)   ()`   ,  `[with](with.html)   ()`   ,  `[write.df](write.df.html)   ()`   ,  `[write.jdbc](write.jdbc.html)   ()`   ,  `[write.json](write.json.html)   ()`   ,  `[write.orc](write.orc.html)   ()`   ,  `[write.parquet](write.parquet.html)   ()`   ,  `[write.stream](write.stream.html)   ()`   ,  `[write.text](write.text.html)   ()`    Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() path <- "path/to/file.json" df <- [read.json](read.json.html)(path) cache(df) }  ```  	Yes, Spark can scale down nodes that have RDD cache. When a node is removed from the cluster, its cached data is lost and the RDDs that were cached on that node will be rebuilt on demand. However, if the RDDs were previously persisted to disk or replicated across the cluster, they will not be lost.	0	0	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-metastore.html	How do I create a unity catalog metastore	Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .	To create a Unity Catalog metastore, you need to configure a storage bucket and IAM role in AWS, then log in to the Databricks account console, click on Data, and then click on Create Metastore. You will need to enter a name for the metastore, the region where the metastore will be deployed, the S3 bucket path, and the IAM role name that you created in step 1. Finally, you will need to select workspaces to link to the metastore.	1	1	1
no_reaction	https://docs.databricks.com/administration-guide/clusters/personal-compute.html	how to disable the personal compute cluster policy?	Manage the Personal Compute cluster policy ============================================    **Personal Compute**   is a Databricks-managed  [cluster policy](policies-best-practices.html)  available, by default, on all Databricks workspaces. Granting users access to this policy enables them to create single-machine compute resources in Databricks for their individual use. Users can create the personal compute resource quickly using shortcuts in either a notebook or the Compute page.       With the Personal Compute policy, administrators have a simple solution for providing limited compute creation privilages. This removes the need for admins to grant users  [general cluster privilages](policies-best-practices.html#unrestricted-env-considerations)  or to manually manage the creation of compute resources. This default policy also serves an an example cluster policy for admins to clone and modify as needed, allowing them to set up appropriate compute governance patterns for their community of users.       By default, all users in a Databricks account have access to the Personal Compute default policy. This can be  [changed by administrators](#manage-policy)  .        Personal Compute policy details ---------------------------------     When a user has access to the Personal Compute policy, they can create Personal Compute resources from either the  [Compute](../../clusters/configure.html)  page or a  [notebook](../../clusters/clusters-manage.html#cluster-start)  . Personal Compute resources are all-purpose clusters (priced according to  [all-purpose compute pricing](https://www.databricks.com/product/aws-pricing)   ) with the following properties:     * Personal Compute resources are  [single-node clusters](../../clusters/single-node.html)  (i.e., “clusters” with no workers and with Spark running in local mode). * They use the single user cluster  [access mode](../../clusters/configure.html#access-mode)  and are Unity Catalog-compatible. * The runtime is set to use the latest version of the Databricks Runtime for Machine Learning. * Both standard instances and GPU-enabled instances are available. * [Auto-termination](../../clusters/clusters-manage.html#automatic-termination)  is set at 72 hours.     No limits are set on the maximum number of Personal Compute resources individual users can create.           Managing access to the Personal Compute policy ------------------------------------------------     Workspace admins can manage access to the Personal Compute policy on individual workspaces using the  [cluster policies UI](policies.html#manage-policy)  .       Account admins can enable or disable access to the Personal Compute policy for all users in an account using the Personal Compute account setting.     1. From the account console, click  ![User Settings Icon](../../_images/user-settings-icon.png) **Settings**   . 2. Click the  **Feature enablement**   tab. 3. Enable the  **Personal Compute**   setting to give all users in the account access to the Personal Compute policy. Or, switch the setting to  **Delegate**   if you want to policy to be managed at the workspace level.         Customize the Personal Compute policy ---------------------------------------     Workspace admins can override or add policy rules for most Personal Compute settings.        Note       Admins can’t edit the Personal Compute policy name, cluster type, cluster mode, or access mode.        To customize a workspace’s Personal Compute policy:     1. Click  ![compute icon](../../_images/clusters-icon.png) **Compute**   in the sidebar. 2. Click the  **Cluster Policies**   tab. 3. Select the  **Personal Compute**   policy. 4. Click  **Edit**   . 5. Under the  **Definitions**   tab, click  **Edit**   . 6. A modal appears where you can override policy definitions. In the  **Overrides**   section, add the updated  [definitions](policies.html#policy-def)  and click  **OK**   .     In the example below, the  `spark\_version`  rule is overridden. The  `spark\_version`  from the default policy is ignored in favor of the override.      ![Override cluster policy](../../_images/overwrite_policy.png)	To disable the Personal Compute cluster policy, workspace admins can manage access to the policy on individual workspaces using the cluster policies UI. Account admins can also disable access to the Personal Compute policy for all users in an account using the Personal Compute account setting.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/get-started.html	How can I implement Unity Catalog on my Databricks account ?	      Get started using Unity Catalog =================================     This article provides step-by-step instructions for setting up Unity Catalog for your organization. It describes how to enable your Databricks account to use Unity Catalog and how to create your first tables in Unity Catalog.          Overview of Unity Catalog setup ---------------------------------     This section provides a high-level overview of how to set up your Databricks account to use Unity Catalog and create your first tables. For detailed step-by-step instructions, see the sections that follow this one.        ###   Set up your Databricks account for Unity Catalog     To enable your Databricks account to use Unity Catalog, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access managed table data in your AWS account. 2. Create a metastore for each region in which your organization operates. This metastore functions as the top-level container for all of your data in Unity Catalog. 3. Assign workspaces to the metastore. Each workspace has the same view of the data that you manage in Unity Catalog. 4. Add users, groups, and service principals to your Databricks account.       For existing Databricks accounts, these identities are already present. 5. (Optional) Transfer your metastore admin role to a group.     ###   Set up data access for your users     To set up data access for your users, you do the following:     1. In a workspace, create at least one compute resource: either a cluster or SQL warehouse.       You will use this compute resource when you run queries and commands, including grant statements on data objects that are secured in Unity Catalog. 2. Create at least one catalog.       Catalogs hold the schemas (databases) that in turn hold the tables that your users work with. 3. Create at least one schema. 4. Create tables.     For each level in the data hierarchy (catalogs, schemas, tables), you grant privileges to users, groups, or service principals. You can also grant row- or column-level privileges using  [dynamic views](create-views.html#dynamic-view)  .          Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships. * You must have at least one workspace that you want to use with Unity Catalog. See  [Create a workspace using the account console](../../administration-guide/workspace/create-workspace.html)  .           Configure a storage bucket and IAM role in AWS ------------------------------------------------     In this step, you create the AWS objects required by Unity Catalog to store and access managed table data in your AWS account.     1. Find your Databricks account ID.      1. Log in to the Databricks   [account console](https://accounts.cloud.databricks.com)    .  2. Click your username.  3. From the menu, copy the   **Account ID**    value. 2. In AWS, create an S3 bucket.       This S3 bucket will be the root storage location for  [managed tables](index.html#managed-table)  in Unity Catalog. Use a dedicated S3 bucket for each metastore and locate it in the same region as the workspaces you want to access the data from. Make a note of the S3 bucket path, which starts with  `s3://`  .       This default storage location can be overridden at the catalog and schema levels.        Important       The bucket name cannot include dot notation (for example,  `incorrect.bucket.name.notation`  ). For more bucket naming guidance, see the  [AWS bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)   .        If you enable KMS encryption on the S3 bucket, make a note of the name of the KMS encryption key. 3. Create an IAM role that will allow access to the S3 bucket.       Role creation is a two-step process. In this step you simply create the role, adding a  *temporary*   trust relationship policy that you then modify in the next step. You must modify the trust policy  *after*   you create the role because your role must be self-assuming—that is, it must be configured to trust itself. The role must therefore exist before you add the self-assumption statement. For information about self-assuming roles, see this  [Amazon blog article](https://aws.amazon.com/blogs/security/announcing-an-update-to-iam-role-trust-policy-behavior/)   .      1. Create the IAM role with a   **Custom Trust Policy**    .  2. In the   **Custom Trust Policy**    field, paste the following policy JSON, replacing   `<DATABRICKS\_ACCOUNT\_ID>`   with the Databricks account ID you found in step 1 (not your AWS account ID).            This policy establishes a cross-account trust relationship so that Unity Catalog can assume the role to access the data in the bucket on behalf of Databricks users. This is specified by the ARN in the   `Principal`   section. It is a static value that references a role created by Databricks. Do not modify it.               ```  {   "Version": "2012-10-17",   "Statement": [{   "Effect": "Allow",   "Principal": {   "AWS": [   "arn:aws:iam::123456767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL"   ]   },   "Action": "sts:AssumeRole",   "Condition": {   "StringEquals": {   "sts:ExternalId": "<DATABRICKS\_ACCOUNT\_ID>"   }   }   }]  }    ```  3. Skip the permissions policy configuration. You’ll go back to add that in a later step.  4. Save the IAM role. 4. Modify the trust relationship policy to make it “self-assuming.”      1. Return to your saved IAM role and go to the   **Trust Relationships**    tab.  2. Edit the trust relationship policy, adding the following ARN to the “Allow” statement.            Replace   `<YOUR\_AWS\_ACCOUNT\_ID>`   and   `<THIS\_ROLE\_NAME>`   with your actual IAM role values.               ```  "arn:aws:iam::<YOUR\_AWS\_ACCOUNT\_ID>:role/<THIS\_ROLE\_NAME>"    ```  Your policy should now look like this (with replacement text updated to use your Databricks account ID and IAM role values):        ``` {  "Version": "2012-10-17",  "Statement": [  {  "Effect": "Allow",  "Principal": {  "AWS": [  "arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL",  "arn:aws:iam::<YOUR\_AWS\_ACCOUNT\_ID>:role/<THIS\_ROLE\_NAME>"  ]  },  "Action": "sts:AssumeRole",  "Condition": {  "StringEquals": {  "sts:ExternalId": "<DATABRICKS\_ACCOUNT\_ID>"  }  }  }  ] }  ``` 5. In AWS, create an IAM policy in the same AWS account as the S3 bucket.       To avoid unexpected issues, you must use the following sample policy, replacing the following values:      * `<BUCKET>`   : The name of the S3 bucket you created in the previous step.  * `<KMS\_KEY>`   : Optional. If encryption is enabled, provide the name of the KMS key that encrypts the S3 bucket contents.   **If encryption is disabled, remove the entire KMS section of the IAM policy.**  * `<AWS\_ACCOUNT\_ID>`   : The Account ID of the current AWS account (not your Databricks account).  * `<AWS\_IAM\_ROLE\_NAME>`   : The name of the AWS IAM role that you created in the previous step.   ``` {  "Version": "2012-10-17",  "Statement": [  {  "Action": [  "s3:GetObject",  "s3:PutObject",  "s3:DeleteObject",  "s3:ListBucket",  "s3:GetBucketLocation",  "s3:GetLifecycleConfiguration",  "s3:PutLifecycleConfiguration"  ],  "Resource": [  "arn:aws:s3:::<BUCKET>/\*",  "arn:aws:s3:::<BUCKET>"  ],  "Effect": "Allow"  },  {  "Action": [  "kms:Decrypt",  "kms:Encrypt",  "kms:GenerateDataKey\*"  ],  "Resource": [  "arn:aws:kms:<KMS\_KEY>"  ],  "Effect": "Allow"  },  {  "Action": [  "sts:AssumeRole"  ],  "Resource": [  "arn:aws:iam::<AWS\_ACCOUNT\_ID>:role/<AWS\_IAM\_ROLE\_NAME>"  ],  "Effect": "Allow"  }  ] }  ```       Note       * If you need a more restrictive IAM policy for Unity Catalog, contact your Databricks representative for assistance.  * Databricks uses   `GetLifecycleConfiguration`   and   `PutLifecycleConfiguration`   to manage lifecycle policies for the personal staging locations used by Partner Connect and the upload data UI. 6. Attach the IAM policy to the IAM role.       On the IAM role’s  **Permissions**   tab, attach the IAM policy you just created.           Create your first metastore and attach a workspace ----------------------------------------------------     To use Unity Catalog, you must create a metastore. A metastore is the top-level container for data in Unity Catalog. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) by which data can be organized.       You create a metastore for each region in which your organization operates. You can link each of these regional metastores to any number of workspaces in that region. Each linked workspace has the same view of the data in the metastore, and data access control can be managed across workspaces. You can access data in other metastores using  [Delta Sharing](../../data-sharing/index.html)  .       The metastore will use the the S3 bucket and IAM role that you created in the previous step.       To create a metastore:     1. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click  **Create Metastore**   . 4. Enter the following:      * A name for the metastore.  * The region where you want to deploy the metastore.            This must be in the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the storage bucket you created earlier.  * The S3 bucket path (you can omit   `s3://`   ) and IAM role name for the bucket and role you created in   [Configure a storage bucket and IAM role in AWS](#cloud-tenant-setup-aws)   . 5. Click  **Create**   . 6. When prompted, select works	To implement Unity Catalog on your Databricks account, you need to follow the steps below:  1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access managed table data in your AWS account. 2. Create a metastore for each region in which your organization operates. This metastore functions as the top-level container for all of your data in Unity Catalog. 3. Assign workspaces to the metastore. Each workspace has the same view of the data that you manage in Unity Catalog. 4. Add users, groups, and service principals to your Databricks account.  For detailed step-by-step instructions, please refer to the "Overview of Unity Catalog setup" section in the document.	0	1	1
no_reaction	https://docs.databricks.com/marketplace/index.html	What are the key concepts in the databricks marketplace	What is Databricks Marketplace? =================================      Preview       This feature is in  [Private Preview](../release-notes/release-types.html)  . To try it, reach out to your Databricks contact.        This article introduces Databricks Marketplace, an open forum for exchanging data products. Databricks Marketplace takes advantage of  [Delta Sharing](../data-sharing/index.html)  to give data providers the tools to share data products securely and data consumers the power to explore and expand their access to the data and data services they need.          How do consumers use Databricks Marketplace? ----------------------------------------------     To find a data product you want on the Databricks Marketplace, simply browse or search provider listings and request access to the data products that you want. Some data products are available immediately, as soon as you request them and agree to terms. Others might require provider approval and transaction completion using provider interfaces. In either case, the Delta Sharing protocol that powers the Marketplace ensures that you can access shared data securely, regardless of whether you use Databricks to work with your data.      ###   Get started accessing data products     To access data products on Databricks Marketplace, you must have a Databricks account and workspace. To learn how to get started, see  [Access data products in Databricks Marketplace](get-started-consumer.html)  .          How do providers use Databricks Marketplace? ----------------------------------------------     Databricks Marketplace gives data providers a secure platform for sharing data products that data scientists and analysts need to help their organizations succeed. Databricks Marketplace uses Delta Sharing to provide security and control over your shared data. You can share public data, free sample data, and commercialized data offerings. In addition to datasets, you can also share Databricks notebooks and other content to demonstrate use cases and show customers how to take full advantage of your data products.      ###   Get started listing data products     To list your data products on Databricks Marketplace, you must have a Databricks account and workspace. To learn how to get started, see  [List your data product in Databricks Marketplace](get-started-provider.html)  .	The key concepts in the Databricks Marketplace are an open forum for exchanging data products, Delta Sharing protocol for secure data sharing, data providers sharing data products, and data consumers accessing data products by browsing or searching provider listings and requesting access.	1	1	1
no_reaction	https://kb.databricks.com/jobs/job-fails-with-spark-shuffle-fetchfailedexception-error	What are some reasons that a large shuffle might run out of memory?	Title: Job fails with ExecutorLostFailure due to “Out of memory” error  Description: Resolve executor failures where the root cause is due to the executor running out of memory..  Answer: Problem Job fails with an ExecutorLostFailure error message. ExecutorLostFailure (executor &lt;1&gt; exited caused by one of the running tasks) Reason: Executor heartbeat timed out after &lt;148564&gt; ms Cause The ExecutorLostFailure error message means one of the executors in the Apache Spark cluster has been lost. This is a generic error message which can have more than one root cause. In this article, we will look how to resolve issues when the root cause is due to the executor running out of memory  Let's say your executor has too much data to process and the amount of memory available in the executor is not sufficient to process the amount of data, then this issue could occur. For e.g. if the executor in your cluster has 24GB capacity and if the cumulative amount of the data size corresponding to all the tasks that are getting executed on that executor is greater than 24GB, then this issue could occur How do you determine if OOM is the reason for the executor getting lost?  Open the Spark UI. Click Stages. Click Failed stages. Click the description that corresponds to the failed stage. Review the bottom of the stage details page. Sort the list of tasks on the error column. The error messages describe why a specific task failed. If you see an error message that says out of memory, or a similar error like java.lang.OutOfMemoryError it means the task failed because the executor ran out of memory. Solution When an executor is failing due to running out of memory, you should review the following items. Is there a data skew?  Check whether the data is equally distributed across executors, or if there is any skew in the data. You can find this by checking the stage summary table on the stage details page of the Spark UI. If there is data skew and if this is the only executor that has more data in it, you need to resolve the skew to prevent the executor from running out of memory.  In most cases Adaptive Query Execution (AQE) automatically detects data skew and resolves the issue. However, there are some edge cases where AQE may not detect data skew correctly. Please review Why didn’t AQE detect my data skew? (AWS | Azure | GCP) for more information. If you are having trouble resolving data skew, you can try increasing the number of partitions or by explicitly mentioning the skew hints as explained in the How to specify skew hints in dataset and DataFrame-based join commands article. A partition is considered skewed when both (partition size &gt; skewedPartitionFactor * median partition size) and (partition size &gt; skewedPartitionThresholdInBytes) are true. For example, given a median partition size of 200 MB, if any partition exceeds 1 GB (200 MB * 5 (five is the default skewedPartitionFactor value)), it is considered skewed. Under this example, if you have a partition size of 900 MB it wouldn't be considered as skewed with the default settings. Now say your application code does a lot of transformations on the data (like explode, cartesian join, etc.). If you are performing a high number of transformations, you can overwhelm the executor, even if the partition isn't normally considered skewed. Using our example defaults, you may find that a 900 MB partition is too much to successfully process. If that is the case, you should reduce the skewedPartitionFactor value. By reducing this value to 4, the system then considers any partition over 800 MB as skewed and automatically assigns the appropriate skew hints. Please review the AQE documentation on dynamically handling skew join (AWS | Azure | GCP) for more information. Is the executor capable enough? If data is equally distributed across all executors and you still see out of memory errors, the executor does not have enough resources to handle the load you are trying to run. Increase horizontally by increasing the number of workers and/or increase vertically by selecting a Worker type with more memory when creating your clusters. Is it a properly configured streaming job? If there is no apparent data skew, but the executor is still getting too much data to process, you should use maxFilesPerTrigger and/or the trigger frequency settings to reduce the amount of data that is processed at any one time. Reducing the load on the executors also helps reduce the memory requirement, at the expense of slightly higher latency. In exchange for the increase in latency, the streaming job processed streaming events in a more controlled manner. A steady flow of events is reliably processed with every micro batch. Please review the Optimize streaming transactions with .trigger article for more information. You should also review the Spark Structured Streaming Programming Guide documentation on input sources and triggers.  If you want to increase the speed of the processing, you need to increase the number of executors in your cluster. You can also repartition the input streaming DataFrame, so the number of tasks is less than or equal to the number of cores in the cluster.	One reason that a large shuffle might run out of memory is if the executor in the Apache Spark cluster has too much data to process and the amount of memory available in the executor is not sufficient to process the amount of data. Another reason could be if there is data skew and if this is the only executor that has more data in it, you need to resolve the skew to prevent the executor from running out of memory. Additionally, if data is equally distributed across all executors and you still see out of memory errors, the executor does not have enough resources to handle the load you are trying to run. Increasing horizontally by increasing the number of workers and/or increasing vertically by selecting a Worker type with more memory when creating your clusters can help in this case.	0	1	1
no_reaction	https://kb.databricks.com/data-sources/sql-access-control-error-when-using-snowflake-as-a-data-source	How do I connect from snowflake	    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format("snowflake")   .option("dbtable", table\_name)   .option("sfUrl", database\_host\_url)   .option("sfUser", username)   .option("sfPassword", password)   .option("sfDatabase", database\_name)   .option("sfSchema", schema\_name)   .option("sfWarehouse", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format("snowflake")  .option("dbtable", table\_name)  .option("sfUrl", database\_host\_url)  .option("sfUser", username)  .option("sfPassword", password)  .option("sfDatabase", database\_name)  .option("sfSchema", schema\_name)  .option("sfWarehouse", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         	To connect from Snowflake, you can configure a connection to Snowflake and then query data using the Snowflake connector provided in the Databricks Runtime. The connector supports reading and writing data from Snowflake. You can use the provided example syntax in Python, SQL, and Scala to query a Snowflake table in Databricks. See the "Using the Spark Connector" and "Setting Configuration Options for the Connector" sections in the Snowflake Connector for Spark documentation for more details and all configuration options.	1	1	1
no_reaction	https://kb.databricks.com/clusters/azure-ssh-cluster-driver-node	How do I ssh into a cluster?	Title: SSH to the cluster driver node  Description: How to SSH to the Apache Spark cluster driver node in an Azure virtual network  Answer: This article explains how to use SSH to connect to an Apache Spark driver node for advanced troubleshooting and installing custom software. Warning You can only use SSH if your workspace is deployed in an Azure Virtual Network (VNet) under your control. If your workspace is NOT VNet injected, the SSH option will not appear. Additionally, NPIP workspaces do not support SSH. Configure an Azure network security group The network security group associated with your VNet must allow SSH traffic. The default port for SSH is 2200. If you are using a custom port, you should make note of it before proceeding. You also have to identify a traffic source. This can be a single IP address, or it can be an IP range that represents your entire office. In the Azure portal, find the network security group. The network security group name can be found in the public subnet. Edit the inbound security rules to allow connections to the SSH port. In this example, we are using the default port. Info Make sure that your computer and office firewall rules allow you to send TCP traffic on the port you are using for SSH. If the SSH port is blocked at your computer or office firewall, you cannot connect to the Azure VNet via SSH. Generate SSH key pair Open a local terminal. Create an SSH key pair by running this command:ssh-keygen -t rsa -b 4096 -C Info You must provide the path to the directory where you want to save the public and private key. The public key is saved with the extension .pub. Configure a new cluster with your public key Copy the ENTIRE contents of the public key file. Open the cluster configuration page. Click Advanced Options. Click the SSH tab. Paste the ENTIRE contents of the public key into the Public key field. Continue with cluster configuration as normal. Configure an existing cluster with your public key If you have an existing cluster and did not provide the public key during cluster creation, you can inject the public key from a notebook. Open any notebook that is attached to the cluster. Copy the following code into the notebook, updating it with your public key as noted:%scala val publicKey = "&lt;put your public key here&gt;" def addAuthorizedPublicKey(key: String): Unit = {   val fw = new java.io.FileWriter("/home/ubuntu/.ssh/authorized_keys", /* append */ true)   fw.write("\n" + key)   fw.close() } addAuthorizedPublicKey(publicKey) Run the code block to inject the public key. SSH into the Spark driver Open the cluster configuration page. Click Advanced Options. Click the SSH tab. Note the Driver Hostname. Open a local terminal. Run the following command, replacing the hostname and private key file path:ssh ubuntu@&lt;hostname&gt; -p 2200 -i &lt;private-key-file-path&gt;	To SSH into an Apache Spark cluster driver node, you need to configure an Azure network security group to allow SSH traffic, generate an SSH key pair, and configure the cluster with your public key. Once configured, you can SSH into the Spark driver by opening the cluster configuration page, noting the Driver Hostname, and running the following command in a local terminal: ssh ubuntu@<hostname> -p 2200 -i <private-key-file-path>.	1	1	1
no_reaction	https://docs.databricks.com/spark/index.html	Why use Apache Spark on Databricks?	Apache Spark on Databricks ============================     This article describes the how Apache Spark is related to Databricks and the Databricks Lakehouse Platform.       Apache Spark is at the heart of the Databricks Lakehouse Platform and is the technology powering compute clusters and SQL warehouses on the platform. Databricks is an optimized platform for Apache Spark, providing an efficient and simple platform for running Apache Spark workloads.            What is the relationship of Apache Spark to Databricks? ---------------------------------------------------------     The Databricks company was founded by the orginal creators of Apache Spark. As an open source software project, Apache Spark has  [committers from many top companies](https://spark.apache.org/committers.html)   , including Databricks.       Databricks continues to develop and release features to Apache Spark. The Databricks Runtime includes additional optimizations and proprietary features that build upon and extend Apache Spark, including  [Photon](../runtime/photon.html)  , an optimized version of Apache Spark rewritten in C++.           How does Apache Spark work on Databricks? -------------------------------------------     Whenever you deploy a compute cluster or SQL warehouse on Databricks, Apache Spark is configured and deployed to virtual machines. You don’t need to worry about configuring or initializing a Spark context or Spark session, as these are managed for you by Databricks.           Can I use Databricks without using Apache Spark? --------------------------------------------------     Databricks supports a variety of workloads and includes a number of other open source libraries in the Databricks Runtime. Databricks SQL uses Apache Spark under the hood, but end users use standard SQL syntax to create and query database objects.       Databricks Runtime for Machine Learning is optimized for ML workloads, and many data scientists use primary open source libraries like TensorFlow and SciKit Learn while working on Databricks. You can use  [workflows](../workflows/index.html)  to schedule arbitrary workloads against compute resources deployed and managed by Databricks.           Why use Apache Spark on Databricks? -------------------------------------     The Databricks Lakehouse Platform provides a secure, collaborative environment for developing and deploying enterprise solutions that scale with your business. Databricks employees representative many of the most knowledgeable Apache Spark maintainers and users in the world, and the company continuously develops and releases new optimizations to ensure that user have access to the fastest environment for running Apache Spark.	You would use Apache Spark on Databricks because Databricks is an optimized platform for Apache Spark, providing an efficient and simple platform for running Apache Spark workloads. The Databricks Lakehouse Platform provides a secure, collaborative environment for developing and deploying enterprise solutions that scale with your business. Additionally, Databricks employees represent many of the most knowledgeable Apache Spark maintainers and users in the world, and the company continuously develops and releases new optimizations to ensure that users have access to the fastest environment for running Apache Spark.	1	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-alter-table.html	alter table change datatype of column	   DataType  [¶](#datatype "Permalink to this headline") =======================================================  *class*  `pyspark.sql.types.`  `DataType`  [[source]](../../../_modules/pyspark/sql/types.html#DataType) [¶](#pyspark.sql.types.DataType "Permalink to this definition")    Base class for data types.     Methods    |  [`fromInternal`](#pyspark.sql.types.DataType.fromInternal "pyspark.sql.types.DataType.fromInternal")  (obj)   |   Converts an internal SQL object into a native Python object.   | |  [`json`](#pyspark.sql.types.DataType.json "pyspark.sql.types.DataType.json")  ()   |   | |  [`jsonValue`](#pyspark.sql.types.DataType.jsonValue "pyspark.sql.types.DataType.jsonValue")  ()   |   | |  [`needConversion`](#pyspark.sql.types.DataType.needConversion "pyspark.sql.types.DataType.needConversion")  ()   |   Does this type needs conversion between Python object and internal SQL object.   | |  [`simpleString`](#pyspark.sql.types.DataType.simpleString "pyspark.sql.types.DataType.simpleString")  ()   |   | |  [`toInternal`](#pyspark.sql.types.DataType.toInternal "pyspark.sql.types.DataType.toInternal")  (obj)   |   Converts a Python object into an internal SQL object.   | |  [`typeName`](#pyspark.sql.types.DataType.typeName "pyspark.sql.types.DataType.typeName")  ()   |   |   Methods Documentation    `fromInternal`    (   *obj     :     Any*   )    → Any  [[source]](../../../_modules/pyspark/sql/types.html#DataType.fromInternal) [¶](#pyspark.sql.types.DataType.fromInternal "Permalink to this definition")    Converts an internal SQL object into a native Python object.    `json`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#DataType.json) [¶](#pyspark.sql.types.DataType.json "Permalink to this definition")   `jsonValue`    (     )    → Union    [    str    ,    Dict    [    str    ,    Any    ]     ]   [[source]](../../../_modules/pyspark/sql/types.html#DataType.jsonValue) [¶](#pyspark.sql.types.DataType.jsonValue "Permalink to this definition")   `needConversion`    (     )    → bool  [[source]](../../../_modules/pyspark/sql/types.html#DataType.needConversion) [¶](#pyspark.sql.types.DataType.needConversion "Permalink to this definition")    Does this type needs conversion between Python object and internal SQL object.     This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.    `simpleString`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#DataType.simpleString) [¶](#pyspark.sql.types.DataType.simpleString "Permalink to this definition")   `toInternal`    (   *obj     :     Any*   )    → Any  [[source]](../../../_modules/pyspark/sql/types.html#DataType.toInternal) [¶](#pyspark.sql.types.DataType.toInternal "Permalink to this definition")    Converts a Python object into an internal SQL object.    *classmethod*  `typeName`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#DataType.typeName) [¶](#pyspark.sql.types.DataType.typeName "Permalink to this definition")   [ByteType](pyspark.sql.types.ByteType.html "previous page")  [DateType](pyspark.sql.types.DateType.html "next page")   	The context you provided is about the `DataType` class in PySpark, which is the base class for data types. It provides several methods such as `fromInternal`, `toInternal`, `simpleString`, `json`, `jsonValue`, `needConversion`, and `typeName`. However, it doesn't provide any information about altering the table or changing the datatype of a column.	0	0	0
no_reaction	https://docs.databricks.com/serverless-compute/index.html	How can I create a serverless cluster on databricks?	Serverless compute ====================     With the serverless compute version of the Databricks platform architecture, the compute layer exists in your Databricks account rather than your AWS account.            Databricks SQL Serverless ---------------------------    [Databricks SQL Serverless](../sql/admin/index.html)  supports serverless compute. Admins can create serverless SQL warehouses (formerly SQL endpoints) that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute clusters in your Databricks account. Use them with Databricks SQL queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.       Databricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it’s connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.       If serverless SQL warehouses are enabled for your account, note the following:     * New SQL warehouses are serverless by default when you create them from the  [UI](../sql/admin/create-sql-warehouse.html#create-warehouse)  . New SQL warehouses are not serverless by default when you create them using the  [API](../sql/api/sql-endpoints.html#create-warehouse)  , which requires that you explicitly specify serverless. You can also create new pro or classic SQL warehouses using either method. For more information about warehouse type defaults, see  [What are the warehouse type defaults?](../sql/index.html#warehouse-type-defaults)  . * You can  [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](../sql/admin/create-sql-warehouse.html#convert-to-serverless)  or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic. * This feature only affects Databricks SQL. It does not affect how Databricks Runtime clusters work with notebooks and jobs in the Data Science & Engineering or Databricks Machine Learning workspace environments. Databricks Runtime clusters always run in the classic data plane in your AWS account. See  [Serverless quotas](#compare)  .     If your account needs updated  [terms of use](../sql/admin/serverless.html#accept-terms)  , workspace admins are prompted in the Databricks SQL UI.       If your workspace has an AWS instance profile,  [you might need to update the trust relationship](../sql/admin/serverless.html#aws-instance-profile-setup)  to support serverless compute, depending on how and when it was created.       For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .         Model Serving ---------------     Model Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your Databricks account. The <ServerlessCompute> resources run as Databricks AWS resources in what is known as the serverless data plane.       In contrast, the legacy model serving architecture is a single-node cluster that runs in your AWS account within the classic data plane.     * **Easy configuration and compute resource management**   : Databricks automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration. * **High availability and scalability**   : Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests. * **Dashboards**   : Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.     For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .       Before you can use serverless Model Serving, you must  [enable serverless compute for your account](../machine-learning/model-serving/migrate-model-serving.html)  .                   Serverless quotas -------------------     Serverless quotas are a safety measure for serverless compute. Serverless quotas restrict how many serverless compute resources a customer can have at any given time. The quota is enforced at the regional level for all workspaces in your account. Quotas are enforced only for  [serverless SQL warehouses](../sql/admin/serverless.html)  . See  [Serverless quotas](serverless-quotas.html)  .         Compare serverless compute to other Databricks architectures --------------------------------------------------------------     Databricks operates out of a  *control plane*   and a  *data plane*   :     * The control plane includes the backend services that Databricks manages in its own AWS account. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest. * The data plane is where data is processed by clusters of compute resources.     There are important differences between the  *classic data plane*   (the original Databricks platform architecture) and the  *serverless data plane*   :     * For a classic data plane, Databricks compute resources run in your AWS account. Clusters perform distributed data analysis using queries (in Databricks SQL) or notebooks (in the Data Science & Engineering or Databricks Machine Learning environments):      + New clusters are created within each workspace’s virtual network in the customer’s AWS account.  + A classic data plane has natural isolation because it runs in each customer’s own AWS account. * For a serverless data plane, Databricks compute resources run in a compute layer within your Databricks account:      + The serverless data plane is used for serverless SQL warehouses and Model Serving. Enabling serverless compute does not change how Databricks Runtime clusters work in the Data Science & Engineering or Databricks Machine Learning environments.  + To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different Databricks customer workspaces and additional network controls between clusters of the same customer.     Databricks creates a serverless data plane in the same AWS region as your workspace’s classic data plane.       Worker nodes are private, which means they do not have public IP addresses.       For communication between the Databricks control plane and the serverless data plane:     * For Databricks SQL Serverless, the communication uses private connectivity. * For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.     When reading or writing to AWS S3 buckets in the same region as your workspace,  [serverless SQL warehouses](../sql/admin/serverless.html)  now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace’s root S3 bucket in your AWS account and to other S3 data sources in the same region.       The following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.      ![Compare classic and serverless data plane for Databricks SQL](../_images/serverless-compute-data-plane-comparison-dbsql-aws.png)   ![Compare classic and serverless data plane for Model Serving](../_images/serverless-compute-data-plane-comparison-models-aws.png)    For more information about secure cluster connectivity, which is mentioned in the diagram, see  [Secure cluster connectivity](../security/network/secure-cluster-connectivity.html)  .       The table below summarizes differences between serverless compute and the classic data plane architecture of Databricks, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about serverless compute security, or if you have questions about items in this table, contact your Databricks representative.               |   Item   |   Serverless data plane (AWS only)   |   Classic data plane (AWS and Azure)   | | --- | --- | --- | |  **Location of control plane resources**   |   Databricks cloud account   |   Databricks cloud account   | |  **Location of data plane compute resources**   |   Serverless data plane (VPC in the Databricks AWS account)   |   Classic data plane (VPC in the customer’s cloud provider account)   | |  **Data plane compute resources**   |   Databricks-managed Kubernetes (EKS) clusters   |   Databricks-managed standalone VMs   | |  **Customer access to data plane**   |   Access through Databricks control plane   | * **AWS**   : Direct access in customer’s AWS account. Additional indirect access through Databricks control plane. * **Azure**   : Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.  | |  **Who pays for unassigned VMs for Databricks SQL?**   |   Databricks   |   Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.   | |  **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**   |   Customer pays based on DBUs until Auto Stop stops the SQL warehouse.   |   Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.   | |  **Virtual private network (VPC) for data plane**   |   VPC in the customer’s Databricks account, with network boundaries between workspaces and between clusters.   | * **AWS**   : Exclusive, the VPC is in the customer’s account. * **Azure**   : Exclusive, the VNet is in the customer’s account.  | |  **OS image**   |   Databricks-modified cloud-managed Amazon-linux2   |   Databricks-managed Ubuntu or CentOS   | |  **Technology that manages default egress from the VPC**   |   Databricks-created AWS internet gateway   |   Default internet gateway or load balancer provided by the cloud   | |  **Customize VPC and firewall settings**   |   No   |   Yes   | |  **Customize CIDR ranges**   |   No   |   Yes   | |  **Public IPs**   |   No   |   Secure cluster connectivity  * When disabled, one public IP for each VM. * When enabled (the default), no public IPs for VMs.  | |  **Container-level network isolation for Databricks Runtime clusters**   |   Uses Kubernetes network policy   |   Uses Databricks-managed  `iptable`  rules   | |  **VM-level network isolation for Databricks Runtime clusters**   |   Security group isolation   |   Security group and isolation of VPC (AWS) or VNet (Azure)   | |  **VM isolation**   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   | |  **Communication between control plane and data plane**   |   For Databricks SQL Serverless, communication uses private connectivity. For Model Serving, communication uses direct mTLS encrypted communication with the connection initiated from the control plane with access limited to control plane IP addresses.   |   Secure cluster connectivity  * When enabled (the default for AWS E2 and Azure): Individual VMs connect to the SCC relay in the control plane during cluster creation * When disabled: Control plane connects to individual VMs using public IPs.  | |  **Credential for initial deployment**   |   Databricks internal IAM roles   | * **AWS**   : IAM roles provided by customers. * **Azure**   : None required.  | |  **Credential for regular data plane operations**   |   Databricks invokes  `sts:AssumeRole`  on customer-provided IAM role.   | * **AWS**   : VMs run with instance profiles that are provided by	To create a serverless cluster on Databricks, you can use the serverless compute version of the Databricks platform architecture, where the compute layer exists in your Databricks account rather than your AWS account. Databricks SQL Serverless supports serverless compute, and admins can create serverless SQL warehouses that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute clusters in your Databricks account. You can create new SQL warehouses from the UI, which are serverless by default, or from the API, which requires that you explicitly specify serverless. For more information, you can refer to the Databricks documentation on serverless compute.	1	1	1
no_reaction	https://docs.databricks.com/introduction/index.html	Tell me about databricks	What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used to process, store, clean, share, analyze, model, and monetize datasets with solutions from BI to machine learning. The Databricks workspace provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks can be interacted with programmatically with REST API, CLI, and Terraform. It works with AWS by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control. Databricks has a variety of use cases, including building an enterprise data lakehouse, ETL and data engineering, machine learning, AI, and data science, data warehousing, analytics, and BI, data governance and secure data sharing, and DevOps, CI/CD, and task orchestration.	0	1	1
no_reaction	https://docs.databricks.com/dev-tools/cli/index.html	configure datatbricks cli	Use the command line ======================     Databricks provides the following command-line tools.          |   Name   |   Use this tool when you want to…   | | --- | --- | |  [Databricks CLI setup & documentation](cli/index.html)  |   Use the command line to work with Databricks workspace assets such as cluster policies, clusters, file systems, groups, pools, jobs, libraries, runs, secrets, and tokens.   | |  [Databricks SQL CLI](databricks-sql-cli.html)  |   Use the command line to run SQL commands and scripts on a Databricks SQL warehouse.   |	To configure Databricks CLI, you can follow the setup and documentation provided in the Databricks CLI setup & documentation tool. This tool allows you to work with Databricks workspace assets such as cluster policies, clusters, file systems, groups, pools, jobs, libraries, runs, secrets, and tokens through the command line.	1	1	1
no_reaction	https://kb.databricks.com/clusters/cluster-failed-launch	what does CLOUD_PROVIDER_LAUNCH_FAILURE mean	Title: Cluster failed to launch  Description: Learn how to resolve <Databricks> cluster launch failures.  Answer: This article describes several scenarios in which a cluster fails to launch, and provides troubleshooting steps for each scenario based on error messages found in logs. Cluster timeout Error messages: Driver failed to start in time INTERNAL_ERROR: The Spark driver failed to start within 300 seconds Cluster failed to be healthy within 200 seconds Cause The cluster can fail to launch if it has a connection to an external Hive metastore and it tries to download all the Hive metastore libraries from a Maven repo. A cluster downloads almost 200 JAR files, including dependencies. If the Databricks cluster manager cannot confirm that the driver is ready within 5 minutes, then cluster launch fails. This can occur because JAR downloading is taking too much time. Solution Store the Hive libraries in DBFS and access them locally from the DBFS location. See Spark Options. Global or cluster-specific init scripts Error message: The cluster could not be started in 50 minutes. Cause: Timed out with exception after &lt;xxx&gt; attempts Cause Init scripts that run during the cluster spin-up stage send an RPC (remote procedure call) to each worker machine to run the scripts locally. All RPCs must return their status before the process continues. If any RPC hits an issue and doesn’t respond back (due to a transient networking issue, for example), then the 1-hour timeout can be hit, causing the cluster setup job to fail. Solution Use a cluster-scoped init script instead of global or cluster-named init scripts. With cluster-scoped init scripts, Databricks does not use synchronous blocking of RPCs to fetch init script execution status. Too many libraries installed in cluster UI Error message: Library installation timed out after 1800 seconds. Libraries that are not yet installed: Cause This is usually an intermittent problem due to network problems. Solution Usually you can fix this problem by re-running the job or restarting the cluster. The library installer is configured to time out after 3 minutes. While fetching and installing jars, a timeout can occur due to network problems. To mitigate this issue, you can download the libraries from Maven to a DBFS location and install it from there. Cloud provider limit Error message: Cluster terminated. Reason: Cloud Provider Limit Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Cloud provider shutdown Error message: Cluster terminated. Reason: Cloud Provider Shutdown Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Instances unreachable (Azure) Error message: Cluster terminated. Reason: Instances Unreachable An unexpected error was encountered while setting up the cluster. Please retry and contact Azure Databricks if the problem persists. Internal error message: Timeout while placing node Cause This error is usually returned by the cloud provider. Typically, it occurs when you have an Azure Databricks workspace deployed to your own virtual network (VNet) (as opposed to the default VNet created when you launch a new Azure Databricks workspace). If the virtual network where the workspace is deployed is already peered or has an ExpressRoute connection to on-premises resources, the virtual network cannot make an ssh connection to the cluster node when Azure Databricks is attempting to create a cluster. Solution Add a user-defined route (UDR) to give the Azure Databricks control plane ssh access to the cluster instances, Blob Storage instances, and artifact resources. This custom UDR allows outbound connections and does not interfere with cluster creation. For detailed UDR instructions, see Step 3: Create user-defined routes and associate them with your Azure Databricks virtual network subnets. For more VNet-related troubleshooting information, see Troubleshooting.	CLOUD_PROVIDER_LAUNCH_FAILURE is not explicitly mentioned in the context. However, the context describes several scenarios in which a cluster fails to launch, and provides troubleshooting steps for each scenario based on error messages found in logs. If the error message is related to cloud provider limit or cloud provider shutdown, the solution is to see the cloud provider error information in cluster unexpected termination.	1	0	0
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-tables.html	When should I use a managed table vs external table?	    Create tables ===============     This article introduces the concept of  *managed*   and  *external*   tables in Unity Catalog and describes how to create tables in Unity Catalog.        Note       When you create a table, be sure to reference a catalog that is governed by Unity Catalog. The catalog  `hive\_metastore`  appears in Data Explorer but is not considered governed by Unity Catalog. It is managed by your Databricks workspace’s Hive metastore. All other catalogs listed are governed by Unity Catalog.       You can use the Unity Catalog table upgrade interface to upgrade existing tables registered in the Hive metastore to Unity Catalog. See  [Upgrade tables and views to Unity Catalog](migrate.html)  .           Managed tables ----------------     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.           External tables -----------------     External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.       When you run  `DROP     TABLE`  on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a  `LOCATION`  path in your  `CREATE     TABLE`  statement. External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     To manage access to the underlying cloud storage for an external table, you must set up  [storage credentials and external locations](manage-external-locations-and-credentials.html)  .       To learn more, see  [Create an external table](#create-an-external-table)  .         Requirements --------------     You must have the  `CREATE     TABLE`  privilege on the schema in which you want to create the table, as well as the  `USE     SCHEMA`  privilege on the schema and the  `USE     CATALOG`  privilege on the parent catalog.       If you are creating an external table, see  [Create an external table](#create-an-external-table)  for additional requirements.         Create a managed table ------------------------     To create a managed table, run the following SQL command. You can also use the  [example notebook](#example-notebook-managed-table)  to create a table. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. This cannot be the  `hive\_metastore`  catalog that is created automatically for the Hive metastore associated with your Databricks workspace. * `<schema\_name>`  : The name of the schema. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column.       ``` CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> (  <column\_specification> );  ```       ``` spark.sql("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "   "("   " <column\_specification>"   ")")  ```       ``` library(SparkR)  sql(paste("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> ",  "(",  " <column\_specification>",  ")",  sep = ""))  ```       ``` spark.sql("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> " +  "(" +  " <column\_specification>" +  ")")  ```       You can also create a managed table by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of table full names by using  [databricks\_tables](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/tables)   .       For example, to create the table  `main.default.department`  and insert five rows into it:         ``` CREATE TABLE main.default.department (  deptcode INT,  deptname STRING,  location STRING );  INSERT INTO main.default.department VALUES  (10, 'FINANCE', 'EDINBURGH'),  (20, 'SOFTWARE', 'PADDINGTON'),  (30, 'SALES', 'MAIDSTONE'),  (40, 'MARKETING', 'DARLINGTON'),  (50, 'ADMIN', 'BIRMINGHAM');  ```       ``` spark.sql("CREATE TABLE main.default.department "   "("   " deptcode INT,"   " deptname STRING,"   " location STRING"   ")"   "INSERT INTO main.default.department VALUES "   " (10, 'FINANCE', 'EDINBURGH'),"   " (20, 'SOFTWARE', 'PADDINGTON'),"   " (30, 'SALES', 'MAIDSTONE'),"   " (40, 'MARKETING', 'DARLINGTON'),"   " (50, 'ADMIN', 'BIRMINGHAM')")  ```       ``` library(SparkR)  sql(paste("CREATE TABLE main.default.department ",  "(",  " deptcode INT,",  " deptname STRING,",  " location STRING",  ")",  "INSERT INTO main.default.department VALUES ",  " (10, 'FINANCE', 'EDINBURGH'),",  " (20, 'SOFTWARE', 'PADDINGTON'),",  " (30, 'SALES', 'MAIDSTONE'),",  " (40, 'MARKETING', 'DARLINGTON'),",  " (50, 'ADMIN', 'BIRMINGHAM')",  sep = ""))  ```       ``` spark.sql("CREATE TABLE main.default.department " +  "(" +  " deptcode INT," +  " deptname STRING," +  " location STRING" +  ")" +  "INSERT INTO main.default.department VALUES " +  " (10, 'FINANCE', 'EDINBURGH')," +  " (20, 'SOFTWARE', 'PADDINGTON')," +  " (30, 'SALES', 'MAIDSTONE')," +  " (40, 'MARKETING', 'DARLINGTON')," +  " (50, 'ADMIN', 'BIRMINGHAM')")  ```        ###   Example notebooks for creating managed tables     You can use the following example notebooks to create a catalog, schema, and managed table, and to manage permissions on them.        ####   Create and manage a table in Unity Catalog with SQL     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-example-notebook.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Create and manage a table in Unity Catalog with Python     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-quickstart-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                   Drop a managed table ----------------------     You must be the table’s owner to drop a table. To drop a managed table, run the following SQL command:        ``` DROP TABLE IF EXISTS catalog\_name.schema\_name.table\_name;  ```      When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.         Create an external table --------------------------     The data in an external table is stored in a path on your cloud tenant. To work with external tables, Unity Catalog introduces two objects to access and work with external cloud storage:     * A  *storage credential*   contains an authentication method for accessing a cloud storage location. The storage credential does not contain a mapping to the path to which it grants access. Storage credentials are access-controlled to determine which users can use the credential. * An  *external location*   maps a storage credential with a cloud storage path to which it grants access. The external location grants access only to that cloud storage path and its contents. External locations are access-controlled to determine which users can use them. An external location is used automatically when your SQL command contains a  `LOCATION`  clause.      ###   Requirements     To create an external table, you must have:     * The  `CREATE     EXTERNAL     TABLE`  privilege on an external location or storage credential, that grants access to the  `LOCATION`  accessed by the external table. * The  `USE     SCHEMA`  permission on the table’s parent schema. * The  `USE     CATALOG`  permission on the table’s parent catalog. * The  `CREATE     TABLE`  permission on the table’s parent schema.     External locations and storage credentials are stored in the top level of the metastore, rather than in a catalog. To create a storage credential or an external location, you must be the metastore admin or an account-level admin. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .       ###   Create a table     Use one of the following command examples in a notebook or the SQL query editor to create an external table.       You can also use an  [example notebook](#example-notebook-external-table)  to create the storage credential, external location, and external table, and also manage permissions for them.       In the following examples, replace the placeholder values:     * `<catalog>`  : The name of the catalog that will contain the table. * `<schema>`  : The name of the schema that will contain the table. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column. * `<bucket\_path>`  : The path on your cloud tenant where the table will be created. * `<table\_directory>`  : A directory where the table will be created. Use a unique directory for each table.      Important       Once a table is created in a path, users can no longer directly access the files in that path from Databricks even if they have been given privileges on an external location or storage credential to do so. This is to ensure that users cannot circumvent access controls applied to tables by reading files from your cloud tenant directly.          ``` CREATE TABLE <catalog>.<schema>.<table\_name> (  <column\_specification> ) LOCATION 's3://<bucket\_path>/<table\_directory>';  ```       ``` spark.sql("CREATE TABLE <catalog>.<schema>.<table\_name> "   "("   " <column\_specification>"   ") "   "LOCATION 's	You should use a managed table when you want Unity Catalog to manage the lifecycle and file layout of the table. Managed tables are stored in the root storage location that you configure when you create a metastore. On the other hand, you should use an external table when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses. External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. When you run `DROP TABLE` on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner.	1	1	1
no_reaction	https://docs.databricks.com/dev-tools/python-api.html	should i use python api or cli	    Databricks CLI setup & documentation ======================================         The Databricks command-line interface (CLI) provides an easy-to-use interface to the Databricks platform. The open source project is hosted on  [GitHub](https://github.com/databricks/databricks-cli)   . The CLI is built on top of the  [Databricks REST API](../api/index.html)  and is organized into command groups based on primary endpoints.       You can use the Databricks CLI to do things such as:     * Provision compute resources in Databricks workspaces. * Run data processing and data analysis tasks. * List, import, and export notebooks and folders in workspaces.      Experimental       This CLI is under active development and is released as an  [Experimental](../../release-notes/release-types.html)  client. This means that interfaces are still subject to change.         Set up the CLI ----------------     This section lists CLI requirements and describes how to install and configure your environment to run the CLI.      ###   Requirements   * **Python 3**   - 3.6 and above * **Python 2**   - 2.7.9 and above        Important       On macOS, the default Python 2 installation does not implement the TLSv1\_2 protocol and running the CLI with this Python installation results in the error:  `AttributeError:     'module'     object     has     no     attribute     'PROTOCOL\_TLSv1\_2'`  . Use  [Homebrew](https://brew.sh/)   to install a version of Python that has  `ssl.PROTOCOL\_TLSv1\_2`  .     ###   Install the CLI     Run  `pip     install     databricks-cli`  using the appropriate version of  `pip`  for your Python installation:        ``` pip install databricks-cli  ```      ###   Update the CLI     Run  `pip     install     databricks-cli     --upgrade`  using the appropriate version of  `pip`  for your Python installation:        ``` pip install databricks-cli --upgrade  ```      To list the version of the CLI that is currently installed, run  `databricks     --version`  (or  `databricks     -v`  ):        ``` databricks --version  # Or...  databricks -v  ```        ###   Set up authentication      Note       As a security best practice, when authenticating with automated tools, systems, scripts, and apps, Databricks recommends you use access tokens belonging to  [service principals](../../administration-guide/users-groups/service-principals.html)  instead of workspace users. To create access tokens for service principals, see  [Manage access tokens for a service principal](../../administration-guide/users-groups/service-principals.html#personal-access-tokens)  .        Before you can run CLI commands, you must set up authentication. To authenticate to the CLI, you can use a  [Databricks personal access token](../auth.html#pat)  . A Databricks username and password are also supported but not recommended.       To configure the CLI to use a personal access token, run the following command:        ``` databricks configure --token  ```      The command begins by issuing the prompt:        ``` Databricks Host (should begin with https://):  ```      Enter your workspace URL, with the format  `https://<instance-name>.cloud.databricks.com`  . To get your workspace URL, see  [Workspace instance names, URLs, and IDs](../../workspace/workspace-details.html#workspace-url)  .       The command continues by issuing the prompt to enter your personal access token:        ``` Token:  ```      After you complete the prompts, your access credentials are stored in the file  `~/.databrickscfg`  on Unix, Linux, or macOS, or  `%USERPROFILE%\.databrickscfg`  on Windows. The file contains a default profile entry:        ``` [DEFAULT] host = <workspace-URL> token = <personal-access-token>  ```      For CLI 0.8.1 and above, you can change the path of this file by setting the environment variable  `DATABRICKS\_CONFIG\_FILE`  .          ``` export DATABRICKS\_CONFIG\_FILE=<path-to-file>  ```         ``` setx DATABRICKS_CONFIG_FILE "<path-to-file>" /M  ```        Although not recommended, it is possible to use your Databricks username and password instead of a Databricks personal access token to authenticate. Run  `databricks     configure`  and follow the prompts. The  `.databrickscfg`  file contains a default profile entry:        ``` [DEFAULT] host = <workspace-URL> username = <username> password = <password>  ```       Important       Beginning with CLI 0.17.2, the CLI does not work with a  [.netrc file](../api/latest/authentication.html#netrc)  . You can have a  `.netrc`  file in your environment for other purposes, but the CLI will not use that  `.netrc`  file.        CLI 0.8.0 and above supports the following Databricks  [environment variables](../auth.html#env-vars)  :     * `DATABRICKS\_HOST` * `DATABRICKS\_USERNAME` * `DATABRICKS\_PASSWORD` * `DATABRICKS\_TOKEN`     An environment variable setting takes precedence over the setting in the configuration file.      ####   Test your authentication setup     To check whether you set up authentication correctly, you can run a command such as the following, replacing  `<someone@example.com>`  with your Databricks workspace username:        ``` databricks workspace ls /Users/<someone@example.com>  ```      If successful, this command  [lists the objects in the specified workspace path](workspace-cli.html#list-objects-in-a-workspace)  .          ###   Connection profiles     The Databricks CLI configuration supports multiple connection profiles. The same installation of Databricks CLI can be used to make API calls on multiple Databricks workspaces.       To add a connection profile, specify a unique name for the profile:        ``` databricks configure --token --profile <profile-name>  ```      The  `.databrickscfg`  file contains a corresponding profile entry:        ``` [<profile-name>] host = <workspace-URL> token = <token>  ```      To use the connection profile:        ``` databricks <group> <command> --profile <profile-name>  ```      If  `--profile     <profile-name>`  is not specified, the default profile is used. If a default profile is not found, you are prompted to configure the CLI with a default profile.       ###   Test your connection profiles     To check whether you set up your connection profiles correctly, you can run a command such as the following, replacing  `<someone@example.com>`  with your Databricks workspace username and  `<DEFAULT>`  with one of your connection profile names:        ``` databricks workspace ls /Users/<someone@example.com> --profile <DEFAULT>  ```      If successful, this command  [lists the objects in the specified workspace path](workspace-cli.html#list-objects-in-a-workspace)  in the workspace for the specified connection profile. Run this command for each connection profile that you want to test.         ###   Alias command groups     Sometimes it can be inconvenient to prefix each CLI invocation with the name of a command group, for example  `databricks     workspace     ls`  . To make the CLI easier to use, you can alias command groups to shorter commands. For example, to shorten  `databricks     workspace     ls`  to  `dw     ls`  in the Bourne again shell, you can add  `alias     dw="databricks     workspace"`  to the appropriate bash profile. Typically, this file is located at  `~/.bash\_profile`  .        Tip       Databricks already aliases  `databricks     fs`  to  `dbfs`  ;  `databricks     fs     ls`  and  `dbfs     ls`  are equivalent.           Use the CLI -------------     This section shows you how to get CLI help, parse CLI output, and invoke commands in each command group.      ###   Display CLI command group help     You list the subcommands for any command group by running  `databricks     <group>     --help`  (or  `databricks     <group>     -h`  ). For example, you list the DBFS CLI subcommands by running  `databricks     fs     -h`  :        ``` databricks fs -h  ```      ###   Display CLI subcommand help     You list the help for a subcommand by running  `databricks     <group>     <subcommand>     --help`  (or  `databricks     <group>     <subcommand>     -h`  ). For example, you list the help for the DBFS copy files subcommand by running  `databricks     fs     cp     -h`  :        ``` databricks fs cp -h  ```        ###   Use  `jq`  to parse CLI output     Some Databricks CLI commands output the JSON response from the API endpoint. Sometimes it can be useful to parse out parts of the JSON to pipe into other commands. For example, to copy a job definition, you must take the  `settings`  field of a  `databricks     jobs     get`  command and use that as an argument to the  `databricks     jobs     create`  command. In these cases, we recommend you to use the utility  `jq`  .       For example, the following command prints the settings of the job with the ID of 233.        ``` databricks jobs list --output JSON | jq '.jobs[] | select(.job\_id == 233) | .settings'  ```       ``` {  "name": "Quickstart",  "new\_cluster": {  "spark\_version": "7.5.x-scala2.12",  "spark\_env\_vars": {  "PYSPARK\_PYTHON": "/databricks/python3/bin/python3"  },  "num\_workers": 8,  ...  },  "email\_notifications": {},  "timeout\_seconds": 0,  "notebook\_task": {  "notebook\_path": "/Quickstart"  },  "max\_concurrent\_runs": 1 }  ```      As another example, the following command prints the names and IDs of all available clusters in the workspace:        ``` databricks clusters list --output JSON | jq '[ .clusters[] | { name: .cluster\_name, id: .cluster\_id } ]'  ```       ``` [  {  "name": "My Cluster 1",  "id": "1234-5	Based on the context, you can use both Python API and CLI to interact with the Databricks platform. The CLI is built on top of the Databricks REST API and provides an easy-to-use interface to the Databricks platform. You can use the Databricks CLI to provision compute resources in Databricks workspaces, run data processing and data analysis tasks, list, import, and export notebooks and folders in workspaces. On the other hand, the Python API provides a programmatic way to interact with the Databricks platform using Python code. The choice between using the Python API or CLI depends on your use case and personal preference.	1	1	1
no_reaction	https://docs.databricks.com/dev-tools/cli/dbfs-cli.html	how to delete file on dbfs?	DBFS CLI ==========     You run Databricks DBFS CLI subcommands appending them to  `databricks     fs`  (or the alias  `dbfs`  ), prefixing all DBFS paths with  `dbfs:/`  . These subcommands call the  [DBFS API 2.0](../api/latest/dbfs.html)  .        ``` databricks fs -h  ```       ``` Usage: databricks fs [OPTIONS] COMMAND [ARGS]...    Utility to interact with DBFS. DBFS paths are all prefixed   with dbfs:/. Local paths can be absolute or local.  Options:   -v, --version   -h, --help     Show this message and exit.  Commands:   cat        Shows the contents of a file. Does not work for directories.   configure   cp         Copies files to and from DBFS.     Options:       -r, --recursive       --overwrite     Overwrites files that exist already.   ls         Lists files in DBFS.     Options:       --absolute      Displays absolute paths.       -l              Displays full information including size and file type.   mkdirs     Makes directories in DBFS.   mv         Moves a file between two DBFS paths.   rm         Removes files from DBFS.     Options:       -r, --recursive  ```      For operations that list, move, or delete more than 10k files, we strongly discourage using the DBFS CLI.     * The  `list`  operation (  `databricks     fs     ls`  ) will time out after approximately 60s. * The  `move`  operation (  `databricks     fs     mv`  ) will time out after approximately 60s, potentially resulting in partially moved data. * The  `delete`  operation (  `databricks     fs     rm`  ) will incrementally delete batches of files.     We recommend that you perform such operations in the context of a cluster, using  [File system utility (dbutils.fs)](../databricks-utils.html#dbutils-fs)  .  `dbutils.fs`  covers the functional scope of the DBFS REST API, but from notebooks. Running such operations using notebooks provides better control, such as selective deletes, manageability, and the possibility to automate periodic jobs.        List the contents of a file -----------------------------     To display usage documentation, run  `databricks     fs     cat     --help`  .        ``` databricks fs cat dbfs:/tmp/my-file.txt  ```       ``` Apache Spark is awesome!  ```        Copy a file -------------     To display usage documentation, run  `databricks     fs     cp     --help`  .        ``` databricks fs cp dbfs:/tmp/your_file.txt dbfs:/parent/child/grandchild/my_file.txt --overwrite  ```      On success, this command displays nothing.         List information about files and directories ----------------------------------------------     To display usage documentation, run  `databricks     fs     ls     --help`  .        ``` databricks fs ls dbfs:/tmp --absolute -l  ```       ``` file 42408084 dbfs:/tmp/LoanStats.csv 1590005159000 file 40 dbfs:/tmp/file\_b.txt 1603991038000 dir 0 dbfs:/tmp/hive 0 dir 0 dbfs:/tmp/mlflow 0 file 385 dbfs:/tmp/multi-line.json 1597770632000 dir 0 dbfs:/tmp/new 0 dir 0 dbfs:/tmp/parent 0 file 243 dbfs:/tmp/test.json 1597770628000 file 40 dbfs:/tmp/test\_dbfs.txt 1603989162000  ```        Create a directory --------------------     To display usage documentation, run  `databricks     fs     mkdirs     --help`  .        ``` databricks fs mkdirs dbfs:/tmp/new-dir  ```      On success, this command displays nothing.         Move a file -------------     To display usage documentation, run  `databricks     fs     mv     --help`  .        ``` databricks fs mv dbfs:/tmp/my-file.txt dbfs:/parent/child/grandchild/my-file.txt  ```      On success, this command displays nothing.         Delete a file ---------------     To display usage documentation, run  `databricks     fs     rm     --help`  .        ``` databricks fs rm dbfs:/tmp/parent/child/grandchild/my-file.txt  ```       ``` Delete finished successfully.  ```	To delete a file on DBFS, you can use the `databricks fs rm` command followed by the path of the file you want to delete, prefixed with `dbfs:/`. For example, to delete a file named `my_file.txt` located in the `/tmp` directory, you can run the command `databricks fs rm dbfs:/tmp/my_file.txt`.	1	1	1
no_reaction	https://docs.databricks.com/ingestion/auto-loader/unity-catalog.html	What is the code to ingest json using autoloader?	      Databricks Runtime 7.6 (Unsupported) ======================================     Databricks released this image in February 2021.       The following release notes provide information about Databricks Runtime 7.6, powered by Apache Spark 3.0.       For help with migration from Databricks Runtime 6.x, see  [Databricks Runtime 7.x migration guide](7.x-migration.html)  .        New features --------------    ###   Better JSON schema evolution handling in Auto Loader     When loading JSON data, the default behavior is that fields that are not part of the original schema are silently dropped. Now with Auto Loader, you can set the JSON option  `unparsedDataColumn`  to specify the name of a column, which will be used to store all the new fields as a JSON string. For example, by setting      `.option("unparsedDataColumn",     "\_unparsed\_data")`      the result will have a column  `\_unparsed\_data`  , where new fields are stored as a JSON string. These new fields can be parsed later for further processing. If there are no new fields, the column value is  `null`  .       ###   Support for reading overwritten files in Auto Loader     You can now configure streams to allow input directory file changes to overwrite existing data. Set  `.option("cloudFiles.allowOverwrites",     "true")`  . For details, see  [What is Auto Loader?](../../ingestion/auto-loader/index.html)  .          Improvements --------------    ###   Better multi-dimensional clustering and data skipping performance with  `OPTIMIZE     ZORDER`     The  `OPTIMIZE     ZORDER`  operation now uses  [Hilbert space-filling curves](https://wikipedia.org/wiki/Hilbert_curve)   by default. This approach provides better clustering characteristics than Z-order in higher dimensions. For Delta tables using  `OPTIMIZE     ZORDER`  with many columns, Hilbert curves can speed up read queries by skipping more data than Z-order. For details, see  [Data skipping with Z-order indexes for Delta Lake](../../delta/data-skipping.html)  .       ###   Performance improvements for some  `MERGE`  operations with only  `MATCHED`  clauses     You may get better performance in  `MERGE`  operations that have only  `MATCHED`  clauses with clause conditions.       ###   Auto-tuning file sizes for  `MERGE`  -heavy Delta tables (Public Preview)     Delta tables that undergo repeated  `MERGE`  operations (for example, continuous upserts of row-level change data) repeatedly rewrite files to update data. Such tables can benefit from having smaller files than those produced by default by data layout operations like  `OPTIMIZE`  , Optimized Write, and Auto Compaction. With Databricks Runtime 7.6, you can automatically detect  `MERGE`  -heavy tables and tune file sizes. Contact Databricks support or your Databricks representative for more information on how to enable this feature.         ###   Multi-cluster writes in SparkR     Multi-cluster writes now support SparkR. You can write to a Delta table using SparkR from multiple clusters in  *the same workspace*   with full ACID guarantees.          Library upgrades ------------------   * Upgraded Python libraries:      + certifi upgraded from 2020.6.20 to 2020.12.5.  + koalas upgraded from 1.4.0 to 1.5.0.  + python-apt upgraded from 1.6.5+ubuntu0.3 to 1.6.5+ubuntu0.4 * Upgraded R library:      + nlme from 3.1-150 to 3.1-151. * Upgraded Java library:      + azure-data-lake-store-sdk upgraded from 2.3.8 to 2.3.9. * OpenJDK 8 build upgraded to Zulu 8.50.0.51-CA-linux64 (build 1.8.0\_275-b01).           Apache Spark --------------     Databricks Runtime 7.6 includes Apache Spark 3.0.1. This release includes all Spark fixes and improvements included in  [Databricks Runtime 7.5 (Unsupported)](7.5.html)  , as well as the following additional bug fixes and improvements made to Spark:     * [[SPARK-33740]](https://issues.apache.org/jira/browse/SPARK-33740)   [SQL]hadoop configs in hive-site.xml can overrides pre-existing hadoop ones * [[SPARK-33822]](https://issues.apache.org/jira/browse/SPARK-33822)   [SQL] Use the  `CastSupport.cast`  method in HashJoin * [[SPARK-33398]](https://issues.apache.org/jira/browse/SPARK-33398)   Fix loading tree models prior to Spark 3.0 * [[SPARK-33029]](https://issues.apache.org/jira/browse/SPARK-33029)   [CORE][WEBUI] Fix to UI executor page incorrectly marking the driver as blocklisted * [[SPARK-34012]](https://issues.apache.org/jira/browse/SPARK-34012)   [SQL] Keep behavior consistent when conf  `spark.sqllegacy.parser.havingWithoutGroupByAsWhere`  is true with migration guide * [[SPARK-33635]](https://issues.apache.org/jira/browse/SPARK-33635)   [SS] Adjust the order of check in KafkaTokenUtil.needTokenUpdate to remedy perf regression * [[SPARK-33900]](https://issues.apache.org/jira/browse/SPARK-33900)   [WEBUI] Show shuffle read size / records correctly when only remotebytesread is available * [[SPARK-33593]](https://issues.apache.org/jira/browse/SPARK-33593)   [SQL] Vector reader got incorrect data with binary partition value * [[SPARK-31170]](https://issues.apache.org/jira/browse/SPARK-31170)   [SQL] Spark SQL Cli should respect hive-site.xml and spark.sql.warehouse.dir * [[SPARK-33853]](https://issues.apache.org/jira/browse/SPARK-33853)   [SQL] EXPLAIN CODEGEN and BenchmarkQueryTest don’t show subquery code * [[SPARK-33786]](https://issues.apache.org/jira/browse/SPARK-33786)   [SQL] The storage level for a cache should be respected when a table name is altered. * [[SPARK-33841]](https://issues.apache.org/jira/browse/SPARK-33841)   [CORE] Fix issue with jobs disappearing intermittently from the SHS under high load * [[SPARK-33480]](https://issues.apache.org/jira/browse/SPARK-33480)   [SQL] do not expose user data in error message * [[SPARK-33756]](https://issues.apache.org/jira/browse/SPARK-33756)   [SQL] Make BytesToBytesMap’s MapIterator idempotent * [[SPARK-33774]](https://issues.apache.org/jira/browse/SPARK-33774)   [UI][CORE]”Back to Master” returns 500 error in Standalone cluster * [[SPARK-33733]](https://issues.apache.org/jira/browse/SPARK-33733)   [SQL] PullOutNondeterministic should check and collect deterministic field * [[SPARK-33670]](https://issues.apache.org/jira/browse/SPARK-33670)   [SQL] Verify the partition provider is Hive in v1 SHOW TABLE EXTENDED * [[SPARK-33788]](https://issues.apache.org/jira/browse/SPARK-33788)   [SQL][3.1][2.4] Throw NoSuchPartitionsException from HiveExternalCatalog.dropPartitions() * [[SPARK-33819]](https://issues.apache.org/jira/browse/SPARK-33819)   [CORE] SingleFileEventLogFileReader/RollingEventLogFilesFileReader should be  `package     private` * [[SPARK-33742]](https://issues.apache.org/jira/browse/SPARK-33742)   [SQL] Throw PartitionsAlreadyExistException from HiveExternalCatalog.createPartitions() * [[SPARK-32110]](https://issues.apache.org/jira/browse/SPARK-32110)   [SQL] normalize special floating numbers in HyperLogLog++ * [[SPARK-33071]](https://issues.apache.org/jira/browse/SPARK-33071)  [[SPARK-33536]](https://issues.apache.org/jira/browse/SPARK-33536)   [SQL] Do not change Alias constructor to not break snowflake connector * [[SPARK-32680]](https://issues.apache.org/jira/browse/SPARK-32680)   [SQL] Don’t Preprocess V2 CTAS with Unresolved Query * [[SPARK-33520]](https://issues.apache.org/jira/browse/SPARK-33520)   [ML][PYSPARK] Cherry-pick: make CrossValidator/TrainValidateSplit/OneVsRest Reader/Writer support Python backend estimator/evaluator * [[SPARK-33592]](https://issues.apache.org/jira/browse/SPARK-33592)   [ML][PYTHON]Pyspark ML Validator params in estimatorParamMaps may be lost after saving and reloading * [[SPARK-33667]](https://issues.apache.org/jira/browse/SPARK-33667)   [SQL] Respect the  `spark.sql.caseSensitive`  config while resolving partition spec in v1  `SHOW     PARTITIONS` * [[SPARK-33141]](https://issues.apache.org/jira/browse/SPARK-33141)   [SQL] Capture SQL configs when creating permanent views * [[SPARK-33636]](https://issues.apache.org/jira/browse/SPARK-33636)   [PYTHON][ML] Add labelsArray to PySpark StringIndexer * [[SPARK-33629]](https://issues.apache.org/jira/browse/SPARK-33629)   [PYTHON]Make spark.buffer.size configuration visible on driver side * [[SPARK-33389]](https://issues.apache.org/jira/browse/SPARK-33389)   [SQL] Make internal classes of SparkSession always using active SQLConf * [[SPARK-33071]](https://issues.apache.org/jira/browse/SPARK-33071)  [[SPARK-33536]](https://issues.apache.org/jira/browse/SPARK-33536)   [SQL] Avoid changing dataset\_id of LogicalPlan in join() to not break DetectAmbiguousSelfJoin * [[SPARK-33551]](https://issues.apache.org/jira/browse/SPARK-33551)   [SQL] Do not use custom shuffle reader for repartition * [[SPARK-33432]](https://issues.apache.org/jira/browse/SPARK-33432)   [SQL] SQL parser should use active SQLConf * [[SPARK-33588]](https://issues.apache.org/jira/browse/SPARK-33588)   [SQL] Respect the  `spark.sql.caseSensitive`  config while resolving partition spec in v1  `SHOW     TABLE     EXTENDED` * [[SPARK-33440]](https://issues.apache.org/jira/browse/SPARK-33440)   [CORE] Use current timestamp with warning log in HadoopFSDelegationTokenProvider when the issue date for token is not set up properly * [[SPARK-33494]](https://issues.apache.	To ingest JSON using autoloader, you can set the JSON option `unparsedDataColumn` to specify the name of a column, which will be used to store all the new fields as a JSON string. For example, by setting `.option("unparsedDataColumn", "\_unparsed\_data")` the result will have a column `\_unparsed\_data`, where new fields are stored as a JSON string. These new fields can be parsed later for further processing.	0	1	1
no_reaction	https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-table.html	how to create a table in Python?	   pyspark.sql.Catalog.createTable  [¶](#pyspark-sql-catalog-createtable "Permalink to this headline") =====================================================================================================  `Catalog.`  `createTable`    (   *tableName     :     str*  ,  *path     :     Optional    [    str    ]     =     None*  ,  *source     :     Optional    [    str    ]     =     None*  ,  *schema     :     Optional    [   [pyspark.sql.types.StructType](pyspark.sql.types.StructType.html#pyspark.sql.types.StructType "pyspark.sql.types.StructType")    ]     =     None*  ,  *description     :     Optional    [    str    ]     =     None*  ,  *\*\*     options     :     str*   )    → pyspark.sql.dataframe.DataFrame  [[source]](../../../_modules/pyspark/sql/catalog.html#Catalog.createTable) [¶](#pyspark.sql.Catalog.createTable "Permalink to this definition")    Creates a table based on the dataset in a data source.     New in version 2.2.0.     Parameters    **tableName**    str     name of the table to create.     Changed in version 3.4.0:    Allow  `tableName`  to be qualified with catalog name.    **path**    str, optional     the path in which the data for this table exists. When  `path`  is specified, an external table is created from the data at the given path. Otherwise a managed table is created.    **source**    str, optional     the source of this table such as ‘parquet, ‘orc’, etc. If  `source`  is not specified, the default data source configured by  `spark.sql.sources.default`  will be used.    **schema**    class:    StructType    , optional     the schema for this table.    **description**    str, optional     the description of this table.     Changed in version 3.1.0:    Added the  `description`  parameter.    **\*\*options**    dict, optional     extra options to specify in the table.     Returns    [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame "pyspark.sql.DataFrame")   The DataFrame associated with the table.     Examples     Creating a managed table.    ``` >>> \_ = spark.catalog.createTable("tbl1", schema=spark.range(1).schema, source='parquet') >>> \_ = spark.sql("DROP TABLE tbl1")  ```   Creating an external table    ``` >>> import tempfile >>> with tempfile.TemporaryDirectory() as d: ...     \_ = spark.catalog.createTable( ...         "tbl2", schema=spark.range(1).schema, path=d, source='parquet') >>> \_ = spark.sql("DROP TABLE tbl2")  ```  [pyspark.sql.Catalog.createExternalTable](pyspark.sql.Catalog.createExternalTable.html "previous page")  [pyspark.sql.Catalog.currentCatalog](pyspark.sql.Catalog.currentCatalog.html "next page")   	To create a table in Python, you can use the `Catalog.createTable` method from the `pyspark.sql` module. This method allows you to create a table based on the dataset in a data source. You need to provide the name of the table to create, and optionally the path in which the data for this table exists, the source of this table such as ‘parquet, ‘orc’, etc., the schema for this table, the description of this table, and extra options to specify in the table.	1	1	1
no_reaction	https://docs.databricks.com/data/manage-permissions.html	how do I use SQL to revoke access to a catalog?	Title: Revoke all user privileges  Description: Use a regex and a series of for loops to revoke all privileges for a single user.  Answer: When user permissions are explicitly granted for individual tables and views, the selected user can access those tables and views even if they don’t have permission to access the underlying database. If you want to revoke a user’s access, you can do so with the REVOKE command. However, the REVOKE command is explicit, and is strictly scoped to the object specified in the command. For example: %sql REVOKE ALL PRIVILEGES ON DATABASE &lt;database-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;` REVOKE SELECT ON &lt;table-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;` If you want to revoke all privileges for a single user you can do it with a series of multiple commands, or you can use a regular expression and a series of for loops to automate the process. Example code This example code matches the &lt;search-string&gt; pattern to the database name and the table name and then revokes the user’s privileges. The search is recursive. %python from re import search databaseQuery = sqlContext.sql("show databases") databaseList = databaseQuery.collect() # This loop revokes at the database level. for db in databaseList:   listTables = sqlContext.sql("show tables from "+dbdatabaseName)   tableRows = listTables.collect()   if search(&lt;search-string&gt;, dbdatabaseName):     revokeDatabase=sqlContext.sql("REVOKE ALL PRIVILAGES ON DATABASE "+dbdatabaseName+" to `&lt;username&gt;`")     display(revokeDatabase)     print("Ran the REVOKE query on "+dbdatabaseName+" for &lt;username&gt;")   # This loop revokes at the table level.   for table in tableRows:     if search(&lt;search-string&gt;,tabletableName):       revokeCommand=sqlContext.sql("REVOKE SELECT ON "+tabledatabase+"."+tabletableName+" FROM `&lt;username&gt;`")       display(revokeCommand)       print("Revoked the SELECT permissions on "+tabledatabase+"."+tabletableName+" for &lt;username&gt;") Info These commands only work if you have enabled table access control for the cluster (AWS | Azure | GCP).	To revoke access to a catalog using SQL, you can use the REVOKE command. However, the REVOKE command is explicit and is strictly scoped to the object specified in the command. If you want to revoke all privileges for a single user, you can use a regular expression and a series of for loops to automate the process. The example code provided in the context matches the search string pattern to the database name and the table name and then revokes the user's privileges. These commands only work if you have enabled table access control for the cluster.	1	1	1
no_reaction	https://docs.databricks.com/structured-streaming/delta-lake.html	How can I do multiple writes to the same delta table in streaming?	    Table streaming reads and writes ==================================     Delta Lake is deeply integrated with  [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)   through  `readStream`  and  `writeStream`  . Delta Lake overcomes many of the limitations typically associated with streaming systems and files, including:     * Coalescing small files produced by low latency ingest * Maintaining “exactly-once” processing with more than one stream (or concurrent batch jobs) * Efficiently discovering which files are new when using files as the source for a stream        Delta table as a source -------------------------     When you load a Delta table as a stream source and use it in a streaming query, the query processes all of the data present in the table as well as any new data that arrives after the stream is started.       You can load both paths and tables as a stream.        ``` spark.readStream.format("delta")  .load("/tmp/delta/events")  import io.delta.implicits.\_ spark.readStream.delta("/tmp/delta/events")  ```      or        ``` import io.delta.implicits.\_  spark.readStream.format("delta").table("events")  ```        Limit input rate ------------------     The following options are available to control micro-batches:     * `maxFilesPerTrigger`  : How many new files to be considered in every micro-batch. The default is 1000. * `maxBytesPerTrigger`  : How much data gets processed in each micro-batch. This option sets a “soft max”, meaning that a batch processes approximately this amount of data and may process more than the limit in order to make the streaming query move forward in cases when the smallest input unit is larger than this limit. If you use  `Trigger.Once`  for your streaming, this option is ignored. This is not set by default.     If you use  `maxBytesPerTrigger`  in conjunction with  `maxFilesPerTrigger`  , the micro-batch processes data until either the  `maxFilesPerTrigger`  or  `maxBytesPerTrigger`  limit is reached.        Note       In cases when the source table transactions are cleaned up due to the  `logRetentionDuration` [configuration](../delta/history.html#data-retention)  and the stream lags in processing, Delta Lake processes the data corresponding to the latest available transaction history of the source table but does not fail the stream. This can result in data being dropped.            Stream a Delta Lake change data capture (CDC) feed ----------------------------------------------------     Delta Lake  [change data feed](../delta/delta-change-data-feed.html)  records changes to a Delta table, including updates and deletes. When enabled, you can stream from a change data feed and write logic to process inserts, updates, and deletes into downstream tables. Although change data feed data output differs slightly from the Delta table it describes, this provides a solution for propagating incremental changes to downstream tables in a  [medallion architecture](../lakehouse/medallion.html)  .           Ignore updates and deletes ----------------------------     Structured Streaming does not handle input that is not an append and throws an exception if any modifications occur on the table being used as a source. There are two main strategies for dealing with changes that cannot be automatically propagated downstream:     * You can delete the output and checkpoint and restart the stream from the beginning. * You can set either of these two options:      + `ignoreDeletes`   : ignore transactions that delete data at partition boundaries.  + `skipChangeCommits`   : ignore transactions that delete or modify existing records.   `skipChangeCommits`   subsumes   `ignoreDeletes`   .      Note       In Databricks Runtime 12.1 and above,  `skipChangeCommits`  deprecates the previous setting  `ignoreChanges`  . In Databricks Runtime 12.0 and lower,  `ignoreChanges`  is the only supported option.       The semantics for  `ignoreChanges`  differ greatly from  `skipChangeCommits`  . With  `ignoreChanges`  enabled, rewritten data files in the source table are re-emitted after a data changing operation such as  `UPDATE`  ,  `MERGE     INTO`  ,  `DELETE`  (within partitions), or  `OVERWRITE`  . Unchanged rows are often emitted alongside new rows, so downstream consumers must be able to handle duplicates. Deletes are not propagated downstream.  `ignoreChanges`  subsumes  `ignoreDeletes`  .      `skipChangeCommits`  disregards file changing operations entirely. Data files that are rewritten in the source table due to data changing operation such as  `UPDATE`  ,  `MERGE     INTO`  ,  `DELETE`  , and  `OVERWRITE`  are ignored entirely. In order to reflect changes in upstream source tables, you must implement separate logic to propagate these changes.       ###   Example     For example, suppose you have a table  `user\_events`  with  `date`  ,  `user\_email`  , and  `action`  columns that is partitioned by  `date`  . You stream out of the  `user\_events`  table and you need to delete data from it due to GDPR.       When you delete at partition boundaries (that is, the  `WHERE`  is on a partition column), the files are already segmented by value so the delete just drops those files from the metadata. Thus, if you just want to delete data from some partitions, you can use:        ``` spark.readStream.format("delta")  .option("ignoreDeletes", "true")  .load("/tmp/delta/user\_events")  ```      However, if you have to delete data based on  `user\_email`  , then you will need to use:        ``` spark.readStream.format("delta")  .option("ignoreChanges", "true")  .load("/tmp/delta/user\_events")  ```      If you update a  `user\_email`  with the  `UPDATE`  statement, the file containing the  `user\_email`  in question is rewritten. When you use  `ignoreChanges`  , the new record is propagated downstream with all other unchanged records that were in the same file. Your logic should be able to handle these incoming duplicate records.          Specify initial position --------------------------     You can use the following options to specify the starting point of the Delta Lake streaming source without processing the entire table.     * `startingVersion`  : The Delta Lake version to start from. All table changes starting from this version (inclusive) will be read by the streaming source. You can obtain the commit versions from the  `version`  column of the  [DESCRIBE HISTORY](../delta/history.html)  command output.       In Databricks Runtime 7.4 and above, to return only the latest changes, specify  `latest`  . * `startingTimestamp`  : The timestamp to start from. All table changes committed at or after the timestamp (inclusive) will be read by the streaming source. One of:      + A timestamp string. For example,   `"2019-01-01T00:00:00.000Z"`   .  + A date string. For example,   `"2019-01-01"`   .     You cannot set both options at the same time; you can use only one of them. They take effect only when starting a new streaming query. If a streaming query has started and the progress has been recorded in its checkpoint, these options are ignored.        Important       Although you can start the streaming source from a specified version or timestamp, the schema of the streaming source is always the latest schema of the Delta table. You must ensure there is no incompatible schema change to the Delta table after the specified version or timestamp. Otherwise, the streaming source may return incorrect results when reading the data with an incorrect schema.         ###   Example     For example, suppose you have a table  `user\_events`  . If you want to read changes since version 5, use:        ``` spark.readStream.format("delta")  .option("startingVersion", "5")  .load("/tmp/delta/user\_events")  ```      If you want to read changes since 2018-10-18, use:        ``` spark.readStream.format("delta")  .option("startingTimestamp", "2018-10-18")  .load("/tmp/delta/user\_events")  ```         Process initial snapshot without data being dropped -----------------------------------------------------      Note       This feature is available on Databricks Runtime 11.1 and above. This feature is in  [Public Preview](../release-notes/release-types.html)  .        When using a Delta table as a stream source, the query first processes all of the data present in the table. The Delta table at this version is called the initial snapshot. By default, the Delta table’s data files are processed based on which file was last modified. However, the last modification time does not necessarily represent the record event time order.       In a stateful streaming query with a defined watermark, processing files by modification time can result in records being processed in the wrong order. This could lead to records dropping as late events by the watermark.       You can avoid the data drop issue by enabling the following option:     * withEventTimeOrder: Whether the initial snapshot should be processed with event time order.     With event time order enabled, the event time range of initial snapshot data is divided into time buckets. Each micro batch processes a bucket by filtering data within the time range. The maxFilesPerTrigger and maxBytesPerTrigger configuration options are still applicable to control the microbatch size but only in an approximate way due to the nature of the processing.       The graphic below shows this process:      ![Initial Snapshot](../_images/delta-initial-snapshot-data-drop.png)    Notable information about this feature:     * The data drop issue only happens when the initial Delta snapshot of a stateful streaming query is processed in the default order. * You cannot change  `withEventTimeOrder`  once the stream query is started while the initial snapshot is still being processed. To restart with  `withEventTimeOrder`  changed, you need to delete the checkpoint. * If you are running a stream query with withEventTimeOrder enabled, you cannot downgrade it to a DBR version which doesn’t support this feature until the initial snapshot processing is completed. If you need to downgrade, you can wait for the initial snapshot to finish, or delete the checkpoint and restart the query. * This feature is not supported in the following uncommon scenarios:      + The event time column is a generated column and there are non-projection transformations between the Delta source and watermark.  + There is a watermark that has more than one Delta source in the stream query. * With event time order enabled, the performance of the Delta initial snapshot processing might be slower. * Each micro batch scans the initial snapshot to filter data within the corresponding event time range. For faster filter action, it is advised to use a Delta source column as the event time so that data skipping can be applied (check  [Data skipping with Z-order indexes for Delta Lake](../delta/data-skipping.html)  for when it’s applicable). Additionally, table partitioning along the event time column can further speed the processing. You can check Spark UI to see how many delta files are scanned for a specific micro batch.      ###   Example     Suppose you have a table  `user\_events`  with an  `event\_time`  column. Your streaming query is an aggregation query. If you want to ensure no data drop during the initial snapshot processing, you can use:        ``` spark.readStream.format("delta")  .option("withEventTimeOrder", "true")  .load("/tmp/delta/user\_events")  .withWatermark("event\_time", "10 seconds")  ```       Note       You can also enable this with Spark config on the cluster which will apply to all streaming queries:  `spark.databricks.delta.withEventTimeOrder.enabled     true`            Delta table as a sink -----------------------     You can also write data into a Delta table using Structured Streaming. The transaction log enables Delta Lake to guarantee exactly	To do multiple writes to the same Delta table in streaming, you can use the `foreachBatch` method in Spark Structured Streaming. This method allows you to apply any arbitrary operation on the output of a streaming query, including writing to a Delta table. You can use the `foreachBatch` method to write the output of a streaming query to a Delta table in batches, which can improve performance and reduce the number of small files produced by low latency ingest.	0	0	1
no_reaction	https://docs.databricks.com/archive/spark-sql-2.x-language-manual/alter-database.html	syntax for changing ownership of a table?	Title: Object ownership is getting changed on dropping and recreating tables  Description: Use TRUNCATE or REPLACE for tables and ALTER VIEW for views instead of dropping and recreating them.  Answer: Problem Ownership of SQL objects changes after dropping and recreating them. This can result in job failures due to permission issues. Cause In Databricks Runtime 7.3 LTS, when jobs are run with table ACLs turned off, any action that drops and recreates tables or views preserves the table ACLs that was set the last time the job was run with table ACLs turned on. In Databricks Runtime 9.1 LTS and above this behavior changed. Any action that drops a table or view clears the table ACL state. Solution You should use TRUNCATE or REPLACE for tables and ALTER VIEW for views instead of dropping and recreating them. To replace a view, you should be the owner of the view or an administrator. Info If you want to restore the behavior from Databricks Runtime 7.3 LTS, you can add spark.databricks.acl.enforceTableOwnerAssignment false to the cluster's Spark config. spark.databricks.acl.enforceTableOwnerAssignment was introduced in Databricks Runtime 9.1 LTS.  Previously, when objects were created outside of a table ACL enabled cluster the ACL system had no knowledge of them. A Databricks administrator would have to set ownership permissions for new objects and clean up dangling permissions for deleted objects. Now, objects created outside of Databricks SQL or table ACL enabled clusters create representations in the ACL system, assigning ownership automatically or dropping permissions as needed.	To change ownership of a table, you should be the owner of the table or an administrator. However, it is recommended to use TRUNCATE or REPLACE for tables and ALTER VIEW for views instead of dropping and recreating them to avoid ownership changes and permission issues.	0	1	1
no_reaction	https://docs.databricks.com/clusters/single-node.html	when should i use a standard cluster over a single node	    Single Node clusters ======================      Note       This article describes the legacy clusters UI. For information about the new clusters UI (in preview), including terminology changes for cluster access modes, see  [Create a cluster](configure.html)  . For a comparison of the new and legacy cluster types, see  [Clusters UI changes and cluster access modes](../archive/compute/cluster-ui-preview.html)  .        A Single Node cluster is a cluster consisting of an Apache Spark driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including  [Delta Lake](../delta/index.html)  . A Standard cluster requires a minimum of one Spark worker to run Spark jobs.       Single Node clusters are helpful for:     * Single-node machine learning workloads that use Spark to load and save data * Lightweight exploratory data analysis      Create a Single Node cluster ------------------------------     To create a Single Node cluster, select the  **Single Node**   button when you  [configure a cluster](configure.html#access-mode)  .         Single Node cluster properties --------------------------------     A Single Node cluster has the following properties:     * Runs Spark locally. * The driver acts as both master and worker, with no worker nodes. * Spawns one executor thread per logical core in the cluster, minus 1 core for the driver. * All  `stderr`  ,  `stdout`  , and  `log4j`  log output is saved in the driver log. * A Single Node cluster can’t be converted to a Multi Node cluster.         Limitations -------------   * Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster. * Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared. * A Multi Node cluster can’t be scaled to 0 workers. Use a Single Node cluster instead. * Single Node clusters are not compatible with process isolation. * GPU scheduling is not enabled on Single Node clusters. * On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:        ``` The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.  ```      To work around this problem, disable the native Parquet reader:        ``` spark.conf.set("spark.databricks.io.parquet.nativeReader.enabled", False)  ```         REST API ----------     You can use the Clusters API to  [create a Single Node cluster](../dev-tools/api/latest/clusters.html#clusterclusterservicecreatecluster)  .           Single Node cluster policy ----------------------------    [Cluster policies](../administration-guide/clusters/policies.html)  simplify cluster configuration for Single Node clusters.       Consider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to create a maximum number of Single Node clusters, using  [pools](pools.html)  and  [cluster policies](../administration-guide/clusters/policies.html)  :     1. Create  [a pool](pools.html)  :      1. Set   **Max capacity**    to   `10`   .  2. In   **Autopilot options**    , enable   **autoscaling enabled for local storage**    .  3. Set   **Instance type**    to   **Single Node cluster**    .  4. Select a Databricks version. Databricks recommends using the latest version if possible.  5. Click   **Create**    .  The pool’s properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool. 2. Create a  [cluster policy](../administration-guide/clusters/policies.html)  :      * Set the pool ID and instance type ID from the pool properties from the pool.  * Specify   [constraints](../administration-guide/clusters/policies.html)   as needed. 3. Grant the cluster policy to the team members. You can use  [Manage users, service principals, and groups](../administration-guide/users-groups/index.html)  to simplify user management.        ``` {  "spark\_conf.spark.databricks.cluster.profile": {  "type": "fixed",  "value": "singleNode",  "hidden": true  },  "instance\_pool\_id": {  "type": "fixed",  "value": "singleNodePoolId1",  "hidden": true  },  "spark\_version": {  "type": "fixed",  "value": "11.3.x-scala2.12",  "hidden": true  },  "autotermination\_minutes": {  "type": "fixed",  "value": 120,  "hidden": true  },  "num\_workers": {  "type": "fixed",  "value": 0,  "hidden": true  },  "docker\_image.url": {  "type": "forbidden",  "hidden": true  } }  ```             Single Node job cluster policy --------------------------------     To set up a cluster policy for jobs, you can define a similar cluster policy. Set the  `cluster\_type.type`  to  `fixed`  and  `cluster\_type.value`  to  `job`  . Remove all references to  `auto\_termination\_minutes`  .        ``` {  "cluster\_type": {  "type": "fixed",  "value": "job"  },  "spark\_conf.spark.databricks.cluster.profile": {  "type": "fixed",  "value": "singleNode",  "hidden": true  },  "instance\_pool\_id": {  "type": "fixed",  "value": "singleNodePoolId1",  "hidden": true  },  "num\_workers": {  "type": "fixed",  "value": 0,  "hidden": true  },  "spark\_version": {  "type": "fixed",  "value": "11.3.x-scala2.12",  "hidden": true  },  "docker\_image.url": {  "type": "forbidden",  "hidden": true  } }  ```                                                           	You should use a standard cluster over a single node when you have large-scale data processing workloads that will exhaust the resources on a Single Node cluster. Single Node clusters are not designed to be shared, so if the cluster must be shared, Databricks recommends using a Multi Node cluster.	0	1	1
no_reaction	https://kb.databricks.com/python/convert-datetime-to-string	how do i get a timestamp into a dataframe	   pyspark.sql.functions.to\_timestamp  [¶](#pyspark-sql-functions-to-timestamp "Permalink to this headline") ============================================================================================================  `pyspark.sql.functions.`  `to_timestamp`    (   *col     :     ColumnOrName*  ,  *format     :     Optional    [    str    ]     =     None*   )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#to_timestamp) [¶](#pyspark.sql.functions.to_timestamp "Permalink to this definition")    Converts a  [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")  into  [`pyspark.sql.types.TimestampType`](pyspark.sql.types.TimestampType.html#pyspark.sql.types.TimestampType "pyspark.sql.types.TimestampType")  using the optionally specified format. Specify formats according to  [datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)   . By default, it follows casting rules to  [`pyspark.sql.types.TimestampType`](pyspark.sql.types.TimestampType.html#pyspark.sql.types.TimestampType "pyspark.sql.types.TimestampType")  if the format is omitted. Equivalent to  `col.cast("timestamp")`  .     New in version 2.2.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **col**   [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")  or str     column values to convert.    **format: str, optional**    format to use to convert timestamp values.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")   timestamp value as  [`pyspark.sql.types.TimestampType`](pyspark.sql.types.TimestampType.html#pyspark.sql.types.TimestampType "pyspark.sql.types.TimestampType")  type.     Examples    ``` >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], t) >>> df.select(to\_timestamp(df.t).alias('dt')).collect() [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]  ```  ``` >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], t) >>> df.select(to\_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect() [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]  ```  [pyspark.sql.functions.unix\_timestamp](pyspark.sql.functions.unix_timestamp.html "previous page")  [pyspark.sql.functions.to\_date](pyspark.sql.functions.to_date.html "next page")   	To get a timestamp into a dataframe, you can use the `to_timestamp` function from the `pyspark.sql.functions` module. This function converts a column into a `pyspark.sql.types.TimestampType` type using the specified format. If the format is omitted, it follows casting rules to `pyspark.sql.types.TimestampType`. Here is an example:  ``` from pyspark.sql.functions import to_timestamp  df = spark.createDataFrame([('1997-02-28 10:30:00',)], t) df.select(to_timestamp(df.t).alias('dt')).show() ```  This will output a dataframe with a new column `dt` containing the timestamp value.	1	1	1
no_reaction	https://docs.databricks.com/data/metastores/index.html	show schemas for a hive_metastore	METASTORE\_PRIVILEGES =======================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime 10.2 and above       INFORMATION\_SCHEMA.METASTORE\_PRIVILEGES lists  [principals](../sql-ref-principal.html)  that have  [privileges](../sql-ref-privileges.html)  on the current  [metastore](metastores.html)  .       This is an extension to the SQL Standard Information Schema.        Definition ------------     The  `METASTORE\_PRIVILEGES`  relation contains the following columns:             |   Name   |   Data type   |   Nullable   |   Description   |  | | --- | --- | --- | --- | --- | |  `GRANTOR`  |  `STRING`  |   No   |  [Principal](../sql-ref-principal.html)  that granted the privilege.   |  | |  `GRANTEE`  |  `STRING`  |   No   |  [Principal](../sql-ref-principal.html)  to which the privilege is granted.   |  | |  `METASTORE\_ID`  |  `STRING`  |   No   |  [Metastore](metastores.html)  on which the privilege is granted.   |  | |  `PRIVILEGE\_TYPE`  |  `STRING`  |   No   |   Privilege being granted.   |  | |  `IS\_GRANTABLE`  |  `STRING`  |   No   |   Always  `NO`  . Reserved for future use.   |  | |  `INHERITED\_FROM`  |  `STRING`  |   No   |   No   |   The ancestor relation that the privilege is inherited from.   |       Constraints -------------     The following constraints apply to the  `METASTORE\_PRIVILEGES`  relation:            |   Class   |   Name   |   Column List   |   Description   | | --- | --- | --- | --- | |   Primary key   |  `METPRIVS\_PK`  |  `GRANTOR`  ,  `GRANTEE`  ,  `METASTORE\_ID`  ,  `PRIVILEGE\_TYPE`  |   Unique identifier for the granted privilege.   | |   Foreign key   |  `METPRIVS\_METS\_FK`  |  `METASTORE\_ID`  |   References  [METASTORES](metastores.html)  |       Examples ----------      ``` > SELECT metastore\_id, grantee  FROM information\_schema.metastore\_privileges;  ```        Related ---------   * [Information schema](../sql-ref-information-schema.html) * [INFORMATION\_SCHEMA.METASTORES](metastores.html) * [SHOW GRANTS](../security-show-grant.html)	The `METASTORE_PRIVILEGES` relation in the Information Schema lists principals that have privileges on the current metastore. It contains columns such as `GRANTOR`, `GRANTEE`, `METASTORE_ID`, `PRIVILEGE_TYPE`, `IS_GRANTABLE`, and `INHERITED_FROM`.	0	1	1
no_reaction	https://docs.databricks.com/administration-guide/account-settings-e2/private-access-settings.html	How can I configure privatelink for my aws workspace?	Enable AWS PrivateLink ========================     This article explains how to use AWS PrivateLink to enable private connectivity between users and their Databricks workspaces and between clusters on the data plane and core services on the control plane within the Databricks workspace infrastructure.        Important       This article mentions the term  *data plane*   , which is the compute layer of the Databricks platform. In the context of this article, data plane refers to the Classic data plane in your AWS account. By contrast, the serverless data plane that supports  [serverless SQL warehouses (Public Preview)](../../../sql/admin/serverless.html)  runs in the Databricks AWS account. To learn more, see  [Serverless compute](../../../serverless-compute/index.html)  .         Overview ----------     AWS PrivateLink provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network. Databricks workspaces on the E2 version of the platform support PrivateLink connections for two connection types:     * **Front-end (user to workspace)**   : A front-end PrivateLink connection allows users to connect to the Databricks web application, REST API, and Databricks Connect API over a VPC interface endpoint. * **Back-end (data plane to control plane)**   : Databricks Runtime clusters in a customer-managed VPC (the  [data plane](../../../getting-started/overview.html)  ) connect to a Databricks workspace’s core services (the  [control plane](../../../getting-started/overview.html)  ) in the Databricks cloud account. Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay. This PrivateLink connection type involves two different VPC interface endpoints because of the two different destination services.     You can implement both front-end and back-end PrivateLink connections or just one of them. This article discusses how to configure either one or both PrivateLink connection types. If you implement PrivateLink for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means Databricks rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.       To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects.       To create configuration objects and create (or update) a workspace, this article describes how to  [use the account console](#account-console)  or  [use the Account API](#account-api)  .       The following table describes important terminology.          |   Terminology   |   Description   | | --- | --- | |   AWS PrivateLink   |   An AWS technology that provides private connectivity from AWS VPCs and on-premises networks to AWS services without exposing the traffic to the public network.   | |   Front-end PrivateLink   |   The PrivateLink connection for users to connect to the Databricks web application, REST API, and Databricks Connect API.   | |   Back-end PrivateLink   |   The PrivateLink connection for the  [data plane](../../../getting-started/overview.html)  in your AWS account to connect to the Databricks  [control plane](../../../getting-started/overview.html)  ).   | |   AWS VPC endpoint service   |   An AWS VPC endpoint service is a PrivateLink-powered service. Each Databricks control plane (typically one per region) publishes two AWS VPC endpoint services for PrivateLink. The workspace VPC endpoint service applies to both a Databricks front-end PrivateLink connection or the Databricks back-end PrivateLink connection for REST APIs. Databricks publishes another VPC endpoint service for its  [secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  relay.   | |   AWS VPC endpoint   |   An  [AWS VPC interface endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html)   enables private connections between your VPC and VPC endpoint services powered by AWS PrivateLink. You must create AWS VPC interface endpoints and then register them with Databricks. Registering a VPC endpoint creates a Databricks-specific object called a VPC endpoint registration that references the AWS VPC endpoint.   | |   Databricks network configuration   |   A Databricks object that describes the important information about a  [Customer-managed VPC](customer-managed-vpc.html)  . If you implement any PrivateLink connection (front-end or back-end), your workspace must use a customer-managed VPC. For PrivateLink back-end support only, your network configuration needs an extra property that identifies the VPC endpoints for the back-end connection.   | |   Databricks private access settings object   |   A Databricks object that describes a workspace’s PrivateLink connectivity. You must attach a private access settings object to the workspace during workspace creation, whether using front-end, back-end, or both. It expresses your intent to use AWS PrivateLink with your workspace. It controls your settings for the front-end use case of AWS PrivateLink for public network access. It controls which VPC endpoints are permitted to access your workspace.   | |   Databricks workspace configuration object   |   A Databricks object that describes a workspace. To enable PrivateLink, this object must reference Databricks private access settings object. For back-end PrivateLink, the workspace must also have a Databricks network configuration object with two extra fields that specify which VPC endpoint registrations to use, one for control plane’s secure cluster connectivity relay and the other connects to the workspace to access REST APIs.   |      ###   Updates of existing PrivateLink configuration objects     This article focuses on the main two use cases of creating a new workspace or enabling PrivateLink on a workspace. You also can make other configuration changes to related objects using the UI or API:     * You can upgrade a workspace’s PrivateLink support to add support for front-end, back-end, or both types of connectivity. Add a private access settings object (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ) or update a private access settings object (  [UI](../../account-settings-e2/private-access-settings.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/replace-private-access-settings)   ). To do so, create a new network configuration with new settings, for example for a new VPC or different PrivateLink support settings, and then update the workspace to use the new network configuration. Note that you cannot remove (downgrade) any existing front-end or back-end PrivateLink support on a workspace. * Add or update a workspace’s registered VPC endpoints by creating a new network configuration object with registered VPC endpoints and then update the workspace’s network configuration (  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   ). * For more information about what kinds of workspace fields can be changed on failed or running workspaces, see information about this task by using the  [UI](../../workspace/update-workspace.html#update)  or  [API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/patch-workspace)   .     Note that not all related objects can be updated. Where update is not possible, create new objects and set their parent objects to reference the new objects. The following rules apply both to the account console UI and the Account API:           |   Object   |   Can be created   |   Can be updated   | | --- | --- | --- | |   Workspace configurations   |   Yes   |   Yes   | |   Private access settings   |   Yes   |   Yes   | |   Network configurations   |   Yes   |  **No**   | |   VPC endpoint registrations   |   Yes   |  **No**   |     To update CIDR ranges on an existing VPC, see  [Updating CIDRs](customer-managed-vpc.html#update-cidr)  .       ###   Network flow     The following diagram shows the network flow in a typical implementation.      ![PrivateLink network architecture](../../../_images/privatelink-network.png)             Requirements --------------    **Databricks account**     * Your Databricks account is on the E2 version of the platform. * Your Databricks account is on the  [Enterprise pricing tier](https://databricks.com/product/aws-pricing)   . * You have your Databricks account ID. Get your account ID from the  [account console](../../account-settings/index.html#account-id)  .    **Databricks workspace**     * Your workspace must be in an  [AWS region that supports the E2 version of the platform](../../../resources/supported-regions.html)  . However, the  `us-west-1`  region does not support PrivateLink even for workspaces on the E2 version of the platform. * Your Databricks workspace must use  [Customer-managed VPC](customer-managed-vpc.html)  to add any PrivateLink connection (even a front-end-only connection). Note that you cannot update an existing workspace with a Databricks-managed VPC and change it to use a customer-managed VPC. * If you implement the back-end PrivateLink connection, your Databricks workspace must use  [Secure cluster connectivity](../../../security/network/secure-cluster-connectivity.html)  , which is the default for new workspaces on the E2 version of the platform. To add back-end PrivateLink to an older existing workspace that does not use secure cluster connectivity, contact your Databricks representative.    **AWS account permissions**     * If you are the user who sets up PrivateLink, you must have all necessary AWS permissions to provision a Databricks workspace and to provision new VPC endpoints for your workspace.    **Network architecture**     * To implement the front-end PrivateLink connection to access the workspace from your on-premises network, add private connectivity from the on-premises network to an AWS VPC using either Direct Connect or VPN. * For guidance for other network objects, see  [Step 1: Configure AWS network objects](#create-vpc)  .         Step 1: Configure AWS network objects ---------------------------------------         You can use the AWS Management Console to create these objects or automate the process with tools such as the  [Terraform provider for networks](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/mws_networks)   .       To configure a VPC, subnets, and security groups:     1. Set up a VPC for your workspace if you haven’t already done so. You may re-use a VPC from another workspace, but you must create separate subnets for each workspace. Every workspace requires at least two private subnets.      1. To create a VPC, see   [Customer-managed VPC](customer-managed-vpc.html)   . If you are updating a workspace for PrivateLink rather than creating a new workspace, note that the workspace must already be using a customer-managed VPC.  2. On your VPC, ensure that you enable both of the settings   **DNS Hostnames**    and   **DNS resolution**    .  3. Ensure that the network ACLs for the subnets have   **bidirectional**    (outbound and inbound) rules that allow TCP access to 0.0.0.0/0 for these ports:          * 443: for Databricks infrastructure, cloud data sources, and library repositories   * 3306: for the metastore   * 6666: for PrivateLink   * 2443: only for use with compliance security profile     Important            If your	To configure PrivateLink for AWS Workspace, you need to create Databricks configuration objects and add new fields to existing configuration objects. Databricks workspaces on the E2 version of the platform support PrivateLink connections for two connection types: front-end (user to workspace) and back-end (data plane to control plane). You can implement both front-end and back-end PrivateLink connections or just one of them. To enable PrivateLink connections, you must create Databricks configuration objects and add new fields to existing configuration objects. To create configuration objects and create (or update) a workspace, you can use the account console or use the Account API.	0	1	1
no_reaction	https://docs.databricks.com/serverless-compute/index.html	What is serverless?	Serverless compute ====================     With the serverless compute version of the Databricks platform architecture, the compute layer exists in your Databricks account rather than your AWS account.            Databricks SQL Serverless ---------------------------    [Databricks SQL Serverless](../sql/admin/index.html)  supports serverless compute. Admins can create serverless SQL warehouses (formerly SQL endpoints) that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute clusters in your Databricks account. Use them with Databricks SQL queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.       Databricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it’s connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.       If serverless SQL warehouses are enabled for your account, note the following:     * New SQL warehouses are serverless by default when you create them from the  [UI](../sql/admin/create-sql-warehouse.html#create-warehouse)  . New SQL warehouses are not serverless by default when you create them using the  [API](../sql/api/sql-endpoints.html#create-warehouse)  , which requires that you explicitly specify serverless. You can also create new pro or classic SQL warehouses using either method. For more information about warehouse type defaults, see  [What are the warehouse type defaults?](../sql/index.html#warehouse-type-defaults)  . * You can  [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](../sql/admin/create-sql-warehouse.html#convert-to-serverless)  or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic. * This feature only affects Databricks SQL. It does not affect how Databricks Runtime clusters work with notebooks and jobs in the Data Science & Engineering or Databricks Machine Learning workspace environments. Databricks Runtime clusters always run in the classic data plane in your AWS account. See  [Serverless quotas](#compare)  .     If your account needs updated  [terms of use](../sql/admin/serverless.html#accept-terms)  , workspace admins are prompted in the Databricks SQL UI.       If your workspace has an AWS instance profile,  [you might need to update the trust relationship](../sql/admin/serverless.html#aws-instance-profile-setup)  to support serverless compute, depending on how and when it was created.       For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .         Model Serving ---------------     Model Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your Databricks account. The <ServerlessCompute> resources run as Databricks AWS resources in what is known as the serverless data plane.       In contrast, the legacy model serving architecture is a single-node cluster that runs in your AWS account within the classic data plane.     * **Easy configuration and compute resource management**   : Databricks automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration. * **High availability and scalability**   : Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests. * **Dashboards**   : Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.     For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .       Before you can use serverless Model Serving, you must  [enable serverless compute for your account](../machine-learning/model-serving/migrate-model-serving.html)  .                   Serverless quotas -------------------     Serverless quotas are a safety measure for serverless compute. Serverless quotas restrict how many serverless compute resources a customer can have at any given time. The quota is enforced at the regional level for all workspaces in your account. Quotas are enforced only for  [serverless SQL warehouses](../sql/admin/serverless.html)  . See  [Serverless quotas](serverless-quotas.html)  .         Compare serverless compute to other Databricks architectures --------------------------------------------------------------     Databricks operates out of a  *control plane*   and a  *data plane*   :     * The control plane includes the backend services that Databricks manages in its own AWS account. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest. * The data plane is where data is processed by clusters of compute resources.     There are important differences between the  *classic data plane*   (the original Databricks platform architecture) and the  *serverless data plane*   :     * For a classic data plane, Databricks compute resources run in your AWS account. Clusters perform distributed data analysis using queries (in Databricks SQL) or notebooks (in the Data Science & Engineering or Databricks Machine Learning environments):      + New clusters are created within each workspace’s virtual network in the customer’s AWS account.  + A classic data plane has natural isolation because it runs in each customer’s own AWS account. * For a serverless data plane, Databricks compute resources run in a compute layer within your Databricks account:      + The serverless data plane is used for serverless SQL warehouses and Model Serving. Enabling serverless compute does not change how Databricks Runtime clusters work in the Data Science & Engineering or Databricks Machine Learning environments.  + To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different Databricks customer workspaces and additional network controls between clusters of the same customer.     Databricks creates a serverless data plane in the same AWS region as your workspace’s classic data plane.       Worker nodes are private, which means they do not have public IP addresses.       For communication between the Databricks control plane and the serverless data plane:     * For Databricks SQL Serverless, the communication uses private connectivity. * For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.     When reading or writing to AWS S3 buckets in the same region as your workspace,  [serverless SQL warehouses](../sql/admin/serverless.html)  now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace’s root S3 bucket in your AWS account and to other S3 data sources in the same region.       The following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.      ![Compare classic and serverless data plane for Databricks SQL](../_images/serverless-compute-data-plane-comparison-dbsql-aws.png)   ![Compare classic and serverless data plane for Model Serving](../_images/serverless-compute-data-plane-comparison-models-aws.png)    For more information about secure cluster connectivity, which is mentioned in the diagram, see  [Secure cluster connectivity](../security/network/secure-cluster-connectivity.html)  .       The table below summarizes differences between serverless compute and the classic data plane architecture of Databricks, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about serverless compute security, or if you have questions about items in this table, contact your Databricks representative.               |   Item   |   Serverless data plane (AWS only)   |   Classic data plane (AWS and Azure)   | | --- | --- | --- | |  **Location of control plane resources**   |   Databricks cloud account   |   Databricks cloud account   | |  **Location of data plane compute resources**   |   Serverless data plane (VPC in the Databricks AWS account)   |   Classic data plane (VPC in the customer’s cloud provider account)   | |  **Data plane compute resources**   |   Databricks-managed Kubernetes (EKS) clusters   |   Databricks-managed standalone VMs   | |  **Customer access to data plane**   |   Access through Databricks control plane   | * **AWS**   : Direct access in customer’s AWS account. Additional indirect access through Databricks control plane. * **Azure**   : Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.  | |  **Who pays for unassigned VMs for Databricks SQL?**   |   Databricks   |   Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.   | |  **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**   |   Customer pays based on DBUs until Auto Stop stops the SQL warehouse.   |   Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.   | |  **Virtual private network (VPC) for data plane**   |   VPC in the customer’s Databricks account, with network boundaries between workspaces and between clusters.   | * **AWS**   : Exclusive, the VPC is in the customer’s account. * **Azure**   : Exclusive, the VNet is in the customer’s account.  | |  **OS image**   |   Databricks-modified cloud-managed Amazon-linux2   |   Databricks-managed Ubuntu or CentOS   | |  **Technology that manages default egress from the VPC**   |   Databricks-created AWS internet gateway   |   Default internet gateway or load balancer provided by the cloud   | |  **Customize VPC and firewall settings**   |   No   |   Yes   | |  **Customize CIDR ranges**   |   No   |   Yes   | |  **Public IPs**   |   No   |   Secure cluster connectivity  * When disabled, one public IP for each VM. * When enabled (the default), no public IPs for VMs.  | |  **Container-level network isolation for Databricks Runtime clusters**   |   Uses Kubernetes network policy   |   Uses Databricks-managed  `iptable`  rules   | |  **VM-level network isolation for Databricks Runtime clusters**   |   Security group isolation   |   Security group and isolation of VPC (AWS) or VNet (Azure)   | |  **VM isolation**   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   | |  **Communication between control plane and data plane**   |   For Databricks SQL Serverless, communication uses private connectivity. For Model Serving, communication uses direct mTLS encrypted communication with the connection initiated from the control plane with access limited to control plane IP addresses.   |   Secure cluster connectivity  * When enabled (the default for AWS E2 and Azure): Individual VMs connect to the SCC relay in the control plane during cluster creation * When disabled: Control plane connects to individual VMs using public IPs.  | |  **Credential for initial deployment**   |   Databricks internal IAM roles   | * **AWS**   : IAM roles provided by customers. * **Azure**   : None required.  | |  **Credential for regular data plane operations**   |   Databricks invokes  `sts:AssumeRole`  on customer-provided IAM role.   | * **AWS**   : VMs run with instance profiles that are provided by	Serverless compute is a version of the Databricks platform architecture where the compute layer exists in your Databricks account rather than your AWS account. It allows for serverless SQL warehouses and model serving, which autoscale and are managed by Databricks. Serverless quotas are in place to restrict how many serverless compute resources a customer can have at any given time.	0	1	1
no_reaction	https://kb.databricks.com/streaming/optimize-streaming-transactions-with-trigger	change data capture to trigger execution	      Change data capture with Delta Live Tables ============================================          Preview       Delta Live Tables support for SCD type 2 is in  [Public Preview](../release-notes/release-types.html)  .        You can use change data capture (CDC) in Delta Live Tables to update tables based on changes in source data. CDC is supported in the Delta Live Tables SQL and Python interfaces. Delta Live Tables supports updating tables with slowly changing dimensions (SCD) type 1 and type 2:     * Use SCD type 1 to update records directly. History is not retained for records that are updated. * Use SCD type 2 to retain a history of records, either on all updates or on updates to a specified set of columns. See  [Track history for only specified columns with SCD type 2](#type2-track-history)     For syntax and other references, see:     * [Change data capture with Python in Delta Live Tables](python-ref.html#cdc) * [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) * [CDC table properties](properties.html#cdc)      Note       This article describes how to update tables in your Delta Live Tables pipeline based on changes in source data. To learn how to record and query row-level change information for Delta tables, see  [Use Delta Lake change data feed on Databricks](../delta/delta-change-data-feed.html)  .           How is CDC implemented with Delta Live Tables? ------------------------------------------------     You must specify a column in the source data on which to sequence records, which Delta Live Tables interprets as a monotonically increasing representation of the proper ordering of the source data. Delta Live Tables automatically handles data that arrives out of order. For SCD Type 2 changes, Delta Live Tables propagates the appropriate sequencing values to the  `\_\_START\_AT`  and  `\_\_END\_AT`  columns of the target table. There should be at most one distinct update per key at each sequencing value, and NULL sequencing values are unsupported.       To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an  `APPLY     CHANGES     INTO`  statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the  `CREATE     OR     REFRESH     STREAMING     TABLE`  statement in SQL or the  `create\_streaming\_live\_table()`  function in Python. To create the statement defining the CDC processing, use the  `APPLY     CHANGES`  statement in SQL or the  `apply\_changes()`  function in Python. For syntax details, see  [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc)  or  [Change data capture with Python in Delta Live Tables](python-ref.html#cdc)  .             What data objects are used for Delta Live Tables CDC processing? ------------------------------------------------------------------     When you declare the target table, two data structures are created in the Hive metastore:     * A view using the name assigned to the target table. * An internal backing table used by Delta Live Tables table to manage CDC processing. This table is named by prepending  `\_\_apply\_changes\_storage\_`  to the target table name.     For example, if you declare a target table named  `dlt\_cdc\_target`  , you will see a view named  `dlt\_cdc\_target`  and a table named  `\_\_apply\_changes\_storage\_dlt\_cdc\_target`  in the metastore. Creating a view allows Delta Live Tables to filter out the extra information (for example, tombstones and versions) that is required to handle out-of-order data. To view the processed data, query the target view. You can also query the raw data in the  `\_\_apply\_changes\_storage\_`  table to see deleted records and extra version columns. If you add data manually to the table, the records are assumed to come before other changes because the version columns are missing.         Limitations -------------   * Metrics for the target table, such as number of output rows, are not available. * SCD type 2 updates will add a history row for every input row, even if no columns have changed. * The target of the  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function cannot be used as a source for a streaming table. A table that reads from the target of an  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function must be a live table. * Expectations are not supported in an  `APPLY     CHANGES     INTO`  query or  `apply\_changes()`  function. To use expectations for the source or target dataset:      + Add expectations on source data by defining an intermediate table with the required expectations and use this dataset as the source for the target table.  + Add expectations on target data with a downstream table that reads input data from the target table.         SCD type 1 and SCD type 2 on Databricks -----------------------------------------     The following sections provide examples that demonstrate Delta Live Tables SCD type 1 and type 2 queries that update target tables based on source events that:     1. Create new user records. 2. Delete a user record. 3. Update user records. In the SCD type 1 example, the last  `UPDATE`  operations arrive late and are dropped from the target table, demonstrating the handling of out of order events.     All of the following examples assume familiarity with configuring and updating Delta Live Tables pipelines. See  [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html)  .       In order to run these examples, you must begin by creating a sample dataset. See  [Generate test data](#generate-data)  .       The following are the input records for these examples:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   124   |   Raul   |   Oaxaca   |   INSERT   |   1   | |   123   |   Isabel   |   Monterrey   |   INSERT   |   1   | |   125   |   Mercedes   |   Tijuana   |   INSERT   |   2   | |   126   |   Lily   |   Cancun   |   INSERT   |   2   | |   123   |   null   |   null   |   DELETE   |   6   | |   125   |   Mercedes   |   Guadalajara   |   UPDATE   |   6   | |   125   |   Mercedes   |   Mexicali   |   UPDATE   |   5   | |   123   |   Isabel   |   Chihuahua   |   UPDATE   |   5   |     If you uncomment the final row in the example data, it will insert the following record that specifies where records should be truncated:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   null   |   null   |   null   |   TRUNCATE   |   3   |      Note       All the following examples include options to specify both  `DELETE`  and  `TRUNCATE`  operations, but each of these are optional.            Process SCD type 1 updates ----------------------------     The following code example demonstrates processing SCD type 1 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format("delta").table("cdc\_data.users")  dlt.create\_streaming\_live\_table("target")  dlt.apply\_changes(   target = "target",   source = "users",   keys = ["userId"],   sequence\_by = col("sequenceNum"),   apply\_as\_deletes = expr("operation = 'DELETE'"),   apply\_as\_truncates = expr("operation = 'TRUNCATE'"),   except\_column\_list = ["operation", "sequenceNum"],   stored\_as\_scd\_type = 1 )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = "DELETE" APPLY AS TRUNCATE WHEN  operation = "TRUNCATE" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 1;  ```       After running the SCD type 1 example, the target table contains the following records:           |   userId   |   name   |   city   | | --- | --- | --- | |   124   |   Raul   |   Oaxaca   | |   125   |   Mercedes   |   Guadalajara   | |   126   |   Lily   |   Cancun   |     After running the SCD type 1 example with the additional  `TRUNCATE`  record, records  `124`  and  `126`  are truncated because of the  `TRUNCATE`  operation at  `sequenceNum=3`  , and the target table contains the following record:           |   userId   |   name   |   city   | | --- | --- | --- | |   125   |   Mercedes   |   Guadalajara   |         Process SCD type 2 updates ----------------------------     The following code example demonstrates processing SCD type 2 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format("delta").table("cdc\_data.users")  dlt.create\_streaming\_live\_table("target")  dlt.apply\_changes(   target = "target",   source = "users",   keys = ["userId"],   sequence\_by = col("sequenceNum"),   apply\_as\_deletes = expr("operation = 'DELETE'"),   except\_column\_list = ["operation", "sequenceNum"],   stored\_as\_scd\_type = "2" )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = "DELETE" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 2;  ```       After running the SCD type 2 example, the target table contains the following records:             |   userId   |   name   |   city   |   \_\_START\_AT   |   \_\_END\_AT   | | --- | --- | --- | --- | --- | |   123   |   Isabel   |   Monterrey   |   1   |   5   | |   123   |   Isabel   |  	To trigger execution using change data capture (CDC) in Delta Live Tables, you must first create a streaming table, and then use an `APPLY CHANGES INTO` statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the `CREATE OR REFRESH STREAMING TABLE` statement in SQL or the `create_streaming_live_table()` function in Python. To create the statement defining the CDC processing, use the `APPLY CHANGES` statement in SQL or the `apply_changes()` function in Python. For syntax details, see [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) or [Change data capture with Python in Delta Live Tables](python-ref.html#cdc).	0	1	1
no_reaction	https://docs.databricks.com/delta/table-properties.html	how can I add custom table properties when writing to a delta table using pyspark?	Delta table properties reference ==================================         Delta Lake reserves Delta table properties starting with  `delta.`  . These properties may have specific meanings, and affect behaviors when these properties are set.        Default table properties --------------------------     Delta Lake configurations set in the SparkSession override the default table properties for new Delta Lake tables created in the session. The prefix used in the SparkSession is different from the configurations used in the table properties.          |   Delta Lake conf   |   SparkSession conf   | | --- | --- | |  `delta.<conf>`  |  `spark.databricks.delta.properties.defaults.<conf>`  |     For example, to set the  `delta.appendOnly     =     true`  property for all new Delta Lake tables created in a session, set the following:        ``` SET spark.databricks.delta.properties.defaults.appendOnly = true  ```      To modify table properties of existing tables, use  [SET TBLPROPERTIES](../sql/language-manual/sql-ref-syntax-ddl-tblproperties.html)  .                     Delta table properties ------------------------     Available Delta table properties include the following:         |   Property   | | --- | |  `delta.appendOnly`   `true`  for this Delta table to be append-only. If append-only, existing records cannot be deleted, and existing values cannot be updated.     See  [Delta table properties reference](#)  .     Data type:  `Boolean`    Default:  `false`  | |  `delta.autoOptimize.autoCompact`   `true`  for Delta Lake to automatically optimize the layout of the files for this Delta table.     See  [Enable auto optimize](../optimizations/auto-optimize.html#enable-auto-optimize)  .     Data type:  `Boolean`    Default: (none)   | |  `delta.autoOptimize.optimizeWrite`   `true`  for Delta Lake to automatically optimize the layout of the files for this Delta table during writes.     See  [Enable auto optimize](../optimizations/auto-optimize.html#enable-auto-optimize)  .     Data type:  `Boolean`    Default: (none)   | |  `delta.checkpoint.writeStatsAsJson`   `true`  for Delta Lake to write file statistics in checkpoints in JSON format for the  `stats`  column.     See  [Manage column-level statistics in checkpoints](best-practices.html#column-stats)  .     Data type:  `Boolean`    Default:  `true`  | |  `delta.checkpoint.writeStatsAsStruct`   `true`  for Delta Lake to write file statistics to checkpoints in struct format for the  `stats\_parsed`  column and to write partition values as a struct for  `partitionValues\_parsed`  .     See  [Manage column-level statistics in checkpoints](best-practices.html#column-stats)  .     Data type:  `Boolean`    Default: (none)   | |  `delta.columnMapping.mode`    Whether column mapping is enabled for Delta table columns and the corresponding Parquet columns that use different names.     See  [Rename and drop columns with Delta Lake column mapping](delta-column-mapping.html)  .     Data type:  `DeltaColumnMappingMode`    Default:  `none`  | |  `delta.compatibility.symlinkFormatManifest.enabled`   `true`  for Delta Lake to configure the Delta table so that all write operations on the table automatically update the manifests.     Data type:  `Boolean`    Default:  `false`  | |  `delta.dataSkippingNumIndexedCols`    The number of columns for Delta Lake to collect statistics about for data skipping. A value of  `-1`  means to collect statistics for all columns. Updating this property does not automatically collect statistics again; instead, it redefines the statistics schema of the Delta table. Specifically, it changes the behavior of future statistics collection (such as during appends and optimizations) as well as data skipping (such as ignoring column statistics beyond this number, even when such statistics exist).     See  [Data skipping with Z-order indexes for Delta Lake](data-skipping.html)  .     Data type:  `Int`    Default:  `32`  | |  `delta.deletedFileRetentionDuration`    The shortest duration for Delta Lake to keep logically deleted data files before deleting them physically. This is to prevent failures in stale readers after compactions or partition overwrites.     This value should be large enough to ensure that:  * It is larger than the longest possible duration of a job if you run  `VACUUM`  when there are concurrent readers or writers accessing the Delta table. * If you run a streaming query that reads from the table, that query does not stop for longer than this value. Otherwise, the query may not be able to restart, as it must still read old files.    See  [Configure data retention for time travel](history.html#data-retention)  .     Data type:  `CalendarInterval`    Default:  `interval     1     week`  | |  `delta.enableChangeDataFeed`   `true`  to enable change data feed.     See  [Enable change data feed](delta-change-data-feed.html#enable-change-data-feed)  .     Data type:  `Boolean`    Default:  `false`  | |  `delta.isolationLevel`    The degree to which a transaction must be isolated from modifications made by concurrent transactions.     Valid values are  `Serializable`  and  `WriteSerializable`  .     See  [Isolation levels and write conflicts on Databricks](../optimizations/isolation-level.html)  .     Data type:  `String`    Default:  `WriteSerializable`  | |  `delta.logRetentionDuration`    How long the history for a Delta table is kept.     Each time a checkpoint is written, Delta Lake automatically cleans up log entries older than the retention interval. If you set this property to a large enough value, many log entries are retained. This should not impact performance as operations against the log are constant time. Operations on history are parallel but will become more expensive as the log size increases.     See  [Configure data retention for time travel](history.html#data-retention)  .     Data type:  `CalendarInterval`    Default:  `interval     30     days`  | |  `delta.minReaderVersion`    The minimum required protocol reader version for a reader that allows to read from this Delta table.     See  [How does Databricks manage Delta Lake feature compatibility?](feature-compatibility.html)  .     Data type:  `Int`    Default:  `1`  | |  `delta.minWriterVersion`    The minimum required protocol writer version for a writer that allows to write to this Delta table.     See  [How does Databricks manage Delta Lake feature compatibility?](feature-compatibility.html)  .     Data type:  `Int`    Default:  `2`  | |  `delta.randomizeFilePrefixes`   `true`  for Delta Lake to generate a random prefix for a file path instead of partition information.     For example, this may improve Amazon S3 performance when Delta Lake needs to send very high volumes of Amazon S3 calls to better partition across S3 servers.     See  [Delta table properties reference](#)  .     Data type:  `Boolean`    Default:  `false`  | |  `delta.randomPrefixLength`    When  `delta.randomizeFilePrefixes`  is set to  `true`  , the number of characters that Delta Lake generates for random prefixes.     See  [Delta table properties reference](#)  .     Data type:  `Int`    Default:  `2`  | |  `delta.setTransactionRetentionDuration`    The shortest duration within which new snapshots will retain transaction identifiers (for example,  `SetTransaction`  s). When a new snapshot sees a transaction identifier older than or equal to the duration specified by this property, the snapshot considers it expired and ignores it. The  `SetTransaction`  identifier is used when making the writes idempotent. See  [Idempotent table writes in foreachBatch](../structured-streaming/delta-lake.html#idempotent-table-writes-in-foreachbatch)  for details.     Data type:  `CalendarInterval`    Default: (none)   | |  `delta.targetFileSize`    The target file size in bytes or higher units for file tuning. For example,  `104857600`  (bytes) or  `100mb`  .     See  [Configure Delta Lake to control data file size](tune-file-size.html)  .     Data type:  `String`    Default: (none)   | |  `delta.tuneFileSizesForRewrites`   `true`  to always use lower file sizes for all data layout optimization operations on the Delta table.    `false`  to never tune to lower file sizes, that is, prevent auto-detection from being activated.     See  [Configure Delta Lake to control data file size](tune-file-size.html)  .     Data type:  `Boolean`    Default: (none)   |	To add custom table properties when writing to a Delta table using PySpark, you can use the `option` method of the DataFrameWriter object and pass in the custom properties as key-value pairs. For example, `df.write.format("delta").option("key", "value").save("path")`. Note that custom properties should start with `delta.` prefix.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/delta-update.html	Is this syntax databricks compatible UPDATE SC_GFLS_ILM_PROD.PROCESSED.OPEN_PURCHASE_ORDERS_GLBL_WHLSL_S4 AS TGT SET MATERIAL_AVAILABILITY_DATE=SOH.MAD FROM NGP_DA_PROD.EIS.SO_HDR AS SOH WHERE TGT.SALES_ORDER_HEADER_NUMBER=SOH.SO_HDRNBR;	UPDATE ========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Updates the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows.       This statement is only supported for Delta Lake tables.        Syntax --------      ``` UPDATE table\_name [table\_alias]    SET  { { column\_name | field\_name }  = [ expr | DEFAULT } [, ...]    [WHERE clause]  ```        Parameters ------------   * **[table\_name](sql-ref-names.html#table-name)**      Identifies table to be updated. The table name must not use a  [temporal specification](sql-ref-names.html#table-name)  . * **[table\_alias](sql-ref-names.html#table-alias)**      Define an alias for the table. The alias must not include a column list. * **[column\_name](sql-ref-names.html#column-name)**      A reference to a column in the table. You may reference each column at most once. * **[field\_name](sql-ref-names.html#field-name)**      A reference to field within a column of type STRUCT. You may reference each field at most once. * **[expr](sql-ref-expression.html)**      An arbitrary expression. If you reference  `table\_name`  columns they represent the state of the row prior the update. * **DEFAULT**      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  [SQL warehouse version 2022.35](../release-notes/index.html)  or higher  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.2 and above       The  `DEFAULT`  expression for the column if one is defined, NULL otherwise. * **[WHERE](sql-ref-syntax-qry-select-where.html)**      Filter rows by predicate. The  `WHERE`  clause may include subqueries with the following exceptions:      + Nested subqueries, that is, a subquery inside another subquery  + A   `NOT        IN`   subquery inside an   `OR`   , for example,   `a        =        3        OR        b        NOT        IN        (SELECT        c        from        t)`  In most cases, you can rewrite  `NOT     IN`  subqueries using  `NOT     EXISTS`  . You should use  `NOT     EXISTS`  whenever possible, as  `UPDATE`  with  `NOT     IN`  subqueries can be slow.       Examples ----------      ``` > UPDATE events SET eventType = 'click' WHERE eventType = 'clk'  > UPDATE all\_events  SET session\_time = 0, ignored = true  WHERE session\_time < (SELECT min(session\_time) FROM good\_events)  > UPDATE orders AS t1  SET order\_status = 'returned'  WHERE EXISTS (SELECT oid FROM returned\_orders WHERE t1.oid = oid)  > UPDATE events  SET category = 'undefined'  WHERE category NOT IN (SELECT category FROM events2 WHERE date > '2001-01-01')  > UPDATE events  SET ignored = DEFAULT  WHERE eventType = 'unknown'  ```        Related articles ------------------   * [COPY](delta-copy-into.html) * [DELETE](delta-delete-from.html) * [INSERT](sql-ref-syntax-dml-insert-into.html) * [MERGE](delta-merge-into.html) * [PARTITION](sql-ref-partition.html#partition) * [query](sql-ref-syntax-qry-query.html)	The syntax you provided is compatible with Databricks SQL. It updates the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows. This statement is only supported for Delta Lake tables.	0	1	1
no_reaction	https://docs.databricks.com/dev-tools/python-sql-connector.html	how do I connect to a databricks database from python	      Databricks SQL Connector for Python =====================================     The  [Databricks SQL Connector for Python](https://github.com/databricks/databricks-sql-python)   is a Python library that allows you to use Python code to run SQL commands on Databricks clusters and Databricks SQL warehouses. The Databricks SQL Connector for Python is easier to set up and use than similar Python libraries such as  [pyodbc](pyodbc.html)  . This library follows  [PEP 249 – Python Database API Specification v2.0](https://www.python.org/dev/peps/pep-0249/)   .        Requirements --------------   * A development machine running Python >=3.7 and <=3.11. * An existing  [cluster](../clusters/index.html)  or  [SQL warehouse](../sql/admin/create-sql-warehouse.html)  .       Get started -------------   * Gather the following information for the cluster or SQL warehouse that you want to use:         + The server hostname of the cluster. You can get this from the   **Server Hostname**    value in the   **[Advanced Options > JDBC/ODBC](../integrations/jdbc-odbc-bi.html#connection-details-cluster)**   tab for your cluster.  + The HTTP path of the cluster. You can get this from the   **HTTP Path**    value in the   **[Advanced Options > JDBC/ODBC](../integrations/jdbc-odbc-bi.html#connection-details-cluster)**   tab for your cluster.  + A Databricks   [personal access token](auth.html#pat)   for the workspace.    Note       As a security best practice, you should not hard-code this information into your code. Instead, you should retrieve this information from a secure location. For example, the code examples later in this article use environment variables.          + The server hostname of the SQL warehouse. You can get this from the   **Server Hostname**    value in the   [Connection Details](../integrations/jdbc-odbc-bi.html#connection-details-sql-warehouse)   tab for your SQL warehouse.  + The HTTP path of the SQL warehouse. You can get this from the   **HTTP Path**    value in the   [Connection Details](../integrations/jdbc-odbc-bi.html#connection-details-sql-warehouse)   tab for your SQL warehouse.  + A Databricks   [personal access token](auth.html#pat)   for the workspace.    Note       As a security best practice, you should not hard-code this information into your code. Instead, you should retrieve this information from a secure location. For example, the code examples later in this article use environment variables. * Install the Databricks SQL Connector for Python library on your development machine by running  `pip     install     databricks-sql-connector`  .       Examples ----------     The following code examples demonstrate how to use the Databricks SQL Connector for Python to query and insert data, query metadata, manage cursors and connections, and configure logging.       These code example retrieve their  `server\_hostname`  ,  `http\_path`  , and  `access\_token`  connection variable values from these environment variables:     * `DATABRICKS\_SERVER\_HOSTNAME`  , which represents the  **Server Hostname**   value from the requirements. * `DATABRICKS\_HTTP\_PATH`  , which represents the  **HTTP Path**   value from the requirements. * `DATABRICKS\_TOKEN`  , which represents your access token from the requirements.     You can use other approaches to retrieving these connection variable values. Using environment variables is just one approach among many.      * [Query data](#query-data) * [Insert data](#insert-data) * [Query metadata](#query-metadata) * [Manage cursors and connections](#manage-cursors-and-connections) * [Configure logging](#configure-logging)     ###  [Query data](#id6)     The following code example demonstrates how to call the Databricks SQL Connector for Python to run a basic SQL command on a cluster or SQL warehouse. This command returns the first two rows from the  `diamonds`  table.       The  `diamonds`  table is included in  [Sample datasets](../dbfs/databricks-datasets.html)  . This table is also featured in  [Tutorial: Query data with notebooks](../getting-started/quick-start.html)  .        ``` from databricks import sql import os  with sql.connect(server\_hostname = os.getenv("DATABRICKS\_SERVER\_HOSTNAME"),                  http\_path       = os.getenv("DATABRICKS\_HTTP\_PATH"),                  access\_token    = os.getenv("DATABRICKS\_TOKEN")) as connection:    with connection.cursor() as cursor:     cursor.execute("SELECT \* FROM default.diamonds LIMIT 2")     result = cursor.fetchall()      for row in result:       print(row)  ```      ###  [Insert data](#id7)     The following example demonstrate how to insert small amounts of data (thousands of rows):        ``` from databricks import sql import os  with sql.connect(server\_hostname = os.getenv("DATABRICKS\_SERVER\_HOSTNAME"),                  http\_path       = os.getenv("DATABRICKS\_HTTP\_PATH"),                  access\_token    = os.getenv("DATABRICKS\_TOKEN")) as connection:    with connection.cursor() as cursor:     cursor.execute("CREATE TABLE IF NOT EXISTS squares (x int, x\_squared int)")      squares = [(i, i \* i) for i in range(100)]     values = ",".join([f"({x}, {y})" for (x, y) in squares])      cursor.execute(f"INSERT INTO squares VALUES {values}")      cursor.execute("SELECT \* FROM squares LIMIT 10")      result = cursor.fetchall()      for row in result:       print(row)  ```      For large amounts of data, you should first upload the data to cloud storage and then execute the  [COPY INTO](../sql/language-manual/delta-copy-into.html)  command.       ###  [Query metadata](#id8)     There are dedicated methods for retrieving metadata. The following example retrieves metadata about columns in a sample table:        ``` from databricks import sql import os  with sql.connect(server\_hostname = os.getenv("DATABRICKS\_SERVER\_HOSTNAME"),                  http\_path       = os.getenv("DATABRICKS\_HTTP\_PATH"),                  access\_token    = os.getenv("DATABRICKS\_TOKEN")) as connection:    with connection.cursor() as cursor:     cursor.columns(schema\_name="default", table\_name="squares")     print(cursor.fetchall())  ```      ###  [Manage cursors and connections](#id9)     It is best practice to close any connections and cursors that have been finished with. This frees resources on Databricks clusters and Databricks SQL warehouses.       You can use a context manager (the  `with`  syntax used in previous examples) to manage the resources, or explicitly call  `close`  :        ``` from databricks import sql import os  connection = sql.connect(server\_hostname = os.getenv("DATABRICKS\_SERVER\_HOSTNAME"),                          http\_path       = os.getenv("DATABRICKS\_HTTP\_PATH"),                          access\_token    = os.getenv("DATABRICKS\_TOKEN"))  cursor = connection.cursor()  cursor.execute("SELECT \* from range(10)") print(cursor.fetchall())  cursor.close() connection.close()  ```      ###  [Configure logging](#id10)     The Databricks SQL Connector uses Python’s  [standard logging module](https://docs.python.org/3/library/logging.html)   . You can configure the logging level similar to the following:        ``` from databricks import sql import os, logging  logging.getLogger("databricks.sql").setLevel(logging.DEBUG) logging.basicConfig(filename = "results.log",                     level    = logging.DEBUG)  connection = sql.connect(server\_hostname = os.getenv("DATABRICKS\_SERVER\_HOSTNAME"),                          http\_path       = os.getenv("DATABRICKS\_HTTP\_PATH"),                          access\_token    = os.getenv("DATABRICKS\_TOKEN"))  cursor = connection.cursor()  cursor.execute("SELECT \* from range(10)")  result = cursor.fetchall()  for row in result:    logging.debug(row)  cursor.close() connection.close()  ```         API reference ---------------    * [Package](#package) * [Module](#module)      + [Methods](#methods)          - [`connect`    method](#connect-method) * [Classes](#classes)      + [`Connection`   class](#connection-class)          - [Methods](#methods)              * [`close`     method](#close-method)    * [`cursor`     method](#cursor-method)  + [`Cursor`   class](#cursor-class)          - [Attributes](#attributes)              * [`arraysize`     attribute](#arraysize-attribute)    * [`description`     attribute](#description-attribute)   - [Methods](#methods)              * [`cancel`     method](#cancel-method)    * [`close`     method](#close-method)    * [`execute`     method](#execute-method)    * [`executemany`     method](#executemany-method)    * [`cat	To connect to a Databricks database from Python, you can use the Databricks SQL Connector for Python library. First, you need to gather the server hostname, HTTP path, and personal access token for the cluster or SQL warehouse that you want to use. Then, you can install the Databricks SQL Connector for Python library on your development machine by running `pip install databricks-sql-connector`. Finally, you can use the `connect` method from the library to establish a connection to the database.	0	0	1
no_reaction	https://docs.databricks.com/serverless-compute/index.html	What is the value of using DB SQL Serverless?	Enable serverless SQL warehouses ==================================     With serverless compute, the compute layer exists in your Databricks account rather than your AWS account. Serverless compute is supported for use with Databricks SQL. Admins can create serverless SQL warehouses that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute resources in your Databricks account. Use serverless SQL warehouses with Databricks SQL queries the same way you would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.       If your workspace satisfies the serverless SQL warehouse requirements:     * New SQL warehouses are serverless by default when created from the UI, but you can also create new pro and classic SQL warehouses. * From the SQL Warehouses API or Terraform, Databricks recommends always explicitly set  `enable\_serverless\_compute`  to  `true`  and  `warehouse\_type`  to  `pro`  to create serverless SQL warehouses. For details about the defaults if omitted, see  [SQL Warehouses API](../api/sql-endpoints.html)  and  [Terraform](../../dev-tools/terraform/index.html)  . * You can  [create serverless SQL warehouses with the UI or API, or convert warehouses to serverless](#test)  .     This feature only affects Databricks SQL. It does not affect how Databricks Runtime clusters work with notebooks and jobs in the Data Science & Engineering or Databricks Machine Learning workspace environments.       Serverless SQL warehouses do  **not**   have public IP addresses. For more architectural information, see  [Serverless compute](../../serverless-compute/index.html)  .       Depending on when your Databricks account was created, to create  [serverless SQL warehouses](../../serverless-compute/index.html)  , your organization may need to perform several tasks:           |   Task   |   Who can do this step?   |   Where is this done?   | | --- | --- | --- | |   If your account needs updated  [terms of use](#accept-terms)  , workspace admins are prompted in the Databricks SQL UI.   |   Account owner or account administrator.   |   The account console’s settings page.   | |   If your workspace uses an instance profile for Databricks SQL, you may need to update its role to add a trust relationship.   |   Workspace administrator must confirm which instance profile your workspace uses for Databricks SQL. An AWS administrator with permissions to view and make changes to AWS IAM policies must check the role’s trust relationship policy or make any necessary changes.   |   The workspace’s Databricks SQL settings page and the AWS console.   |     This article describes how to perform these steps. If you are not able to perform all of these roles (for example, you are a workspace admin but not an account admin or you do not have access to your AWS IAM roles), you may need to contact others in your organization to perform some steps.       If acceptance of terms is required for your account to enable serverless compute, any workspace admin that uses Databricks SQL sees a banner on the top of each page to indicate that your account admin must  [accept the terms of use](#accept-terms)  . If the workspace admin is not an account admin, contact your account admin to do this step.        Note       Databricks changed the name from  *SQL endpoint*   to  *SQL warehouse*   because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it’s connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.           Requirements --------------     Account requirements:     * Your Databricks account must be on the E2 version of the platform. * Your Databricks workspace must be on the  [Premium or higher pricing tier](https://databricks.com/product/aws-pricing)   . * Your Databricks account must not be on a free trial. * Your Databricks account must not have the  [compliance security profile](../../security/privacy/security-profile.html)  enabled at the account level.     Workspace requirements:     * Your Databricks account must not have the  [compliance security profile](../../security/privacy/security-profile.html)  enabled at the workspace level for any workspaces that you intend to use with any Serverless compute features such as <ServerlessWarehouses>. * Your workspace must not use an  [external Hive legacy metastore](../../data/metastores/external-hive-metastore.html)  . * Your workspace must not use S3 access policies.     Other feature interactions:     * [Cluster policies, including spot instance policies](../../administration-guide/clusters/policies.html)  are unsupported. * [Customer-managed VPCs](../../administration-guide/cloud-configurations/aws/customer-managed-vpc.html)  are not applicable to compute resources for serverless SQL warehouses. * Although the Serverless data plane for serverless SQL warehouses does not use the  [customer-configurable AWS PrivateLink connectivity for the Classic data plane](../../administration-guide/cloud-configurations/aws/privatelink.html)  , it does use private connectivity to connect to the Databricks  [control plane](../../getting-started/overview.html)  . * Although the serverless data plane does not use the  [secure cluster connectivity relay for the classic data plane](../../security/network/secure-cluster-connectivity.html)  , serverless SQL warehouses do  **not**   have public IP addresses. * Serverless SQL warehouses do not use customer-managed keys for EBS storage encryption, which is an optional part of the  [customer-managed keys for workspace storage](../../security/keys/customer-managed-keys-storage-aws.html)  feature configuration. Disks for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. For example, when serverless SQL warehouses are stopped or scaled down, the VMs and their storage are destroyed. See  [Serverless compute and customer-managed keys](../../security/keys/customer-managed-keys.html#serverless)  .     For a list of regions that support serverless SQL warehouses, see  [Databricks clouds and regions](../../resources/supported-regions.html)  .       Also note that the Databricks  [documentation on cluster size instance types and CPU quotas](create-sql-warehouse.html#cluster-size)  applies only to pro and classic SQL warehouses, not to serverless SQL warehouses.           Serverless quotas -------------------         Serverless quotas are a safety measure for serverless compute. Serverless quotas restrict how many serverless compute resources a customer can have at any given time. The quota is enforced at the regional level for all workspaces in your account. Quotas are enforced only for  [serverless SQL warehouses](#)  . See  [Serverless quotas](../../serverless-compute/serverless-quotas.html)  .           Step 1: If prompted, accept updated account terms of use ----------------------------------------------------------      Important       If your account needs updated  [terms of use](#accept-terms)  , workspace admins are prompted in the Databricks SQL UI. If you are a workspace admin and you do not see a yellow notification when using Databricks SQL, you can skip this step.        If you are not an account owner or account administrator, you cannot perform this step. Contact the account owner or an account administrator before continuing to the next steps in this article.     1. As an account owner or account administrator, go to the  [feature enablement tab of the account console settings page](https://accounts.cloud.databricks.com/settings/feature-enablement)   . 2. Next to  **Enable use of serverless compute**   , click the blue button  **Enable**   .       If the blue button does not appear but there is text that says  **Enabled**   , this step is already complete. Continue to  [Step 2: Confirm or set up an AWS instance profile to use with your serverless SQL warehouses](#aws-instance-profile-setup)  . 3. A pop-up appears about agreeing to applicable terms of use. Click the link to open the applicable terms in a new browser tab. When complete, return to the original tab and click the  **Enable**   button in the pop-up.           Step 2: Confirm or set up an AWS instance profile to use with your serverless SQL warehouses ----------------------------------------------------------------------------------------------     An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. You can optionally  [configure an AWS instance profile](../../aws/iam/instance-profile-tutorial.html)  for Databricks SQL to connect to AWS S3 buckets other than your root bucket.       If you already use an instance profile with Databricks SQL, the role associated with the instance profile needs a Databricks Serverless compute trust relationship statement so that serverless SQL warehouses can use it.       Depending on how and when your instance profile was created, you might not need to modify the role because it may already have the trust relationship. If the instance profile was created in the following ways, it likely has the trust relationship statement:     * After June 24, 2022, your instance profile was created as part of creating a Databricks workspace by using AWS Quickstart. * After June 24, 2022, someone in your organization followed steps in the Databricks article to  [create the instance profile manually](../../aws/iam/instance-profile-tutorial.html)  .     This section describes how to  **confirm or update**   that the role associated with the instance profile has the trust relationship statement. That enables your serverless SQL warehouses to use the role to access your S3 buckets.        Important       To perform these steps, you must be a Databricks workspace admin to confirm which instance profile your workspace uses for Databricks SQL. You must also be an AWS account administrator to check the role’s trust relationship policy or make any necessary changes. If you are not both of these types of admin, contact the appropriate admins in your organization to complete these steps.      1. In the  [admin console](../../administration-guide/workspace/index.html#admin-console)  , click the  **SQL Warehouse Settings**   tab. 2. Look in the  **Data Security**   section for the  **Instance Profile**   field. Confirm whether your workspace is configured to use an AWS instance profile for Databricks SQL to connect to AWS S3 buckets other than your root bucket.      * If you are using an instance profile, its name is visible in the   **Instance Profile**    field. Make a note of it for the next step.  * If the field value is   **None**    , you are not using an   [instance profile](../../aws/iam/instance-profile-tutorial.html)   to access S3 buckets other than your workspace’s root bucket. Setup is complete. Skip to   [Step 3](#test)   . 3. Confirm whether your instance profile name matches the associated role name.      1. In the AWS console, go to the   [IAM service’s Roles tab](https://console.aws.amazon.com/iam/home#/roles)    . It lists all the IAM roles in your account.  2. Click the role with the name that matches the instance profile name in the Databricks SQL admin settings in the   **Data Security**    section for the   **Instance Profile**    field that you found earlier in this section.  3. In the summary area, find the   **Role ARN**    and   **Instance Profile ARNs**    fields.  4. Check if the last part of those two fields have matching names after the final slash. For example:           ![Does instance profile name and role arn name match](../../_images/serverless-compute-aws-console-instance-profile-names.png) 4. If you determined in the previous step that the role name (the text after the last slash in the role ARN) and the instance profile name (the text after the last slash in the instance profile ARN) do not match,  [edit your instance profile registration](../../aws/iam/instance-profile-tutorial.html#edit-instance-profile)  to specify your IAM role ARN.      1. To	Using DB SQL Serverless allows for the creation of serverless SQL warehouses that enable instant compute and are managed by Databricks. These warehouses use compute resources in your Databricks account and can be used with Databricks SQL queries the same way you would with the original customer-hosted SQL warehouses. Serverless SQL warehouses do not have public IP addresses and have certain requirements and quotas that need to be met.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-functions-builtin-alpha.html	What is the list of databricks functions?	Alphabetical list of built-in functions =========================================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       This article provides an alphabetically-ordered list of built-in functions and operators in Databricks.      * [`abs`  function](functions/abs.html) * [`acos`  function](functions/acos.html) * [`acosh`  function](functions/acosh.html) * [`add\_months`  function](functions/add_months.html) * [`aes\_decrypt`  function](functions/aes_decrypt.html) * [`aes\_encrypt`  function](functions/aes_encrypt.html) * [`aggregate`  function](functions/aggregate.html) * [`&`  (ampersand sign) operator](functions/ampersandsign.html) * [`and`  predicate](functions/and.html) * [`any`  aggregate function](functions/any.html) * [`any\_value`  aggregate function](functions/any_value.html) * [`approx\_count\_distinct`  aggregate function](functions/approx_count_distinct.html) * [`approx\_percentile`  aggregate function](functions/approx_percentile.html) * [`approx\_top\_k`  aggregate function](functions/approx_top_k.html) * [`array`  function](functions/array.html) * [`array\_agg`  aggregate function](functions/array_agg.html) * [`array\_append`  function](functions/array_append.html) * [`array\_compact`  function](functions/array_compact.html) * [`array\_contains`  function](functions/array_contains.html) * [`array\_distinct`  function](functions/array_distinct.html) * [`array\_except`  function](functions/array_except.html) * [`array\_intersect`  function](functions/array_intersect.html) * [`array\_join`  function](functions/array_join.html) * [`array\_max`  function](functions/array_max.html) * [`array\_min`  function](functions/array_min.html) * [`array\_position`  function](functions/array_position.html) * [`array\_remove`  function](functions/array_remove.html) * [`array\_repeat`  function](functions/array_repeat.html) * [`array\_size`  function](functions/array_size.html) * [`array\_sort`  function](functions/array_sort.html) * [`array\_union`  function](functions/array_union.html) * [`arrays\_overlap`  function](functions/arrays_overlap.html) * [`arrays\_zip`  function](functions/arrays_zip.html) * [`ascii`  function](functions/ascii.html) * [`asin`  function](functions/asin.html) * [`asinh`  function](functions/asinh.html) * [`assert\_true`  function](functions/assert_true.html) * [`\*`  (asterisk sign) operator](functions/asterisksign.html) * [`atan`  function](functions/atan.html) * [`atan2`  function](functions/atan2.html) * [`atanh`  function](functions/atanh.html) * [`avg`  aggregate function](functions/avg.html) * [`!=`  (bangeq sign) operator](functions/bangeqsign.html) * [`!`  (bang sign) operator](functions/bangsign.html) * [`base64`  function](functions/base64.html) * [`between`  predicate](functions/between.html) * [`bigint`  function](functions/bigint.html) * [`bin`  function](functions/bin.html) * [`binary`  function](functions/binary.html) * [`bit\_and`  aggregate function](functions/bit_and.html) * [`bit\_count`  function](functions/bit_count.html) * [`bit\_get`  function](functions/bit_get.html) * [`bit\_length`  function](functions/bit_length.html) * [`bit\_or`  aggregate function](functions/bit_or.html) * [`bit\_reverse`  function](functions/bit_reverse.html) * [`bit\_xor`  aggregate function](functions/bit_xor.html) * [`bool\_and`  aggregate function](functions/bool_and.html) * [`bool\_or`  aggregate function](functions/bool_or.html) * [`boolean`  function](functions/boolean.html) * [`[     ]`  (bracket sign) operator](functions/bracketsign.html) * [`bround`  function](functions/bround.html) * [`btrim`  function](functions/btrim.html) * [`cardinality`  function](functions/cardinality.html) * [`^`  (caret sign) operator](functions/caretsign.html) * [`case`  expression](functions/case.html) * [`cast`  function](functions/cast.html) * [`cbrt`  function](functions/cbrt.html) * [`ceil`  function](functions/ceil.html) * [`ceiling`  function](functions/ceiling.html) * [`char`  function](functions/char.html) * [`char\_length`  function](functions/char_length.html) * [`character\_length`  function](functions/character_length.html) * [`charindex`  function](functions/charindex.html) * [`chr`  function](functions/chr.html) * [`cloud\_files\_state`  table-valued function](functions/cloud_files_state.html) * [`coalesce`  function](functions/coalesce.html) * [`collect\_list`  aggregate function](functions/collect_list.html) * [`collect\_set`  aggregate function](functions/collect_set.html) * [`::`  (colon colon sign) operator](functions/coloncolonsign.html) * [`:`  (colon sign) operator](functions/colonsign.html) * [`concat`  function](functions/concat.html) * [`concat\_ws`  function](functions/concat_ws.html) * [`contains`  function](functions/contains.html) * [`conv`  function](functions/conv.html) * [`corr`  aggregate function](functions/corr.html) * [`cos`  function](functions/cos.html) * [`cosh`  function](functions/cosh.html) * [`cot`  function](functions/cot.html) * [`count`  aggregate function](functions/count.html) * [`count\_if`  aggregate function](functions/count_if.html) * [`count\_min\_sketch`  aggregate function](functions/count_min_sketch.html) * [`covar\_pop`  aggregate function](functions/covar_pop.html) * [`covar\_samp`  aggregate function](functions/covar_samp.html) * [`crc32`  function](functions/crc32.html) * [`csc`  function](functions/csc.html) * [`cube`  function](functions/cube.html) * [`cume\_dist`  analytic window function](functions/cume_dist.html) * [`curdate`  function](functions/curdate.html) * [`current\_catalog`  function](functions/current_catalog.html) * [`current\_database`  function](functions/current_database.html) * [`current\_date`  function](functions/current_date.html) * [`current\_metastore`  function](functions/current_metastore.html) * [`current\_schema`  function](functions/current_schema.html) * [`current\_timestamp`  function](functions/current_timestamp.html) * [`current\_timezone`  function](functions/current_timezone.html) * [`current\_user`  function](functions/current_user.html) * [`current\_version`  function](functions/current_version.html) * [`date`  function](functions/date.html) * [`date\_add`  function](functions/date_add.html) * [`date\_format`  function](functions/date_format.html) * [`date\_from\_unix\_date`  function](functions/date_from_unix_date.html) * [`date\_part`  function](functions/date_part.html) * [`date\_sub`  function](functions/date_sub.html) * [`date\_trunc`  function](functions/date_trunc.html) * [`dateadd`  function](functions/dateadd.html) * [`dateadd`  (days) function](functions/dateadd2.html) * [`datediff`  function](functions/datediff.html) * [`datediff`  (timestamp) function](functions/datediff3.html) * [`day`  function](functions/day.html) * [`dayofmonth`  function](functions/dayofmonth.html) * [`dayofweek`  function](functions/dayofweek.html) * [`dayofyear`  function](functions/dayofyear.html) * [`decimal`  function](functions/decimal.html) * [`decode`  function](functions/decode.html) * [`decode`  (character set) function](functions/decode_cs.html) * [`degrees`  function](functions/degrees.html) * [`dense\_rank`  ranking window function](functions/dense_rank.html) * [`div`  operator](functions/div.html) * [`.`  (dot sign) operator](functions/dotsign.html) * [`double`  function](functions/double.html) * [`e`  function](functions/e.html) * [`element\_at`  function](functions/element_at.html) * [`elt`  function](functions/elt.html) * [`encode`  function](functions/encode.html) * [`endswith`  function](functions/endswith.html) * [`==`  (eq eq sign) operator](functions/eqeqsign.html) * [`=`  (eq sign) operator](functions/eqsign.html) * [`equal\_null`  function](functions/equal_null.html) * [`every`  aggregate function](functions/every.html) * [`exists`  function](functions/exists.html) * [`exp`  function](functions/exp.	The list of Databricks functions can be found in an alphabetically-ordered list in the Databricks documentation. The list includes functions such as `abs`, `acos`, `add_months`, `aggregate`, `array`, `ascii`, `avg`, `base64`, `ceil`, `concat`, `count`, `date`, `decode`, `double`, `element_at`, `endswith`, `exp`, and many more.	1	1	1
no_reaction	https://docs.databricks.com/archive/spark-sql-2.x-language-manual/merge-into.html	can i use merge into statement inside pipeline ?	MERGE INTO ============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Merges a set of updates, insertions, and deletions based on a source table into a target Delta table.       This statement is supported only for Delta Lake tables.        Syntax --------      ``` MERGE INTO target\_table\_name [target\_alias]    USING source\_table\_reference [source\_alias]    ON merge\_condition    { WHEN MATCHED [ AND matched\_condition ] THEN matched\_action |      WHEN NOT MATCHED [BY TARGET] [ AND not\_matched\_condition ] THEN not\_matched\_action |      WHEN NOT MATCHED BY SOURCE [ AND not\_matched\_by\_source\_condition ] THEN not\_matched\_by\_source\_action } [...]  matched\_action  { DELETE |    UPDATE SET \* |    UPDATE SET { column = { expr | DEFAULT } } [, ...] }  not\_matched\_action  { INSERT \* |    INSERT (column1 [, ...] ) VALUES ( expr | DEFAULT ] [, ...] )  not\_matched\_by\_source\_action  { DELETE |    UPDATE SET { column = { expr | DEFAULT } } [, ...] }  ```        Parameters ------------   * **[target\_table\_name](sql-ref-names.html#table-name)**      A  [Table name](sql-ref-names.html#table-name)  identifying the table being modified. The table referenced must be a Delta table. * **[target\_alias](sql-ref-names.html#table-alias)**      A  [Table alias](sql-ref-names.html#table-alias)  for the target table. The alias must not include a column list. * **[source\_table\_reference](sql-ref-names.html#table-name)**      A  [Table name](sql-ref-names.html#table-name)  identifying the source table to be merged into the target table. * **[source\_alias](sql-ref-names.html#table-alias)**      A  [Table alias](sql-ref-names.html#table-alias)  for the source table. The alias must not include a column list. * **ON  [merge\_condition](sql-ref-expression.html)**      How the rows from one relation are combined with the rows of another relation. An expression with a return type of BOOLEAN. * **`WHEN     MATCHED     [     AND` [matched\_condition](sql-ref-expression.html) `]`**     `WHEN     MATCHED`  clauses are executed when a source row matches a target table row based on the  `merge\_condition`  and the optional  `match\_condition`  . * **matched\_action**      + **`DELETE`**           Deletes the matching target table row.            Multiple matches are allowed when matches are unconditionally deleted. An unconditional delete is not ambiguous, even if there are multiple matches.  + **`UPDATE`**           Updates the matched target table row.            To update all the columns of the target Delta table with the corresponding columns of the source dataset, use   `UPDATE        SET        \*`   .  This is equivalent to   `UPDATE        SET        col1        =        source.col1        [,        col2        =        source.col2        ...]`   for all the columns of the target Delta table.  Therefore, this action assumes that the source table has the same columns as those in the target table, otherwise the query will throw an analysis error.              Note            This behavior changes when automatic schema migration is enabled. See   [Automatic schema evolution for Delta Lake merge](../../delta/update-schema.html#merge-schema-evolution)   for details.             **Applies to:**   ![check marked yes](../../_images/check.png)   Databricks SQL   [SQL warehouse version 2022.35](../release-notes/index.html)   or higher   ![check marked yes](../../_images/check.png)   Databricks Runtime 11.2 and above            You can specify   `DEFAULT`   as   `expr`   to explicitly update the column to its default value.  If there are multiple  `WHEN     MATCHED`  clauses, then they are evaluated in the order they are specified. Each  `WHEN     MATCHED`  clause, except the last one, must have a  `matched\_condition`  . Otherwise, the query returns a  [NON\_LAST\_MATCHED\_CLAUSE\_OMIT\_CONDITION](../../error-messages/index.html#non_last_matched_clause_omit_condition)  error.       If none of the  `WHEN     MATCHED`  conditions evaluate to true for a source and target row pair that matches the  `merge\_condition`  , then the target row is left unchanged. * **`WHEN     NOT     MATCHED     [BY     TARGET]     [     AND` [not\_matched\_condition](sql-ref-expression.html) `]`**     `WHEN     NOT     MATCHED`  clauses insert a row when a source row does not match any target row based on the  `merge\_condition`  and the optional  `not\_matched\_condition`  .      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 12.1 and above      `WHEN     NOT     MATCHED     BY     TARGET`  can be used as an alias for  `WHEN     NOT     MATCHED`  .      `not\_matched\_condition`  must be a Boolean expression.      + `INSERT        \*`           Inserts all the columns of the target Delta table with the corresponding columns of the source dataset.  This is equivalent to   `INSERT        (col1        [,        col2        ...])        VALUES        (source.col1        [,        source.col2        ...])`   for all the columns of the target Delta table.  This action requires that the source table has the same columns as those in the target table.              Note            This behavior changes when automatic schema migration is enabled. See   [Automatic schema evolution for Delta Lake merge](../../delta/update-schema.html#merge-schema-evolution)   for details.  + `INSERT        (        ...        )        VALUES        (        ...        )`           The new row is generated based on the specified column and corresponding expressions.  All the columns in the target table do not need to be specified.  For unspecified target columns, the column default is inserted, or   `NULL`   if none exists.           **Applies to:**   ![check marked yes](../../_images/check.png)   Databricks SQL   [SQL warehouse version 2022.35](../release-notes/index.html)   or higher   ![check marked yes](../../_images/check.png)   Databricks Runtime 11.2 and above            You can specify   `DEFAULT`   as an expression to explicitly insert the column default for a target column.  If there are multiple  `WHEN     NOT     MATCHED`  clauses, then they are evaluated in the order they are specified. All  `WHEN     NOT     MATCHED`  clauses, except the last one, must have  `not\_matched\_condition`  s. Otherwise, the query returns a  [NON\_LAST\_NOT\_MATCHED\_CLAUSE\_OMIT\_CONDITION](../../error-messages/index.html#non_last_not_matched_clause_omit_condition)  error. * **`WHEN     NOT     MATCHED     BY     SOURCE     [     AND` [not\_matched\_by\_source\_condition](sql-ref-expression.html) `]`**     **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 12.1 and above      `WHEN     NOT     MATCHED     BY     SOURCE`  clauses are executed when a target row does not match any rows in the source table based on the  `merge\_condition`  and the optional  `not\_match\_by\_source\_condition`  evaluates to true.      `not\_matched\_by\_source\_condition`  must be a Boolean expression that only references columns from the target table. * **not\_matched\_by\_source\_action**      + **`DELETE`**           Deletes the target table row.  + **`UPDATE`**           Updates the target table row.   `expr`   may only reference columns from the target table, otherwise the query will throw an analysis error.           **Applies to:**   ![check marked yes](../../_images/check.png)   Databricks SQL   [SQL warehouse version 2022.35](../release-notes/index.html)   or higher   ![check marked yes](../../_images/check.png)   Databricks Runtime 11.2 and above            You can specify   `DEFAULT`   as   `expr`   to explicitly update the column to its default value.   Important       Adding a  `WHEN     NOT     MATCHED     BY     SOURCE`  clause to update or delete target rows when the  `merge\_condition`  evaluates to false can lead to a large number of target rows being modified. For best performance, apply  `not\_matched\_by\_source\_condition`  s to limit the number of target rows updated or deleted.        If there are multiple  `WHEN     NOT     MATCHED     BY     SOURCE     clauses`  , then they are evaluated in the order they are specified. Each  `WHEN     NOT     MATCHED     BY     SOURCE`  clause, except the last one, must have a	Yes, you can use the MERGE INTO statement inside a pipeline in Databricks. The MERGE INTO statement is supported in Databricks SQL and Databricks Runtime for Delta Lake tables.	0	1	1
no_reaction	https://docs.databricks.com/dbfs/file-browser.html	dbfs read-only file	What is the Databricks File System (DBFS)? ============================================     The Databricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls.        Note       Databricks workspaces deploy with a  [DBFS root volume](#dbfs-root)  , accessible to all users by default. Databricks recommends against storing production data in this location.           What can you do with DBFS? ----------------------------     DBFS provides convenience by mapping cloud object storage URIs to relative paths.     * Allows you to  [interact with object storage](#interact-files)  using directory and file semantics instead of cloud-specific API commands. * Allows you to  [mount](#mount-storage)  cloud object storage locations so that you can map storage credentials to paths in the Databricks workspace. * Simplifies the process of persisting files to object storage, allowing virtual machines and attached volume storage to be safely deleted on cluster termination. * Provides a convenient location for storing init scripts, JARs, libraries, and configurations for cluster initialization. * Provides a convenient location for checkpoint files created during model training with OSS deep learning libraries.      Note       DBFS is the Databricks implementation for FUSE. See  [How to work with files on Databricks](../files/index.html)  .            Interact with files in cloud-based object storage ---------------------------------------------------     DBFS provides many options for interacting with files in cloud object storage:         * [How to work with files on Databricks](../files/index.html) * [List, move, copy, and delete files with Databricks Utilities](../dev-tools/databricks-utils.html#file-system-utility-dbutilsfs) * [Browse files in DBFS](file-browser.html) * [Upload files to DBFS with the UI](../archive/legacy/data-tab.html) * [Interact with DBFS files using the Databricks CLI](../dev-tools/cli/dbfs-cli.html) * [Interact with DBFS files using the Databricks REST API](../dev-tools/api/latest/dbfs.html)             Mount object storage ----------------------     Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage, so you do not need to specify these settings in code or during cluster configuration.       For more information, see  [Mounting cloud object storage on Databricks](mounts.html)  .           What is the DBFS root? ------------------------     The  *DBFS root*   is the default storage location for a Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Databricks workspace. For details on Databricks Filesystem root configuration and deployment, see  [Configure AWS storage](../administration-guide/cloud-configurations/aws/aws-storage.html)  . For best practices around securing data in the DBFS root, see  [Recommendations for working with DBFS root](dbfs-root.html)  .       Some users of Databricks may refer to the DBFS root as “DBFS” or “the DBFS”; it is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a cloud object storage location. You use DBFS to interact with the DBFS root, but they are distinct concepts, and DBFS has many applications beyond the DBFS root.       The DBFS root contains a number of special locations that serve as defaults for various actions performed by users in the workspace. For details, see  [What directories are in DBFS root by default?](root-locations.html)  .           How does DBFS work with Unity Catalog? ----------------------------------------     Unity Catalog adds the concepts of external locations and managed storage credentials to help organizations provide least privileges access to data in cloud object storage. Unity Catalog also provides a new default storage location for managed tables. Some security configurations provide direct access to both Unity Catalog-managed resources and DBFS. Databricks has compiled recommendations for  [using DBFS and Unity Catalog](unity-catalog.html)  .	DBFS is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. It is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls. DBFS provides convenience by mapping cloud object storage URIs to relative paths and allows you to interact with object storage using directory and file semantics instead of cloud-specific API commands. It also allows you to mount cloud object storage locations so that you can map storage credentials to paths in the Databricks workspace.	0	1	1
no_reaction	https://docs.databricks.com/machine-learning/model-serving/migrate-model-serving.html	ow do you deploy my model for serving?	Migrate to Model Serving ==========================     This article demonstrates how to enable Model Serving on your workspace and switch your models from using  [Legacy MLflow Model Serving](../../archive/legacy-model-serving/model-serving.html)  to the new  [Model Serving](index.html)  experience built on serverless compute.        Requirements --------------   * Registered model in the MLflow Model Registry. * Permissions on the registered models as described in the  [access control guide](../../security/auth-authz/access-control/serving-endpoint-acl.html)  .       Significant changes ---------------------   * In Model Serving, the format of the request to the endpoint and the response from the endpoint are slightly different from Legacy MLflow Model Serving. See  [Scoring a model endpoint](create-manage-serving-endpoints.html#score)  for details on the new format protocol. * In Model Serving, the endpoint URL includes  `serving-endpoints`  instead of  `model`  . * Model Serving includes full support for  [managing resources with API workflows](create-manage-serving-endpoints.html)  . * Model Serving is production-ready and backed by the Databricks SLA.       Enable Model Serving for your workspace -----------------------------------------     To use Model Serving, your account admin must read and accept the terms and conditions in the account console.       No additional steps are required to enable Model Serving in your workspace.         Migrate served models to Model Serving ----------------------------------------     You can create a Model Serving endpoint, and flexibly transition model serving workflows without disabling Legacy MLflow Model Serving.       The following steps show how to accomplish this with the UI. For each model on which you have Legacy MLflow Model Serving enabled:     1. Navigate to  **Serving endpoints**   on the sidebar of your machine learning workspace. 2. Follow the workflow described in  [UI workflow](create-manage-serving-endpoints.html#model-serving-ui)  on how to create a serving endpoint with your model. 3. Transition your application to use the new URL provided by the serving endpoint to query the model, along with the new scoring format. 4. When your models are transitioned over, you can navigate to  **Models**   on the sidebar of your machine learning workspace. 5. Select the model for which you want to disable Legacy MLflow Model Serving. 6. On the  **Serving**   tab, select  **Stop**   . 7. A message appears to confirm. Select  **Stop Serving**   .       Additional resources ----------------------   * [Manage Model Serving endpoints](create-manage-serving-endpoints.html)	To deploy your model for serving, you need to enable Model Serving on your workspace and migrate your models from using Legacy MLflow Model Serving to the new Model Serving experience built on serverless compute. You can create a Model Serving endpoint and transition your application to use the new URL provided by the serving endpoint to query the model, along with the new scoring format.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/index.html	how is the unity catalog credentials secured	Data discovery and collaboration in the lakehouse ===================================================     Databricks designed Unity Catalog to help organizations reduce time to insights by empowering a broader set of data users to discover and analyze data at scale. Data stewards can securely grant access to data assets for diverse teams of end users in Unity Catalog. These users can then use a variety of languages and tools, including SQL and Python, to create derivative datasets, models, and dashboards that can be shared across teams.        Manage permissions at scale -----------------------------     Unity Catalog provides administrators a unified location to assign permissions for catalogs, databases, tables, and views to groups of users. Privileges and metastores are shared across workspaces, allowing administrators to set secure permissions once against groups synced from identity providers and know that end users only have access to the proper data in any Databricks workspace they enter.       Unity Catalog also allows administrators to define storage credentials, a secure way to store and share permissions on cloud storage infrastructure. You can grant privileges on these securables to power users within the organization so they can define external locations against cloud object storage locations, allowing data engineers to self-service for new workloads without needing to provide elevated permissions in cloud account consoles.         Discover data on Databricks -----------------------------     Users can browse available data objects in Unity Catalog using the  [Data Explorer](../data/index.html)  . Data Explorer uses the privileges configured by Unity Catalog administrators to ensure that users are only able to see catalogs, databases, tables, and views that they have permissions to query. Once users find a dataset of interest, they can review field names and types, read comments on tables and individual fields, and preview a sample of the data. Users can also review the full history of the table to understand when and how data has changed, and the lineage feature allows users to track how certain datasets are derived from upstream jobs and used in downstream jobs.       Storage credentials and external locations are also displayed in Data Explorer, allowing each user to fully grasp the privileges they have to read and write data across available locations and resources.         Accelerate time to production with the lakehouse --------------------------------------------------     Databricks supports workloads in SQL, Python, Scala, and R, allowing users with diverse skill sets and technical backgrounds to leverage their knowledge to derive analytic insights. You can use all languages supported by Databricks to define production jobs, and notebooks can leverage a combination of languages. This means that you can promote queries written by SQL analysts for last mile ETL into production data engineering code with almost no effort. Queries and workloads defined by personas across the organization leverage the same datasets, so there’s no need to reconcile field names or make sure dashboards are up to date before sharing code and results with other teams. You can securely share code, notebooks, queries, and dashboards, all powered by the same scalable cloud infrastructure and defined against the same curated data sources.	Unity Catalog allows administrators to define storage credentials, which is a secure way to store and share permissions on cloud storage infrastructure. These credentials are displayed in Data Explorer, allowing each user to fully grasp the privileges they have to read and write data across available locations and resources.	0	1	1
thumb_down	https://docs.databricks.com/ingestion/index.html	1. How do I load files into Databricks?	LOAD DATA ===========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Loads the data into a Hive SerDe table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the  `LOAD     DATA`  statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table.       If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the table or the dependents are accessed the next time.        Syntax --------      ``` LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table\_name [ PARTITION clause ]  ```        Parameters ------------   * **path**       Path of the file system. It can be either an absolute or a relative path. * **[table\_name](sql-ref-names.html#table-name)**      Identifies the table to be inserted to. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the table cannot be found Databricks raises a  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error. * **[PARTITION clause](sql-ref-partition.html#partition)**      An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition. * **LOCAL**       If specified, it causes the  `INPATH`  to be resolved against the local file system, instead of the default file system, which is typically a distributed storage. * **OVERWRITE**       By default, new data is appended to the table. If  `OVERWRITE`  is used, the table is instead overwritten with new data.       Examples ----------      ``` -- Example without partition specification. -- Assuming the students table has already been created and populated. > SELECT \* FROM students;  name address student\_id  --------- ---------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111   > CREATE TABLE test\_load (name VARCHAR(64), address VARCHAR(64), student\_id INT) USING HIVE;  -- Assuming the students table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test\_load;  > SELECT \* FROM test\_load;  name address student\_id  --------- ---------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111  -- Example with partition specification. > CREATE TABLE test\_partition (c1 INT, c2 INT, c3 INT) PARTITIONED BY (c2, c3);  > INSERT INTO test\_partition PARTITION (c2 = 2, c3 = 3) VALUES (1);  > INSERT INTO test\_partition PARTITION (c2 = 5, c3 = 6) VALUES (4);  > INSERT INTO test\_partition PARTITION (c2 = 8, c3 = 9) VALUES (7);  > SELECT \* FROM test\_partition;  c1 c2 c3  --- --- ---  1 2 3  4 5 6  7 8 9  > CREATE TABLE test\_load\_partition (c1 INT, c2 INT, c3 INT) USING HIVE PARTITIONED BY (c2, c3);  -- Assuming the test\_partition table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/test\_partition/c2=2/c3=3'  OVERWRITE INTO TABLE test\_load\_partition PARTITION (c2=2, c3=3);  > SELECT \* FROM test\_load\_partition;  c1 c2 c3  --- --- ---  1 2 3  ```        Related articles ------------------   * [INSERT INTO](sql-ref-syntax-dml-insert-into.html) * [COPY INTO](delta-copy-into.html)	To load files into Databricks, you can use the `LOAD DATA` statement to load data into a Hive SerDe table from the user-specified directory or file. If a directory is specified, then all the files from the directory are loaded. If a file is specified, then only the single file is loaded. Additionally, the `LOAD DATA` statement takes an optional partition specification. When a partition is specified, the data files (when the input source is a directory) or the single file (when the input source is a file) are loaded into the partition of the target table.	1	1	1
no_reaction	https://docs.databricks.com/machine-learning/train-model/pytorch.html	How Can  Install pytorch?	    PyTorch =========    [PyTorch project](https://github.com/pytorch)   is a Python package that provides GPU accelerated tensor computation and high level functionalities for building deep learning networks. For licensing details, see the PyTorch  [license doc on GitHub](https://github.com/pytorch/pytorch/blob/a90c259edad1ea4fa1b8773e3cb37240df680d62/LICENSE)   .       To monitor and debug your PyTorch models, consider using  [TensorBoard](tensorflow.html#using-tensorboard)  .       PyTorch is included in Databricks Runtime for Machine Learning. If you are using Databricks Runtime, see  [Install PyTorch](#install-pytorch)  for instructions on installing PyTorch.        Note       This is not a comprehensive guide to PyTorch. For more information, see the  [PyTorch website](https://pytorch.org/)   .         Single node and distributed training --------------------------------------     To test and migrate single-machine workflows, use a  [Single Node cluster](../../clusters/single-node.html)  .       For distributed training options for deep learning, see  [Distributed training](distributed-training/index.html)  .         Example notebook ------------------      ###   PyTorch notebook     [Open notebook in new tab](/_extras/notebooks/source/deep-learning/pytorch-single-node.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Install PyTorch -----------------      ###   Databricks Runtime for ML    [Introduction to Databricks Runtime for Machine Learning](../../runtime/mlruntime.html)  includes PyTorch so you can create the cluster and start using PyTorch. For the version of PyTorch installed in the Databricks Runtime ML version you are using, see the  [release notes](../../release-notes/runtime/releases.html)  .         ###   Databricks Runtime     Databricks recommends that you use the PyTorch included on  [Introduction to Databricks Runtime for Machine Learning](../../runtime/mlruntime.html)  . However, if you must use  [Databricks Runtime](../../runtime/dbr.html)  , PyTorch can be installed as a  [Databricks PyPI library](../../libraries/index.html)  . The following example shows how to install PyTorch 1.5.0:     * On GPU clusters, install  `pytorch`  and  `torchvision`  by specifying the following:      + `torch==1.5.0`  + `torchvision==0.6.0` * On CPU clusters, install  `pytorch`  and  `torchvision`  by using the following wheel files:        ``` https://download.pytorch.org/whl/cpu/torch-1.5.0%2Bcpu-cp37-cp37m-linux_x86_64.whl  https://download.pytorch.org/whl/cpu/torchvision-0.6.0%2Bcpu-cp37-cp37m-linux_x86_64.whl  ```          Errors and troubleshooting for distributed PyTorch ----------------------------------------------------     The following sections describe common error messages and troubleshooting guidance for the classes:  [PyTorch DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)   or  [PyTorch DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel)   . Most of these errors can likely be resolved with  [TorchDistributor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.torch.distributor.TorchDistributor.html)   , which is available on Databricks Runtime ML 13.0 and above. However, if  `TorchDistributor`  is not a viable solution, recommended solutions are also provided within each section.       The following is an example of how to use TorchDistributor:        ```  from pyspark.ml.torch.distributor import TorchDistributor  def train\_fn(learning\_rate):         # ...  num\_processes=2 distributor = TorchDistributor(num\_processes=num\_processes, local\_mode=True)  distributor.run(train\_fn, 1e-3)  ```     ###  `process     0     terminated     with     exit     code     1`     This error occurs when using notebooks, regardless of environment: Databricks, local machine, etc. To avoid this error, use  `torch.multiprocessing.start\_processes`  with  `start\_method=fork`  instead of  `torch.multiprocessing.spawn`  .       For example:        ``` import torch  def train\_fn(rank, learning\_rate):     # required setup, e.g. setup(rank)         # ...  num\_processes = 2 torch.multiprocessing.start\_processes(train\_fn, args=(1e-3,), nprocs=num\_processes, start\_method="fork")  ```      ###  `The     server     socket     has     failed     to     bind     to     [::]:{PORT     NUMBER}     (errno:     98     -     Address     already     in     use).`     This is error appears when you restart the distributed training after interrupting the cell while training is happening.       To resolve, restart the cluster. If that does not solve the problem, there may be an error in the training function code.       ###   CUDA related errors     You can run into additional issues with CUDA since  `start\_method=”fork”`  is  [not CUDA-compatible](https://github.com/pytorch/pytorch/blob/master/torch/multiprocessing/spawn.py#L173)   . Using any  `.cuda`  commands in any cell might lead to failures. To avoid these errors, add the following check before you call  `torch.multiprocessing.start\_method`  :        ``` if torch.cuda.is\_initialized():     raise Exception("CUDA was initialized; distributed training will fail.") # or something similar  ```                                                        	To install PyTorch, you can follow the instructions provided in the context. If you are using Databricks Runtime for Machine Learning, PyTorch is already included. If you are using Databricks Runtime, you can install PyTorch as a Databricks PyPI library. The context provides an example of how to install PyTorch 1.5.0 on CPU clusters and GPU clusters.	0	1	1
no_reaction	https://docs.databricks.com/repos/index.html	how to pull changes from an external git provider back to a databricks workspace	Git integration with Databricks Repos =======================================     Databricks Repos is a visual Git client in Databricks. It supports common Git operations such a cloning a repository, committing and pushing, pulling, branch management, and visual comparison of diffs when committing.       Within Repos you can develop code in notebooks or other files and follow data science and engineering code development best practices using Git for version control, collaboration, and CI/CD.        What can you do with Databricks Repos? ----------------------------------------     Databricks Repos provides source control for data and AI projects by integrating with Git providers.       In Databricks Repos, you can use Git functionality to:     * Clone, push to, and pull from a remote Git repository. * Create and manage branches for development work. * Create notebooks, and edit notebooks and other files. * Visually compare differences upon commit.     For step-by-step instructions, see  [Clone a Git repo & other common Git operations](git-operations-with-repos.html)  . Databricks Repos also has an  [API](../dev-tools/api/latest/repos.html)  that you can integrate with your CI/CD pipeline. For example, you can programmatically update a Databricks repo so that it always has the most recent version of the code. For information about best practices for code development using Databricks Repos, see  [CI/CD workflows with Git integration and Databricks Repos](ci-cd-techniques-with-repos.html)  .       For following tasks, work in your Git provider:     * Create a pull request. * Resolve merge conflicts. * Merge or delete branches. * Rebase a branch.       Supported Git providers -------------------------     Databricks supports the following Git providers:     * GitHub * Bitbucket Cloud * GitLab * Azure DevOps * AWS CodeCommit * GitHub AE     See  [Get a Git access token & connect a remote repo to Databricks](get-access-tokens-from-git-provider.html)  .       Databricks Repos also supports Bitbucket Server, GitHub Enterprise Server, and GitLab self-managed integration, if the server is internet accessible. To integrate with a private Git server instance that is not internet-accessible, get in touch with your Databricks representative.       Support for arbitrary files in Databricks Repos is available in Databricks Runtime 8.4 and above. See  [What are workspace files?](../files/workspace.html)  .         Next step -----------    [Set up Databricks Repos](repos-setup.html)	To pull changes from an external Git provider back to a Databricks workspace, you can use the Git functionality within Databricks Repos. You can clone, push to, and pull from a remote Git repository, create and manage branches for development work, and visually compare differences upon commit. For step-by-step instructions, you can refer to the "Clone a Git repo & other common Git operations" section in the provided context. Additionally, Databricks Repos has an API that you can integrate with your CI/CD pipeline to programmatically update a Databricks repo so that it always has the most recent version of the code.	0	1	1
no_reaction	https://docs.databricks.com/administration-guide/workspace/create-workspace.html	create new workspace	Manage your Databricks workspaces ===================================     This article is an overview of your options for creating and managing workspaces. It also includes guidance on accessing the admin console and workspace settings.          What is a workspace? ----------------------     A workspace is a Databricks deployment in a cloud service account. It provides a unified environment for working with Databricks assets for a specified set of users.         Create a workspace --------------------     There are a few different methods you can use to create new workspaces. These are:     * Using the Databricks  [account console](create-workspace.html)  UI * Calling the Databricks  [Account API](create-workspace-api.html) * Using the Databricks  [Terraform Provider](../../dev-tools/terraform/e2-workspace.html) * [AWS Quick Start](templates.html#quickstart)  (not formally supported by Databricks)         Access the admin console --------------------------     The admin console is where you can manage features and settings for your Databricks workspace. To access the admin console, click your username in the top bar of the Databricks workspace and select  **Admin Console**   .         Manage workspace settings ---------------------------     The admin console includes a tab for  **Workspace Settings**   where you can manage workspace-specific behavior. For more information, see  [Manage workspace settings](settings/index.html)  .	To create a new workspace, there are a few different methods you can use, including using the Databricks account console UI, calling the Databricks Account API, using the Databricks Terraform Provider, or AWS Quick Start (not formally supported by Databricks).	1	1	1
no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.merge.html	pyspark merge	   pyspark.pandas.merge  [¶](#pyspark-pandas-merge "Permalink to this headline") ===============================================================================  `pyspark.pandas.`  `merge`    (   *obj     :     pyspark.pandas.frame.DataFrame*  ,  *right     :     pyspark.pandas.frame.DataFrame*  ,  *how     :     str     =     'inner'*  ,  *on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *left\_on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *right\_on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *left\_index     :     bool     =     False*  ,  *right\_index     :     bool     =     False*  ,  *suffixes     :     Tuple    [    str    ,    str    ]     =     '\_x', '\_y'*   )    → pyspark.pandas.frame.DataFrame  [[source]](../../../_modules/pyspark/pandas/namespace.html#merge) [¶](#pyspark.pandas.merge "Permalink to this definition")    Merge DataFrame objects with a database-style join.     The index of the resulting DataFrame will be one of the following:    * 0…n if no index is used for merging * Index of the left DataFrame if merged only on the index of the right DataFrame * Index of the right DataFrame if merged only on the index of the left DataFrame * All involved indices if merged using the indices of both DataFrames     e.g. if    left    with indices (a, x) and    right    with indices (b, x), the result will be an index (x, a, b)   Parameters    **right: Object to merge with.**   **how: Type of merge to be performed.**    {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’     left: use only keys from left frame, like a SQL left outer join; preserve key     order.     right: use only keys from right frame, like a SQL right outer join; preserve key     order.     outer: use union of keys from both frames, like a SQL full outer join; sort keys     lexicographically.     inner: use intersection of keys from both frames, like a SQL inner join;     preserve the order of the left keys.    **on: Column or index level names to join on. These must be found in both DataFrames. If on**    is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.    **left\_on: Column or index level names to join on in the left DataFrame. Can also**    be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.    **right\_on: Column or index level names to join on in the right DataFrame. Can also**    be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.    **left\_index: Use the index from the left DataFrame as the join key(s). If it is a**    MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.    **right\_index: Use the index from the right DataFrame as the join key. Same caveats as**    left\_index.    **suffixes: Suffix to apply to overlapping column names in the left and right side,**    respectively.     Returns     DataFrame     A DataFrame of the two merged objects.     Notes     As described in #263, joining string columns currently returns None for missing values     instead of NaN.     Examples    ``` >>> df1 = ps.DataFrame({'lkey': foo', 'bar', 'baz', 'foo, ...                     'value': [1, 2, 3, 5]}, ...                    columns=lkey', 'value) >>> df2 = ps.DataFrame({'rkey': foo', 'bar', 'baz', 'foo, ...                     'value': [5, 6, 7, 8]}, ...                    columns=rkey', 'value) >>> df1  lkey value 0 foo 1 1 bar 2 2 baz 3 3 foo 5 >>> df2  rkey value 0 foo 5 1 bar 6 2 baz 7 3 foo 8  ```   Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, \_x and \_y, appended.    ``` >>> merged = ps.merge(df1, df2, left\_on='lkey', right\_on='rkey') >>> merged.sort\_values(by=lkey', 'value\_x', 'rkey', 'value\_y)    lkey value\_x rkey value\_y ...bar 2 bar 6 ...baz 3 baz 7 ...foo 1 foo 5 ...foo 1 foo 8 ...foo 5 foo 5 ...foo 5 foo 8  ```  ``` >>> left\_psdf = ps.DataFrame({'A': [1, 2]}) >>> right\_psdf = ps.DataFrame({'B': x', 'y}, index=[1, 2])  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True).sort\_index()  A B 1 2 x  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='left').sort\_index()  A B 0 1 None 1 2 x  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='right').sort\_index()  A B 1 2.0 x 2 NaN y  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='outer').sort\_index()  A B 0 1.0 None 1 2.0 x 2 NaN y  ```  [pyspark.pandas.melt](pyspark.pandas.melt.html "previous page")  [pyspark.pandas.merge\_asof](pyspark.pandas.merge_asof.html "next page")   	The `pyspark.pandas.merge` function is used to merge DataFrame objects with a database-style join. It takes in two DataFrames, `left` and `right`, and performs a merge based on the specified `how` parameter, which can be 'left', 'right', 'outer', or 'inner'. The resulting DataFrame will have an index that depends on the type of merge performed. The `on` parameter specifies the column or index level names to join on, and if it is None and not merging on indexes, it defaults to the intersection of the columns in both DataFrames. The `left_on` and `right_on` parameters specify the column or index level names to join on in the left and right DataFrames, respectively. The `left_index` and `right_index` parameters specify whether to use the index from the left or right DataFrame as the join key(s). The `suffixes` parameter specifies the suffix to apply to overlapping column names in the left and right side, respectively.	1	1	1
no_reaction	https://docs.databricks.com/delta-live-tables/python-ref.html	how to write create or refresh streaming live table in python?	   pyspark.sql.streaming.DataStreamWriter.toTable  [¶](#pyspark-sql-streaming-datastreamwriter-totable "Permalink to this headline") ===================================================================================================================================  `DataStreamWriter.`  `toTable`    (   *tableName     :     str*  ,  *format     :     Optional    [    str    ]     =     None*  ,  *outputMode     :     Optional    [    str    ]     =     None*  ,  *partitionBy     :     Union[str, List[str], None]     =     None*  ,  *queryName     :     Optional    [    str    ]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → pyspark.sql.streaming.query.StreamingQuery  [[source]](../../../_modules/pyspark/sql/streaming/readwriter.html#DataStreamWriter.toTable) [¶](#pyspark.sql.streaming.DataStreamWriter.toTable "Permalink to this definition")    Starts the execution of the streaming query, which will continually output results to the given table as new data arrives.     The returned  [`StreamingQuery`](pyspark.sql.streaming.StreamingQuery.html#pyspark.sql.streaming.StreamingQuery "pyspark.sql.streaming.StreamingQuery")  object can be used to interact with the stream.     New in version 3.1.0.     Parameters    **tableName**    str     string, for the name of the table.    **format**    str, optional     the format used to save.    **outputMode**    str, optional     specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.    * append    : Only the new rows in the streaming DataFrame/Dataset will be written to the sink * complete    : All the rows in the streaming DataFrame/Dataset will be written to the sink every time these are some updates * update    : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to    append    mode.  **partitionBy**    str or list, optional     names of partitioning columns    **queryName**    str, optional     unique name for the query    **\*\*options**    dict     All other string options. You may want to provide a    checkpointLocation    .     Notes     This API is evolving.     For v1 table, partitioning columns provided by    partitionBy    will be respected no matter the table exists or not. A new table will be created if the table not exists.     For v2 table,    partitionBy    will be ignored if the table already exists.    partitionBy    will be respected only if the v2 table does not exist. Besides, the v2 table created by this API lacks some functionalities (e.g., customized properties, options, and serde info). If you need them, please create the v2 table manually before the execution to avoid creating a table with incomplete information.     Examples     Save a data stream to a table.    ``` >>> import tempfile >>> import time >>> \_ = spark.sql("DROP TABLE IF EXISTS my\_table2") >>> with tempfile.TemporaryDirectory() as d: ...     # Create a table with Rate source. ...     q = spark.readStream.format("rate").option( ...         "rowsPerSecond", 10).load().writeStream.toTable( ...             "my\_table2", ...             queryName='that\_query', ...             outputMode="append", ...             format='parquet', ...             checkpointLocation=d) ...     time.sleep(3) ...     q.stop() ...     spark.read.table("my\_table2").show() ...     \_ = spark.sql("DROP TABLE my\_table2") +...---------+-----+ |...timestamp|value| +...---------+-----+ ...  ```  [pyspark.sql.streaming.DataStreamWriter.start](pyspark.sql.streaming.DataStreamWriter.start.html "previous page")  [pyspark.sql.streaming.DataStreamWriter.trigger](pyspark.sql.streaming.DataStreamWriter.trigger.html "next page")   	To create or refresh a streaming live table in Python, you can use the `toTable` method of the `DataStreamWriter` class in PySpark. This method starts the execution of the streaming query, which will continually output results to the given table as new data arrives. The returned `StreamingQuery` object can be used to interact with the stream. You can specify the table name, format, output mode, partitioning columns, query name, and other options using the method parameters. Here is an example code snippet:  ``` from pyspark.sql.streaming import DataStreamWriter  streamingDF.writeStream \     .format("memory") \     .queryName("my_table") \     .outputMode("append") \     .start() \     .awaitTermination() ```  Note that this API is evolving and the behavior may differ for v1 and v2 tables.	1	1	1
no_reaction	https://docs.databricks.com/migration/index.html	What's the best way to move data from an on-premise database to Databricks?	Migrate data applications to Databricks =========================================     This article provides an introduction to migrating existing data applications to Databricks. Databricks provides a unified approach that lets you work with data from many source systems on a single platform.       For an overview of platform capabilities, see  [What is Databricks?](../introduction/index.html)  .          Migrate ETL jobs to Databricks --------------------------------     You can migrate Apache Spark jobs used to extract, transform, and load data from on-premises or cloud-native implementations to Databricks with just a few steps. See  [Adapt your exisiting Apache Spark code for Databricks](spark.html)  .       Databricks extends the functionality of Spark SQL with pre-configured open source integrations, partner integrations, and enterprise product offerings. If your ETL workloads are written in SQL or Hive, you can migrate to Databricks with minimal refactoring. Learn more about Databricks SQL offerings:     * [What is data warehousing on Databricks?](../sql/index.html) * [What is Delta Live Tables?](../delta-live-tables/index.html) * [What is Databricks Partner Connect?](../partner-connect/index.html)     For specific instructions on migrating from various source systems to Databricks, see  [Migrate ETL pipelines to Databricks](etl.html)  .         Replace your enterprise data warehouse with a lakehouse ---------------------------------------------------------     Databricks provides optimal value and performance when workloads align around data stored in the lakehouse. Many enterprise data stacks include both a data lake and an enterprise data warehouse, and organizations create complex ETL workflows to try to keep these systems and data in sync. The lakehouse allows you to use the same data, stored in the data lake, across queries and systems that usually rely on a separate data warehouse. For more on the lakehouse, see  [What is the Databricks Lakehouse?](../lakehouse/index.html)  . For more on data warehousing on Databricks, see  [What is data warehousing on Databricks?](../sql/index.html)  .       Migrating from an enterprise data warehouse to the lakehouse generally involves reducing the complexity of your data architecture and workflows, but there are some caveats and best practices to keep in mind while completing this work. See  [Migrate your data warehouse to the Databricks Lakehouse](warehouse-to-lakehouse.html)  .         Unify your ML, data science, and analytics workloads ------------------------------------------------------     Because the lakehouse provides optimized access to cloud-based data files through table queries or file paths, you can do ML, data science, and analytics on a single copy of your data. Databricks makes it easy to move workloads from both open source and proprietary tools, and maintains updated versions of many of open source libraries used by analysts and data scientists.       Pandas workloads in Jupyter notebooks can be synced and run using  [Databricks Repos](../repos/index.html)  . Databricks provides  [native support for pandas](../pandas/index.html)  in all Databricks Runtime versions, and configures many popular ML and deep learning libraries in the  [Databricks ML Runtime](../runtime/mlruntime.html)  . If you sync your local workloads using Git and  [Files in Repos](../files/workspace.html)  , you can use the same relative paths for data and custom libaries present in your local environment.        Note       By default, Databricks maintains  `.ipynb`  extensions for Jupyter notebooks synced with Databricks Repos, but automatically converts Jupyter notebooks to Databricks notebooks when  [imported with the UI](../notebooks/notebook-export-import.html#import-notebook)  . Databricks notebooks save with a  `.py`  extension, and so can live side-by-side with Jupyter notebooks in a Git repository.	To move data from an on-premise database to Databricks, you can migrate Apache Spark jobs used to extract, transform, and load data from on-premises or cloud-native implementations to Databricks with just a few steps. For specific instructions on migrating from various source systems to Databricks, see [Migrate ETL pipelines to Databricks](etl.html).	1	1	1
no_reaction	https://docs.databricks.com/query-federation/snowflake.html	Is Databricks or Snowflake better	    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format("snowflake")   .option("dbtable", table\_name)   .option("sfUrl", database\_host\_url)   .option("sfUser", username)   .option("sfPassword", password)   .option("sfDatabase", database\_name)   .option("sfSchema", schema\_name)   .option("sfWarehouse", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format("snowflake")  .option("dbtable", table\_name)  .option("sfUrl", database\_host\_url)  .option("sfUser", username)  .option("sfPassword", password)  .option("sfDatabase", database\_name)  .option("sfSchema", schema\_name)  .option("sfWarehouse", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         	Based on the context provided, the document explains how to read and write data from Snowflake using Databricks. It provides examples of syntax in Python, SQL, and Scala. It also includes notebooks that demonstrate how to use the Snowflake Connector for Spark to write data to Snowflake, use Snowflake for basic data manipulation, train a machine learning model in Databricks, and write the results back to Snowflake. The document also includes a FAQ section that answers common questions about using the Snowflake Connector for Spark. However, it does not provide a direct comparison between Databricks and Snowflake to determine which one is better.	1	0	0
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-alter-table-add-constraint.html	Does Delta support Foreign Key constraints	Constraints on Databricks ===========================     Databricks supports standard SQL constraint management clauses. Constraints fall into two categories:     * Enforced contraints ensure that the quality and integrity of data added to a table is automatically verified. * Informational primary key and foreign key constraints encode relationships between fields in tables and are not enforced.     All constraints on Databricks require Delta Lake.       Delta Live Tables has a similar concept known as expectations. See  [Manage data quality with Delta Live Tables](../delta-live-tables/expectations.html)  .        Enforced constraints on Databricks ------------------------------------     When a constraint is violated, the transaction fails with an error. Two types of constraints are supported:     * `NOT     NULL`  : indicates that values in specific columns cannot be null. * `CHECK`  : indicates that a specified boolean expression must be true for each input row.      Important       Adding a constraint automatically upgrades the table writer protocol version if the previous writer version was less than 3. See  [How does Databricks manage Delta Lake feature compatibility?](../delta/feature-compatibility.html)  to understand table protocol versioning and what it means to upgrade the protocol version.            Set a  `NOT     NULL`  constraint in Databricks -------------------------------------------------      Note     * `SET     NOT     NULL`  is available in Databricks Runtime 7.4 and above.      You specify  `NOT     NULL`  constraints in the schema when you create a table. You drop or add  `NOT     NULL`  constraints using the  `ALTER     TABLE     ALTER     COLUMN`  command.        ``` CREATE TABLE people10m (  id INT NOT NULL,  firstName STRING,  middleName STRING NOT NULL,  lastName STRING,  gender STRING,  birthDate TIMESTAMP,  ssn STRING,  salary INT ) USING DELTA;  ALTER TABLE people10m ALTER COLUMN middleName DROP NOT NULL; ALTER TABLE people10m ALTER COLUMN ssn SET NOT NULL;  ```      Before adding a  `NOT     NULL`  constraint to a table, Databricks verifies that all existing rows satisfy the constraint.       If you specify a  `NOT     NULL`  constraint on a column nested within a struct, the parent struct must also be not null. Columns nested within array or map types do not accept  `NOT     NULL`  constraints.       See  [CREATE TABLE [USING]](../sql/language-manual/sql-ref-syntax-ddl-create-table-using.html)  and  [ALTER TABLE ALTER COLUMN](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html)  .           Set a  `CHECK`  constraint in Databricks ------------------------------------------      Note     * Available in Databricks Runtime 7.4 and above. * In Databricks Runtime 7.3 LTS you can write to tables with  `CHECK`  constraints defined but you cannot create  `CHECK`  constraints.      You manage  `CHECK`  constraints using the  `ALTER     TABLE     ADD     CONSTRAINT`  and  `ALTER     TABLE     DROP     CONSTRAINT`  commands.  `ALTER     TABLE     ADD     CONSTRAINT`  verifies that all existing rows satisfy the constraint before adding it to the table.        ``` CREATE TABLE people10m (  id INT,  firstName STRING,  middleName STRING,  lastName STRING,  gender STRING,  birthDate TIMESTAMP,  ssn STRING,  salary INT ) USING DELTA;  ALTER TABLE people10m ADD CONSTRAINT dateWithinRange CHECK (birthDate > '1900-01-01'); ALTER TABLE people10m DROP CONSTRAINT dateWithinRange;  ```      See  [ALTER TABLE ADD CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html#add-constraint)  and  [ALTER TABLE DROP CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html#drop-constraint)  .      `CHECK`  constraints are exposed as table properties in the output of the  `DESCRIBE     DETAIL`  and  `SHOW     TBLPROPERTIES`  commands.        ``` ALTER TABLE people10m ADD CONSTRAINT validIds CHECK (id > 1 and id < 99999999);  DESCRIBE DETAIL people10m;  SHOW TBLPROPERTIES people10m;  ```          Declare primary key and foreign key relationships ---------------------------------------------------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .         Note     * Primary key and foreign key constraints are available in Databricks Runtime 11.1 and above. * Primary key and foreign key constraints require Unity Catalog and Delta Lake.      You can use primary key and foreign key relationships on fields in Unity Catalog tables. Primary and foreign keys are informational only and are not enforced. Foreign keys must reference a primary key in another table.       You can declare primary keys and foreign keys as part of the table specification clause during table creation. This clause is not allowed during CTAS statements. You can also add constraints to existing tables.        ``` CREATE TABLE T(pk1 INTEGER NOT NULL, pk2 INTEGER NOT NULL,  CONSTRAINT t\_pk PRIMARY KEY(pk1, pk2)); CREATE TABLE S(pk INTEGER NOT NULL PRIMARY KEY,  fk1 INTEGER, fk2 INTEGER,  CONSTRAINT s\_t\_fk FOREIGN KEY(fk1, fk2) REFERENCES T);  ```      You can query the  `information\_schema`  or use  `DESCRIBE`  to get details about how constraints are applied across a given catalog.       See:     * [ALTER TABLE](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html) * [ADD CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table-add-constraint.html) * [DROP CONSTRAINT](../sql/language-manual/sql-ref-syntax-ddl-alter-table-drop-constraint.html) * [CONSTRAINT clause](../sql/language-manual/sql-ref-syntax-ddl-create-table-constraint.html)	Yes, Delta supports Foreign Key constraints. However, it's important to note that foreign key constraints in Delta are informational only and are not enforced. Foreign keys must reference a primary key in another table.	0	1	1
no_reaction	https://docs.databricks.com/data-sharing/share-data-open.html	how can I share data	Access data shared with you using Delta Sharing =================================================     This article shows how to to access data that has been shared with you using Delta Sharing.          Delta Sharing and data recipients -----------------------------------     Delta Sharing is an open standard for secure data sharing. A Databricks user, called a  *data provider*   , can use Delta Sharing to share data with a person or group outside of their organization, called a  *data recipient*   .        ###   Databricks-to-Databricks sharing and open sharing     How you access the data depends on whether you yourself are a Databricks user and whether or not your data provider configured the data being shared with you for  *Databricks-to-Databricks*   sharing or  *open sharing*   .      **In the Databricks-to-Databricks model**   , you must be a user on a Databricks workspace that is enabled for  [Unity Catalog](../data-governance/unity-catalog/index.html)  . A member of your team provides the data provider with a unique identifier for your Databricks workspace, and the data provider uses that to create a secure sharing connection. The shared data simply becomes available for access in your workspace. If necessary, a member of your team configures granular access control on that data.      **In the open sharing model**   , you can use any tool you like (including Databricks) to access the shared data. The data provider sends you an activation URL over a secure channel. You follow it to download a credential file that lets you access the data shared with you.       ###   Terms of use     The shared data is not provided by Databricks directly but by data providers running on Databricks.        Note       By accessing a data provider’s shared data as a data recipient, data recipient represents that it has been authorized to access the data share(s) provided to it by the data provider and acknowledges that (1) Databricks has no liability for such data or data recipient’s use of such shared data, and (2) Databricks may collect information about data recipient’s use of and access to the shared data (including identifying any individual or company who accesses the data using the credential file in connection with such information) and may share it with the applicable data provider.           Get access to the data shared with you ----------------------------------------     How you access the data depends on whether your data provider shared data with you using the open sharing protocol or the Databricks-to-Databricks sharing protocol. See  [Databricks-to-Databricks sharing and open sharing](#open-sharing-vs-db-to-db)  .        ###   Get access in the Databricks-to-Databricks model     In the Databricks-to-Databricks model:     1. The data provider sends you instructions for finding a unique identifier for the  [Unity Catalog](../data-governance/unity-catalog/index.html)  metastore associated with your Databricks workspace, and you send it to them.       The sharing identifier is a string consisting of the metastore’s cloud, region, and UUID (the unique identifier for the metastore), in the format  `<cloud>:<region>:<uuid>`  . For example,  `aws:eu-west-1:b0c978c8-3e68-4cdf-94af-d05c120ed1ef`  .       To get the sharing identifier using Data Explorer:      1. In your Databricks workspace, click   ![Data Icon](../_images/data-icon.png)  **Data**    .  2. In the left pane, expand the   **Delta Sharing**    menu and select   **Shared with me**    .  3. Above the Providers tab, click the   **Sharing identifier**    copy icon.  To get the sharing identifier using a notebook or Databricks SQL query, use the default SQL function  `CURRENT\_METASTORE`  . If you use a notebook, it must run on a  [Unity-Catalog-capable cluster](../data-governance/unity-catalog/compute.html#access-mode)  in the workspace you will use to access the shared data.        ``` SELECT CURRENT\_METASTORE();  ``` 2. The data provider creates:      * A   *recipient*    in their Databricks account to represent you and the users in your organization who will access the data.  * A   *share*    , which is a representation of the tables and views to be shared with you. 3. You access the data shared with you. You or someone on your team can, if necessary, configure granular data access on that data for your users.       ###   Get access in the open sharing model     In the open sharing model:     1. The data provider creates:      * A   *recipient*    in their Databricks account to represent you and the users in your organization who will access the data. A token and credential file are generated as part of this configuration.  * A   *share*    , which is a representation of the tables and partitions to be shared with you. 2. The data provider sends you an activation URL over a secure channel. You follow it to download a credential file that lets you access the data shared with you.        Important       Don’t share the activation link with anyone. You can download a credential file only once. If you visit the activation link again after the credential file has already downloaded, the  **Download Credential File**   button is disabled.        If you lose the activation link before you use it, contact the data provider. 3. Store the credential file in a secure location.       Don’t share the credential file with anyone outside the group of users who should have access to the shared data. If you need to share it with someone in your organization, Databricks recommends using a password manager.        Read the shared data ----------------------     How you read data that has been shared securely with you using Delta Sharing depends on whether you received a credential file (the open sharing model) or you are using a Databricks workspace and you provided the data provider with your sharing identifier (the Databricks-to-Databricks model).      ###   Read shared data using a credential file (open sharing)     If data has been shared with you using the Delta Sharing open sharing protocol, you use the credential file that you downloaded to authenticate to the data provider’s Databricks account and read the shared data. Access persists until the provider stops sharing the data with you. Updates to the data are available to you in near real time. You can read and make copies of the shared data, but you can’t modify the source data.       To learn how to access and read shared data using the credential file in Databricks, Apache Spark, pandas, and Power BI, see  [Read data shared using Delta Sharing open sharing](read-data-open.html)  .       ###   Read shared data using Databricks-to-Databricks sharing     If data has been shared with you using the Databricks-to-Databricks model, then no credential file is required to access the shared data. Databricks takes care of the secure connection, and the shared data is automatically discoverable in your Databricks workspace.       To learn how to find, read, and manage that shared data in your Databricks workspace, see  [Read data shared using Databricks-to-Databricks Delta Sharing](read-data-databricks.html)  .          Audit usage of shared data ----------------------------     If you have access to a Databricks workspace, you can use Databricks audit logs to understand who in your organization is accessing which data using Delta Sharing. See  [Audit and monitor data access using Delta Sharing (for recipients)](audit-logging-recipient.html)  .         Next steps ------------   * [Learn more about Databricks](../getting-started/index.html) * [Learn more about Delta Sharing](https://delta.io/sharing/) * [Learn more about Unity Catalog](../data-governance/unity-catalog/index.html)	To share data using Delta Sharing, a data provider can use Delta Sharing to share data with a person or group outside of their organization, called a data recipient. The data recipient can access the shared data either through the Databricks-to-Databricks sharing model or the open sharing model. In the Databricks-to-Databricks model, the data recipient must be a user on a Databricks workspace that is enabled for Unity Catalog, and a member of their team provides the data provider with a unique identifier for their Databricks workspace. In the open sharing model, the data provider sends the data recipient an activation URL over a secure channel, and the data recipient follows it to download a credential file that lets them access the data shared with them.	0	1	1
no_reaction	https://docs.databricks.com/ingestion/file-metadata-column.html	How do I get file metadata in databricks	    File metadata column ======================     You can get metadata information for input files with the  `\_metadata`  column. The  `\_metadata`  column is a  *hidden*   column, and is available for all input file formats. To include the  `\_metadata`  column in the returned DataFrame, you must explicitly reference it in your query.       If the data source contains a column named  `\_metadata`  , queries return the column from the data source, and not the file metadata.        Warning       New fields might be added to the  `\_metadata`  column in future releases. To prevent schema evolution errors if the  `\_metadata`  column is updated, Databricks recommends selecting specific fields from the column in your queries. See  [examples](#metadata-examples)  .         Supported metadata --------------------     The  `\_metadata`  column is a  `STRUCT`  containing the following fields:             |   Name   |   Type   |   Description   |   Example   |   Minimum Databricks Runtime release   | | --- | --- | --- | --- | --- | |  **file\_path**   |  `STRING`  |   File path of the input file.   |  `file:/tmp/f0.csv`  |   10.5   | |  **file\_name**   |  `STRING`  |   Name of the input file along with its extension.   |  `f0.csv`  |   10.5   | |  **file\_size**   |  `LONG`  |   Length of the input file, in bytes.   |   628   |   10.5   | |  **file\_modification\_time**   |  `TIMESTAMP`  |   Last modification timestamp of the input file.   |  `2021-12-20     20:05:21`  |   10.5   | |  **file\_block\_start**   |  `LONG`  |   Start offset of the block being read, in bytes.   |   0   |   13.0   | |  **file\_block\_length**   |  `LONG`  |   Length of the block being read, in bytes.   |   628   |   13.0   |         Examples ----------    ###   Use in a basic file-based data source reader       ``` df = spark.read \   .format("csv") \   .schema(schema) \   .load("dbfs:/tmp/\*") \   .select("\*", "\_metadata")  display(df)  ''' Result: +---------+-----+----------------------------------------------------+ | name | age | \_metadata | +=========+=====+====================================================+ | | | { | | | | "file\_path": "dbfs:/tmp/f0.csv", | | Debbie | 18 | "file\_name": "f0.csv", | | | | "file\_size": 12, | | | | "file\_block\_start": 0, | | | | "file\_block\_length": 12, | | | | "file\_modification\_time": "2021-07-02 01:05:21" | | | | } | +---------+-----+----------------------------------------------------+ | | | { | | | | "file\_path": "dbfs:/tmp/f1.csv", | | Frank | 24 | "file\_name": "f1.csv", | | | | "file\_size": 12, | | | | "file\_block\_start": 0, | | | | "file\_block\_length": 12, | | | | "file\_modification\_time": "2021-12-20 02:06:21" | | | | } | +---------+-----+----------------------------------------------------+ '''  ```       ``` val df = spark.read  .format("csv")  .schema(schema)  .load("dbfs:/tmp/\*")  .select("\*", "\_metadata")  display(df\_population)  /\* Result: +---------+-----+----------------------------------------------------+ | name | age | \_metadata | +=========+=====+====================================================+ | | | { | | | | "file\_path": "dbfs:/tmp/f0.csv", | | Debbie | 18 | "file\_name": "f0.csv", | | | | "file\_size": 12, | | | | "file\_block\_start": 0, | | | | "file\_block\_length": 12, | | | | "file\_modification\_time": "2021-07-02 01:05:21" | | | | } | +---------+-----+----------------------------------------------------+ | | | { | | | | "file\_path": "dbfs:/tmp/f1.csv", | | Frank | 24 | "file\_name": "f1.csv", | | | | "file\_size": 10, | | | | "file\_block\_start": 0, | | | | "file\_block\_length": 12, | | | | "file\_modification\_time": "2021-12-20 02:06:21" | | | | } | +---------+-----+----------------------------------------------------+ \*/  ```       ###   Select specific fields       ``` spark.read \   .format("csv") \   .schema(schema) \   .load("dbfs:/tmp/\*") \   .select("\_metadata.file\_name", "\_metadata.file\_size")  ```       ``` spark.read  .format("csv")  .schema(schema)  .load("dbfs:/tmp/\*")  .select("\_metadata.file\_name", "\_metadata.file\_size")  ```       ###   Use in filters       ``` spark.read \   .format("csv") \   .schema(schema) \   .load("dbfs:/tmp/\*") \   .select("\*") \   .filter(col("\_metadata.file\_name") == lit("test.csv"))  ```       ``` spark.read  .format("csv")  .schema(schema)  .load("dbfs:/tmp/\*")  .select("\*")  .filter(col("\_metadata.file\_name") === lit("test.csv"))  ```       ###   Use in COPY INTO      ``` COPY INTO my\_delta\_table FROM (  SELECT \*, \_metadata FROM 's3://my-bucket/csvData' ) FILEFORMAT = CSV  ```      ###   Use in Auto Loader      Note       When writing the  `\_metadata`  column, we rename it to  `source\_metadata`  . Writing it as  `\_metadata`  would make it impossible to access the metadata column in the target table, because if the data source contains a column named  `\_metadata`  , queries will return the column from the data source, and not the file metadata.          ``` spark.readStream \   .format("cloudFiles") \   .option("cloudFiles.format", "csv") \   .schema(schema) \   .load("s3://my-bucket/csvData") \   .selectExpr("\*", "\_metadata as source\_metadata") \   .writeStream \   .format("delta") \   .option("checkpointLocation", checkpointLocation) \   .start(targetTable)  ```       ``` spark.readStream  .format("cloudFiles")  .option("cloudFiles.format", "csv")  .schema(schema)  .load("s3://my-bucket/csvData")  .selectExpr("\*", "\_metadata as source\_metadata")  .writeStream  .format("delta")  .option("checkpointLocation", checkpointLocation)  .start(targetTable)  ```          Related articles ------------------   * [COPY INTO](../sql/language-manual/delta-copy-into.html) * [Auto Loader](auto-loader/index.html) * [Structured Streaming](../structured-streaming/index.html)                                                      	To get metadata information for input files in Databricks, you can use the `_metadata` column. The `_metadata` column is a hidden column that is available for all input file formats. To include the `_metadata` column in the returned DataFrame, you must explicitly reference it in your query. The `_metadata` column is a `STRUCT` containing fields such as `file_path`, `file_name`, `file_size`, `file_modification_time`, `file_block_start`, and `file_block_length`. To prevent schema evolution errors if the `_metadata` column is updated, Databricks recommends selecting specific fields from the column in your queries.	1	1	1
no_reaction	https://docs.databricks.com/dbfs/dbfs-root.html	Is dbfs secure?	What is the Databricks File System (DBFS)? ============================================     The Databricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls.        Note       Databricks workspaces deploy with a  [DBFS root volume](#dbfs-root)  , accessible to all users by default. Databricks recommends against storing production data in this location.           What can you do with DBFS? ----------------------------     DBFS provides convenience by mapping cloud object storage URIs to relative paths.     * Allows you to  [interact with object storage](#interact-files)  using directory and file semantics instead of cloud-specific API commands. * Allows you to  [mount](#mount-storage)  cloud object storage locations so that you can map storage credentials to paths in the Databricks workspace. * Simplifies the process of persisting files to object storage, allowing virtual machines and attached volume storage to be safely deleted on cluster termination. * Provides a convenient location for storing init scripts, JARs, libraries, and configurations for cluster initialization. * Provides a convenient location for checkpoint files created during model training with OSS deep learning libraries.      Note       DBFS is the Databricks implementation for FUSE. See  [How to work with files on Databricks](../files/index.html)  .            Interact with files in cloud-based object storage ---------------------------------------------------     DBFS provides many options for interacting with files in cloud object storage:         * [How to work with files on Databricks](../files/index.html) * [List, move, copy, and delete files with Databricks Utilities](../dev-tools/databricks-utils.html#file-system-utility-dbutilsfs) * [Browse files in DBFS](file-browser.html) * [Upload files to DBFS with the UI](../archive/legacy/data-tab.html) * [Interact with DBFS files using the Databricks CLI](../dev-tools/cli/dbfs-cli.html) * [Interact with DBFS files using the Databricks REST API](../dev-tools/api/latest/dbfs.html)             Mount object storage ----------------------     Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage, so you do not need to specify these settings in code or during cluster configuration.       For more information, see  [Mounting cloud object storage on Databricks](mounts.html)  .           What is the DBFS root? ------------------------     The  *DBFS root*   is the default storage location for a Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Databricks workspace. For details on Databricks Filesystem root configuration and deployment, see  [Configure AWS storage](../administration-guide/cloud-configurations/aws/aws-storage.html)  . For best practices around securing data in the DBFS root, see  [Recommendations for working with DBFS root](dbfs-root.html)  .       Some users of Databricks may refer to the DBFS root as “DBFS” or “the DBFS”; it is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a cloud object storage location. You use DBFS to interact with the DBFS root, but they are distinct concepts, and DBFS has many applications beyond the DBFS root.       The DBFS root contains a number of special locations that serve as defaults for various actions performed by users in the workspace. For details, see  [What directories are in DBFS root by default?](root-locations.html)  .           How does DBFS work with Unity Catalog? ----------------------------------------     Unity Catalog adds the concepts of external locations and managed storage credentials to help organizations provide least privileges access to data in cloud object storage. Unity Catalog also provides a new default storage location for managed tables. Some security configurations provide direct access to both Unity Catalog-managed resources and DBFS. Databricks has compiled recommendations for  [using DBFS and Unity Catalog](unity-catalog.html)  .	DBFS provides convenience by mapping cloud object storage URIs to relative paths. It allows you to interact with object storage using directory and file semantics instead of cloud-specific API commands. DBFS also simplifies the process of persisting files to object storage, allowing virtual machines and attached volume storage to be safely deleted on cluster termination. However, the security of DBFS depends on how it is configured and used. Databricks recommends against storing production data in the DBFS root volume, which is accessible to all users by default. For best practices around securing data in the DBFS root, see Recommendations for working with DBFS root.	0	0	1
no_reaction	https://kb.databricks.com/clusters/autotermination-disabled-error-creating-job	How do I terminate a cluster?	Title: Unexpected cluster termination  Description: Learn how to troubleshoot a Databricks cluster that stopped unexpectedly.  Answer: Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination. A cluster can be terminated for many reasons. Some terminations are initiated by Databricks and others are initiated by the cloud provider. This article describes termination reasons and steps for remediation. Databricks initiated request limit exceeded To defend against API abuses, ensure quality of service, and prevent you from accidentally creating too many large clusters, Databricks throttles all cluster up-sizing requests, including cluster creation, starting, and resizing. The throttling uses the token bucket algorithm to limit the total number of nodes that anyone can launch over a defined interval across your Databricks deployment, while allowing burst requests of certain sizes. Requests coming from both the web UI and the APIs are subject to rate limiting. When cluster requests exceed rate limits, the limit-exceeding request fails with a REQUEST_LIMIT_EXCEEDED error. Solution If you hit the limit for your legitimate workflow, Databricks recommends that you do the following: Retry your request a few minutes later. Spread out your recurring workflow evenly in the planned time frame. For example, instead of scheduling all of your jobs to run at an hourly boundary, try distributing them at different intervals within the hour. Consider using clusters with a larger node type and smaller number of nodes. Use autoscaling clusters. If these options don’t work for you, contact Databricks Support to request a limit increase for the core instance. For other Databricks initiated termination reasons, see Termination Code. Cloud provider initiated terminations This article lists common cloud provider related termination reasons and remediation steps. AWS Provider limit Databricks launches a cluster by requesting resources on behalf of your cloud account. Sometimes, these requests fail because they would exceed your cloud account’s resource limits. In AWS, common error codes include: InstanceLimitExceeded AWS limits the number of running instances for each node type. Possible solutions include: Request a cluster with fewer nodes. Request a cluster with a different node type. Ask AWS support to increase instance limits. Client.VolumeLimitExceeded The cluster creation request exceeded the EBS volume limit. AWS has two types of volume limits: a limit on the total number of EBS volumes, and a limit on the total storage size of EBS volumes. Potential remediation steps: Request a cluster with fewer nodes. Check which of the two limits was exceeded. (AWS trusted advisor shows service limits for free). If the request exceeded the total number of EBS volumes, try reducing the requested number of volumes per node. If the request exceeded the total EBS storage size, try reducing the requested storage size and/or the number of EBS volumes. Ask AWS support to increase EBS volume limits. RequestLimitExceeded AWS limits the rate of API requests made for an AWS account. Wait a while before retrying the request. Provider shutdown The Spark driver is a single point of failure because it holds all cluster state. If the instance hosting the driver node is shut down, Databricks terminates the cluster. In AWS, common error codes include: Client.UserInitiatedShutdown Instance was terminated by a direct request to AWS which did not originate from Databricks. Contact your AWS administrator for more details. Server.InsufficientInstanceCapacity AWS could not satisfy the instance request. Wait a while and retry the request. Contact AWS support if the problem persists. Server.SpotInstanceTermination Instance was terminated by AWS because the current spot price has exceeded the maximum bid made for this instance. Use an on-demand instance for the driver, choose a different availability zone, or specify a higher spot bid price. For other shutdown-related error codes, refer to AWS docs. Launch failure AWS In AWS, common error codes include: UnauthorizedOperation Databricks was not authorized to launch the requested instances. Possible reasons include: Your AWS administrator invalidated the AWS access key or IAM role used to launch instances. You are trying to launch a cluster using an IAM role that Databricks does not have permission to use. Contact the AWS administrator who set up the IAM role. For more information, see Secure Access to S3 Buckets Using IAM Roles. Unsupported with message “EBS-optimized instances are not supported for your requested configuration” The selected instance type is not available in the selected availability zone (AZ). It does not actually have anything to do with EBS-optimization being enabled. To remediate, you can choose a different instance type or AZ. AuthFailure.ServiceLinkedRoleCreationNotPermitted The provided credentials do not have permission to create the service-linked role for EC2 spot instances. The Databricks administrator needs to update the credentials used to launch instances in your account. Instructions and the updated policy can be found AWS Account. See Error Codes for a complete list of AWS error codes. Azure This termination reason occurs when Azure Databricks fails to acquire virtual machines. The error code and message from the API are propagated to help you troubleshoot the issue. OperationNotAllowed You have reached a quota limit, usually number of cores, that your subscription can launch. Request a limit increase in Azure portal. See Azure subscription and service limits, quotas, and constraints. PublicIPCountLimitReached You have reached the limit of the public IPs that you can have running. Request a limit increase in Azure Portal. SkuNotAvailable The resource SKU you have selected (such as VM size) is not available for the location you have selected. To resolve, see Resolve errors for SKU not available. ReadOnlyDisabledSubscription Your subscription was disabled. Follow the steps in Why is my Azure subscription disabled and how do I reactivate it? to reactivate your subscription. ResourceGroupBeingDeleted Can occur if someone cancels your Azure Databricks workspace in the Azure portal and you try to create a cluster at the same time. The cluster fails because the resource group is being deleted. SubscriptionRequestsThrottled Your subscription is hitting the Azure Resource Manager request limit (see Throttling Resource Manager requests). Typical cause is that another system outside Azure Databricks) making a lot of API calls to Azure. Contact Azure support to identify this system and then reduce the number of API calls. Communication lost Databricks was able to launch the cluster, but lost the connection to the instance hosting the Spark driver. AWS Caused by an incorrect networking configuration (for example, changing security group settings for Databricks workers) or a transient AWS networking issue. Azure Caused by the driver virtual machine going down or a networking issue.	To terminate a Databricks cluster, you can go to the Clusters tab in the Databricks workspace, select the cluster you want to terminate, and click the "Terminate" button.	0	1	1
no_reaction	https://docs.databricks.com/dev-tools/python-sql-connector.html	How do i get Row samples with Databricks SQL Connector for Python	###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) + [ANSI Compliance](sql-ref-ansi-compliance.html) + [Data Types](sql-ref-datatypes.html) + [Datetime Pattern](sql-ref-datetime-pattern.html) + [Number Pattern](sql-ref-number-pattern.html) + [Functions](sql-ref-functions.html) + [Identifiers](sql-ref-identifier.html) + [Literals](sql-ref-literals.html) + [Null Semantics](sql-ref-null-semantics.html) + [SQL Syntax](sql-ref-syntax.html) - [Data Definition Statements](sql-ref-syntax.html#ddl-statements) - [Data Manipulation Statements](sql-ref-syntax.html#dml-statements) - [Data Retrieval(Queries)](sql-ref-syntax.html#data-retrieval-statements) - [Auxiliary Statements](sql-ref-syntax.html#auxiliary-statements)  * [Error Conditions](sql-error-conditions.html)   Sampling Queries ==================  ###   Description   The  `TABLESAMPLE`   statement is used to sample the table. It supports the following sampling methods:    * `TABLESAMPLE`   (x  `ROWS`   ): Sample the table down to the given number of rows. * `TABLESAMPLE`   (x  `PERCENT`   ): Sample the table down to the given percentage. Note that percentages are defined as a number between 0 and 100. * `TABLESAMPLE`   (  `BUCKET`   x  `OUT OF`   y): Sample the table down to a  `x`   out of  `y`   fraction.  **Note:**  `TABLESAMPLE`   returns the approximate number of rows or fraction requested.    ###   Syntax  ``` TABLESAMPLE ({ integer\_expression | decimal\_expression } PERCENT)     | TABLESAMPLE ( integer\_expression ROWS )     | TABLESAMPLE ( BUCKET integer\_expression OUT OF integer\_expression )  ```  ###   Examples  ``` SELECT \* FROM test; +--+----+ |id|name| +--+----+ | 5|Alex| | 8|Lucy| | 2|Mary| | 4|Fred| | 1|Lisa| | 9|Eric| |10|Adam| | 6|Mark| | 7|Lily| | 3|Evan| +--+----+  SELECT \* FROM test TABLESAMPLE (50 PERCENT); +--+----+ |id|name| +--+----+ | 5|Alex| | 2|Mary| | 4|Fred| | 9|Eric| |10|Adam| | 3|Evan| +--+----+  SELECT \* FROM test TABLESAMPLE (5 ROWS); +--+----+ |id|name| +--+----+ | 5|Alex| | 8|Lucy| | 2|Mary| | 4|Fred| | 1|Lisa| +--+----+  SELECT \* FROM test TABLESAMPLE (BUCKET 4 OUT OF 10); +--+----+ |id|name| +--+----+ | 8|Lucy| | 2|Mary| | 9|Eric| | 6|Mark| +--+----+  ```  ###   Related Statements  * [SELECT](sql-ref-syntax-qry-select.html)	To get row samples with Databricks SQL Connector for Python, you can use the `TABLESAMPLE` statement. It supports three sampling methods: `TABLESAMPLE (x ROWS)`, `TABLESAMPLE (x PERCENT)`, and `TABLESAMPLE (BUCKET x OUT OF y)`. You can use these methods to sample the table down to the given number of rows, percentage, or fraction. Note that `TABLESAMPLE` returns the approximate number of rows or fraction requested.	1	1	1
no_reaction	https://docs.databricks.com/ingestion/auto-loader/schema.html	databricks autoloader schema inference	      Configure schema inference and evolution in Auto Loader =========================================================     You can configure Auto Loader to automatically detect the schema of loaded data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time.       Auto Loader can also “rescue” data that was unexpected (for example, of differing data types) in a JSON blob column, that you can choose to access later using the  [semi-structured data access APIs](../../optimizations/semi-structured.html)  .       The following formats are supported for schema inference and evolution:          |   File format   |   Supported versions   | | --- | --- | |  `JSON`  |   Databricks Runtime 8.2 and above   | |  `CSV`  |   Databricks Runtime 8.3 and above   | |  `Avro`  |   Databricks Runtime 10.2 and above   | |  `Parquet`  |   Databricks Runtime 11.1 and above   | |  `ORC`  |   Unsupported   | |  `Text`  |   Not applicable (fixed-schema)   | |  `Binaryfile`  |   Not applicable (fixed-schema)   |      Syntax for schema inference and evolution -------------------------------------------     Specifying a target directory for the option  `cloudFiles.schemaLocation`  enables schema inference and evolution. You can choose to use the same directory you specify for the  `checkpointLocation`  . If you use  [Delta Live Tables](../../delta-live-tables/index.html)  , Databricks manages schema location and other checkpoint information automatically.        Note       If you have more than one source data location being loaded into the target table, each Auto Loader ingestion workload requires a separate streaming checkpoint.        The following example uses  `parquet`  for the  `cloudFiles.format`  . Use  `csv`  ,  `avro`  , or  `json`  for other file sources. All other settings for read and write stay the same for the default behaviors for each format.         ``` (spark.readStream.format("cloudFiles")   .option("cloudFiles.format", "parquet")   # The schema location directory keeps track of your data schema over time   .option("cloudFiles.schemaLocation", "<path\_to\_checkpoint>")   .load("<path\_to\_source\_data>")   .writeStream   .option("checkpointLocation", "<path\_to\_checkpoint>")   .start("<path\_to\_target") )  ```       ``` spark.readStream.format("cloudFiles")  .option("cloudFiles.format", "parquet")  // The schema location directory keeps track of your data schema over time  .option("cloudFiles.schemaLocation", "<path\_to\_checkpoint>")  .load("<path\_to\_source\_data>")  .writeStream  .option("checkpointLocation", "<path\_to\_checkpoint>")  .start("<path\_to\_target")  ```             How does Auto Loader schema inference work? ---------------------------------------------     To infer the schema when first reading data, Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first. Auto Loader stores the schema information in a directory  `\_schemas`  at the configured  `cloudfFiles.schemaLocation`  to track schema changes to the input data over time.        Note       To change the size of the sample that’s used you can set the SQL configurations:        ``` spark.databricks.cloudFiles.schemaInference.sampleSize.numBytes  ```      (byte string, for example  `10gb`  )       and        ``` spark.databricks.cloudFiles.schemaInference.sampleSize.numFiles  ```      (integer)        By default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don’t encode data types (JSON and CSV), Auto Loader infers all columns as strings (including nested fields in JSON files). For formats with typed schema (Parquet and Avro), Auto Loader samples a subset of files and merges the schemas of individual files. This behavior is summarized in the following table:          |   File format   |   Default inferred data type   | | --- | --- | |  `JSON`  |   String   | |  `CSV`  |   String   | |  `Avro`  |   Types encoded in Avro schema   | |  `Parquet`  |   Types encoded in Parquet schema   |     The Apache Spark DataFrameReader uses different behavior for schema inference, selecting data types for columns in JSON and CSV sources based on sample data. To enable this behavior with Auto Loader, set the option  `cloudFiles.inferColumnTypes`  to  `true`  .        Note       When inferring schema for CSV data, Auto Loader assumes that the files contain headers. If your CSV files do not contain headers, provide the option  `.option("header",     "false")`  . In addition, Auto Loader merges the schemas of all the files in the sample to come up with a global schema. Auto Loader can then read each file according to its header and parse the CSV correctly.         Note       When a column has different data types in two Parquet files, Auto Loader attempts to  `upcast`  one type to the other. If upcasting is not possible, data inference fails. See the following table for examples:           |   Type 1   |   Type 2   |   Upcast type   | | --- | --- | --- | |  `int`  |  `double`  |  `double`  | |  `double`  |  `string`  |  `string`  | |  `int`  |  `struct`  |   Inference fails   |     After merging data types on inference, files containing records of the unselected type are loaded to the  [rescued data column](#rescue)  , because the data type is different from the inferred schema.              How does Auto Loader schema evolution work? ---------------------------------------------     Auto Loader detects the addition of new columns as it processes your data. When Auto Loader detects a new column, the stream stops with an  `UnknownFieldException`  . Before your stream throws this error, Auto Loader performs schema inference on the latest micro-batch of data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged.       Databricks recommends configuring Auto Loader streams with  [workflows](../../workflows/index.html)  to restart automatically after such schema changes.       Auto Loader supports the following modes for schema evolution, which you set in the option  `cloudFiles.schemaEvolutionMode`  :          |   Mode   |   Behavior on reading new column   | | --- | --- | |  `addNewColumns`  (default)   |   Stream fails. New columns are added to the schema. Existing columns do not evolve data types.   | |  `rescue`  |   Schema is never evolved and stream does not fail due to schema changes. All new columns are recorded in the  [rescued data column](#rescue)  .   | |  `failOnNewColumns`  |   Stream fails. Stream does not restart unless the provided schema is updated, or the offending data file is removed.   | |  `none`  |   Does not evolve the schema, new columns are ignored, and data is not rescued unless the  `rescuedDataColumn`  option is set. Stream does not fail due to schema changes.   |         How do partitions work with Auto Loader? ------------------------------------------     Auto Loader attempts to infer partition columns from the underlying directory structure of the data if the data is laid out in Hive style partitioning. For example, the file path  `base\_path/event=click/date=2021-04-01/f0.json`  results in the inference of  `date`  and  `event`  as partition columns. If the underlying directory structure contains conflicting Hive partitions or doesn’t contain Hive style partitioning, partition columns are ignored.       Binary file (  `binaryFile`  ) and  `text`  file formats have fixed data schemas, but support partition column inference. Databricks recommends setting  `cloudFiles.schemaLocation`  for these file formats. This avoids any potential errors or information loss and prevents inference of partitions columns each time an Auto Loader begins.       Partition columns are not considered for schema evolution. If you had an initial directory structure like  `base\_path/event=click/date=2021-04-01/f0.json`  , and then start receiving new files as  `base\_path/event=click/date=2021-04-01/hour=01/f1.json`  , Auto Loader ignores the hour column. To capture information for new partition columns, set  `cloudFiles.partitionColumns`  to  `event,date,hour`  .        Note       The option  `cloudFiles.partitionColumns`  takes a comma-separated list of column names. Only columns that exist as  `key=value`  pairs in your directory structure are parsed.            What is the rescued data column? ----------------------------------     When Auto Loader infers the schema, a rescued data column is automatically added to your schema as  `\_rescued\_data`  . You can rename the column or include it in cases where you provide a schema by setting the option  `rescuedDataColumn`  .       The rescued data column ensures that columns that don’t match with the schema are rescued instead of being dropped. The rescued data column contains any data that isn’t parsed for the following reasons:     * The column is missing from the schema. * Type mismatches. * Case mismatches.     The rescued data column contains a JSON containing the rescued columns and the source file path of the record.        Note       The JSON and CSV parsers support three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records are dropped or throw errors, such as incomplete or malformed JSON or CSV. If you use  `badRecordsPath`  when parsing JSON or CSV, data type mismatches are not considered as bad records when using the  `rescuedDataColumn`  . Only incomplete and malformed JSON or CSV records are stored in  `badRecordsPath`  .            Change case-sensitive behavior --------------------------------     Unless case sensitivity is enabled, the columns  `abc`  ,  `Abc`  , and  `ABC`  are considered the same column for the purposes of schema inference. The case that is chosen is arbitrary and depends on the sampled data. You can use  [schema hints](#schema-hints)  to enforce which case should be used. Once a selection has been made and the schema is inferred, Auto Loader does not consider the casing variants that were not selected consistent with the schema.       When  [rescued data column](#rescue)  is enabled, fields named in a case other than that of the schema are loaded to the  `\_rescued\_data`  column. Change this behavior by setting the option  `readerCaseSensitive`  to false, in which case Auto Loader reads data in a case-insensitive way.           Override schema inference with schema hints ---------------------------------------------     You can use schema hints to enforce the schema information that you know and expect on an inferred schema. When you know that a column is of a specific data type, or if you want to choose a more general data type (for example,	Auto Loader can be configured to automatically detect the schema of loaded data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time. Auto Loader supports schema inference and evolution for JSON, CSV, Avro, Parquet, Binaryfile, and Text file formats. To enable schema inference and evolution, you need to specify a target directory for the option `cloudFiles.schemaLocation`. When first reading data, Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first, to infer the schema. Auto Loader detects the addition of new columns as it processes your data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged. Auto Loader supports different modes for schema evolution, which you set in the option `cloudFiles.schemaEvolutionMode`. You can also use schema hints to enforce the schema information that you know and expect on an inferred schema.	1	1	1
no_reaction	https://docs.databricks.com/data-sharing/index.html	what's delta sharing?	Delta Sharing ===============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data.       There are three components to Delta Sharing:     * **[Providers](#providers)**      A provider is an entity which has made data available for sharing. * **[Shares](#shares)**      A share defines a logical grouping for the tables you intend to share. * **[Recipients](#recipients)**      A recipient identifies an organization with which you want to share any number of shares.     For a detailed guide on how to use Delta Sharing see  [Share data securely using Delta Sharing](../../data-sharing/index.html)  .        Providers -----------    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.1 and above       A data provider is an object representing the organization in the real world who shares the data. A provider contains shares which further contain the shared data. Once a provider has added you as a  **[Recipients](#recipients)**  you can     * Customize the local name of the provider using  [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html)  . * List the sets of data shared with you with  [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html)  . * Create a catalog using shares of interest to you with  [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html)  .    ###   Examples      ``` -- Change the data provider name locally. > ALTER PROVIDER `Center for Disease Control` RENAME TO cdc;  -- List the shares the provider has granted you access too. > SHOW SHARES IN PROVIDER cdc;  vaccinedata  -- Make the share accessible locally as a catalog. > CREATE CATALOG cdcdata USING cdc.vaccinedata;  -- Use the data. > USE CATALOG cdcdata; > SELECT COUNT(\*) FROM information\_schema.tables;  10  ```      ###   Related articles   * [DESCRIBE PROVIDER](sql-ref-syntax-aux-describe-provider.html) * [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html) * [DROP PROVIDER](sql-ref-syntax-ddl-drop-provider.html) * [SHOW PROVIDERS](sql-ref-syntax-aux-show-providers.html) * [COMMENT ON PROVIDER](sql-ref-syntax-ddl-comment.html) * [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html) * [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Shares --------     A share is a container instantiated with the  [CREATE SHARE](sql-ref-syntax-ddl-create-share.html)  command. Once created you can iteratively register a collection of existing tables defined within the metastore using the  [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html)  command. You can register tables under their original name, qualified by their original schema, or provide alternate exposed names.       You must be a metastore admin or account admin to create, alter, and drop shares.        ###   Examples      ``` -- Create share `customer\_share` only if share with same name doesn't exist, with a comment. > CREATE SHARE IF NOT EXISTS customer\_share COMMENT 'This is customer share';  -- Add 2 tables to the share. -- Expose my\_schema.tab1 a different name. -- Expose only two partitions of other\_schema.tab2 > ALTER SHARE customer\_share ADD TABLE my\_schema.tab1 AS their\_schema.tab1; > ALTER SHARE customer\_share ADD TABLE other\_schema.tab2 PARTITION (c1 = 5), (c1 = 7);  -- List the content of the share > SHOW ALL IN SHARE customer\_share;  name type shared\_object added\_at added\_by comment partitions  ----------------- ---- ---------------------- ---------------------------- -------------------------- ------- -----------------  other\_schema.tab2 TABLE main.other\_schema.tab2 2022-01-01T00:00:01.000+0000 alwaysworks@databricks.com NULL  their\_schema.tab1 TABLE main.myschema.tab2 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com NULL (c1 = 5), (c1 = 7)  ```        ###   Related articles   * [Change data feed](../../delta/delta-change-data-feed.html) * [CREATE SHARE](sql-ref-syntax-ddl-create-share.html) * [DESCRIBE SHARE](sql-ref-syntax-aux-describe-share.html) * [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html) * [DROP SHARE](sql-ref-syntax-ddl-drop-share.html) * [SHOW SHARES](sql-ref-syntax-aux-show-shares.html) * [SHOW ALL IN SHARE](sql-ref-syntax-aux-show-all-in-share.html) * [COMMENT ON SHARE](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Recipients ------------     A recipient is an object you create using  [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html)  to represent an organization which you want to allow access shares. When you create a recipient Databricks SQL generates an activation link you can send to the organization. To retrieve the activation link after creation you use  [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html)  .       Once a recipient has been created you can give it  `SELECT`  privileges on shares of your choice using  [GRANT ON SHARE](security-grant-share.html)  .       You must be a metastore administrator to create recipients, drop recipients, and grant access to shares.        ###   Examples      ``` -- Create a recipient. > CREATE RECIPIENT IF NOT EXISTS other\_org COMMENT 'other.org';  -- Retrieve the activation link to send to other.org > DESCRIBE RECIPIENT other\_org;  name created\_at created\_by comment activation\_link active\_token\_id active\_token\_expiration\_time rotated\_token\_id rotated\_token\_expiration\_time  --------- ---------------------------- -------------------------- --------- --------------- ------------------------------------ ---------------------------- ---------------- -----------------------------  other\_org 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com other.org https://.... 0160c81f-5262-40bb-9b03-3ee12e6d98d7 9999-12-31T23:59:59.999+0000 NULL NULL  -- Choose shares that other.org has access to > GRANT SELECT ON SHARE customer\_share TO RECIPIENT other\_org;  ```        ###   Related articles   * [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html) * [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html) * [ALTER RECIPIENT](sql-ref-syntax-ddl-alter-recipient.html) * [DROP RECIPIENT](sql-ref-syntax-ddl-drop-recipient.html) * [SHOW RECIPIENTS](sql-ref-syntax-aux-show-recipients.html) * [COMMENT ON RECIPIENT](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)	Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data.	1	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/delta-update.html	convert this to databricks update syntax UPDATE SC_GFLS_ILM_PROD.PROCESSED.OPEN_PURCHASE_ORDERS_GLBL_WHLSL_S4 AS TGT SET MATERIAL_AVAILABILITY_DATE=SOH.MAD FROM NGP_DA_PROD.EIS.SO_HDR AS SOH WHERE TGT.SALES_ORDER_HEADER_NUMBER=SOH.SO_HDRNBR;	UPDATE ========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Updates the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows.       This statement is only supported for Delta Lake tables.        Syntax --------      ``` UPDATE table\_name [table\_alias]    SET  { { column\_name | field\_name }  = [ expr | DEFAULT } [, ...]    [WHERE clause]  ```        Parameters ------------   * **[table\_name](sql-ref-names.html#table-name)**      Identifies table to be updated. The table name must not use a  [temporal specification](sql-ref-names.html#table-name)  . * **[table\_alias](sql-ref-names.html#table-alias)**      Define an alias for the table. The alias must not include a column list. * **[column\_name](sql-ref-names.html#column-name)**      A reference to a column in the table. You may reference each column at most once. * **[field\_name](sql-ref-names.html#field-name)**      A reference to field within a column of type STRUCT. You may reference each field at most once. * **[expr](sql-ref-expression.html)**      An arbitrary expression. If you reference  `table\_name`  columns they represent the state of the row prior the update. * **DEFAULT**      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  [SQL warehouse version 2022.35](../release-notes/index.html)  or higher  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.2 and above       The  `DEFAULT`  expression for the column if one is defined, NULL otherwise. * **[WHERE](sql-ref-syntax-qry-select-where.html)**      Filter rows by predicate. The  `WHERE`  clause may include subqueries with the following exceptions:      + Nested subqueries, that is, a subquery inside another subquery  + A   `NOT        IN`   subquery inside an   `OR`   , for example,   `a        =        3        OR        b        NOT        IN        (SELECT        c        from        t)`  In most cases, you can rewrite  `NOT     IN`  subqueries using  `NOT     EXISTS`  . You should use  `NOT     EXISTS`  whenever possible, as  `UPDATE`  with  `NOT     IN`  subqueries can be slow.       Examples ----------      ``` > UPDATE events SET eventType = 'click' WHERE eventType = 'clk'  > UPDATE all\_events  SET session\_time = 0, ignored = true  WHERE session\_time < (SELECT min(session\_time) FROM good\_events)  > UPDATE orders AS t1  SET order\_status = 'returned'  WHERE EXISTS (SELECT oid FROM returned\_orders WHERE t1.oid = oid)  > UPDATE events  SET category = 'undefined'  WHERE category NOT IN (SELECT category FROM events2 WHERE date > '2001-01-01')  > UPDATE events  SET ignored = DEFAULT  WHERE eventType = 'unknown'  ```        Related articles ------------------   * [COPY](delta-copy-into.html) * [DELETE](delta-delete-from.html) * [INSERT](sql-ref-syntax-dml-insert-into.html) * [MERGE](delta-merge-into.html) * [PARTITION](sql-ref-partition.html#partition) * [query](sql-ref-syntax-qry-query.html)	This is a Databricks update syntax for updating the column values for the rows that match a predicate. The syntax updates the `MATERIAL_AVAILABILITY_DATE` column values in the `SC_GFLS_ILM_PROD.PROCESSED.OPEN_PURCHASE_ORDERS_GLBL_WHLSL_S4` table from the `NGP_DA_PROD.EIS.SO_HDR` table where `TGT.SALES_ORDER_HEADER_NUMBER=SOH.SO_HDRNBR`.	0	1	1
no_reaction	https://spark.apache.org/docs/latest/sql-data-sources-csv.html	sample code for reading csv files	      CSV file ==========     This article provides examples for reading and writing to CSV files with Databricks using Python, Scala, R, and SQL.        Note       You can use SQL to read CSV data directly or by using a temporary view. Databricks recommends using a temporary view. Reading the CSV file directly has the following drawbacks:     * You can’t  [specify data source options](#options)  . * You can’t  [specify the schema](#specify-schema)  for the data.     See  [Examples](#examples)  .         Options ---------     You can configure several options for CSV file data sources. See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#csv(path:String):Unit) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html#pyspark.sql.DataFrameWriter.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#csv(path:String):Unit)         Work with malformed CSV records ---------------------------------     When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:     * `PERMISSIVE`  (default): nulls are inserted for fields that could not be parsed correctly * `DROPMALFORMED`  : drops lines that contain fields that could not be parsed * `FAILFAST`  : aborts the reading if any malformed data is found     To set the mode, use the  `mode`  option.        ``` diamonds\_df = (spark.read   .format("csv")   .option("mode", "PERMISSIVE")   .load("/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv") )  ```      In the  `PERMISSIVE`  mode it is possible to inspect the rows that could not be parsed correctly using one of the following methods:     * You can provide a custom path to the option  `badRecordsPath`  to record corrupt records to a file. * You can add the column  `\_corrupt\_record`  to the schema provided to the DataFrameReader to review corrupt records in the resultant DataFrame.      Note       The  `badRecordsPath`  option takes precedence over  `\_corrupt\_record`  , meaning that malformed rows written to the provided path do not appear in the resultant DataFrame.       Default behavior for malformed records changes when using the  [rescued data column](#rescued-data)  .         ###   Find malformed rows notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-corrupt-record.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                    Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.3 (Unsupported)](../release-notes/runtime/8.3.html)  and above.        When using the  `PERMISSIVE`  mode, you can enable the rescued data column to capture any data that wasn’t parsed because one or more fields in a record have one of the following issues:     * Absent from the provided schema. * Does not match the data type of the provided schema. * Has a case mismatch with the field names in the provided schema.     The rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set("spark.databricks.sql.rescuedDataColumn.filePath.enabled",     "false")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name when reading data, such as  `\_rescued\_data`  with  `spark.read.option("rescuedDataColumn",     "\_rescued\_data").format("csv").load(<path>)`  .       The CSV parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors.       When  `rescuedDataColumn`  is used in  `PERMISSIVE`  mode, the following rules apply to  [corrupt records](#corrupt-records)  :     * The first row of the file (either a header row or a data row) sets the expected row length. * A row with a different number of columns is considered incomplete. * Data type mismatches are not considered corrupt records. * Only incomplete and malformed CSV records are considered corrupt and recorded to the  `\_corrupt\_record`  column or  `badRecordsPath`  .       Examples ----------     These examples use the  [diamonds dataset](../dbfs/databricks-datasets.html)  . Specify the path to the dataset as well as any options that you would like.        In this section:     * [Read file in any language](#read-file-in-any-language) * [Specify schema](#specify-schema) * [Pitfalls of reading a subset of columns](#pitfalls-of-reading-a-subset-of-columns)     ###  [Read file in any language](#id1)     This notebook shows how to read a file, display sample data, and print the data schema using Scala, R, Python, and SQL.        ####   Read CSV files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Specify schema](#id2)     When the schema of the CSV file is known, you can specify the desired schema to the CSV reader with the  `schema`  option.        ####   Read CSV files with schema notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-schema.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Pitfalls of reading a subset of columns](#id3)     The behavior of the CSV parser depends on the set of columns that are read. If the specified schema is incorrect, the results might differ considerably depending on the subset of columns that is accessed. The following notebook presents the most common pitfalls.        ####   Caveats of reading a subset of columns of a CSV file notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-column-subset.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                  	To read CSV files with Databricks, you can use Python, Scala, R, or SQL. Databricks recommends using a temporary view to read CSV data with SQL. When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. You can configure several options for CSV file data sources, such as specifying the mode for handling malformed CSV records. The CSV parser supports three modes when parsing records: PERMISSIVE, DROPMALFORMED, and FAILFAST. When used together with rescuedDataColumn, data type mismatches do not cause records to be dropped in DROPMALFORMED mode or throw an error in FAILFAST mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors. There are several examples provided in the context for reading CSV files with Databricks, including reading a file in any language, specifying a schema, and pitfalls of reading a subset of columns.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-tables.html	can i convert managed tables to external tables	    Create tables ===============     This article introduces the concept of  *managed*   and  *external*   tables in Unity Catalog and describes how to create tables in Unity Catalog.        Note       When you create a table, be sure to reference a catalog that is governed by Unity Catalog. The catalog  `hive\_metastore`  appears in Data Explorer but is not considered governed by Unity Catalog. It is managed by your Databricks workspace’s Hive metastore. All other catalogs listed are governed by Unity Catalog.       You can use the Unity Catalog table upgrade interface to upgrade existing tables registered in the Hive metastore to Unity Catalog. See  [Upgrade tables and views to Unity Catalog](migrate.html)  .           Managed tables ----------------     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.           External tables -----------------     External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.       When you run  `DROP     TABLE`  on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a  `LOCATION`  path in your  `CREATE     TABLE`  statement. External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     To manage access to the underlying cloud storage for an external table, you must set up  [storage credentials and external locations](manage-external-locations-and-credentials.html)  .       To learn more, see  [Create an external table](#create-an-external-table)  .         Requirements --------------     You must have the  `CREATE     TABLE`  privilege on the schema in which you want to create the table, as well as the  `USE     SCHEMA`  privilege on the schema and the  `USE     CATALOG`  privilege on the parent catalog.       If you are creating an external table, see  [Create an external table](#create-an-external-table)  for additional requirements.         Create a managed table ------------------------     To create a managed table, run the following SQL command. You can also use the  [example notebook](#example-notebook-managed-table)  to create a table. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. This cannot be the  `hive\_metastore`  catalog that is created automatically for the Hive metastore associated with your Databricks workspace. * `<schema\_name>`  : The name of the schema. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column.       ``` CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> (  <column\_specification> );  ```       ``` spark.sql("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "   "("   " <column\_specification>"   ")")  ```       ``` library(SparkR)  sql(paste("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> ",  "(",  " <column\_specification>",  ")",  sep = ""))  ```       ``` spark.sql("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> " +  "(" +  " <column\_specification>" +  ")")  ```       You can also create a managed table by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of table full names by using  [databricks\_tables](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/tables)   .       For example, to create the table  `main.default.department`  and insert five rows into it:         ``` CREATE TABLE main.default.department (  deptcode INT,  deptname STRING,  location STRING );  INSERT INTO main.default.department VALUES  (10, 'FINANCE', 'EDINBURGH'),  (20, 'SOFTWARE', 'PADDINGTON'),  (30, 'SALES', 'MAIDSTONE'),  (40, 'MARKETING', 'DARLINGTON'),  (50, 'ADMIN', 'BIRMINGHAM');  ```       ``` spark.sql("CREATE TABLE main.default.department "   "("   " deptcode INT,"   " deptname STRING,"   " location STRING"   ")"   "INSERT INTO main.default.department VALUES "   " (10, 'FINANCE', 'EDINBURGH'),"   " (20, 'SOFTWARE', 'PADDINGTON'),"   " (30, 'SALES', 'MAIDSTONE'),"   " (40, 'MARKETING', 'DARLINGTON'),"   " (50, 'ADMIN', 'BIRMINGHAM')")  ```       ``` library(SparkR)  sql(paste("CREATE TABLE main.default.department ",  "(",  " deptcode INT,",  " deptname STRING,",  " location STRING",  ")",  "INSERT INTO main.default.department VALUES ",  " (10, 'FINANCE', 'EDINBURGH'),",  " (20, 'SOFTWARE', 'PADDINGTON'),",  " (30, 'SALES', 'MAIDSTONE'),",  " (40, 'MARKETING', 'DARLINGTON'),",  " (50, 'ADMIN', 'BIRMINGHAM')",  sep = ""))  ```       ``` spark.sql("CREATE TABLE main.default.department " +  "(" +  " deptcode INT," +  " deptname STRING," +  " location STRING" +  ")" +  "INSERT INTO main.default.department VALUES " +  " (10, 'FINANCE', 'EDINBURGH')," +  " (20, 'SOFTWARE', 'PADDINGTON')," +  " (30, 'SALES', 'MAIDSTONE')," +  " (40, 'MARKETING', 'DARLINGTON')," +  " (50, 'ADMIN', 'BIRMINGHAM')")  ```        ###   Example notebooks for creating managed tables     You can use the following example notebooks to create a catalog, schema, and managed table, and to manage permissions on them.        ####   Create and manage a table in Unity Catalog with SQL     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-example-notebook.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Create and manage a table in Unity Catalog with Python     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-quickstart-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                   Drop a managed table ----------------------     You must be the table’s owner to drop a table. To drop a managed table, run the following SQL command:        ``` DROP TABLE IF EXISTS catalog\_name.schema\_name.table\_name;  ```      When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.         Create an external table --------------------------     The data in an external table is stored in a path on your cloud tenant. To work with external tables, Unity Catalog introduces two objects to access and work with external cloud storage:     * A  *storage credential*   contains an authentication method for accessing a cloud storage location. The storage credential does not contain a mapping to the path to which it grants access. Storage credentials are access-controlled to determine which users can use the credential. * An  *external location*   maps a storage credential with a cloud storage path to which it grants access. The external location grants access only to that cloud storage path and its contents. External locations are access-controlled to determine which users can use them. An external location is used automatically when your SQL command contains a  `LOCATION`  clause.      ###   Requirements     To create an external table, you must have:     * The  `CREATE     EXTERNAL     TABLE`  privilege on an external location or storage credential, that grants access to the  `LOCATION`  accessed by the external table. * The  `USE     SCHEMA`  permission on the table’s parent schema. * The  `USE     CATALOG`  permission on the table’s parent catalog. * The  `CREATE     TABLE`  permission on the table’s parent schema.     External locations and storage credentials are stored in the top level of the metastore, rather than in a catalog. To create a storage credential or an external location, you must be the metastore admin or an account-level admin. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .       ###   Create a table     Use one of the following command examples in a notebook or the SQL query editor to create an external table.       You can also use an  [example notebook](#example-notebook-external-table)  to create the storage credential, external location, and external table, and also manage permissions for them.       In the following examples, replace the placeholder values:     * `<catalog>`  : The name of the catalog that will contain the table. * `<schema>`  : The name of the schema that will contain the table. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column. * `<bucket\_path>`  : The path on your cloud tenant where the table will be created. * `<table\_directory>`  : A directory where the table will be created. Use a unique directory for each table.      Important       Once a table is created in a path, users can no longer directly access the files in that path from Databricks even if they have been given privileges on an external location or storage credential to do so. This is to ensure that users cannot circumvent access controls applied to tables by reading files from your cloud tenant directly.          ``` CREATE TABLE <catalog>.<schema>.<table\_name> (  <column\_specification> ) LOCATION 's3://<bucket\_path>/<table\_directory>';  ```       ``` spark.sql("CREATE TABLE <catalog>.<schema>.<table\_name> "   "("   " <column\_specification>"   ") "   "LOCATION 's	Yes, you can convert managed tables to external tables. To do so, you can create an external table with the same schema as the managed table and then insert the data from the managed table into the external table. Alternatively, you can use the CTAS (Create Table As Select) statement to create an external table from a managed table.	1	1	1
no_reaction	https://kb.databricks.com/scala/spark-job-fail-parquet-column-convert	Possible cause: Parquet column cannot be converted.	Title: Apache Spark job fails with Parquet column cannot be converted error  Description: Parquet column cannot be converted error appears when you are reading decimal data in Parquet format and writing to a Delta table.  Answer: Problem You are reading data in Parquet format and writing to a Delta table when you get a Parquet column cannot be converted error message. The cluster is running Databricks Runtime 7.3 LTS or above. org.apache.spark.SparkException: Task failed while writing rows. Caused by: com.databricks.sql.io.FileReadException: Error while reading file s3://bucket-name/landing/edw/xxx/part-xxxx-tid-c00.snappy.parquet. Parquet column cannot be converted. Column: [Col1], Expected: DecimalType(10,0), Found: FIXED_LEN_BYTE_ARRAY Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException. Cause The vectorized Parquet reader is decoding the decimal type column to a binary format. The vectorized Parquet reader is enabled by default in Databricks Runtime 7.3 and above for reading datasets in Parquet files. The read schema uses atomic data types: binary, boolean, date, string, and timestamp. Info This error only occurs if you have decimal type columns in the source data. Solution If you have decimal type columns in your source data, you should disable the vectorized Parquet reader. Set spark.sql.parquet.enableVectorizedReader to false in the cluster’s Spark configuration to disable the vectorized Parquet reader at the cluster level. You can also disable the vectorized Parquet reader at the notebook level by running: %scala spark.conf.set("spark.sql.parquet.enableVectorizedReader","false") Info The vectorized Parquet reader enables native record-level filtering using push-down filters, improving memory locality, and cache utilization. If you disable the vectorized Parquet reader, there may be a minor performance impact. You should only disable it, if you have decimal type columns in your source data.	Possible cause for the Parquet column cannot be converted error is when you are reading decimal data in Parquet format and writing to a Delta table. This error occurs if you have decimal type columns in the source data and the vectorized Parquet reader is enabled by default in Databricks Runtime 7.3 and above for reading datasets in Parquet files. To solve this issue, you can disable the vectorized Parquet reader at the cluster or notebook level by setting spark.sql.parquet.enableVectorizedReader to false or running %scala spark.conf.set("spark.sql.parquet.enableVectorizedReader","false"). However, disabling the vectorized Parquet reader may have a minor performance impact.	1	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-ansi-compliance.html	does databricks support ansi sql	ANSI compliance in Databricks Runtime =======================================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       This article describes ANSI compliance in Databricks Runtime. For ANSI mode in Databricks SQL, see  [ANSI\_MODE](parameters/ansi_mode.html)  .       Spark SQL has two options to support compliance with the ANSI SQL standard:  `spark.sql.ansi.enabled`  and  `spark.sql.storeAssignmentPolicy`  .       When  `spark.sql.ansi.enabled`  is set to  `true`  , Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results if the inputs to a SQL operator/function are invalid. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL’s style.       Moreover, Spark SQL has an independent option to control implicit casting behaviours when inserting rows in a table. The casting behaviours are defined as store assignment rules in the standard.       When  `spark.sql.storeAssignmentPolicy`  is set to  `ANSI`  , Spark SQL complies with the ANSI store assignment rules. This is a separate configuration because its default value is  `ANSI`  , while the configuration  `spark.sql.ansi.enabled`  is disabled by default.       The following table summarizes the behavior:           |   Property Name   |   Default   |   Meaning   | | --- | --- | --- | |  `spark.sql.ansi.enabled`  |   false   |   When true, Spark attempts to conform to the ANSI SQL specification:  * Throws a runtime exception if an overflow occurs in any operation on an integer or decimal field. * Forbids using the reserved keywords of ANSI SQL as identifiers in the SQL parser.  | |  `spark.sql.storeAssignmentPolicy`  |   ANSI   |   When inserting a value into a column with a different data type, Spark performs type conversion. There are three policies for the type coercion rules:  `ANSI`  ,  `legacy`  , and  `strict`  .  * `ANSI`  : Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting string to int or double to boolean. * `legacy`  : Spark allows the type coercion as long as it is a valid Cast, which is very loose. For example, converting string to int or double to boolean is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. * `strict`  : Spark doesn’t allow any possible precision loss or data truncation in type coercion, for example, converting double to int or decimal to double is not allowed.  |     The following subsections present behavior changes in arithmetic operations, type conversions, and SQL parsing when ANSI mode is enabled. For type conversions in Spark SQL, there are three kinds of them and this article will introduce them one by one: cast, store assignment and type coercion.        Arithmetic operations -----------------------     In Spark SQL, arithmetic operations performed on numeric types (with the exception of decimal) are not checked for overflows by default. This means that in case an operation causes overflows, the result is the same with the corresponding operation in a Java or Scala program (For example, if the sum of 2 integers is higher than the maximum value representable, the result is a negative number). On the other hand, Spark SQL returns null for decimal overflows. When  `spark.sql.ansi.enabled`  is set to  `true`  and an overflow occurs in numeric and interval arithmetic operations, it throws an arithmetic exception at runtime.        ``` -- `spark.sql.ansi.enabled=true` > SELECT 2147483647 + 1;  error: integer overflow  -- `spark.sql.ansi.enabled=false` > SELECT 2147483647 + 1;  -2147483648  ```        Cast ------     When  `spark.sql.ansi.enabled`  is set to  `true`  , explicit casting by  `CAST`  syntax throws a runtime exception for illegal cast patterns defined in the standard, such as casts from a string to an integer.       The  `CAST`  clause of Spark ANSI mode follows the syntax rules of section 6.13 “cast specification” in  [ISO/IEC 9075-2:2011 Information technology — Database languages - SQL — Part 2: Foundation (SQL/Foundation)](https://www.iso.org/standard/53682.html)   , except it specially allows the following straightforward type conversions which are disallowed as per the ANSI standard:     * NumericType <=> BooleanType * StringType <=> BinaryType     The valid combinations of source and target data type in a  `CAST`  expression are given by the following table. “Y” indicates that the combination is syntactically valid without restriction and “N” indicates that the combination is not valid.                   |   SourceTarget   |   Numeric   |   String   |   Date   |   Timestamp   |   Interval   |   Boolean   |   Binary   |   Array   |   Map   |   Struct   | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | |   Numeric   |   Y   |   Y   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   | |   String   |   Y   |   Y   |   Y   |   Y   |   Y   |   Y   |   Y   |   N   |   N   |   N   | |   Date   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Timestamp   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Interval   |   N   |   Y   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   |   N   | |   Boolean   |   Y   |   Y   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   | |   Binary   |   Y   |   N   |   N   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   | |   Array   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   Y   |   N   |   N   | |   Map   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   Y   |   N   | |   Struct   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   Y   |      ``` -- Examples of explicit casting  -- `spark.sql.ansi.enabled=true` > SELECT CAST('a' AS INT);  error: invalid input syntax for type numeric: a  > SELECT CAST(2147483648L AS INT);  error: Casting 2147483648 to int causes overflow  > SELECT CAST(DATE'2020-01-01' AS INT)  error: cannot resolve 'CAST(DATE '2020-01-01' AS INT)' due to data type mismatch: cannot cast date to int. To convert values from date to int, you can use function UNIX\_DATE instead.  -- `spark.sql.ansi.enabled=false` (This is a default behavior) > SELECT cast('a' AS INT);  null  > SELECT CAST(2147483648L AS INT);  -2147483648  > SELECT CAST(DATE'2020-01-01' AS INT);  null  -- Examples of store assignment rules > CREATE TABLE t (v INT);  -- `spark.sql.storeAssignmentPolicy=ANSI` > INSERT INTO t VALUES ('1');  error: Cannot write incompatible data to table '`default`.`t`': - Cannot safely cast 'v': string to int;  -- `spark.sql.storeAssignmentPolicy=LEGACY` (This is a legacy behavior until Spark 2.x) > INSERT INTO t VALUES ('1'); > SELECT \* FROM t;  1  ```        Store assignment ------------------     As mentioned at the beginning, when  `spark.sql.storeAssignmentPolicy`  is set to  `ANSI`  (which is the default value), Spark SQL complies with the ANSI store assignment rules on table insertions. The valid combinations of source and target data type in table insertions are given by the following table.                   |   SourceTarget   |   Numeric   |   String   |   Date   |   Timestamp   |   Interval   |   Boolean   |   Binary   |   Array   |   Map   |   Struct   | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | |   Numeric   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   | |   String   |   N   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   | |   Date   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Timestamp   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Interval   |   N   |   Y   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   |   N   | |   Boolean   |   N   |   Y   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   | |   Binary   |   N   |   Y	Yes, Databricks supports ANSI SQL. Databricks Runtime has two options to support compliance with the ANSI SQL standard: `spark.sql.ansi.enabled` and `spark.sql.storeAssignmentPolicy`. When `spark.sql.ansi.enabled` is set to `true`, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results if the inputs to a SQL operator/function are invalid.	1	1	1
no_reaction	https://docs.databricks.com/ingestion/auto-loader/options.html	how to declare schema with cloudfiles	Title: Generate schema from case class  Description: Learn how to generate a schema from a Scala case class.  Answer: Spark provides an easy way to generate a schema from a Scala case class. For case class A, use the method ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]. For example: %scala import org.apache.spark.sql.types.StructType import org.apache.spark.sql.catalyst.ScalaReflection case class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]]) val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] schema.printTreeString	To declare schema with cloud files, you can use the method ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] where A is the case class that defines the schema. This method generates a schema from a Scala case class and can be used with cloud files.	0	0	0
no_reaction	https://docs.databricks.com/files/download-internet-files.html	how to download a csv file from the dbfs to a local machine?	      CSV file ==========     This article provides examples for reading and writing to CSV files with Databricks using Python, Scala, R, and SQL.        Note       You can use SQL to read CSV data directly or by using a temporary view. Databricks recommends using a temporary view. Reading the CSV file directly has the following drawbacks:     * You can’t  [specify data source options](#options)  . * You can’t  [specify the schema](#specify-schema)  for the data.     See  [Examples](#examples)  .         Options ---------     You can configure several options for CSV file data sources. See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#csv(path:String):Unit) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html#pyspark.sql.DataFrameWriter.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#csv(path:String):Unit)         Work with malformed CSV records ---------------------------------     When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:     * `PERMISSIVE`  (default): nulls are inserted for fields that could not be parsed correctly * `DROPMALFORMED`  : drops lines that contain fields that could not be parsed * `FAILFAST`  : aborts the reading if any malformed data is found     To set the mode, use the  `mode`  option.        ``` diamonds\_df = (spark.read   .format("csv")   .option("mode", "PERMISSIVE")   .load("/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv") )  ```      In the  `PERMISSIVE`  mode it is possible to inspect the rows that could not be parsed correctly using one of the following methods:     * You can provide a custom path to the option  `badRecordsPath`  to record corrupt records to a file. * You can add the column  `\_corrupt\_record`  to the schema provided to the DataFrameReader to review corrupt records in the resultant DataFrame.      Note       The  `badRecordsPath`  option takes precedence over  `\_corrupt\_record`  , meaning that malformed rows written to the provided path do not appear in the resultant DataFrame.       Default behavior for malformed records changes when using the  [rescued data column](#rescued-data)  .         ###   Find malformed rows notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-corrupt-record.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                    Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.3 (Unsupported)](../release-notes/runtime/8.3.html)  and above.        When using the  `PERMISSIVE`  mode, you can enable the rescued data column to capture any data that wasn’t parsed because one or more fields in a record have one of the following issues:     * Absent from the provided schema. * Does not match the data type of the provided schema. * Has a case mismatch with the field names in the provided schema.     The rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set("spark.databricks.sql.rescuedDataColumn.filePath.enabled",     "false")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name when reading data, such as  `\_rescued\_data`  with  `spark.read.option("rescuedDataColumn",     "\_rescued\_data").format("csv").load(<path>)`  .       The CSV parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors.       When  `rescuedDataColumn`  is used in  `PERMISSIVE`  mode, the following rules apply to  [corrupt records](#corrupt-records)  :     * The first row of the file (either a header row or a data row) sets the expected row length. * A row with a different number of columns is considered incomplete. * Data type mismatches are not considered corrupt records. * Only incomplete and malformed CSV records are considered corrupt and recorded to the  `\_corrupt\_record`  column or  `badRecordsPath`  .       Examples ----------     These examples use the  [diamonds dataset](../dbfs/databricks-datasets.html)  . Specify the path to the dataset as well as any options that you would like.        In this section:     * [Read file in any language](#read-file-in-any-language) * [Specify schema](#specify-schema) * [Pitfalls of reading a subset of columns](#pitfalls-of-reading-a-subset-of-columns)     ###  [Read file in any language](#id1)     This notebook shows how to read a file, display sample data, and print the data schema using Scala, R, Python, and SQL.        ####   Read CSV files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Specify schema](#id2)     When the schema of the CSV file is known, you can specify the desired schema to the CSV reader with the  `schema`  option.        ####   Read CSV files with schema notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-schema.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Pitfalls of reading a subset of columns](#id3)     The behavior of the CSV parser depends on the set of columns that are read. If the specified schema is incorrect, the results might differ considerably depending on the subset of columns that is accessed. The following notebook presents the most common pitfalls.        ####   Caveats of reading a subset of columns of a CSV file notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-column-subset.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                  	To download a CSV file from the DBFS to a local machine, you can use the Databricks CLI command `databricks fs cp` followed by the path of the file in the DBFS and the path of the destination on the local machine. For example, to download a file named `example.csv` from the DBFS to the current directory on the local machine, you can run the command `databricks fs cp dbfs:/path/to/example.csv ./example.csv`.	1	0	1
no_reaction	https://docs.databricks.com/notebooks/run-notebook.html	How can i prevent user to not run any notebooks ?	Run Databricks notebooks ==========================     Before you can run any cell in a notebook, you must  [attach the notebook to a cluster](notebook-ui.html#attach)  .       To run all the cells in a notebook, select  **Run All**   in the notebook toolbar.        Important       Do not use  **Run All**   if steps for  [mount and unmount](../dbfs/mounts.html)  are in the same notebook. It could lead to a race condition and possibly corrupt the mount points.        To run a single cell, click in the cell and press  **shift+enter**   . You can also run a subset of lines in a cell; see  [Run selected text](notebooks-code.html#run-selected-text)  .       To run all cells before or after a cell, use the cell actions menu  ![Cell actions](../_images/cell-actions.png)  at the far right. Click  ![Run Menu](../_images/run-menu.png)  and select  **Run All Above**   or  **Run All Below**   .  **Run All Below**   includes the cell you are in;  **Run All Above**   does not.       When a notebook is running, the icon in the notebook tab changes from  ![notebook tab icon](../_images/nb-not-running-icon.png)  to  ![running notebook tab icon](../_images/nb-running-icon.png)  . If notifications are enabled in your browser and you navigate to a different tab while a notebook is running, a notification appears when the notebook finishes.       To stop or interrupt a running notebook, select  ![the interrupt button](../_images/nb-interrupt-button.png)  in the notebook toolbar. You can also select  **Run > Interrupt execution**   , or use the keyboard shortcut  `I     I`  .        Schedule a notebook run -------------------------     To automatically run a notebook on a regular schedule,  [create a notebook job](schedule-notebook-jobs.html)  .           Run a Delta Live Tables pipeline from a notebook --------------------------------------------------     For information about starting a Delta Live Tables run from a notebook, see  [Open or run a Delta Live Tables pipeline from a notebook](notebooks-dlt-pipeline.html)  .         Notifications ---------------     Notifications alert you to certain events, such as which command is currently running and which commands are in error state. When your notebook is showing multiple error notifications, the first one will have a link that allows you to clear all notifications.      ![Notebook notifications](../_images/notification.png)    Notebook notifications are enabled by default. You can disable them in  [user settings](notebooks-manage.html#configure-notebook-settings)  .      ###   Background notifications     If you start a notebook run and then navigate away from the tab or window that the notebook is running in, a notification appears when the notebook is completed. You can disable this notification in your browser settings.         ###   Databricks Advisor     Databricks Advisor automatically analyzes commands every time they are run and displays appropriate advice in the notebooks. The advice notices provide information that can assist you in improving the performance of workloads, reducing costs, and avoiding common mistakes.      ####   View advice     A blue box with a lightbulb icon signals that advice is available for a command. The box displays the number of distinct pieces of advice.      ![Databricks advice](../_images/advice-collapsed.png)    Click the lightbulb to expand the box and view the advice. One or more pieces of advice will become visible.      ![View advice](../_images/advice-expanded.png)    Click the  **Learn more**   link to view documentation providing more information related to the advice.       Click the  **Don’t show me this again**   link to hide the piece of advice. The advice of this type will no longer be displayed. This action can be  [reversed in Editor settings](#advice-settings)  .       Click the lightbulb again to collapse the advice box.          ###   Advice settings     To enable or disable Databricks Advisor, go to  [user settings](notebooks-manage.html#configure-notebook-settings)  or click the gear icon in the expanded advice box.       Toggle the  **Turn on Databricks Advisor**   option to enable or disable advice.       The  **Reset hidden advice**   link is displayed if one or more types of advice is currently hidden. Click the link to make that advice type visible again.	To prevent users from running any notebooks, you can revoke their access to the cluster or workspace. This can be done through the Databricks UI by modifying the user's permissions.	0	0	1
no_reaction	https://docs.databricks.com/partners/prep/dbt-cloud.html	dbt cli	Connect to dbt Cloud ======================     dbt (data build tool) is a development environment that enables data analysts and data engineers to transform data by simply writing select statements. dbt handles turning these select statements into tables and views. dbt compiles your code into raw SQL and then runs that code on the specified database in Databricks. dbt supports collaborative coding patterns and best practices such as version control, documentation, and modularity.       dbt does not extract or load data. dbt focuses on the transformation step only, using a “transform after load” architecture. dbt assumes that you already have a copy of your data in your database.       This article focuses on dbt Cloud. dbt Cloud comes equipped with turnkey support for scheduling jobs, CI/CD, serving documentation, monitoring and alerting, and an integrated development environment (IDE).       A local version of dbt called dbt Core is also available. dbt Core enables you to write dbt code in the text editor or IDE of your choice on your local development machine and then run dbt from the command line. dbt Core includes the dbt Command Line Interface (CLI). The dbt CLI is free to use and open source. For more information, see  [Connect to dbt Core](dbt.html)  .       Because dbt Cloud and dbt Core can use hosted git repositories (for example, on GitHub, GitLab or BitBucket), you can use dbt Cloud to create a dbt project and then make it available to your dbt Cloud and dbt Core users. For more information, see  [Creating a dbt project](https://docs.getdbt.com/docs/building-a-dbt-project/projects#creating-a-dbt-project)   and  [Using an existing project](https://docs.getdbt.com/docs/building-a-dbt-project/projects#using-an-existing-project)   on the dbt website.       For a general overview of dbt, watch the following YouTube video (26 minutes).                      Connect to dbt Cloud using Partner Connect --------------------------------------------     This section describes how to connect your Databricks workspace to dbt Cloud using Partner Connect, then give dbt Cloud read access to your data.        Note       Partner Connect does not connect Databricks  [clusters](../../clusters/index.html)  to dbt Cloud. To connect a cluster to dbt Cloud, connect to dbt Cloud manually.         ###   Differences between standard connections and dbt Cloud     To connect to dbt Cloud using Partner Connect, you follow the steps in  [Connect to data prep partners using Partner Connect](../../partner-connect/prep.html)  . The dbt Cloud connection is different from standard data preparation and transformation connections in the following ways:     * In addition to a service principal and a personal access token, Partner Connect creates a SQL warehouse (formerly SQL endpoint) named  **DBT\_CLOUD\_ENDPOINT**   by default.       ###   Steps to connect     To connect to dbt Cloud using Partner Connect, do the following:     1. [Connect to data prep partners using Partner Connect](../../partner-connect/prep.html)  . 2. After you connect to dbt Cloud, your dbt Cloud dashboard appears. To explore your dbt Cloud project, in the menu bar, next to the dbt logo, select your dbt account name from the first drop-down if it is not displayed, and then select the  **Databricks Partner Connect Trial**   project from the second drop-down menu if it is not displayed.        Tip       To view your project’s settings, click the “three stripes” or “hamburger” menu, click  **Account Settings > Projects**   , and click the name of the project. To view the connection settings, click the link next to  **Connection**   . To change any settings, click  **Edit**   .       To view the Databricks personal access token information for this project, click the “person” icon on the menu bar, click  **Profile > Credentials > Databricks Partner Connect Trial**   , and click the name of the project. To make a change, click  **Edit**   .       ###   Steps to give dbt Cloud read access to your data     Partner Connect gives create-only permission to the  **DBT\_CLOUD\_USER**   service principal only on the default catalog. Follow these steps in your Databricks workspace to give the  **DBT\_CLOUD\_USER**   service principal read access to the data that you choose.        Warning       You can adapt these steps to give dbt Cloud additional access across catalogs, databases, and tables within your workspace. However, as a security best practice, Databricks strongly recommends that you give access only to the individual tables that you need the  **DBT\_CLOUD\_USER**   service principal to work with and only read access to those tables.      1. Switch your workspace to the  **SQL**   persona. To do this, in your workspace’s  [sidebar](../../workspace/index.html#use-the-sidebar)  , click the icon below the Databricks logo  ![Databricks logo](../../_images/databricks-logo.png)  , and then select  **SQL**   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   in the sidebar. 3. Select the SQL warehouse (  **DBT\_CLOUD\_ENDPOINT**   ) in the drop-down list at the top right.      ![Select warehouse](../../_images/select-endpoint.png)   1. Under   **Data Explorer**    , select the catalog that contains the database for your table.  2. Select the database that contains your table.  3. Select your table.   Tip       If you do not see your catalog, database, or table listed, enter any portion of the name in the  **Select Catalog**   ,  **Select Database**   , or  **Filter tables**   boxes, respectively, to narrow down the list.      ![Filter tables](../../_images/filter-tables.png) 4. Click  **Permissions**   . 5. Click  **Grant**   . 6. For  **Type to add multiple users or groups**   , select  **DBT\_CLOUD\_USER**   . This is the Databricks service principal that Partner Connect created for you in the previous section.        Tip       If you do not see  **DBT\_CLOUD\_USER**   , begin typing  `DBT\_CLOUD\_USER`  in the  **Type to add multiple users or groups**   box until it appears in the list, and then select it. 7. Grant read access only by selecting  **SELECT**   and  **READ\_METADATA**   . 8. Click  **OK**   .     Repeat steps 4-9 for each additional table that you want to give dbt Cloud read access to.         ###   Troubleshoot the dbt Cloud connection     If someone deletes the project in dbt Cloud for this account, and you the click the  **dbt**   tile, an error message appears, stating that the project cannot be found. To fix this, click  **Delete connection**   , and then start from the beginning of this procedure to create the connection again.            Connect to dbt Cloud manually -------------------------------     This section describes how to connect a Databricks  [cluster](../../clusters/configure.html)  or a Databricks SQL  [warehouse](../../sql/admin/create-sql-warehouse.html)  in your Databricks workspace to  [dbt Cloud](https://docs.getdbt.com/docs/dbt-cloud/cloud-overview)   .        Note       To create a new SQL warehouse and then quickly connect it to dbt Cloud, use Partner Connect instead.         ###   Requirements   * A cluster or SQL warehouse in your Databricks workspace.      + [Create a cluster](../../clusters/configure.html)   .  + [Configure SQL warehouses](../../sql/admin/create-sql-warehouse.html)   . * The connection details for your cluster or SQL warehouse, specifically the  **Server Hostname**   ,  **Port**   , and  **HTTP Path**   values.      + [Get connection details for a cluster](../../integrations/jdbc-odbc-bi.html#connection-details-cluster)   .  + [Get connection details for a SQL warehouse](../../integrations/jdbc-odbc-bi.html#sql)   . * A Databricks  [personal access token](../../dev-tools/auth.html#pat)  .        Note       As a security best practice, when authenticating with automated tools, systems, scripts, and apps, Databricks recommends you use access tokens belonging to  [service principals](../../administration-guide/users-groups/service-principals.html)  instead of workspace users. To create access tokens for service principals, see  [Manage access tokens for a service principal](../../administration-guide/users-groups/service-principals.html#personal-access-tokens)  .   * To connect dbt Cloud to data managed by Unity Catalog, dbt version 1.1 or above.       The steps in this article create a new environment that uses the latest dbt version. For information about upgrading the dbt version for an existing environment, see  [Upgrading to the latest version of dbt in Cloud](https://docs.getdbt.com/docs/dbt-versions/upgrade-core-in-cloud#upgrading-to-the-latest-version-of-dbt-in-cloud)   in the dbt documentation.       ###   Step 1: Sign up for dbt Cloud     Go to  [dbt Cloud - Signup](https://www.getdbt.com/signup/)   and enter your email, name, and company information. Create a password and click  **Create my account**   .         ###   Step 2: Create a dbt project     In this step, you create a dbt  *project*   , which contains a connection to a Databricks  [cluster](../../clusters/configure.html)  or a  [SQL warehouse](../../sql/admin/create-sql-warehouse.html)  , a repository that contains your source code, and one or more environments (such as testing and production environments).     1. [Sign in to dbt Cloud](https://cloud.getdbt.com/login/)   . 2. Click the settings icon, and then click  **Account Settings**   . 3. Click  **New Project**   . 4. For  **Name**   , enter a unique name for your project, and then click  **Continue**   . 5. For  **Choose a warehouse**   , click  **Databricks**   , and then click  **Next**   . 6. For  **Name**   , enter a unique name for this connection. 7. For  **Select Adapter**   , select  **Databricks (dbt-databricks)**   .        Note       By default, new projects use the Databricks (dbt-databricks) adapter, which supports Unity Catalog. To migrate an existing project to Databricks (dbt-databricks), see  [Migrating from dbt-spark to dbt-databricks](https://docs.getdbt.com/guides/migration/tools/migrating-from-spark-to-databricks)   in the dbt documentation. 8. For  **Server Hostname**   , enter the server hostname value from the requirements. 9. For  **HTTP Path**   , enter the HTTP path value from the requirements. 10. If your workspace is Unity Catalog-enabled, under  **Optional Settings**   , enter the name of the catalog for dbt Cloud to use. 11. For  **Token**   , enter the personal access token from the requirements. 12. For  **Schema**   , enter the name of the schema where you want dbt Cloud to create the tables and views (for example,  `default`  ). 13. Click  **Test Connection**	The dbt Command Line Interface (CLI) is a free and open-source tool that is part of dbt Core. It enables you to write dbt code in the text editor or IDE of your choice on your local development machine and then run dbt from the command line.	0	1	1
no_reaction	https://docs.databricks.com/delta-live-tables/event-hubs.html	what is the best connector to read data from event hub?	    Azure Event Hubs ==================    [Azure Event Hubs](https://azure.microsoft.com/services/event-hubs/)   is a hyper-scale telemetry ingestion service that collects, transforms, and stores millions of events. As a distributed streaming platform, it gives you low latency and configurable time retention, which enables you to ingress massive amounts of telemetry into the cloud and read the data from multiple applications using publish-subscribe semantics.       This article explains how to use Structured Streaming with Azure Event Hubs and Databricks clusters.        Requirements --------------     For current release support, see “Latest Releases” in the Azure Event Hubs Spark Connector project  [readme file](https://github.com/Azure/azure-event-hubs-spark/blob/master/README.md#latest-releases)   .     1. [Create a library](../libraries/workspace-libraries.html#maven-libraries)  in your Databricks workspace using the Maven coordinate  `com.microsoft.azure:azure-eventhubs-spark\_2.11:2.3.17`  .        Note       This connector is updated regularly, and a more recent version may be available: we recommend that you pull the latest connector from the  [Maven repository](https://mvnrepository.com/artifact/com.microsoft.azure/azure-eventhubs-spark) 2. [Install the created library](../libraries/cluster-libraries.html#install-libraries)  into your cluster.       Schema --------     The schema of the records is:          |   Column   |   Type   | | --- | --- | |   body   |   binary   | |   partition   |   string   | |   offset   |   string   | |   sequenceNumber   |   long   | |   enqueuedTime   |   timestamp   | |   publisher   |   string   | |   partitionKey   |   string   | |   properties   |   map[string,json]   |     The  `body`  is always provided as a byte array. Use  `cast("string")`  to explicitly deserialize the  `body`  column.         Quick Start -------------     Let’s start with a quick example: WordCount. The following notebook is all that it takes to run WordCount using Structured Streaming with Azure Event Hubs.        ###   Azure Event Hubs WordCount with Structured Streaming notebook     [Open notebook in new tab](/_extras/notebooks/source/structured-streaming-event-hubs-wordcount.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Configuration ---------------     This section discusses the configuration settings you need to work with Event Hubs.       For detailed guidance on configuring Structured Streaming with Azure Event Hubs, see the  [Structured Streaming and Azure Event Hubs Integration Guide](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md)   developed by Microsoft.       For detailed guidance on using Structured Streaming, see  [What is Apache Spark Structured Streaming?](index.html)  .      ###   Connection string     An Event Hubs connection string is required to connect to the Event Hubs service. You can get the connection string for your Event Hubs instance from the  [Azure portal](https://portal.azure.com)   or by using the  `ConnectionStringBuilder`  in the library.      ####   Azure portal     When you get the connection string from the Azure portal, it may or may not have the  `EntityPath`  key. Consider:        ```  // Without an entity path val without = "Endpoint=<endpoint>;SharedAccessKeyName=<key-name>;SharedAccessKey=<key>"  // With an entity path val with = "Endpoint=sb://<sample>;SharedAccessKeyName=<key-name>;SharedAccessKey=<key>;EntityPath=<eventhub-name>"  ```      To connect to your EventHubs, an  `EntityPath`  must be present. If your connection string doesn’t have one, don’t worry. This will take care of it:        ``` import org.apache.spark.eventhubs.ConnectionStringBuilder  val connectionString = ConnectionStringBuilder(without) // defined in the previous code block  .setEventHubName("<eventhub-name>")  .build  ```      ####   ConnectionStringBuilder     Alternatively, you can use the  `ConnectionStringBuilder`  to make your connection string.        ``` import org.apache.spark.eventhubs.ConnectionStringBuilder  val connectionString = ConnectionStringBuilder()  .setNamespaceName("<namespace-name>")  .setEventHubName("<eventhub-name>")  .setSasKeyName("<key-name>")  .setSasKey("<key>")  .build  ```       ###   EventHubsConf     All configuration relating to Event Hubs happens in your  `EventHubsConf`  . To create an  `EventHubsConf`  , you must pass a connection string:        ``` val connectionString = "<event-hub-connection-string>" val eventHubsConf = EventHubsConf(connectionString)  ```      See  [Connection String](#connection-string)  for more information about obtaining a valid connection string.       For a complete list of configurations, see  [EventHubsConf](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#eventhubsconf)   . Here is a  *subset of configurations*   to get you started:             |   Option   |   Value   |   Default   |   Query type   |   Description   | | --- | --- | --- | --- | --- | |   consumerGroup   |   String   |   “$Default”   |   Streaming and batch   |   A consumer group is a view of an entire event hub. Consumer groups enable multiple consuming applications to each have a separate view of the event stream, and to read the stream independently at their own pace and with their own offsets. More information is available in the  [Microsoft documentation](https://learn.microsoft.com/azure/event-hubs/event-hubs-features#event-consumers)   .   | |   startingPosition   |   EventPosition   |   Start of stream   |   Streaming and batch   |   The starting position for your Structured Streaming job. See  [startingPositions](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#eventhubsconf)   for information about the order in which options are read.   | |   maxEventsPerTrigger   |   long   |   partitionCount  * 1000  |   Streaming query   |   Rate limit on maximum number of events processed per trigger interval. The specified total number of events will be proportionally split across partitions of different volume.   |     For each option, there exists a corresponding setting in  `EventHubsConf`  . For example:        ``` import org.apache.spark.eventhubs.  val cs = "<your-connection-string>" val eventHubsConf = EventHubsConf(cs)  .setConsumerGroup("sample-cg")  .setMaxEventsPerTrigger(10000)  ```     ####   EventPosition    `EventHubsConf`  allows users to specify starting (and ending) positions with the  `EventPosition`  class.  `EventPosition`  defines the position of an event in an Event Hub partition. The position can be an enqueued time, offset, sequence number, the start of the stream, or the end of the stream.        ``` import org.apache.spark.eventhubs.\_  EventPosition.fromOffset("246812") // Specifies offset 246812 EventPosition.fromSequenceNumber(100L) // Specifies sequence number 100 EventPosition.fromEnqueuedTime(Instant.now) // Any event after the current time EventPosition.fromStartOfStream // Specifies from start of stream EventPosition.fromEndOfStream // Specifies from end of stream  ```      If you would like to start (or end) at a specific position, simply create the correct  `EventPosition`  and set it in your  `EventHubsConf`  :        ``` val connectionString = "<event-hub-connection-string>" val eventHubsConf = EventHubsConf(connectionString)  .setStartingPosition(EventPosition.fromEndOfStream)  ```          Production Structured Streaming with Azure Event Hubs -------------------------------------------------------     When you run streaming queries in production, you probably want more robustness and uptime guarantees than you would have when you simply attach a notebook to a cluster and run your streaming queries interactively. Import and run the following notebook for a demonstration of how to configure and run Structured Streaming in production with Azure Event Hubs and Databricks.       For more information, see  [Production considerations for Structured Streaming](production.html)  .        ###   Production Structured Streaming with Azure Event Hubs notebook     [Open notebook in new tab](/_extras/notebooks/source/structured-streaming-event-hubs-integration.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                 	The best connector to read data from Event Hub is the Azure Event Hubs Spark Connector.	0	1	1
no_reaction	https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html	Transition a model’s stage using the MLflow Client API.	      Log, load, register, and deploy MLflow models ===============================================     An MLflow  [Model](https://mlflow.org/docs/latest/models.html)   is a standard format for packaging machine learning models that can be used in a variety of downstream tools—for example, batch inference on Apache Spark or real-time serving through a REST API. The format defines a convention that lets you save a model in different  [flavors](https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors)   (python-function, pytorch, sklearn, and so on), that can be understood by different model  [serving and inference platforms](https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools)   .          Log and load models ---------------------     When you log a model, MLflow automatically logs  `requirements.txt`  and  `conda.yaml`  files. You can use these files to recreate the model development environment and reinstall dependencies using  `virtualenv`  (recommended) or  `conda`  .        Important       Anaconda Inc. updated their  [terms of service](https://www.anaconda.com/terms-of-service)   for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See  [Anaconda Commercial Edition FAQ](https://www.anaconda.com/blog/anaconda-commercial-edition-faq)   for more information. Your use of any Anaconda channels is governed by their terms of service.       MLflow models logged before  [v1.18](https://mlflow.org/news/2021/06/18/1.18.0-release/index.html)   (Databricks Runtime 8.3 ML or earlier) were by default logged with the conda  `defaults`  channel (  <https://repo.anaconda.com/pkgs/>  ) as a dependency. Because of this license change, Databricks has stopped the use of the  `defaults`  channel for models logged using MLflow v1.18 and above. The default channel logged is now  `conda-forge`  , which points at the community managed  <https://conda-forge.org/>  .       If you logged a model before MLflow v1.18 without excluding the  `defaults`  channel from the conda environment for the model, that model may have a dependency on the  `defaults`  channel that you may not have intended. To manually confirm whether a model has this dependency, you can examine  `channel`  value in the  `conda.yaml`  file that is packaged with the logged model. For example, a model’s  `conda.yaml`  with a  `defaults`  channel dependency may look like this:        ``` channels: - defaults dependencies: - python=3.8.8 - pip - pip:  - mlflow  - scikit-learn==0.23.2  - cloudpickle==1.6.0  name: mlflow-env  ```      Because Databricks can not determine whether your use of the Anaconda repository to interact with your models is permitted under your relationship with Anaconda, Databricks is not forcing its customers to make any changes. If your use of the Anaconda.com repo through the use of Databricks is permitted under Anaconda’s terms, you do not need to take any action.       If you would like to change the channel used in a model’s environment, you can re-register the model to the model registry with a new  `conda.yaml`  . You can do this by specifying the channel in the  `conda\_env`  parameter of  `log\_model()`  .       For more information on the  `log\_model()`  API, see the MLflow documentation for the model flavor you are working with, for example,  [log\_model for scikit-learn](https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)   .       For more information on  `conda.yaml`  files, see the  [MLflow documentation](https://www.mlflow.org/docs/latest/models.html#additional-logged-files)   .       ###   API commands     To log a model to the MLflow  [tracking server](tracking.html)  , use  `mlflow.<model-type>.log\_model(model,     ...)`  .       To load a previously logged model for inference or further development, use  `mlflow.<model-type>.load\_model(modelpath)`  , where  `modelpath`  is one of the following:     * a run-relative path (such as  `runs:/{run\_id}/{model-path}`  ) * a DBFS path * a  [registered model](model-registry.html)  path (such as  `models:/{model\_name}/{model\_stage}`  ).     For a complete list of options for loading MLflow models, see  [Referencing Artifacts in the MLflow documentation](https://www.mlflow.org/docs/latest/concepts.html#artifact-locations)   .       For Python MLflow models, an additional option is to use  `mlflow.pyfunc.load\_model()`  to load the model as a generic Python function. You can use the following code snippet to load the model and score data points.        ``` model = mlflow.pyfunc.load\_model(model\_path) model.predict(model\_input)  ```      As an alternative, you can export the model as an Apache Spark UDF to use for scoring on a Spark cluster, either as a batch job or as a real-time  [Spark Streaming](../structured-streaming/index.html)  job.        ``` # load input data table as a Spark DataFrame input\_data = spark.table(input\_table\_name) model\_udf = mlflow.pyfunc.spark\_udf(model\_path) df = input\_data.withColumn("prediction", model\_udf())  ```      ###   Log model dependencies     To accurately load a model, you should make sure the model dependencies are loaded with the correct versions into the notebook environment. In Databricks Runtime 10.5 ML and above, MLflow warns you if a mismatch is detected between the current environment and the model’s dependencies.       Additional functionality to simplify restoring model dependencies is included in Databricks Runtime 11.0 ML and above. In Databricks Runtime 11.0 ML and above, for  `pyfunc`  flavor models, you can call  `mlflow.pyfunc.get\_model\_dependencies`  to retrieve and download the model dependencies. This function returns a path to the dependencies file which you can then install by using  `%pip     install     <file-path>`  . When you load a model as a PySpark UDF, specify  `env\_manager="virtualenv"`  in the  `mlflow.pyfunc.spark\_udf`  call. This restores model dependencies in the context of the PySpark UDF and does not affect the outside environment.       You can also use this functionality in Databricks Runtime 10.5 or below by manually installing  [MLflow version 1.25.0 or above](https://www.mlflow.org/docs/latest/index.html)   :        ``` %pip install "mlflow>=1.25.0"  ```      For additional information on how to log model dependencies (Python and non-Python) and artifacts, see  [Log model dependencies](log-model-dependencies.html)  .       Learn how to log model dependencies and custom artifacts for model serving:     * [Deploy models with dependencies](log-model-dependencies.html#deploy-dependencies) * [Use custom Python libraries with Model Serving](../machine-learning/model-serving/private-libraries-model-serving.html) * [Package custom artifacts for Model Serving](../machine-learning/model-serving/model-serving-custom-artifacts.html)    * [Log model dependencies](log-model-dependencies.html)      ###   Automatically generated code snippets in the MLflow UI     When you log a model in a Databricks notebook, Databricks automatically generates code snippets that you can copy and use to load and run the model. To view these code snippets:     1. Navigate to the Runs screen for the run that generated the model. (See  [View notebook experiment](tracking.html#view-notebook-experiment)  for how to display the Runs screen.) 2. Scroll to the  **Artifacts**   section. 3. Click the name of the logged model. A panel opens to the right showing code you can use to load the logged model and make predictions on Spark or pandas DataFrames.    ![Artifact panel code snippets](../_images/code-snippets.png)    ###   Examples     For examples of logging models, see the examples in  [Track machine learning training runs examples](tracking.html#tracking-examples)  . For an example of loading a logged model for inference, see the following example.      * [Model inference example](model-example.html)         Register models in the Model Registry ---------------------------------------     You can register models in the  [MLflow Model Registry](model-registry.html)  , a centralized model store that provides a UI and set of APIs to manage the full lifecycle of MLflow Models. For general information about the Model Registry, see  [MLflow Model Registry on Databricks](model-registry.html)  . For instructions on how to use the Model Registry to manage models in Databricks, see  [Manage model lifecycle](../machine-learning/manage-model-lifecycle/index.html)  .       To register a model using the API, use  `mlflow.register\_model("runs:/{run\_id}/{model-path}",     "{registered-model-name}")`  .         Save models to DBFS ---------------------     To save a model locally, use  `mlflow.<model-type>.save\_model(model,     modelpath)`  .  `modelpath`  must be a  [DBFS](../dbfs/index.html)  path. For example, if you use a DBFS location  `dbfs:/my\_project\_models`  to store your project work, you must use the model path  `/dbfs/my\_project\_models`  :        ```   modelpath = "/dbfs/my\_project\_models/model-%f-%f" % (alpha, l1\_ratio)   mlflow.sklearn.save\_model(lr, modelpath)  ```      For MLlib models, use  [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines)   .         Download model artifacts --------------------------     You can download the logged model artifacts (such as model files, plots, and metrics) for a registered model with various APIs.      [Python API](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_model_version_download_uri)   example:        ``` from mlflow.store.artifact.models\_artifact\_repo import ModelsArtifactRepository  model\_uri = MlflowClient.get\_model\_version\_download\_uri(model\_name, model\_version) ModelsArtifactRepository(model\_uri).download\_artifacts(artifact\_path="")  ```     [Java API](https://mlflow.org/docs/latest/java_api/org/mlflow/tracking/MlflowClient.html#downloadModelVersion-java.lang	To transition a model's stage using the MLflow Client API, you can use the `transition_model_version_stage` method of the `MlflowClient` class. This method allows you to move a registered model version from one stage to another. The stages available are `None`, `Staging`, and `Production`. Here is an example code snippet:  ``` import mlflow  # Set the tracking URI to the Databricks workspace mlflow.set_tracking_uri("databricks")  # Set the Databricks host and token mlflow_databricks = {     "host": "<databricks-instance>",     "token": "<databricks-token>" }  # Create an MLflow client client = mlflow.tracking.MlflowClient(     tracking_uri=mlflow.get_tracking_uri(),     registry_uri=f"databricks://{mlflow_databrickshost}",     artifact_uri=mlflow.get_artifact_uri(),     backend_store_uri=mlflow.get_tracking_uri(),     default_artifact_root=mlflow.get_artifact_uri(),     store_secret=None, )  # Get the registered model version model_version = client.get_model_version("<model-name>", <model-version>)  # Transition the model version to the Staging stage client.transition_model_version_stage(     name="<model-name>",     version=model_version.version,     stage="Staging" ) ```  Note that you need to set the tracking URI to the Databricks workspace and provide the Databricks host and token to create the MLflow client. Also, you need to replace `<model-name>` and `<model-version>` with the actual name and version of the registered model version you want to transition.	0	0	0
no_reaction	https://kb.databricks.com/jobs/update-notification-settings-for-jobs-with-the-jobs-api	For job alerts how do I configure email	Title: Update notification settings for jobs with the Jobs API  Description: You can use the Jobs API to add email notifications to some, or all, of the jobs in your workspace.  Answer: Email notifications can be useful when managing multiple jobs. If you have many jobs configured without notifications, manually adding notifications can be time consuming. Instead, you can use the Jobs API (AWS | Azure | GCP) to add email notifications to the jobs in your workspace. Instructions In order to call the Jobs API, you first need to setup a personal access token and a secret scope. This allows you to interact with the API via a script. After the secret scopes have been setup, you can run the example script in a notebook to update all the jobs in your workspace at once. Create a Databricks personal access token Follow the Personal access tokens for users (AWS | Azure | GCP) documentation to create a personal access token. Create a secret scope Follow the Create a Databricks-backed secret scope (AWS | Azure | GCP) documentation to create a secret scope. Store your personal access token and your Databricks instance in the secret scope Follow the Create a secret in a Databricks-backed scope (AWS | Azure | GCP) documentation to store the personal access token you created and your Databricks instance as new secrets within your secret scope. Your Databricks instance is the hostname for your workspace, for example, xxxxx.cloud.databricks.com. Use a Python script to update job notifications for all the jobs in the workspace  You need to replace the following values in the script before running: &lt;email-address&gt;  - Email address to notify. &lt;scope-name&gt; - The name of your scope that holds the secrets. &lt;secret-name-1&gt; - The name of the secret that holds your Databricks instance. &lt;secret-name-2&gt; - The name of the secret that holds your personal access token. import json import requests API_URL = dbutils.secrets.get(scope = "&lt;scope-name&gt;", key = "&lt;secret-name1&gt;") TOKEN = dbutils.secrets.get(scope = "&lt;scope-name&gt;", key = "&lt;secret-name2&gt;")   url = f"{API_URL}/api/2.0/jobs/list" #Get all the jobs created inside the workspace payload={} headers = {   'Authorization': 'Bearer ' + TOKEN } response = requests.request("GET", url, headers=headers, data=payload) response.json() for job in response.json()jobs:     job_id = job["job_id"]     payload = { "job_id": job_id, "new_settings": { "email_notifications": { "on_start": [ "&lt;email-address&gt;" #user's email ID ], "on_success": [ "&lt;email-address&gt;" ], "on_failure": [ "&lt;email-address&gt;" ] } }     }     url2 = f"{API_URL}/api/2.1/jobs/update"     r = requests.post(url2, data = json.dumps(payload), headers=headers) print("successfully added the email_notification to jobs") Info You can modify the sample script to add additional filtering options if you don't want to add notifications to all jobs. For example, you can filter based on the job creator and only add notifications to the filtered jobs. This version of the sample code adds an if condition, checking an email address against the value creator_user_name. This filters the jobs based on the job creator. Replace &lt;job-creator-email-address&gt; with the email address you want to filter on. import json import requests API_URL = dbutils.secrets.get(scope = "&lt;scope-name&gt;", key = "&lt;secret-name1&gt;")  TOKEN = dbutils.secrets.get(scope = "&lt;scope-name&gt;", key = "&lt;secret-name2&gt;")   url = f"{API_URL}/api/2.0/jobs/list"  #Get all the jobs created inside the workspace payload={} headers = {   'Authorization': 'Bearer ' + TOKEN } response = requests.request("GET", url, headers=headers, data=payload) response.json() for job in response.json()jobs:     if jobcreator_user_name=="&lt;job-creator-email-address&gt;":  # filtering the jobs based on the job creator         job_id = job["job_id"]         payload = {         "job_id": job_id,         "new_settings": {         "email_notifications": {         "on_start": [         "&lt;email-address&gt;"  #user's email ID                       ],         "on_success": [         "&lt;email-address&gt;"                        ],         "on_failure": [         "&lt;email-address&gt;"                        ]                     }                         }                     }         url2 = f"{API_URL}/api/2.1/jobs/update"         r = requests.post(url2, data = json.dumps(payload), headers=headers) print("successfully added the email_notification to jobs") Verify status in the Job UI Once the code runs successfully you can verify the updated notification status by checking your jobs in the Job UI. In the left nav menu, click on Workflows. Click the name of a job you want to verify. On the right side of the job details page, scroll down to the Notifications section. The email address you added to the sample script is now present and configured for notifications.	To configure email notifications for jobs, you can use the Jobs API to add email notifications to some or all of the jobs in your workspace. To call the Jobs API, you first need to set up a personal access token and a secret scope. After the secret scopes have been set up, you can run the example script in a notebook to update all the jobs in your workspace at once. You need to replace the following values in the script before running: <email-address> - Email address to notify. <scope-name> - The name of your scope that holds the secrets. <secret-name-1> - The name of the secret that holds your Databricks instance. <secret-name-2> - The name of the secret that holds your personal access token. Once the code runs successfully, you can verify the updated notification status by checking your jobs in the Job UI.	0	1	1
no_reaction	https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html	How can I deploy Private Link on an existing Databricks workspace?	Manage private access settings ================================         This article describes a process that is available only for  [accounts on the E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.       This article discusses how to create private access settings objects, which are a required object as part of enabling  [AWS PrivateLink](https://aws.amazon.com/privatelink)   . This article does not contain all the information necessary to configure PrivateLink for your workspace. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see  [Enable AWS PrivateLink](../cloud-configurations/aws/privatelink.html)  .       The following related sections discuss updating existing network and configuration objects:     * [Update a running or failed workspace](../workspace/update-workspace.html)  . * [Updates of existing PrivateLink configuration objects](../cloud-configurations/aws/privatelink.html#update-related)  .      What is a private access settings object? -------------------------------------------     A private access settings object is a Databricks object that describes a workspace’s PrivateLink connectivity. Create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region.       This object serves several purposes:     * It expresses your intent to use AWS PrivateLink with your workspace. * It controls your settings for the front-end use case of AWS PrivateLink for public network access. * It controls which VPC endpoints are permitted to access your workspace.     Create a private access settings object using the account console or the  [Account API 2.0](../../dev-tools/api/latest/account.html)  . You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you  *must*   attach a private access settings object to the workspace during workspace creation.           Create a private access settings object -----------------------------------------      Note       These instructions show you how to create the private access object from the  **Cloud resources**   page in the account console before you create a new workspace. You can also create the private access settings in a similar way as part of the flow of creating a new workspace and choosing  **Add a new private access object**   from the picker instead of choosing an existing object. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .      1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. Click  **Add private access settings**   .      ![private access settings object](../../_images/privatelink-vpc-pas.png) 5. Enter a name for your new private access settings object. 6. For the region, be sure to match the region of your workspace as this is not validated immediately and workspace deployment will fail if it does not match. It is validated only during the actual creation of the workspace. 7. Set the  **Public access enabled**   field, which configures public access to the front-end connection (the web application and REST APIs) for your workspace.      * If set to   **False**    (the default), the front-end connection can be accessed only using PrivateLink connectivity and not from the public internet. When public access is disabled, the   [IP access lists for workspaces](../../security/network/ip-access-list-workspace.html)   feature is unsupported.  * If set to   **True**    , the front-end connection can be accessed either from PrivateLink connectivity or from the public internet. Any IP access lists only limit connections from the public internet but not traffic through the PrivateLink connection. 8. Set the  **Private Access Level**   field to the value that best represents which VPC endpoints to allow for your workspace.      * Set to   **Account**    to limit connections to those VPC endpoints that are registered in your Databricks account.  * Set to   **Endpoint**    to limit connections to an explicit set of VPC endpoints, which you can enter in a field that appears. It lets you select VPC endpoint registrations that you’ve already created. Be sure to include your   *front-end*    VPC endpoint registration if you created one. 9. Click  **Add**   .         Update a private access settings object -----------------------------------------     To update fields on a private access object:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Update**   . 5. Change any fields. For guidance on specific fields, see  [Create a private access settings object](#create)  .        Note       The private access access level  `ANY`  is deprecated. If the object previously had this value and you use the account console to update the private access settings for any fields, you must change the private access level to a non-deprecated value. To make changes to other fields without changing the  `ANY`  private access level at this time, use the  [Account API](../../dev-tools/api/latest/account.html)  . See  [AWS PrivateLink private access level ANY is deprecated](../../release-notes/product/2022/august.html#privatelink-private-access-level-any-deprecated)  . 6. Click  **Update private access setting**   .         Delete a private access settings object -----------------------------------------     Private access settings object cannot be edited after creation. If the configuration has incorrect data or if you no longer need it for any workspaces, delete it:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Delete**   . 5. In the confirmation dialog, click  **Confirm Delete**   .	To deploy Private Link on an existing Databricks workspace, you need to create a private access settings object that describes the workspace's PrivateLink connectivity. You can create a new private access settings object just for this workspace or re-use and share an existing private access setting object among multiple workspaces, but they must be in the same AWS region. You can create a private access settings object using the account console or the Account API 2.0. You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object, but to use PrivateLink, you must attach a private access settings object to the workspace during workspace creation. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see Enable AWS PrivateLink.	0	1	1
no_reaction	https://kb.databricks.com/streaming/ss-read-from-last-offset	How do you use offset in streaming query	Title: How to restart a structured streaming query from last written offset  Description: Learn how to restart a structured streaming query from the last written offset.  Answer: Scenario You have a stream, running a windowed aggregation query, that reads from Apache Kafka and writes files in Append mode. You want to upgrade the application and restart the query with the offset equal to the last written offset. You want to discard all state information that hasn’t been written to the sink, start processing from the earliest offsets that contributed to the discarded state, and modify the checkpoint directory accordingly. However, if you use existing checkpoints after upgrading the application code, old states and objects from the previous application version are re-used, which results in unexpected output such as reading from old sources or processing with old application code. Solution Apache Spark maintains state across the execution and binary objects on checkpoints. Therefore you cannot modify the checkpoint directory. As an alternative, copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. Make sure to delete the checkpoint directory. You can get the current offsets by using asynchronous APIs: %scala spark.streams.addListener(new StreamingQueryListener() {     override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {         println("Query started:" + queryStarted.id)     }     override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {         println("Query terminated" + queryTerminated.id)     }     override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {      println("Query made progress")         println("Starting offset:" + queryProgress.progress.sources(0).startOffset)         println("Ending offset:" + queryProgress.progress.sources(0).endOffset)         //Logic to save these offsets     } }) You can use readStream with the latest offset written by the process shown above: %scala option("startingOffsets",  """ {"articleA":{"0":23,"1":-1},"articleB":{"0":-2}} """) The input schema for streaming records is: root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- article: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) Also, you can implement logic to save and update the offset to a database and read it at the next restart.	To use offset in streaming query, you can copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. You can get the current offsets by using asynchronous APIs and use readStream with the latest offset written by the process shown above.	1	1	1
no_reaction	https://docs.databricks.com/ingestion/index.html	how do I load files into databricks	LOAD DATA ===========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Loads the data into a Hive SerDe table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the  `LOAD     DATA`  statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table.       If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the table or the dependents are accessed the next time.        Syntax --------      ``` LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table\_name [ PARTITION clause ]  ```        Parameters ------------   * **path**       Path of the file system. It can be either an absolute or a relative path. * **[table\_name](sql-ref-names.html#table-name)**      Identifies the table to be inserted to. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the table cannot be found Databricks raises a  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error. * **[PARTITION clause](sql-ref-partition.html#partition)**      An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition. * **LOCAL**       If specified, it causes the  `INPATH`  to be resolved against the local file system, instead of the default file system, which is typically a distributed storage. * **OVERWRITE**       By default, new data is appended to the table. If  `OVERWRITE`  is used, the table is instead overwritten with new data.       Examples ----------      ``` -- Example without partition specification. -- Assuming the students table has already been created and populated. > SELECT \* FROM students;  name address student\_id  --------- ---------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111   > CREATE TABLE test\_load (name VARCHAR(64), address VARCHAR(64), student\_id INT) USING HIVE;  -- Assuming the students table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test\_load;  > SELECT \* FROM test\_load;  name address student\_id  --------- ---------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111  -- Example with partition specification. > CREATE TABLE test\_partition (c1 INT, c2 INT, c3 INT) PARTITIONED BY (c2, c3);  > INSERT INTO test\_partition PARTITION (c2 = 2, c3 = 3) VALUES (1);  > INSERT INTO test\_partition PARTITION (c2 = 5, c3 = 6) VALUES (4);  > INSERT INTO test\_partition PARTITION (c2 = 8, c3 = 9) VALUES (7);  > SELECT \* FROM test\_partition;  c1 c2 c3  --- --- ---  1 2 3  4 5 6  7 8 9  > CREATE TABLE test\_load\_partition (c1 INT, c2 INT, c3 INT) USING HIVE PARTITIONED BY (c2, c3);  -- Assuming the test\_partition table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/test\_partition/c2=2/c3=3'  OVERWRITE INTO TABLE test\_load\_partition PARTITION (c2=2, c3=3);  > SELECT \* FROM test\_load\_partition;  c1 c2 c3  --- --- ---  1 2 3  ```        Related articles ------------------   * [INSERT INTO](sql-ref-syntax-dml-insert-into.html) * [COPY INTO](delta-copy-into.html)	To load files into Databricks, you can use the `LOAD DATA` statement to load data into a Hive SerDe table from a specified directory or file. If a directory is specified, then all the files from the directory are loaded. If a file is specified, then only the single file is loaded. Additionally, the `LOAD DATA` statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table. Here is an example syntax:  ``` LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table_name [ PARTITION clause ] ```	0	1	1
no_reaction	https://docs.databricks.com/notebooks/best-practices.html	what is the best practice for using a repo?	Set up Databricks Repos =========================     Learn how to set up Databricks Repos for version control. Once you set up Databricks Repos, you can use it to perform common Git operations such as clone, checkout, commit, push, pull, and branch management. You can also see diffs for your changes as you develop with notebooks and files in Databricks.        Configure user settings -------------------------     Databricks Repos uses a personal access token (PAT) or an equivalent credential to authenticate with your Git provider to perform operations such as clone, push, pull etc. To use Repos you first need to add your Git PAT and Git provider username to Databricks. See  [Get a Git access token & connect a remote repo to Databricks](get-access-tokens-from-git-provider.html)  .       You can clone public remote repositories without Git credentials (a personal access token and a username). To modify a public remote repository, or to clone or modify a private remote repository, you must have a Git provider username and personal access token with read and write permissions for the remote repository.         Add Git credentials to Databricks -----------------------------------   1. Click  ![User Settings Icon](../_images/user-settings-icon.png) **Settings**   at the top right of your screen and select  **User Settings**   . 2. Click the  **Git Integration**   tab. 3. If you have previously entered credentials, click the  **Change settings**   button. 4. In the Git provider drop-down, select the provider name. 5. Get a personal access token (PAT) or other credentials from your Git provider. For details, see  [Get a Git access token & connect a remote repo to Databricks](get-access-tokens-from-git-provider.html)       Important       Databricks recommends that you set an expiration date for all personal access tokens.   1. For Azure DevOps, Git integration does not support Azure Active Directory tokens. You must use an Azure DevOps personal access token. 2. If your organization has SAML SSO enabled in GitHub, ensure that you have  [authorized your personal access token for SSO](https://docs.github.com/en/github/authenticating-to-github/authorizing-a-personal-access-token-for-use-with-saml-single-sign-on)   .   1. Paste your Git provider PAT token into the  **Token**   field. 2. Enter your username into the  **Git provider username**   field and click  **Save**   .     You can also save a Git PAT token and username to Databricks by using the  [Databricks Repos API](../dev-tools/api/latest/repos.html)  .         Network connectivity between Databricks Repos and a Git provider ------------------------------------------------------------------     Databricks Repos needs network connectivity to your Git provider to function. Ordinarily, this is over the internet and works out of the box. However, you might have setup additional restrictions on your Git provider for controlling access. For example, you might have a IP allow list in place or you might host your own private Git server—through services like GitHub Enterprise(GHE), Bitbucket Server(BBS), or Gitlab Self-managed—and your Git server might not be accessible via the internet.        Note     * If your Git server is internet-accessible but has an IP allow list in place, for example  [GitHub allow lists](https://docs.github.com/organizations/keeping-your-organization-secure/managing-security-settings-for-your-organization/managing-allowed-ip-addresses-for-your-organization)   , you must add Databricks control plane NAT IPs to the Git server’s IP allow list. For a list of control plane NAT IP addresses by region, see  [Databricks clouds and regions](../resources/supported-regions.html)  . Use the IP for the region that your Databricks workspace is in. * If you are privately hosting a Git server, get in touch with your Databricks representative for onboarding instructions for access.        Security features in Databricks Repos ---------------------------------------     Databricks Repos comes with several security features. Following sections walk you through their setup and use:     * Use of encrypted Git credentials. * An allow list * Workspace access control * Audit logging * Secrets detection             Bring your own key: Encrypt Git credentials ---------------------------------------------     You can use AWS Key Management Service to encrypt a Git personal access token (PAT) or other Git credential. Using a key from an encryption service is referred to as a customer-managed key (CMK) or bring your own key (BYOK).       For more information, see  [Customer-managed keys for managed services](../security/keys/customer-managed-keys-managed-services-aws.html)        Restrict usage to URLs in an allow list -----------------------------------------     An admin can limit which remote repositories users can clone from and commit & push to. This helps prevent exfiltration of your code, for example, users cannot push code to an arbitrary repository if you have turned on the allow list restrictions. You can also prevent users from using unlicensed code by restricting clone operation to a list of allowed repositories.       To set up an allow list:     1. Go to the  [Admin Console](../administration-guide/workspace/index.html#admin-console)  . 2. Click the  **Workspace Settings**   tab. 3. In the  **Repos**   section, choose an option from  **Repos Git Allow List**   :      * **Disabled (no restrictions)**    : There are no checks against the allow list.  * **Restrict clone, commit & push to allowed Git repositories**    : Clone, commit, and push operations are allowed only for repository URLs in the allow list.  * **Only restrict commit & push to allowed Git repositories**    : Commit and push operations are allowed only for repository URLs in the allow list. Clone and pull operations are not restricted. 4. In the field next to  **Repos Git URL Allow List: Empty list**   , enter a comma-separated list of URL prefixes. 5. Click  **Save**   .     To allow access to all repositories, choose  **Disable (no restrictions)**   .        Note     * The list you save overwrites the existing set of saved URL prefixes. * It can take up to 15 minutes for the changes to take effect.        Control access to a Repo in your workspace --------------------------------------------     Set permissions for a Repo to control access. Permissions for a Repo apply to all content in that Repo. Menu options are  **Can Manage**   ,  **Can Edit**   ,  **Can Run**   and  **Can View**   .      ![Select Permissions to control access to a Repo.](../_images/repos-permissions.png)    When you create a repo, you have  **Can Manage**   permission on it. This lets you modify content in the Repo, perform Git operations or modify the remote repository. Users should develop in their own isolated Repo and collaborate on a shared code base via Git branches and PRs. Thus, you should not give other users  **Can Edit**   or  **Can Manage**   access to your development Repo.           Allow committing .ipynb notebook output -----------------------------------------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .        By default, the admin setting for Repos doesn’t allow .ipynb notebook output to be committed. Admins can change this setting:     1. Go to  **Admin settings > Workspace**   settings. 2. Under  **Repos > Allow Repos to Export IPYNB outputs**   , select  **Allow: IPYNB outputs can be toggled on**   .      ![Admin console: Allow Repos to Export IPYNB outputs.](../_images/allow-commit-ipynb.png)      Important       When outputs are included, the visualization and dashboard configs are preserved with the .ipynb file format.       ###   Use a repo config file     You can add settings for each notebook to your Repo in a  `.databricks/commit\_outputs`  file that you create manually.       Specify which notebook should include outputs using patterns similar to  [gitignore patterns](https://git-scm.com/docs/gitignore)   .       ###   Patterns for a repo config file     The file contains positive and negative file path patterns. File path patterns include notebook file extension such as .ipynb. Positive patterns enable outputs inclusion for matching notebooks. Negative patterns disable outputs inclusion for matching notebooks. Patterns are evaluated in order for all notebooks. Invalid paths or paths not resolving to .ipynb notebooks are ignored.       For example, to include outputs from a notebook path  `folder/innerfolder/notebook.ipynb`  , use following patterns:        ``` \*\*/\* folder/\*\* folder/innerfolder/note\*  ```      To exclude outputs for a notebook, make sure none of the positive patterns match or add a negative pattern in a correct spot of the configuration file. Negative (exclude) patterns start with  `!`  :        ``` !folder/innerfolder/*.ipynb !folder/**/*.ipynb !**/notebook.ipynb  ```         Audit logging ---------------     When  [audit logging](../administration-guide/account-settings/audit-logs.html)  is enabled, audit events are logged when you interact with a Databricks repo. For example, an audit event is logged when you create, update, or delete a Databricks repo, when you list all Databricks Repos associated with a workspace, and when you sync changes between your Databricks repo and the remote Git repo.           Secrets detection -------------------     Databricks Repos scans code for access key IDs that begin with the prefix  `AKIA`  and warns the user before committing.         Next steps ------------   * [Clone a Git repo & other common Git operations](git-operations-with-repos.html) * [What are workspace files?](../files/workspace.html) * [CI/CD workflows with Git integration and Databricks Repos](ci-cd-techniques-with-repos.html) * [Run a first dbt job with Repos](../workflows/jobs/how-to/use-dbt-in-workflows.html#first-dbt-job)	The best practice for using a repo is to set up Databricks Repos for version control, configure user settings, add Git credentials to Databricks, restrict usage to URLs in an allow list, control access to a Repo in your workspace, and use a repo config file. Additionally, it is recommended to use encrypted Git credentials, restrict usage to URLs in an allow list, and control access to a Repo in your workspace.	0	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-location.html	How do I create an External Location on the UI?	CREATE EXTERNAL LOCATION ==========================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Creates an external location with the specified name. If a location with the same name already exists, an exception is thrown.        Syntax --------      ``` CREATE EXTERNAL LOCATION [IF NOT EXISTS] location\_name     URL url     WITH (STORAGE CREDENTIAL credential\_name)     [COMMENT comment]  ```        Parameters ------------   * **[location\_name](sql-ref-names.html#location-name)**      The name of the location to be created. * **IF NOT EXISTS**       Creates a location with the given name if it does not exist. If a location with the same name already exists, nothing will happen. * **url**       A  `STRING`  literal with the location of the cloud storage described as an absolute URL. * **[credential\_name](sql-ref-names.html#credential-name)**      The named credential used to connect to this location. * **comment**       An optional description for the location, or  `NULL`  . The default is  `NULL`  .       Examples ----------      ``` -- Create a location accessed using the s3\_remote\_cred credential > CREATE EXTERNAL LOCATION s3\_remote URL 's3://us-east-1/location'  WITH (STORAGE CREDENTIAL s3\_remote\_cred)  COMMENT 'Default source for AWS exernal data';  ```        Related articles ------------------   * [ALTER EXTERNAL LOCATION](sql-ref-syntax-ddl-alter-location.html) * [DESCRIBE EXTERNAL LOCATION](sql-ref-syntax-aux-describe-location.html) * [DROP EXTERNAL LOCATION](sql-ref-syntax-ddl-drop-location.html) * [External locations and storage credentials](sql-ref-external-locations.html) * [SHOW EXTERNAL LOCATIONS](sql-ref-syntax-aux-show-locations.html)	To create an External Location on the UI, you can use the `CREATE EXTERNAL LOCATION` command with the specified name, URL, and storage credential. The syntax for the command is as follows:  ``` CREATE EXTERNAL LOCATION [IF NOT EXISTS] location_name     URL url     WITH (STORAGE CREDENTIAL credential_name)     [COMMENT comment] ```  You can replace `location_name` with the name you want to give to the location, `url` with the absolute URL of the cloud storage, and `credential_name` with the named credential used to connect to this location. The `COMMENT` parameter is optional.	1	1	1
thumb_up	https://docs.databricks.com/data-governance/unity-catalog/index.html	What is Unity Catalog?	What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	Unity Catalog is a unified governance solution for data and AI assets on the Lakehouse. It provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces. Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. Its security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data. The hierarchy of primary data objects in Unity Catalog flows from metastore to table: Metastore, Catalog, Schema, and Table. You reference all data in Unity Catalog using a three-level namespace.	1	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/delta-describe-history.html	Databricks SQL how long does query history remain?	Query history ===============     The query history shows SQL queries performed using  [SQL warehouses](create-sql-warehouse.html)  .       You can use the information available through this screen to help you debug issues with queries.       This section describes how to work with query history using the UI. To work with query history using the API, see  [Query History API 2.0](../api/query-history.html)  .        Important       The time recorded in query history for a SQL query is only the time the SQL warehouse spends actually executing the query. It does not record any additional overhead associated with getting ready to execute the query, such as internal queuing, or additional time related to the data upload and download process.         View query history --------------------     To view the history of all executions of a query:     1. Click  ![History Icon](../../_images/history-icon.png) **Query History**   in the sidebar. 2. Optionally, click  **Duration**   to sort the list by duration. By default, the list is sorted by start time. 3. Click the name of a query to see more details, such as the SQL command and the  [execution details](query-profile.html)  .     You can filter the list by user, date range, SQL warehouse, and query status.       If you’re a non-admin user without  `CAN\_MANAGE`  permissions, you can only view your own queries in  **Query History**   .        Note       Queries shared by a user with  **Run as Owner**   permissions to another user with  **Can Run**   permissions appear in the query history of the user executing the query and not the user that shared the query.          View query details --------------------     To view details about a query, such as its duration, SQL command, number of rows returned, and I/O performance:     1. View  [query history](#view-query-history)  . 2. Click the name of a query.      ![Query history details](../../_images/query-details.png)      Brief information about a query’s performance appears, such as time spent in each task, rows returned, and I/O performance. 3. For more detailed information about the query’s performance,including its execution plan, click  **View Query Profile**   at the bottom of the page. For more details, see  [Query profile](query-profile.html)  .       Terminate an executing query ------------------------------     To terminate a long-running query started by you or another user:     1. View  [query history](#view-query-history)  . 2. Click the name of a query. 3. Next to  **Status**   , click  **Cancel**   .        Note      **Cancel**   only appears when a query is running.     The query is terminated and its status changes to  **Canceled**   .	The query history in Databricks SQL shows SQL queries performed using SQL warehouses. The time recorded in query history for a SQL query is only the time the SQL warehouse spends actually executing the query. It does not record any additional overhead associated with getting ready to execute the query, such as internal queuing, or additional time related to the data upload and download process. The duration of query history in Databricks SQL is not mentioned in the context.	0	0	0
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-external-tables.html	use SQL "select *" for a parquet file in an UC external storage	Parquet file ==============    [Apache Parquet](https://parquet.apache.org/)   is a columnar file format that provides optimizations to speed up queries. It is a far more efficient file format than  [CSV](csv.html)  or  [JSON](json.html)  .       For more information, see  [Parquet Files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#parquet(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#parquet(path:String):Unit)     The following notebook shows how to read and write data to Parquet files.        ###   Reading Parquet files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-parquet-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import	Based on the context, we can use SQL "select *" to read all columns from a Parquet file stored in an external storage system using the supported read options provided by Apache Spark.	1	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/data-types/struct-type.html	i got this error "AnalysisException: Can't extract value from name#478: need struct type but got string"	   StructField  [¶](#structfield "Permalink to this headline") =============================================================  *class*  `pyspark.sql.types.`  `StructField`    (   *name     :     str*  ,  *dataType     :    [pyspark.sql.types.DataType](pyspark.sql.types.DataType.html#pyspark.sql.types.DataType "pyspark.sql.types.DataType")*  ,  *nullable     :     bool     =     True*  ,  *metadata     :     Optional    [    Dict    [    str    ,    Any    ]     ]     =     None*   )   [[source]](../../../_modules/pyspark/sql/types.html#StructField) [¶](#pyspark.sql.types.StructField "Permalink to this definition")    A field in  [`StructType`](pyspark.sql.types.StructType.html#pyspark.sql.types.StructType "pyspark.sql.types.StructType")  .     Parameters    **name**    str     name of the field.    **dataType**   [`DataType`](pyspark.sql.types.DataType.html#pyspark.sql.types.DataType "pyspark.sql.types.DataType")  [`DataType`](pyspark.sql.types.DataType.html#pyspark.sql.types.DataType "pyspark.sql.types.DataType")  of the field.    **nullable**    bool, optional     whether the field can be null (None) or not.    **metadata**    dict, optional     a dict from string to simple type that can be toInternald to JSON automatically     Examples    ``` >>> from pyspark.sql.types import StringType, StructField >>> (StructField("f1", StringType(), True) ...      == StructField("f1", StringType(), True)) True >>> (StructField("f1", StringType(), True) ...      == StructField("f2", StringType(), True)) False  ```   Methods    |  [`fromInternal`](#pyspark.sql.types.StructField.fromInternal "pyspark.sql.types.StructField.fromInternal")  (obj)   |   Converts an internal SQL object into a native Python object.   | |  [`fromJson`](#pyspark.sql.types.StructField.fromJson "pyspark.sql.types.StructField.fromJson")  (json)   |   | |  [`json`](#pyspark.sql.types.StructField.json "pyspark.sql.types.StructField.json")  ()   |   | |  [`jsonValue`](#pyspark.sql.types.StructField.jsonValue "pyspark.sql.types.StructField.jsonValue")  ()   |   | |  [`needConversion`](#pyspark.sql.types.StructField.needConversion "pyspark.sql.types.StructField.needConversion")  ()   |   Does this type needs conversion between Python object and internal SQL object.   | |  [`simpleString`](#pyspark.sql.types.StructField.simpleString "pyspark.sql.types.StructField.simpleString")  ()   |   | |  [`toInternal`](#pyspark.sql.types.StructField.toInternal "pyspark.sql.types.StructField.toInternal")  (obj)   |   Converts a Python object into an internal SQL object.   | |  [`typeName`](#pyspark.sql.types.StructField.typeName "pyspark.sql.types.StructField.typeName")  ()   |   |   Methods Documentation    `fromInternal`    (   *obj     :     T*   )    → T  [[source]](../../../_modules/pyspark/sql/types.html#StructField.fromInternal) [¶](#pyspark.sql.types.StructField.fromInternal "Permalink to this definition")    Converts an internal SQL object into a native Python object.    *classmethod*  `fromJson`    (   *json     :     Dict    [    str    ,    Any    ]*   )    →  [pyspark.sql.types.StructField](#pyspark.sql.types.StructField "pyspark.sql.types.StructField")  [[source]](../../../_modules/pyspark/sql/types.html#StructField.fromJson) [¶](#pyspark.sql.types.StructField.fromJson "Permalink to this definition")   `json`    (     )    → str  [¶](#pyspark.sql.types.StructField.json "Permalink to this definition")   `jsonValue`    (     )    → Dict    [    str    ,    Any    ]   [[source]](../../../_modules/pyspark/sql/types.html#StructField.jsonValue) [¶](#pyspark.sql.types.StructField.jsonValue "Permalink to this definition")   `needConversion`    (     )    → bool  [[source]](../../../_modules/pyspark/sql/types.html#StructField.needConversion) [¶](#pyspark.sql.types.StructField.needConversion "Permalink to this definition")    Does this type needs conversion between Python object and internal SQL object.     This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.    `simpleString`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#StructField.simpleString) [¶](#pyspark.sql.types.StructField.simpleString "Permalink to this definition")   `toInternal`    (   *obj     :     T*   )    → T  [[source]](../../../_modules/pyspark/sql/types.html#StructField.toInternal) [¶](#pyspark.sql.types.StructField.toInternal "Permalink to this definition")    Converts a Python object into an internal SQL object.    `typeName`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#StructField.typeName) [¶](#pyspark.sql.types.StructField.typeName "Permalink to this definition")   [VarcharType](pyspark.sql.types.VarcharType.html "previous page")  [StructType](pyspark.sql.types.StructType.html "next page")   	The error "AnalysisException: Can't extract value from name#478: need struct type but got string" suggests that there is an issue with extracting a value from a field named "name" with field type string, when a struct type is expected. The context is a documentation for the `StructField` class in PySpark, which defines a field in a `StructType`. The documentation provides information on the parameters and methods of the `StructField` class.	1	1	1
no_reaction	https://docs.databricks.com/mlflow/index.html	Can we set authority in mlflow.	MLflow API 2.0 ================	Yes, MLflow allows you to set authority for tracking and managing experiments, models, and other artifacts. You can set permissions for users and groups using access control lists (ACLs) in the MLflow tracking server. This allows you to control who can view, modify, or delete experiments and models.	1	1	1
no_reaction	https://docs.databricks.com/dev-tools/databricks-utils.html	DBUtils	    Databricks Utilities ======================     Databricks Utilities (  `dbutils`  ) make it easy to perform powerful combinations of tasks. You can use the utilities to work with object storage efficiently, to chain and parameterize notebooks, and to work with secrets.  `dbutils`  are not supported outside of notebooks.        Important       Calling  `dbutils`  inside of executors can produce unexpected results. To learn more about limitations of  `dbutils`  and alternatives that could be used instead, see  [Limitations](#limitations)  .       `dbutils`  utilities are available in Python, R, and Scala notebooks.      **How to**   :  [List utilities](#dbutils-utilities)  ,  [list commands](#dbutils-commands)  ,  [display command help](#dbutils-help)     **Utilities**   :  [credentials](#dbutils-credentials)  ,  [data](#dbutils-data)  ,  [fs](#dbutils-fs)  ,  [jobs](#dbutils-jobs)  ,  [library](#dbutils-library)  ,  [notebook](#dbutils-workflow)  ,  [secrets](#dbutils-secrets)  ,  [widgets](#dbutils-widgets)  ,  [Utilities API library](#dbutils-api)         List available utilities --------------------------     To list available utilities along with a short description for each utility, run  `dbutils.help()`  for Python or Scala.       This example lists available commands for the Databricks Utilities.         ``` dbutils.help()  ```       ``` dbutils.help()  ```        ``` This module provides various utilities for users to interact with the rest of Databricks.  credentials: DatabricksCredentialUtils -> Utilities for interacting with credentials within notebooks fs: DbfsUtils -> Manipulates the Databricks filesystem (DBFS) from the console jobs: JobsUtils -> Utilities for leveraging jobs features library: LibraryUtils -> Utilities for session isolated libraries notebook: NotebookUtils -> Utilities for the control flow of a notebook (EXPERIMENTAL) secrets: SecretUtils -> Provides utilities for leveraging secrets within notebooks widgets: WidgetsUtils -> Methods to create and get bound value of input widgets inside notebooks  ```          List available commands for a utility ---------------------------------------     To list available commands for a utility along with a short description of each command, run  `.help()`  after the programmatic name for the utility.       This example lists available commands for the Databricks File System (DBFS) utility.         ``` dbutils.fs.help()  ```       ``` dbutils.fs.help()  ```       ``` dbutils.fs.help()  ```        ``` dbutils.fs provides utilities for working with FileSystems. Most methods in this package can take either a DBFS path (e.g., "/foo" or "dbfs:/foo"), or another FileSystem URI. For more info about a method, use dbutils.fs.help("methodName"). In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps straightforwardly onto dbutils calls. For example, "%fs head --maxBytes=10000 /file/path" translates into "dbutils.fs.head("/file/path", maxBytes = 10000)".  fsutils  cp(from: String, to: String, recurse: boolean = false): boolean -> Copies a file or directory, possibly across FileSystems head(file: String, maxBytes: int = 65536): String -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8 ls(dir: String): Seq -> Lists the contents of a directory mkdirs(dir: String): boolean -> Creates the given directory if it does not exist, also creating any necessary parent directories mv(from: String, to: String, recurse: boolean = false): boolean -> Moves a file or directory, possibly across FileSystems put(file: String, contents: String, overwrite: boolean = false): boolean -> Writes the given String out to a file, encoded in UTF-8 rm(dir: String, recurse: boolean = false): boolean -> Removes a file or directory  mount  mount(source: String, mountPoint: String, encryptionType: String = "", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Mounts the given source directory into DBFS at the given mount point mounts: Seq -> Displays information about what is mounted within DBFS refreshMounts: boolean -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information unmount(mountPoint: String): boolean -> Deletes a DBFS mount point updateMount(source: String, mountPoint: String, encryptionType: String = "", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Similar to mount(), but updates an existing mount point instead of creating a new one  ```          Display help for a command ----------------------------     To display help for a command, run  `.help("<command-name>")`  after the command name.       This example displays help for the DBFS copy command.         ``` dbutils.fs.help("cp")  ```       ``` dbutils.fs.help("cp")  ```       ``` dbutils.fs.help("cp")  ```        ``` /** * Copies a file or directory, possibly across FileSystems. * * Example: cp("/mnt/my-folder/a", "dbfs:/a/b") * * @param from FileSystem URI of the source file or directory * @param to FileSystem URI of the destination file or directory * @param recurse if true, all files and directories will be recursively copied * @return true if all files were successfully copied */ cp(from: java.lang.String, to: java.lang.String, recurse: boolean = false): boolean  ```                  Credentials utility (dbutils.credentials) -------------------------------------------    **Commands**   :  [assumeRole](#dbutils-credentials-assumerole)  ,  [showCurrentRole](#dbutils-credentials-showcurrentrole)  ,  [showRoles](#dbutils-credentials-showroles)      The credentials utility allows you to interact with credentials within notebooks. This utility is usable only on clusters with  [credential passthrough](../data-governance/credential-passthrough/iam-passthrough.html)  enabled. To list the available commands, run  `dbutils.credentials.help()`  .        ``` assumeRole(role: String): boolean -> Sets the role ARN to assume when looking for credentials to authenticate with S3 showCurrentRole: List -> Shows the currently set role showRoles: List -> Shows the set of possible assumed roles  ```       ###   assumeRole command (dbutils.credentials.assumeRole)     Sets the Amazon Resource Name (ARN) for the AWS Identity and Access Management (IAM) role to assume when looking for credentials to authenticate with Amazon S3. After you run this command, you can run S3 access commands, such as  `sc.textFile("s3a://my-bucket/my-file.csv")`  to access an object.       To display help for this command, run  `dbutils.credentials.help("assumeRole")`  .         ``` dbutils.credentials.assumeRole("arn:aws:iam::123456789012:roles/my-role")  # Out[1]: True  ```       ``` dbutils.credentials.assumeRole("arn:aws:iam::123456789012:roles/my-role")  # TRUE  ```       ``` dbutils.credentials.assumeRole("arn:aws:iam::123456789012:roles/my-role")  // res0: Boolean = true  ```         ###   showCurrentRole command (dbutils.credentials.showCurrentRole)     Lists the currently set AWS Identity and Access Management (IAM) role.       To display help for this command, run  `dbutils.credentials.help("showCurrentRole")`  .         ``` dbutils.credentials.showCurrentRole()  # Out[1]: arn:aws:iam::123456789012:role/my-role-a  ```       ``` dbutils.credentials.showCurrentRole()  # [[1]] # [1] "arn:aws:iam::123456789012:role/my-role-a"  ```       ``` dbutils.credentials.showCurrentRole()  // res0: java.util.List[String] = [arn:aws:iam::123456789012:role/my-role-a]  ```         ###   showRoles command (dbutils.credentials.showRoles)     Lists the set of possible assumed AWS Identity and Access Management (IAM) roles.       To display help for this command, run  `dbutils.credentials.help("showRoles")`  .         ``` dbutils.credentials.showRoles()  # Out[1]: arn:aws:iam::123456789012:role/my-role-a', 'arn:aws:iam::123456789012:role/my-role-b  ```       ``` dbutils.credentials.showRoles()  # [[1]] # [1] "arn:aws:iam::123456789012:role/my-role-a" # # [[2]] # [1] "arn:aws:iam::123456789012:role/my-role-b"  ```       ``` dbutils.credentials.showRoles()  // res0: java.util.List[String] = [arn:aws:iam::123456789012:role/my-role-a, arn:aws:iam::123456789012:role/my-role-b]  ```          Data utility (dbutils.data) -----------------------------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .         Note       Available in Databricks Runtime 9.0 and above.       **Commands**   :  [summarize](#dbutils-data-summarize)      The data utility allows you to understand and interpret datasets. To list the available commands, run  `dbutils.data.help()`  .        ``` dbutils.data provides utilities for understanding and interpreting datasets. This module is currently in preview and may be unstable. For more info about a method, use dbutils.data.help("methodName").  summarize(df: Object, precise: boolean): void -> Summarize a Spark DataFrame and visualize the statistics to get quick insights  ```       ###   summarize command (dbutils.data.summarize)     Calculates and displays summary statistics of an Apache Spark DataFrame or pandas DataFrame. This command is available for Python, Scala and R.       To display help for this command, run  `dbutils.data.help("summarize")`  .       In Databricks Runtime 10.1 and above, you can use the additional  `precise`  parameter to adjust the precision of the computed statistics.        Note       This feature is in  [Public Preview](../release-notes/release-types.html)  .      * When  `precise`  is set to false (the default), some	The context is about Databricks Utilities, which are a set of tools that make it easy to perform powerful combinations of tasks. The utilities can be used to work with object storage efficiently, to chain and parameterize notebooks, and to work with secrets. `dbutils` are not supported outside of notebooks. The `dbutils` utilities are available in Python, R, and Scala notebooks. There are several commands available for the utilities, including `credentials`, `data`, `fs`, `jobs`, `library`, `notebook`, `secrets`, and `widgets`. The `dbutils.help()` command can be used to list available utilities and commands, and the `.help()` command can be used to display help for a specific command. The `dbutils.data` utility allows you to understand and interpret datasets, and the `summarize` command can be used to calculate and display summary statistics of a Spark DataFrame or pandas DataFrame.	0	0	0
no_reaction	https://spark.apache.org/docs/latest/api/scala/org/apache/spark/streaming/dstream/InputDStream.html	how do I give input as a text in spark streaming	   c    [org](../../../../index.html)   .  [apache](../../../index.html)   .  [spark](../../index.html)   .  [streaming](../index.html)   .  [dstream](index.html)    InputDStream   [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html "Permalink") =======================================================================================================  ###   ####    abstract     class     InputDStream     [    T    ]     extends  [DStream](DStream.html)   [    T    ]   This is the abstract base class for all input streams. This class provides methods start() and stop() which are called by Spark Streaming system to start and stop receiving data, respectively. Input streams that can generate RDDs from new data by running a service/thread only on the driver node (that is, without running a receiver on worker nodes), can be implemented by directly inheriting this InputDStream. For example, FileInputDStream, a subclass of InputDStream, monitors an HDFS directory from the driver for new files and generates RDDs with the new files. For implementing input streams that requires running a receiver on the worker nodes, use  [org.apache.spark.streaming.dstream.ReceiverInputDStream](ReceiverInputDStream.html)   as the parent class.     Source    [InputDStream.scala](https://github.com/apache/spark/tree/v3.4.0/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala)    Linear Supertypes    [DStream](DStream.html)   [    T    ],    Logging    ,    Serializable    ,    Serializable    ,  [AnyRef](../../../../../scala/index.html#AnyRef=Object)   ,    Any     Known Subclasses    [ConstantInputDStream](ConstantInputDStream.html)   ,  [ReceiverInputDStream](ReceiverInputDStream.html)   **   **    Ordering   1. Alphabetic 2. By Inheritance   Inherited      1. InputDStream 2. DStream 3. Logging 4. Serializable 5. Serializable 6. AnyRef 7. Any  1. Hide All 2. Show All   Visibility   1. Public 2. All  ###   Instance Constructors  1. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#<init>(_ssc:org.apache.spark.streaming.StreamingContext)(implicitevidence$1:scala.reflect.ClassTag[T]):org.apache.spark.streaming.dstream.InputDStream[T] "Permalink")   new     InputDStream     (    \_ssc:  [StreamingContext](../StreamingContext.html)    )     (    implicit     arg0:    ClassTag    [    T    ]    )     \_ssc     Streaming context that will execute this input stream  ###   Abstract Value Members  1. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#compute(validTime:org.apache.spark.streaming.Time):Option[org.apache.spark.rdd.RDD[T]] "Permalink")   abstract     def     compute     (    validTime:  [Time](../Time.html)    )     :    Option    [  [RDD](../../rdd/RDD.html)   [    T    ]]     Method that generates an RDD for the given time     Method that generates an RDD for the given time     Definition Classes    [DStream](DStream.html) 2. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#start():Unit "Permalink")   abstract     def     start     ()     :    Unit     Method called to start receiving data.     Method called to start receiving data. Subclasses must implement this method. 3. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#stop():Unit "Permalink")   abstract     def     stop     ()     :    Unit     Method called to stop receiving data.     Method called to stop receiving data. Subclasses must implement this method.  ###   Concrete Value Members  1. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#!=(x$1:Any):Boolean "Permalink")   final     def     !=     (    arg0:    Any     )     :    Boolean     Definition Classes     AnyRef → Any 2. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html###():Int "Permalink")   final     def     ##     ()     :    Int     Definition Classes     AnyRef → Any 3. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#==(x$1:Any):Boolean "Permalink")   final     def     ==     (    arg0:    Any     )     :    Boolean     Definition Classes     AnyRef → Any 4. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#asInstanceOf[T0]:T0 "Permalink")   final     def     asInstanceOf     [    T0    ]     :    T0     Definition Classes     Any 5. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#baseScope:Option[String] "Permalink")   val     baseScope     :    Option    [    String    ]     The base scope associated with the operation that created this DStream.     The base scope associated with the operation that created this DStream.     For InputDStreams, we use the name of this DStream as the scope name. If an outer scope is given, we assume that it includes an alternative name for this stream.     Attributes     protected[  [streaming](../index.html)   ]     Definition Classes    InputDStream  →  [DStream](DStream.html) 6. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#cache():org.apache.spark.streaming.dstream.DStream[T] "Permalink")   def     cache     ()     :  [DStream](DStream.html)   [    T    ]     Persist RDDs of this DStream with the default storage level (MEMORY\_ONLY\_SER)     Persist RDDs of this DStream with the default storage level (MEMORY\_ONLY\_SER)     Definition Classes    [DStream](DStream.html) 7. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#checkpoint(interval:org.apache.spark.streaming.Duration):org.apache.spark.streaming.dstream.DStream[T] "Permalink")   def     checkpoint     (    interval:  [Duration](../Duration.html)    )     :  [DStream](DStream.html)   [    T    ]     Enable periodic checkpointing of RDDs of this DStream     Enable periodic checkpointing of RDDs of this DStream     interval     Time interval after which generated RDD will be checkpointed     Definition Classes    [DStream](DStream.html) 8. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#clone():Object "Permalink")   def     clone     ()     :  [AnyRef](../../../../../scala/index.html#AnyRef=Object)    Attributes     protected[    lang    ]     Definition Classes     AnyRef     Annotations     @throws     (     ...     )     @native     () 9. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#context:org.apache.spark.streaming.StreamingContext "Permalink")   def     context     :  [StreamingContext](../StreamingContext.html)    Return the StreamingContext associated with this DStream     Return the StreamingContext associated with this DStream     Definition Classes    [DStream](DStream.html) 10. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#count():org.apache.spark.streaming.dstream.DStream[Long] "Permalink")   def     count     ()     :  [DStream](DStream.html)   [    Long    ]     Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream.     Return a new DStream in which each RDD has a single element generated by counting each RDD of this DStream.     Definition Classes    [DStream](DStream.html) 11. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#countByValue(numPartitions:Int)(implicitord:Ordering[T]):org.apache.spark.streaming.dstream.DStream[(T,Long)] "Permalink")   def     countByValue     (    numPartitions:    Int    =    [ssc.sc.defaultParallelism](../../SparkContext.html#defaultParallelism:Int)    )     (    implicit     ord:  [Ordering](../../../../../scala/index.html#Ordering[T]=scala.math.Ordering[T])   [    T    ] =    null     )     :  [DStream](DStream.html)   [(    T    ,    Long    )]     Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream.     Return a new DStream in which each RDD contains the counts of each distinct value in each RDD of this DStream. Hash partitioning is used to generate the RDDs with  `numPartitions`   partitions (Spark's default number of partitions if  `numPartitions`   not specified).     Definition Classes    [DStream](DStream.html) 12. [**](../../../../../org/apache/spark/streaming/dstream/InputDStream.html#countByValueAndWindow(windowDuration:org.apache.spark.streaming.Duration,slideDuration:org.apache.spark.streaming.Duration,numPartitions:Int)(implicitord:Ordering[T]):org.apache.spark.streaming.	The InputDStream class in Spark Streaming provides methods start() and stop() which are called by Spark Streaming system to start and stop receiving data, respectively. To give input as text in Spark Streaming, you can use a subclass of InputDStream, such as FileInputDStream, which monitors an HDFS directory from the driver for new files and generates RDDs with the new files.	0	1	1
no_reaction	https://docs.databricks.com/delta-live-tables/index.html	How to create a delta live table?	      Delta Live Tables CLI =======================     You run Databricks Delta Live Tables CLI subcommands by appending them to  `databricks     pipelines`  . These subcommands call the  [Delta Live Tables API](../../delta-live-tables/api-guide.html)  .        ``` databricks pipelines -h  ```       ``` Usage: databricks pipelines [OPTIONS] COMMAND [ARGS]...    Utility to interact with Databricks Delta Live Tables Pipelines.  Options:   -v, --version   0.17.0   --debug         Debug Mode. Shows full stack trace on error.   --profile TEXT  CLI connection profile to use. The default profile is                   "DEFAULT".    -h, --help      Show this message and exit.  Commands:   create  Creates a pipeline.   delete  Deletes the pipeline and cancels any active updates.   deploy  [Deprecated] This command is deprecated, use create and edit           commands instead.  Creates or edits a pipeline specified by the           pipeline settings.    edit    Edits a pipeline.   get     Gets a pipeline's current settings and status.   list    Lists all pipelines and their statuses.   reset   [Deprecated] Use the "start --full-refresh" command instead. Resets           a pipeline so that data can be reprocessed from the beginning.    run     [Deprecated] Use the "start" command instead. Starts a pipeline           update.    start   Starts a pipeline update.   stop    Stops the pipeline by cancelling any active update.  ```       Create a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     create     --help`  .      ###   Usage      ``` databricks pipelines create --settings create-pipeline.json  ```     `create-pipeline.json`  :        ``` {  "name": "Wikipedia pipeline (SQL)",  "storage": "/Users/username/data",  "clusters": [  {  "label": "default",  "autoscale": {  "min\_workers": 1,  "max\_workers": 5  }  }  ],  "libraries": [  {  "notebook": {  "path": "/Users/username/Delta Live Tables quickstart (SQL)"  }  }  ],  "target": "wikipedia\_analysis",  "continuous": false }  ```       ``` Successfully created pipeline: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003 with ID: 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Edit a pipeline -----------------     To display usage documentation, run  `databricks     pipelines     edit     --help`  .        ###   Usage      ``` databricks pipelines edit --settings edit-pipeline.json  ```     `edit-pipeline.json`  :        ``` {  "id": "4ee6b5c2-7017-11ec-90d6-0242ac120003",  "name": "Wikipedia pipeline (SQL)",  "storage": "/Users/username/data",  "clusters": [  {  "label": "default",  "autoscale": {  "min\_workers": 1,  "max\_workers": 3  }  }  ],  "libraries": [  {  "notebook": {  "path": "/Users/username/Delta Live Tables quickstart (SQL)"  }  }  ],  "target": "wikipedia\_quickstart\_data",  "continuous": false }  ```       ``` Successfully edited pipeline settings: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Delete a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     delete     --help`  .        ###   Usage      ``` databricks pipelines delete --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003 deleted  ```         Get information about a pipeline ----------------------------------     To display usage documentation, run  `databricks     pipelines     get     --help`  .        ###   Usage      ``` databricks pipelines get --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` {  "pipeline\_id": "4ee6b5c2-7017-11ec-90d6-0242ac120003",  "spec": {  "id": "4ee6b5c2-7017-11ec-90d6-0242ac120003",  "name": "Wikipedia pipeline (SQL)",  "storage": "/Users/username/data",  "clusters": [  {  "label": "default",  "autoscale": {  "min\_workers": 1,  "max\_workers": 5  }  }  ],  "libraries": [  {  "notebook": {  "path": "/Users/username/Delta Live Tables quickstart (SQL)"  }  }  ],  "target": "wikipedia\_analysis",  "continuous": false  },  "state": "IDLE",  "name": "Wikipedia pipeline (SQL)",  "creator\_user\_name": "username@databricks.com",  "last\_modified": 1641600056265 }  ```         List all pipelines and information on their status ----------------------------------------------------     To display usage documentation, run  `databricks     pipelines     list     --help`  .        ###   Usage      ``` databricks pipelines list  ```       ``` [  {  "pipeline\_id": "8a93e1b4-7024-11ec-90d6-0242ac120003",  "state": "IDLE",  "name": "Retail sales pipeline (SQL)",  "latest\_updates": [  {  "update\_id": "9df03924-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-29T16:18:57.982Z"  },  {  "update\_id": "a57e601c-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-28T18:19:30.371Z"  }  ],  "creator\_user\_name": "user@databricks.com"  },  {  "pipeline\_id": "b46e2670-7024-11ec-90d6-0242ac120003",  "state": "IDLE",  "name": "DLT quickstart (Python)",  "latest\_updates": [  {  "update\_id": "bcd8fa2e-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-12-16T18:19:25.827Z"  },  {  "update\_id": "c2c7a2c8-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-29T22:22:32.586Z"  },  {  "update\_id": "c88d2818-7024-11ec-90d6-0242ac120003",  "state": "COMPLETED",  "creation\_time": "2021-10-29T22:16:18.258Z"  }  ],  "creator\_user\_name": "user@databricks.com"  } ]  ```         Start a pipeline update -------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Started an update c6462e2e-beb7-4c24-a9d3-962eaa19781b for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```        Start a pipeline update with full refresh -------------------------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ###   Usage      ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003 --full-refresh  ```       ``` Started an update ed885d3d-4fc0-44fd-b5dd-59471c789327 with full refresh for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Stop a pipeline update ------------------------     To display usage documentation, run  `databricks     pipelines     stop     --help`  .        ###   Usage      ``` databricks pipelines stop --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Stopped pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```                                                        	To create a delta live table, you can use the Databricks Delta Live Tables CLI subcommands by appending them to `databricks pipelines`. The `create` subcommand can be used to create a pipeline, and you can specify the pipeline settings in a JSON file. The `target` field in the JSON file specifies the name of the Delta table to be created.	0	1	1
no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.max.html	I have a dataframe with the fields "id" and "date". How can i get the max date value per id	   pyspark.pandas.Index.max  [¶](#pyspark-pandas-index-max "Permalink to this headline") =======================================================================================  `Index.`  `max`    (     )    → Union[int, float, bool, str, bytes, decimal.Decimal, datetime.date, datetime.datetime, None, Tuple[Union[int, float, bool, str, bytes, decimal.Decimal, datetime.date, datetime.datetime, None], …]]  [[source]](../../../_modules/pyspark/pandas/indexes/base.html#Index.max) [¶](#pyspark.pandas.Index.max "Permalink to this definition")    Return the maximum value of the Index.     Returns     scalar     Maximum value.     See also    [`Index.min`](pyspark.pandas.Index.min.html#pyspark.pandas.Index.min "pyspark.pandas.Index.min")   Return the minimum value in an Index.    [`Series.max`](pyspark.pandas.Series.max.html#pyspark.pandas.Series.max "pyspark.pandas.Series.max")   Return the maximum value in a Series.    [`DataFrame.max`](pyspark.pandas.DataFrame.max.html#pyspark.pandas.DataFrame.max "pyspark.pandas.DataFrame.max")   Return the maximum values in a DataFrame.     Examples    ``` >>> idx = ps.Index([3, 2, 1]) >>> idx.max() 3  ```  ``` >>> idx = ps.Index(c', 'b', 'a) >>> idx.max() 'c'  ```   For a MultiIndex, the maximum is determined lexicographically.    ``` >>> idx = ps.MultiIndex.from\_tuples([('a', 'x', 1), ('b', 'y', 2)]) >>> idx.max() ('b', 'y', 2)  ```  [pyspark.pandas.Index.min](pyspark.pandas.Index.min.html "previous page")  [pyspark.pandas.Index.map](pyspark.pandas.Index.map.html "next page")   	The `max` function of the `DataFrame` can be used to get the maximum value of a column. In this case, you can group the dataframe by the "id" column and then apply the `max` function to the "date" column to get the maximum date value per id. Here is an example code snippet:  ``` from pyspark.sql.functions import max  df.groupBy("id").agg(max("date").alias("max_date_per_id")) ```	0	1	1
no_reaction	https://docs.databricks.com/dev-tools/index-ci-cd.html	how do i do CI/CD on databricks	Use CI/CD ===========     To manage the lifecycle of Databricks assets and data, you can use continuous integration and continuous delivery (CI/CD) and data pipeline tools.          |   Area   |   Use these tools when you want to…   | | --- | --- | |  [CI/CD workflows with Git and Databricks Repos](../repos/ci-cd-techniques-with-repos.html)  |   Use GitHub and Databricks Repos for source control and CI/CD workflows.   | |  [Continuous integration and delivery using GitHub Actions](ci-cd/ci-cd-github.html)  |   Build a CI/CD workflow on GitHub that uses GitHub Actions developed for Databricks.   | |  [CI/CD with Jenkins on Databricks](ci-cd/ci-cd-jenkins.html)  |   Develop a CI/CD pipeline for Databricks that uses Jenkins.   | |  [Orchestrate Databricks jobs with Apache Airflow](../workflows/jobs/how-to/use-airflow-with-jobs.html)  |   Manage and schedule a data pipeline that uses Apache Airflow.   | |  [Service principals for CI/CD](ci-cd/ci-cd-sp.html)  |   Use service principals, instead of users, with CI/CD systems.   |	To do CI/CD on Databricks, you can use continuous integration and continuous delivery (CI/CD) and data pipeline tools such as CI/CD workflows with Git and Databricks Repos, Continuous integration and delivery using GitHub Actions, CI/CD with Jenkins on Databricks, Orchestrate Databricks jobs with Apache Airflow, and Service principals for CI/CD. These tools can be used for source control and CI/CD workflows, building a CI/CD workflow on GitHub that uses GitHub Actions developed for Databricks, developing a CI/CD pipeline for Databricks that uses Jenkins, managing and scheduling a data pipeline that uses Apache Airflow, and using service principals, instead of users, with CI/CD systems.	1	1	1
no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_format.html	how do I format date columns in pyspark	   pyspark.sql.functions.date\_format  [¶](#pyspark-sql-functions-date-format "Permalink to this headline") ==========================================================================================================  `pyspark.sql.functions.`  `date_format`    (   *date     :     ColumnOrName*  ,  *format     :     str*   )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#date_format) [¶](#pyspark.sql.functions.date_format "Permalink to this definition")    Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.     A pattern could be for instance    dd.MM.yyyy    and could return a string like ‘18.03.1993’. All pattern letters of  [datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)   . can be used.     New in version 1.5.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **date**   [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")  or str     input column of values to format.    **format: str**    format to use to represent datetime values.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column "pyspark.sql.Column")   string value representing formatted datetime.     Notes     Whenever possible, use specialized functions like    year    .     Examples    ``` >>> df = spark.createDataFrame([('2015-04-08',)], dt) >>> df.select(date\_format('dt', 'MM/dd/yyy').alias('date')).collect() [Row(date='04/08/2015')]  ```  [pyspark.sql.functions.date\_add](pyspark.sql.functions.date_add.html "previous page")  [pyspark.sql.functions.date\_sub](pyspark.sql.functions.date_sub.html "next page")   	To format date columns in PySpark, you can use the `date_format` function from the `pyspark.sql.functions` module. This function takes two arguments: the date column to format and the format string to use. The format string specifies the pattern to use for the date format, and can include pattern letters from the [datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html) documentation. Here is an example usage:  ``` from pyspark.sql.functions import date_format  df = spark.createDataFrame([('2015-04-08',)], dt) df.select(date_format('dt', 'MM/dd/yyyy').alias('date')).show() ```  This will output a DataFrame with a new column named `date`, where the date values are formatted in the `MM/dd/yyyy` format.	1	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/index.html	how do I set up unity catalog?	    Create and manage catalogs ============================     This article shows how to create and manage catalogs in Unity Catalog. A catalog contains  [schemas (databases)](create-schemas.html)  , and a schema contains tables and views.        Requirements --------------   * You must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore * Your Databricks account must be on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * You must have a Unity Catalog metastore  [linked to the workspace](create-metastore.html)  where you perform the catalog creation. * The  [compute resource](compute.html)  that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode.         Create a catalog ------------------     To create a catalog, you can use Data Explorer or a SQL command.       1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the  **Create Catalog**   button. 4. (Optional) Specify the location where data for  [managed tables](create-tables.html#managed-table)  in the catalog will be stored.       Specify a location here only if you do not want managed tables in this catalog to be stored in the default root storage location that was configured for the metastore. See  [Create a Unity Catalog metastore](create-metastore.html)  .       The path that you specify must be defined in an external location configuration, and you must have the  `CREATE     MANAGED     STORAGE`  privilege on that external location. You can also use a subpath of that path. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  . 5. Assign permissions for your catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  . 6. Click  **Save**   .     1. Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : Optional description or other comment.   ``` CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name>  [ MANAGED LOCATION '<location\_path>' ]  [ COMMENT <comment> ];  ```      For example, to create a catalog named  `example`  :        ``` CREATE CATALOG IF NOT EXISTS example;  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql("CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql("CREATE CATALOG IF NOT EXISTS example")  ```      For parameter descriptions, see  [CREATE CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-create-catalog.html)  . 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` library(SparkR)  sql("CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]")  ```      For example, to create a catalog named  `example`  :        ``` library(SparkR)  sql("CREATE CATALOG IF NOT EXISTS example")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql("CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql("CREATE CATALOG IF NOT EXISTS example")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .       When you create a catalog, two schemas (databases) are automatically created:  `default`  and  `information\_schema`  .       You can also create a catalog by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog)   . You can retrieve information about catalogs by using  [databricks\_catalogs](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/catalogs)   .      ###   Next steps     Now you can  [add schemas (databases) to your catalog](create-schemas.html)  .          Delete a catalog ------------------     To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.         You must delete all schemas in the catalog except  `information\_schema`  before you can delete a catalog. This includes the auto-created  `default`  schema.     1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. In the  **Data**   pane, on the left, click the catalog you want to delete. 4. In the detail pane, click the three-dot menu to the left of the  **Create database**   button and select  **Delete**   . 5. On the  **Delete catalog**   dialog, click  **Delete**   .       Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` DROP CATALOG [ IF EXISTS ] <catalog\_name> [ RESTRICT | CASCADE ]  ```      For example, to delete a catalog named  `vaccine`  and its schemas:        ``` DROP CATALOG vaccine CASCADE  ```        Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` spark.sql("DROP CATALOG [ IF EXISTS ] <catalog\_name>	To set up Unity Catalog, you need to be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore. Your Databricks account must be on the Premium plan and above. You must have a Unity Catalog metastore linked to the workspace where you perform the catalog creation. The compute resource that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode. To create a catalog, you can use Data Explorer or a SQL command. There are different SQL commands to create a catalog in a notebook or Databricks SQL editor. Once you create a catalog, two schemas (databases) are automatically created: default and information_schema. To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.	1	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/index.html	how does Unity Catalog work with Tableau	What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	Based on the context, Unity Catalog is a unified governance solution for data and AI assets on the Lakehouse. It provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces. To use Unity Catalog with Tableau, clusters must be configured with the correct access mode. For complete setup instructions, please refer to the documentation.	0	1	1
no_reaction	https://kb.databricks.com/data-sources/delete-table-if-s3-bucket-deleted	When I drop a table does the underlying data get deleted?	Drop Table ============      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [DROP TABLE](../../sql/language-manual/sql-ref-syntax-ddl-drop-table.html)  .         ``` DROP TABLE [IF EXISTS] [db\_name.]table\_name  ```      Drop a table and delete the directory associated with the table from the file system if this is not an  `EXTERNAL`  table. If the table to drop does not exist, an exception is thrown.      **`IF     EXISTS`**      If the table does not exist, nothing happens.	Yes, when you drop a table, the underlying data gets deleted as well, but only if it is not an external table. If it is an external table, only the metadata is dropped, and the data remains in the external location.	1	1	1
no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.mode.html	Describe when replacing missing values with the mode value is an appropriate way to handle missing values.	   pyspark.pandas.DataFrame.mode  [¶](#pyspark-pandas-dataframe-mode "Permalink to this headline") =================================================================================================  `DataFrame.`  `mode`    (   *axis     :     Union    [    int    ,    str    ]     =     0*  ,  *numeric\_only     :     bool     =     False*  ,  *dropna     :     bool     =     True*   )    → pyspark.pandas.frame.DataFrame  [[source]](../../../_modules/pyspark/pandas/frame.html#DataFrame.mode) [¶](#pyspark.pandas.DataFrame.mode "Permalink to this definition")    Get the mode(s) of each element along the selected axis.     The mode of a set of values is the value that appears most often. It can be multiple values.     New in version 3.4.0.     Parameters    **axis**    {0 or ‘index’}, default 0     Axis for the function to be applied on.    **numeric\_only**    bool, default False     If True, only apply to numeric columns.    **dropna**    bool, default True     Don’t consider counts of NaN/NaT.     Returns     DataFrame     The modes of each column or row.     See also    [`Series.mode`](pyspark.pandas.Series.mode.html#pyspark.pandas.Series.mode "pyspark.pandas.Series.mode")   Return the highest frequency value in a Series.    [`Series.value\_counts`](pyspark.pandas.Series.value_counts.html#pyspark.pandas.Series.value_counts "pyspark.pandas.Series.value_counts")   Return the counts of values in a Series.     Examples    ``` >>> df = ps.DataFrame([('bird', 2, 2), ...                    ('mammal', 4, np.nan), ...                    ('arthropod', 8, 0), ...                    ('bird', 2, np.nan)], ...                   index=('falcon', 'horse', 'spider', 'ostrich'), ...                   columns=('species', 'legs', 'wings')) >>> df  species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN  ```   By default missing values are not considered, and the mode of wings are both 0 and 2. Because the resulting DataFrame has two rows, the second row of  `species`  and  `legs`  contains  `NaN`  .    ``` >>> df.mode()  species legs wings 0 bird 2.0 0.0 1 None NaN 2.0  ```   Setting  `dropna=False` `NaN`  values are considered and they can be the mode (like for wings).    ``` >>> df.mode(dropna=False)  species legs wings 0 bird 2 NaN  ```   Setting  `numeric\_only=True`  , only the mode of numeric columns is computed, and columns of other types are ignored.    ``` >>> df.mode(numeric\_only=True)  legs wings 0 2.0 0.0 1 NaN 2.0  ```  [pyspark.pandas.DataFrame.median](pyspark.pandas.DataFrame.median.html "previous page")  [pyspark.pandas.DataFrame.pct\_change](pyspark.pandas.DataFrame.pct_change.html "next page")   	Replacing missing values with the mode value is an appropriate way to handle missing values when dealing with categorical data. The mode is the value that appears most often in a set of values, and it can be multiple values. The `DataFrame.mode` function in PySpark can be used to get the mode(s) of each element along the selected axis. By default, missing values are not considered, but they can be considered by setting `dropna=False`.	0	1	1
no_reaction	https://docs.databricks.com/error-messages/user-defined-functions-error-class.html	can I trigger a user defined function based upon an update to a table	Functions ===========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Spark SQL provides two function features to meet a wide range of needs: built-in functions and user-defined functions (UDFs).        Built-in functions --------------------     This article presents the usages and descriptions of categories of frequently used built-in functions for aggregation, arrays and maps, dates and timestamps, and JSON data.      * [Built-in functions](sql-ref-functions-builtin.html)        SQL user-defined functions ----------------------------     SQL user-defined functions (UDFs) are functions you can define yourself which can return scalar values or result sets.       See  [CREATE FUNCTION (SQL)](sql-ref-syntax-ddl-create-sql-function.html)  for more information.      ###   User-defined functions     UDFs allow you to define your own functions when the system’s built-in functions are not enough to perform the desired task. To use UDFs, you first define the function, then register the function with Spark, and finally call the registered function. A UDF can act on a single row or act on multiple rows at once. Spark SQL also supports integration of existing Hive implementations of UDFs, user defined aggregate functions (UDAF), and user defined table functions (UDTF).      * [User-defined aggregate functions (UDAFs)](sql-ref-functions-udf-aggregate.html) * [Integration with Hive UDFs, UDAFs, and UDTFs](sql-ref-functions-udf-hive.html) * [User-defined scalar functions (UDFs)](sql-ref-functions-udf-scalar.html)	Yes, you can trigger a user-defined function based upon an update to a table. However, the process for doing so may vary depending on the specific database or platform being used. The context provides information on user-defined functions (UDFs) in Spark SQL, which can be defined and registered with Spark to act on single or multiple rows.	1	1	1
no_reaction	https://docs.databricks.com/structured-streaming/tutorial.html	When to use delta live tables vs. structured streaming?	      Run your first Structured Streaming workload ==============================================     This article provides code examples and explanation of basic concepts necessary to run your first Structured Streaming queries on Databricks. You can use Structured Streaming for near real-time and incremental processing workloads.       Structured Streaming is one of several technologies that power streaming tables in Delta Live Tables. Databricks recommends using Delta Live Tables for all new ETL, ingestion, and Structured Streaming workloads. See  [What is Delta Live Tables?](../delta-live-tables/index.html)  .        Note       While Delta Live Tables provides a slightly modified syntax for declaring streaming tables, the general syntax for configuring streaming reads and transformations applies to all streaming use cases on Databricks. Delta Live Tables also simplifies streaming by managing state information, metadata, and numerous configurations.         Read from a data stream -------------------------     You can use Structured Streaming to incrementally ingest data from supported data sources. Some of the most common data sources used in Databricks Structured Streaming workloads include the following:     * Data files in cloud object storage * Message buses and queues * Delta Lake     Databricks recommends using Auto Loader for streaming ingestion from cloud object storage. Auto Loader supports most file formats supported by Structured Streaming. See  [What is Auto Loader?](../ingestion/auto-loader/index.html)  .       Each data source provides a number of options to specify how to load batches of data. During reader configuration, the main options you might need to set fall into the following categories:     * Options that specify the data source or format (for example, file type, delimiters, and schema). * Options that configure access to source systems (for example, port settings and credentials). * Options that specify where to start in a stream (for example, Kafka offsets or reading all existing files). * Options that control how much data is processed in each batch (for example, max offsets, files, or bytes per batch).         Use Auto Loader to read streaming data from object storage ------------------------------------------------------------     The following example demonstrates loading JSON data with Auto Loader, which uses  `cloudFiles`  to denote format and options. The  `schemaLocation`  option enables schema inference and evolution. Paste the following code in a Databricks notebook cell and run the cell to create a streaming DataFrame named  `raw\_df`  :        ``` file\_path = "/databricks-datasets/structured-streaming/events" checkpoint\_path = "/tmp/ss-tutorial/\_checkpoint"  raw\_df = (spark.readStream     .format("cloudFiles")     .option("cloudFiles.format", "json")     .option("cloudFiles.schemaLocation", checkpoint\_path)     .load(file\_path) )  ```      Like other read operations on Databricks, configuring a streaming read does not actually load data. You must trigger an action on the data before the stream begins.        Note       Calling  `display()`  on a streaming DataFrame starts a streaming job. For most Structured Streaming use cases, the action that triggers a stream should be writing data to a sink. See  [Preparing your Structured Streaming code for production](#production)  .          Perform a streaming transformation ------------------------------------     Structured Streaming supports most transformations that are available in Databricks and Spark SQL. You can even load MLflow models as UDFs and make streaming predictions as a transformation.       The following code example completes a simple transformation to enrich the ingested JSON data with additional information using Spark SQL functions:        ``` from pyspark.sql.functions import input\_file\_name, current\_timestamp  transformed\_df = (raw\_df.select(     "\*",     input\_file\_name().alias("source\_file"),     current\_timestamp().alias("processing\_time")     ) )  ```      The resulting  `transformed\_df`  contains query instructions to load and transform each record as it arrives in the data source.        Note       Structured Streaming treats data sources as unbounded or infinite datasets. As such, some transformations are not supported in Structured Streaming workloads because they would require sorting an infinite number of items.       Most aggregations and many joins require managing state information with watermarks, windows, and output mode. See  [Apply watermarks to control data processing thresholds](watermarks.html)  .          Write to a data sink ----------------------     A data sink is the target of a streaming write operation. Common sinks used in Databricks streaming workloads include the following:     * Delta Lake * Message buses and queues * Key-value databases     As with data sources, most data sinks provide a number of options to control how data is written to the target system. During writer configuration, the main options you might need to set fall into the following categories:     * Output mode (append by default). * A checkpoint location (required for each  **writer**   ). * Trigger intervals; see  [Configure Structured Streaming trigger intervals](triggers.html)  . * Options that specify the data sink or format (for example, file type, delimiters, and schema). * Options that configure access to target systems (for example, port settings and credentials).         Perform an incremental batch write to Delta Lake --------------------------------------------------     The following example writes to Delta Lake using a specified file path and checkpoint.        Important       Always make sure you specify a unique checkpoint location for each streaming writer you configure. The checkpoint provides the unique identity for your stream, tracking all records processed and state information associated with your streaming query.        The  `availableNow`  setting for the trigger instructs Structured Streaming to process all previously unprocessed records from the source dataset and then shut down, so you can safely execute the following code without worrying about leaving a stream running:        ``` target\_path = "/tmp/ss-tutorial/" checkpoint\_path = "/tmp/ss-tutorial/\_checkpoint"  transformed\_df.writeStream     .trigger(availableNow=True)     .option("checkpointLocation", checkpoint\_path)     .option("path", target\_path)     .start()  ```      In this example, no new records arrive in our data source, so repeat execution of this code does not ingest new records.        Warning       Structured Streaming execution can prevent auto termination from shutting down compute resources. To avoid unexpected costs, be sure to terminate streaming queries.              Preparing your Structured Streaming code for production ---------------------------------------------------------     Databricks recommends using Delta Live Tables for most Structured Streaming workloads. The following recommendations provide a starting point for preparing Structured Streaming workloads for production:     * Remove unnecessary code from notebooks that would return results, such as  `display`  and  `count`  . * Do not run Structured Streaming workloads on interactive clusters; always schedule streams as jobs. * To help streaming jobs recover automatically, configure jobs with infinite retries. * Do not use auto-scaling for workloads with Structured Streaming.     For more recommendations, see  [Production considerations for Structured Streaming](production.html)  .             Read data from Delta Lake, transform, and write to Delta Lake ---------------------------------------------------------------     Delta Lake has extensive support for working with Structured Streaming as both a source and a sink. See  [Table streaming reads and writes](delta-lake.html)  .       The following example shows example syntax to incrementally load all new records from a Delta table, join them with a snapshot of another Delta table, and write them to a Delta table:        ``` (spark.readStream     .table("<table\_name1>")     .join(spark.read.table("<table\_name2>"), on="<id>", how="left")     .writeStream     .trigger(availableNow=True)     .option("checkpointLocation", "<checkpoint\_path>")     .toTable("<table\_name3>") )  ```      You must have proper permissions configured to read source tables and write to target tables and the specified checkpoint location. Fill in all parameters denoted with angle brackets (  `<>`  ) using the relevant values for your data sources and sinks.        Note       Delta Live Tables provides a fully declarative syntax for creating Delta Lake pipelines and manages properties like triggers and checkpoints automatically. See  [What is Delta Live Tables?](../delta-live-tables/index.html)  .            Read data from Kafka, transform, and write to Kafka -----------------------------------------------------     Apache Kafka and other messaging buses provide some of the lowest latency available for large datasets. You can use Databricks to apply transformations to data ingested from Kafka and then write data back to Kafka.        Note       Writing data to cloud object storage adds additional latency overhead. If you wish to store data from a messaging bus in Delta Lake but require the lowest latency possible for streaming workloads, Databricks recommends configuring separate streaming jobs to ingest data to the lakehouse and apply near real-time transformations for downstream messaging bus sinks.        The following code example demonstrates a simple pattern to enrich data from Kafka by joining it with data in a Delta table and then writing back to Kafka:        ``` (spark.readStream     .format("kafka")     .option("kafka.bootstrap.servers", "<server:ip>")     .option("subscribe", "<topic>")     .option("startingOffsets", "latest")     .load()     .join(spark.read.table("<table\_name>"), on="<id>", how="left")     .writeStream     .format("kafka")     .option("kafka.bootstrap.servers", "<server:ip>")     .option("topic", "<topic>")     .option("checkpointLocation", "<checkpoint\_path>")     .start() )  ```      You must have proper permissions configured for access to your Kafka service. Fill in all parameters denoted with angle brackets (  `<>`  ) using the relevant values for your data sources and sinks. See  [Stream processing with Apache Kafka and Databricks](kafka.html)  .                                                        	When to use delta live tables vs. structured streaming?	0	0	0
no_reaction	https://docs.databricks.com/dbfs/root-locations.html	what's the difference between root dbfs and dbfs mount	What directories are in DBFS root by default? ===============================================     Each Databricks workspace has several directories configured in the DBFS root storage container by default. Some of these directories link to locations on the DBFS root, while others are virtual mounts. If you are unable to access data in any of these directories, contact your workspace administrator.     * `/FileStore` * `/databricks-datasets` * `/databricks-results` * `/databricks/init` * `/user/hive/warehouse`        What is stored in the  `/Filestore`  directory? -------------------------------------------------     Data and libraries uploaded through the Databricks UI go to the  `/Filestore`  location by default. Generated plots are also stored in this directory.           What is stored in the  `/databricks-datasets`  directory? -----------------------------------------------------------     Databricks provides a number of open source datasets in this directory. Many of the tutorials and demos provided by Databricks reference these datasets, but you can also use them to indepedently explore the functionality of Databricks.               What is stored in the  `/databricks-results`  directory? ----------------------------------------------------------    `/databricks-results`  stores files generated by downloading the  [full results](../notebooks/notebook-outputs.html#download-full-results)  of a query.         What is stored in the  `/databricks/init`  directory? -------------------------------------------------------     This directory contains global  [init scripts](../clusters/init-scripts.html)  .           What is stored in the  `/user/hive/warehouse`  directory? -----------------------------------------------------------     Databricks stores managed tables in the  `hive\_metastore`  here by default.	The root DBFS (Databricks File Store) is the top-level directory in DBFS, which contains all other directories and files. On the other hand, DBFS mount is a way to mount an external storage system, such as Amazon S3 or Azure Blob Storage, to DBFS. This allows you to access data stored in the external system as if it were stored in DBFS.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-tables.html	when should you use an external table instead of managed table	    Create tables ===============     This article introduces the concept of  *managed*   and  *external*   tables in Unity Catalog and describes how to create tables in Unity Catalog.        Note       When you create a table, be sure to reference a catalog that is governed by Unity Catalog. The catalog  `hive\_metastore`  appears in Data Explorer but is not considered governed by Unity Catalog. It is managed by your Databricks workspace’s Hive metastore. All other catalogs listed are governed by Unity Catalog.       You can use the Unity Catalog table upgrade interface to upgrade existing tables registered in the Hive metastore to Unity Catalog. See  [Upgrade tables and views to Unity Catalog](migrate.html)  .           Managed tables ----------------     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.           External tables -----------------     External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.       When you run  `DROP     TABLE`  on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a  `LOCATION`  path in your  `CREATE     TABLE`  statement. External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     To manage access to the underlying cloud storage for an external table, you must set up  [storage credentials and external locations](manage-external-locations-and-credentials.html)  .       To learn more, see  [Create an external table](#create-an-external-table)  .         Requirements --------------     You must have the  `CREATE     TABLE`  privilege on the schema in which you want to create the table, as well as the  `USE     SCHEMA`  privilege on the schema and the  `USE     CATALOG`  privilege on the parent catalog.       If you are creating an external table, see  [Create an external table](#create-an-external-table)  for additional requirements.         Create a managed table ------------------------     To create a managed table, run the following SQL command. You can also use the  [example notebook](#example-notebook-managed-table)  to create a table. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. This cannot be the  `hive\_metastore`  catalog that is created automatically for the Hive metastore associated with your Databricks workspace. * `<schema\_name>`  : The name of the schema. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column.       ``` CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> (  <column\_specification> );  ```       ``` spark.sql("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "   "("   " <column\_specification>"   ")")  ```       ``` library(SparkR)  sql(paste("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> ",  "(",  " <column\_specification>",  ")",  sep = ""))  ```       ``` spark.sql("CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> " +  "(" +  " <column\_specification>" +  ")")  ```       You can also create a managed table by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of table full names by using  [databricks\_tables](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/tables)   .       For example, to create the table  `main.default.department`  and insert five rows into it:         ``` CREATE TABLE main.default.department (  deptcode INT,  deptname STRING,  location STRING );  INSERT INTO main.default.department VALUES  (10, 'FINANCE', 'EDINBURGH'),  (20, 'SOFTWARE', 'PADDINGTON'),  (30, 'SALES', 'MAIDSTONE'),  (40, 'MARKETING', 'DARLINGTON'),  (50, 'ADMIN', 'BIRMINGHAM');  ```       ``` spark.sql("CREATE TABLE main.default.department "   "("   " deptcode INT,"   " deptname STRING,"   " location STRING"   ")"   "INSERT INTO main.default.department VALUES "   " (10, 'FINANCE', 'EDINBURGH'),"   " (20, 'SOFTWARE', 'PADDINGTON'),"   " (30, 'SALES', 'MAIDSTONE'),"   " (40, 'MARKETING', 'DARLINGTON'),"   " (50, 'ADMIN', 'BIRMINGHAM')")  ```       ``` library(SparkR)  sql(paste("CREATE TABLE main.default.department ",  "(",  " deptcode INT,",  " deptname STRING,",  " location STRING",  ")",  "INSERT INTO main.default.department VALUES ",  " (10, 'FINANCE', 'EDINBURGH'),",  " (20, 'SOFTWARE', 'PADDINGTON'),",  " (30, 'SALES', 'MAIDSTONE'),",  " (40, 'MARKETING', 'DARLINGTON'),",  " (50, 'ADMIN', 'BIRMINGHAM')",  sep = ""))  ```       ``` spark.sql("CREATE TABLE main.default.department " +  "(" +  " deptcode INT," +  " deptname STRING," +  " location STRING" +  ")" +  "INSERT INTO main.default.department VALUES " +  " (10, 'FINANCE', 'EDINBURGH')," +  " (20, 'SOFTWARE', 'PADDINGTON')," +  " (30, 'SALES', 'MAIDSTONE')," +  " (40, 'MARKETING', 'DARLINGTON')," +  " (50, 'ADMIN', 'BIRMINGHAM')")  ```        ###   Example notebooks for creating managed tables     You can use the following example notebooks to create a catalog, schema, and managed table, and to manage permissions on them.        ####   Create and manage a table in Unity Catalog with SQL     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-example-notebook.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Create and manage a table in Unity Catalog with Python     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-quickstart-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                   Drop a managed table ----------------------     You must be the table’s owner to drop a table. To drop a managed table, run the following SQL command:        ``` DROP TABLE IF EXISTS catalog\_name.schema\_name.table\_name;  ```      When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.         Create an external table --------------------------     The data in an external table is stored in a path on your cloud tenant. To work with external tables, Unity Catalog introduces two objects to access and work with external cloud storage:     * A  *storage credential*   contains an authentication method for accessing a cloud storage location. The storage credential does not contain a mapping to the path to which it grants access. Storage credentials are access-controlled to determine which users can use the credential. * An  *external location*   maps a storage credential with a cloud storage path to which it grants access. The external location grants access only to that cloud storage path and its contents. External locations are access-controlled to determine which users can use them. An external location is used automatically when your SQL command contains a  `LOCATION`  clause.      ###   Requirements     To create an external table, you must have:     * The  `CREATE     EXTERNAL     TABLE`  privilege on an external location or storage credential, that grants access to the  `LOCATION`  accessed by the external table. * The  `USE     SCHEMA`  permission on the table’s parent schema. * The  `USE     CATALOG`  permission on the table’s parent catalog. * The  `CREATE     TABLE`  permission on the table’s parent schema.     External locations and storage credentials are stored in the top level of the metastore, rather than in a catalog. To create a storage credential or an external location, you must be the metastore admin or an account-level admin. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .       ###   Create a table     Use one of the following command examples in a notebook or the SQL query editor to create an external table.       You can also use an  [example notebook](#example-notebook-external-table)  to create the storage credential, external location, and external table, and also manage permissions for them.       In the following examples, replace the placeholder values:     * `<catalog>`  : The name of the catalog that will contain the table. * `<schema>`  : The name of the schema that will contain the table. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column. * `<bucket\_path>`  : The path on your cloud tenant where the table will be created. * `<table\_directory>`  : A directory where the table will be created. Use a unique directory for each table.      Important       Once a table is created in a path, users can no longer directly access the files in that path from Databricks even if they have been given privileges on an external location or storage credential to do so. This is to ensure that users cannot circumvent access controls applied to tables by reading files from your cloud tenant directly.          ``` CREATE TABLE <catalog>.<schema>.<table\_name> (  <column\_specification> ) LOCATION 's3://<bucket\_path>/<table\_directory>';  ```       ``` spark.sql("CREATE TABLE <catalog>.<schema>.<table\_name> "   "("   " <column\_specification>"   ") "   "LOCATION 's	You should use an external table instead of a managed table when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses. External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. When you run `DROP TABLE` on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. External tables can use various file formats such as DELTA, CSV, JSON, AVRO, PARQUET, ORC, and TEXT. To manage access to the underlying cloud storage for an external table, you must set up storage credentials and external locations.	1	1	1
no_reaction	https://docs.databricks.com/repos/index.html	repo git provider	Git integration with Databricks Repos =======================================     Databricks Repos is a visual Git client in Databricks. It supports common Git operations such a cloning a repository, committing and pushing, pulling, branch management, and visual comparison of diffs when committing.       Within Repos you can develop code in notebooks or other files and follow data science and engineering code development best practices using Git for version control, collaboration, and CI/CD.        What can you do with Databricks Repos? ----------------------------------------     Databricks Repos provides source control for data and AI projects by integrating with Git providers.       In Databricks Repos, you can use Git functionality to:     * Clone, push to, and pull from a remote Git repository. * Create and manage branches for development work. * Create notebooks, and edit notebooks and other files. * Visually compare differences upon commit.     For step-by-step instructions, see  [Clone a Git repo & other common Git operations](git-operations-with-repos.html)  . Databricks Repos also has an  [API](../dev-tools/api/latest/repos.html)  that you can integrate with your CI/CD pipeline. For example, you can programmatically update a Databricks repo so that it always has the most recent version of the code. For information about best practices for code development using Databricks Repos, see  [CI/CD workflows with Git integration and Databricks Repos](ci-cd-techniques-with-repos.html)  .       For following tasks, work in your Git provider:     * Create a pull request. * Resolve merge conflicts. * Merge or delete branches. * Rebase a branch.       Supported Git providers -------------------------     Databricks supports the following Git providers:     * GitHub * Bitbucket Cloud * GitLab * Azure DevOps * AWS CodeCommit * GitHub AE     See  [Get a Git access token & connect a remote repo to Databricks](get-access-tokens-from-git-provider.html)  .       Databricks Repos also supports Bitbucket Server, GitHub Enterprise Server, and GitLab self-managed integration, if the server is internet accessible. To integrate with a private Git server instance that is not internet-accessible, get in touch with your Databricks representative.       Support for arbitrary files in Databricks Repos is available in Databricks Runtime 8.4 and above. See  [What are workspace files?](../files/workspace.html)  .         Next step -----------    [Set up Databricks Repos](repos-setup.html)	Databricks Repos is a visual Git client that integrates with Git providers such as GitHub, Bitbucket Cloud, GitLab, Azure DevOps, AWS CodeCommit, and GitHub AE. It supports common Git operations such as cloning a repository, committing and pushing, pulling, branch management, and visual comparison of diffs when committing. Within Repos, you can develop code in notebooks or other files and follow data science and engineering code development best practices using Git for version control, collaboration, and CI/CD. Databricks Repos also has an API that you can integrate with your CI/CD pipeline. For more information about best practices for code development using Databricks Repos, see CI/CD workflows with Git integration and Databricks Repos.	1	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/functions/list_secrets.html	I want to use dbutils.secrets.list() to list all secrets	`list\_secrets`  table function ================================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL preview  ![check marked yes](../../../_images/check.png)  Databricks Runtime 11.3 and above       Returns the keys which the user is authorized to see from  [Databricks secret service](../../../security/secrets/index.html)  .        Syntax --------      ``` list\_secrets()  ```        Arguments -----------     This function takes no arguments.         Returns ---------     A table with two columns:     * `scope     STRING     NOT     NULL`  : The namespace of the key. * `key     STYRING     NOT     NULL`  : The key itself     To return the secret values for specific keys use  [`secret(scope, key)`](secret.html)  .         Error conditions ------------------   * `INVALID\_SECRET\_LOOKUP.LIST\_SECRETS\_FAILED`  : Databricks secret service is not available.       Examples ----------      ``` > SELECT \* FROM list\_secrets();  scope key  ------------ ---------------  secrets.r.us theAnswerToLife  > SELECT secret('secrets.r.us', 'theAnswerToLife');  fourtyTwo  ```        Related functions -------------------   * [list\_secrets table function](#)	To list all secrets using `dbutils.secrets.list()`, you can use the `list_secrets()` function. However, the context you provided seems to be a documentation page for the `list_secrets()` function in Databricks SQL preview and Databricks Runtime 11.3 and above. It provides information on the syntax, arguments, returns, error conditions, and examples of using the `list_secrets()` function.	0	0	1
no_reaction	https://docs.databricks.com/sql/language-manual/functions/current_metastore.html	how do i get the metastore id in databricks sql	`current\_metastore`  function ===============================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime 11.1 and above       Returns the current metastore id.        Syntax --------      ``` current\_metastore()  ```        Arguments -----------     This function takes no arguments.         Returns ---------     A STRING.       The metastore id is of the form:  `<cloud>:<region>:<uuid>`        Examples ----------      ``` > SELECT current\_metastore();  azure:westus:f64dcb39-5046-4d4c-9588-c6ac84f9b38a  ```        Related functions -------------------   * [CREATE RECIPIENT](../sql-ref-syntax-ddl-create-recipient.html)	To get the metastore id in Databricks SQL, you can use the `current_metastore()` function, which returns the current metastore id in the form of `<cloud>:<region>:<uuid>`.	1	1	1
thumb_up	https://docs.databricks.com/archive/spark-sql-2.x-language-manual/show-create-table.html	How to create a table in SQL?	Create Table ==============      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [CREATE TABLE](../../sql/language-manual/sql-ref-syntax-ddl-create-table.html)  .         Create Table Using --------------------      ``` CREATE TABLE [IF NOT EXISTS] [db\_name.]table\_name  [(col\_name1 col\_type1 [COMMENT col\_comment1], ...)]  USING data\_source  [OPTIONS (key1 [ = ] val1, key2 [ = ] val2, ...)]  [PARTITIONED BY (col\_name1, col\_name2, ...)]  [CLUSTERED BY (col\_name3, col\_name4, ...) INTO num\_buckets BUCKETS]  [LOCATION path]  [COMMENT table\_comment]  [TBLPROPERTIES (key1 [ = ] val1, key2 [ = ] val2, ...)]  [AS select\_statement]  ```      Create a table using a data source. If a table with the same name already exists in the database, an exception is thrown.      **`IF     NOT     EXISTS`**      If a table with the same name already exists in the database, nothing will happen.      **`USING     data\_source`**      The file format to use for the table.  `data\_source`  must be one of  `TEXT`  ,  `AVRO`  ,  `CSV`  ,  `JSON`  ,  `JDBC`  ,  `PARQUET`  ,  `ORC`  ,  `HIVE`  ,  `DELTA`  , or  `LIBSVM`  , or a fully-qualified class name of a custom implementation of  `org.apache.spark.sql.sources.DataSourceRegister`  .      `HIVE`  is supported to create a Hive SerDe table. You can specify the Hive-specific  `file\_format`  and  `row\_format`  using the  `OPTIONS`  clause, which is a case-insensitive string map. The option keys are  `FILEFORMAT`  ,  `INPUTFORMAT`  ,  `OUTPUTFORMAT`  ,  `SERDE`  ,  `FIELDDELIM`  ,  `ESCAPEDELIM`  ,  `MAPKEYDELIM`  , and  `LINEDELIM`  .      **`OPTIONS`**      Table options used to optimize the behavior of the table or configure  `HIVE`  tables.        Note       This clause is not supported by Delta Lake.       **`PARTITIONED     BY     (col\_name1,     col\_name2,     ...)`**      Partition the created table by the specified columns. A directory is created for each partition.      **`CLUSTERED     BY     col\_name3,     col\_name4,     ...)`**      Each partition in the created table will be split into a fixed number of buckets by the specified columns. This is typically used with partitioning to read and shuffle less data.      **`LOCATION     path`**      The directory to store the table data. This clause automatically implies  `EXTERNAL`  .        Warning       If a schema (database) is registered in your workspace-level Hive metastore, dropping that schema using the  `CASCADE`  option causes all files in that schema location to be deleted recursively, regardless of the table type (managed or external).       If the schema is registered to a Unity Catalog metastore, the files for Unity Catalog  [managed tables](../../data-governance/unity-catalog/create-tables.html#managed-table)  are deleted recursively. However, the files for  [external tables](../../data-governance/unity-catalog/create-tables.html#external-table)  are not deleted. You must manage those files using the cloud storage provider directly.       Therefore, to avoid accidental data loss, you should never register a schema in a Hive metastore to a location with existing data. Nor should you create new external tables in a location managed by Hive metastore schemas or containing Unity Catalog managed tables.       **`AS     select\_statement`**      Populate the table with input data from the  `SELECT`  statement. This cannot contain a column list.             Create Table Using Delta (Delta Lake on Databricks) -----------------------------------------------------      ``` CREATE [OR REPLACE] TABLE [IF NOT EXISTS] [db\_name.]table\_name  [(col\_name1 col\_type1 [NOT NULL] [COMMENT col\_comment1], ...)]  USING DELTA  [LOCATION <path-to-delta-files>]  ```     **`NOT     NULL`**      Indicate that a column value cannot be  `NULL`  . If specified, and an  [Insert](insert.html)  or  [Update a table](../../delta/tutorial.html#update)  statements sets a column value to  `NULL`  , a  `SparkException`  is thrown. The default is to allow a  `NULL`  value.      **`LOCATION     <path-to-delta-files>`**      If you specify a  `LOCATION`  that already contains data stored in Delta Lake, Delta Lake does the following:     * If you specify only the table name and location, for example:        ``` CREATE TABLE events  USING DELTA  LOCATION '/mnt/delta/events'  ```      the table in the Hive metastore automatically inherits the schema, partitioning, and table properties of the existing data. This functionality can be used to “import” data into the metastore. * If you specify any configuration (schema, partitioning, or table properties), Delta Lake verifies that the specification exactly matches the configuration of the existing data.      Warning       If the specified configuration does not exactly match the configuration of the data, Delta Lake throws an exception that describes the discrepancy.       ###   Examples      ``` CREATE TABLE boxes (width INT, length INT, height INT) USING CSV  CREATE TABLE boxes  (width INT, length INT, height INT)  USING PARQUET  OPTIONS ('compression'='snappy')  CREATE TABLE rectangles  USING PARQUET  PARTITIONED BY (width)  CLUSTERED BY (length) INTO 8 buckets  AS SELECT \* FROM boxes  -- CREATE a HIVE SerDe table using the CREATE TABLE USING syntax. CREATE TABLE my\_table (name STRING, age INT, hair\_color STRING)  USING HIVE  OPTIONS(  INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat',  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat',  SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe')  PARTITIONED BY (hair\_color)  TBLPROPERTIES ('status'='staging', 'owner'='andrew')  ```         Create Table with Hive format -------------------------------      ``` CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [db\_name.]table\_name  [(col\_name1[:] col\_type1 [COMMENT col\_comment1], ...)]  [COMMENT table\_comment]  [PARTITIONED BY (col\_name2[:] col\_type2 [COMMENT col\_comment2], ...)]  [ROW FORMAT row\_format]  [STORED AS file\_format]  [LOCATION path]  [TBLPROPERTIES (key1=val1, key2=val2, ...)]  [AS select\_statement]  row\_format:  : SERDE serde\_cls [WITH SERDEPROPERTIES (key1=val1, key2=val2, ...)]  | DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]]  [COLLECTION ITEMS TERMINATED BY char]  [MAP KEYS TERMINATED BY char]  [LINES TERMINATED BY char]  [NULL DEFINED AS char]  file\_format:  : TEXTFILE | SEQUENCEFILE | RCFILE | ORC | PARQUET | AVRO  | INPUTFORMAT input\_fmt OUTPUTFORMAT output\_fmt  ```      Create a table using the Hive format. If a table with the same name already exists in the database, an exception will be thrown. When the table is dropped later, its data will be deleted from the file system.        Note       This command is supported only when Hive support is enabled.       **`EXTERNAL`**      The table uses the custom directory specified with  `LOCATION`  . Queries on the table access existing data previously stored in the directory. When an  `EXTERNAL`  table is dropped, its data is not deleted from the file system. This flag is implied if  `LOCATION`  is specified.      **`IF     NOT     EXISTS`**      If a table with the same name already exists in the database, nothing will happen.      **`PARTITIONED     BY     (col\_name2[:]     col\_type2     [COMMENT     col\_comment2],     ...)`**      Partition the table by the specified columns. This set of columns must be distinct from the set of non-partitioned columns. You cannot specify partitioned columns with  `AS     select\_statement`  .      **`ROW     FORMAT`**      Use the  `SERDE`  clause to specify a custom SerDe for this table. Otherwise, use the  `DELIMITED`  clause to use the native SerDe and specify the delimiter, escape character, null character, and so on.      **`STORED     AS     file\_format`**      Specify the file format for this table. Available formats include  `TEXTFILE`  ,  `SEQUENCEFILE`  ,  `RCFILE`  ,  `ORC`  ,  `PARQUET`  , and  `AVRO`  . Alternatively, you can specify your own input and output formats through  `INPUTFORMAT`  and  `OUTPUTFORMAT`  . Only formats  `TEXTFILE`  ,  `SEQUENCEFILE`  , and  `RCFILE`  can be used with  `ROW     FORMAT     SERDE`  and only  `TEXTFILE`  can be used with  `ROW     FORMAT     DELIMITED`  .      **`LOCATION     path`**      The directory to store the table data. This clause automatically implies  `EXTERNAL`  .      **`AS     select\_statement`**      Populate the table with input data from the select statement. You cannot specify this with  `PARTITIONED     BY`  .         Data types ------------     Spark SQL supports the following data types:     * Numeric types      + `ByteType`   : Represents 1-byte signed integer numbers. The range of numbers is from   `-128`   to   `127`   .  + `ShortType`   : Represents 2-byte signed integer numbers. The range of numbers is from   `-32768`   to   `32767`   .  + `IntegerType`   : Represents 4-byte signed integer numbers. The range of numbers is from   `-2147483648`   to   `2147483647`   .  + `LongType`   : Represents 8-byte signed integer numbers. The range of numbers is from   `-92233720368547	To create a table in SQL, you can use the CREATE TABLE statement. Here is an example syntax:  ``` CREATE TABLE table_name (   column1 datatype,   column2 datatype,   column3 datatype,   ... ); ```  You can replace `datatype` with the specific data type you want to use for each column. Additionally, you can add constraints and other options to the statement.	0	1	1
no_reaction	https://docs.databricks.com/getting-started/admin-get-started.html	How do I get started with Databricks?	Get started articles, tutorials, and best practices =====================================================     Databricks documentation includes many tutorials, Get started articles, and best practices guides.        Get started articles vs. tutorials ------------------------------------     Get started articles provide a shortcut to understanding Databricks features or typical tasks you can perform in Databricks. Most of our Get started articles are intended for new users trying out Databricks.       Tutorials are slightly more complex, prescriptive steps for typical workflows in Databricks that you can use as examples for your projects.         Video tours -------------   * [Video: Databricks Data Science and Engineering workspace](https://www.youtube.com/watch?v=YGq150jtc58&index=5) * [Video: Databricks SQL workspace](https://www.youtube.com/watch?v=zDOVr-Wxyd0&list=PLTPXxbhUt-YWPEK3NYh433T8TwrpVbyce&index=6) * [Video: Databricks Machine Learning workspace](https://www.youtube.com/watch?v=JvXg1iycvQc&list=PLTPXxbhUt-YWPEK3NYh433T8TwrpVbyce&index=7) * [Video: Notebook basics](https://youtu.be/zhUtu0J8LWg)         Get started with Databricks Data Science & Engineering --------------------------------------------------------   * [Get started: Query data from a notebook](../quick-start.html) * [Get started: Build a basic ETL pipeline](../etl-quick-start.html) * [Tutorial: Run an end-to-end lakehouse analytics pipeline](/getting-started/lakehouse-e2e.html) * [Tutorial: Build an end-to-end data pipeline](../data-pipeline-get-started.html)         Get started with Databricks Machine Learning ----------------------------------------------   * [Get started: Databricks Machine Learning in-product quickstart](../../machine-learning/tutorial/ml-quickstart.html) * [10-min tutorials: ML notebooks](../../machine-learning/tutorial/index.html) * [Get started: MLflow quickstart notebooks](../../mlflow/quick-start.html)         Get started with Databricks SQL ---------------------------------   * [Databricks SQL user quickstart: Import and explore sample dashboards](../../sql/get-started/sample-dashboards.html) * [Databricks SQL user quickstart: Run and visualize a query](../../sql/get-started/user-quickstart.html)       Best practices for Databricks -------------------------------     The Databricks documentation includes a number of best practices articles to help you get the best performance at the lowest cost when using and administering Databricks.      ###   Data science and engineering best practices   * [Delta Lake](../../delta/best-practices.html) * [Hyperparameter tuning with Hyperopt](../../machine-learning/automl-hyperparam-tuning/hyperopt-best-practices.html) * [Deep learning in Databricks](../../machine-learning/train-model/dl-best-practices.html) * [Delta Lake Structured Streaming with Amazon Kinesis](../../structured-streaming/kinesis-best-practices.html) * [CI/CD](../../dev-tools/index-ci-cd.html#dev-tools-ci-cd) * [MLOps worflows](../../machine-learning/mlops/mlops-workflow.html)     ###   Best practices for Databricks admins   * [Cluster configuration](../../clusters/cluster-config-best-practices.html) * [Pools](../../clusters/pool-best-practices.html) * [Cluster policies](../../administration-guide/clusters/policies-best-practices.html) * [Data governance](../../data-governance/best-practices.html) * [GDPR and CCPA compliance using Delta Lake](../../security/privacy/gdpr-delta.html)	To get started with Databricks, you can refer to the Get started articles, tutorials, and best practices guides available in the Databricks documentation. The Get started articles provide a shortcut to understanding Databricks features or typical tasks you can perform in Databricks, while tutorials are slightly more complex, prescriptive steps for typical workflows in Databricks that you can use as examples for your projects. Additionally, there are video tours available for Databricks Data Science and Engineering workspace, Databricks SQL workspace, Databricks Machine Learning workspace, and Notebook basics. You can also refer to the specific Get started guides for Databricks Data Science & Engineering, Databricks Machine Learning, and Databricks SQL. Finally, there are best practices articles available for both data science and engineering and Databricks admins.	0	1	1
no_reaction	https://docs.databricks.com/files/download-internet-files.html	how to download a spark dataframe into a local machine in a csv format?	   pyspark.pandas.Series.to\_csv  [¶](#pyspark-pandas-series-to-csv "Permalink to this headline") ================================================================================================  `Series.`  `to_csv`    (   *path     :     Optional    [    str    ]     =     None*  ,  *sep     :     str     =     ','*  ,  *na\_rep     :     str     =     ''*  ,  *columns     :     Optional[List[Union[Any, Tuple[Any, …]]]]     =     None*  ,  *header     :     bool     =     True*  ,  *quotechar     :     str     =     '"'*  ,  *date\_format     :     Optional    [    str    ]     =     None*  ,  *escapechar     :     Optional    [    str    ]     =     None*  ,  *num\_files     :     Optional    [    int    ]     =     None*  ,  *mode     :     str     =     'w'*  ,  *partition\_cols     :     Union[str, List[str], None]     =     None*  ,  *index\_col     :     Union[str, List[str], None]     =     None*  ,  *\*\*     options     :     Any*   )    → Optional    [    str    ]   [¶](#pyspark.pandas.Series.to_csv "Permalink to this definition")    Write object to a comma-separated values (csv) file.     Note     pandas-on-Spark    to\_csv    writes files to a path or URI. Unlike pandas’, pandas-on-Spark respects HDFS’s property such as ‘fs.default.name’.     Note     pandas-on-Spark writes CSV files into the directory,    path    , and writes multiple    part-…    files in the directory when    path    is specified. This behavior was inherited from Apache Spark. The number of partitions can be controlled by    num\_files    . This is deprecated. Use    DataFrame.spark.repartition    instead.     Parameters    **path: str, default None**    File path. If None is provided the result is returned as a string.    **sep: str, default ‘,’**    String of length 1. Field delimiter for the output file.    **na\_rep: str, default ‘’**    Missing data representation.    **columns: sequence, optional**    Columns to write.    **header: bool or list of str, default True**    Write out the column names. If a list of strings is given it is assumed to be aliases for the column names.    **quotechar: str, default ‘”’**    String of length 1. Character used to quote fields.    **date\_format: str, default None**    Format string for datetime objects.    **escapechar: str, default None**    String of length 1. Character used to escape    sep    and    quotechar    when appropriate.    **num\_files: the number of partitions to be written in `path` directory when**    this is a path. This is deprecated. Use    DataFrame.spark.repartition    instead.    **mode: str**    Python write mode, default ‘w’.     Note     mode can accept the strings for Spark writing mode. Such as ‘append’, ‘overwrite’, ‘ignore’, ‘error’, ‘errorifexists’.    * ‘append’ (equivalent to ‘a’): Append the new data to existing data. * ‘overwrite’ (equivalent to ‘w’): Overwrite existing data. * ‘ignore’: Silently ignore this operation if data already exists. * ‘error’ or ‘errorifexists’: Throw an exception if data already exists.  **partition\_cols: str or list of str, optional, default None**    Names of partitioning columns    **index\_col: str or list of str, optional, default: None**    Column names to be used in Spark to represent pandas-on-Spark’s index. The index name in pandas-on-Spark is ignored. By default, the index is always lost.    **options: keyword arguments for additional options specific to PySpark.**    These kwargs are specific to PySpark’s CSV options to pass. Check the options in PySpark’s API documentation for spark.write.csv(…). It has higher priority and overwrites all other options. This parameter only works when    path    is specified.     Returns     str or None     See also    [`read\_csv`](pyspark.pandas.read_csv.html#pyspark.pandas.read_csv "pyspark.pandas.read_csv")  [`DataFrame.to\_delta`](pyspark.pandas.DataFrame.to_delta.html#pyspark.pandas.DataFrame.to_delta "pyspark.pandas.DataFrame.to_delta")  [`DataFrame.to\_table`](pyspark.pandas.DataFrame.to_table.html#pyspark.pandas.DataFrame.to_table "pyspark.pandas.DataFrame.to_table")  [`DataFrame.to\_parquet`](pyspark.pandas.DataFrame.to_parquet.html#pyspark.pandas.DataFrame.to_parquet "pyspark.pandas.DataFrame.to_parquet")  [`DataFrame.to\_spark\_io`](pyspark.pandas.DataFrame.to_spark_io.html#pyspark.pandas.DataFrame.to_spark_io "pyspark.pandas.DataFrame.to_spark_io")   Examples    ``` >>> df = ps.DataFrame(dict( ...    date=list(pd.date\_range('2012-1-1 12:00:00', periods=3, freq='M')), ...    country=KR', 'US', 'JP, ...    code=[1, 2 ,3]), columns=date', 'country', 'code) >>> df.sort\_values(by="date")    date country code ... 2012-01-31 12:00:00      KR     1 ... 2012-02-29 12:00:00      US     2 ... 2012-03-31 12:00:00      JP     3  ```  ``` >>> print(df.to\_csv())   date,country,code 2012-01-31 12:00:00,KR,1 2012-02-29 12:00:00,US,2 2012-03-31 12:00:00,JP,3  ```  ``` >>> df.cummax().to\_csv(path=r'%s/to\_csv/foo.csv' % path, num\_files=1) >>> ps.read\_csv( ...    path=r'%s/to\_csv/foo.csv' % path ... ).sort\_values(by="date")    date country code ... 2012-01-31 12:00:00      KR     1 ... 2012-02-29 12:00:00      US     2 ... 2012-03-31 12:00:00      US     3  ```   In case of Series,    ``` >>> print(df.date.to\_csv())   date 2012-01-31 12:00:00 2012-02-29 12:00:00 2012-03-31 12:00:00  ```  ``` >>> df.date.to\_csv(path=r'%s/to\_csv/foo.csv' % path, num\_files=1) >>> ps.read\_csv( ...     path=r'%s/to\_csv/foo.csv' % path ... ).sort\_values(by="date")    date ... 2012-01-31 12:00:00 ... 2012-02-29 12:00:00 ... 2012-03-31 12:00:00  ```   You can preserve the index in the roundtrip as below.    ``` >>> df.set\_index("country", append=True, inplace=True) >>> df.date.to\_csv( ...     path=r'%s/to\_csv/bar.csv' % path, ...     num\_files=1, ...     index\_col=["index1", "index2"]) >>> ps.read\_csv( ...     path=r'%s/to\_csv/bar.csv' % path, index\_col=["index1", "index2"] ... ).sort\_values(by="date")    date index1 index2 ...    ...    2012-01-31 12:00:00 ...    ...    2012-02-29 12:00:00 ...    ...    2012-03-31 12:00:00  ```  [pyspark.pandas.Series.to\_json](pyspark.pandas.Series.to_json.html "previous page")  [pyspark.pandas.Series.to\_excel](pyspark.pandas.Series.to_excel.html "next page")   	You can use the `to_csv` method of a Spark DataFrame to write it to a CSV file. The method takes a file path as an argument, and if you provide `None` as the file path, the result will be returned as a string. Here is an example:  ``` df.write.csv('/path/to/file.csv') ```  This will write the DataFrame to a CSV file at the specified path. If you want to write the DataFrame to a local file on your machine, you can use a file path that starts with `file://`. For example:  ``` df.write.csv('file:///path/to/file.csv') ```  This will write the DataFrame to a CSV file at `/path/to/file.csv` on your local machine.	1	1	1
no_reaction	https://docs.databricks.com/machine-learning/model-serving/deploy-custom-models.html	write a custom pyfunc for mlflow	   Source code for pyspark.pandas.mlflow =======================================  ```  # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the "License"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. #  """ MLflow-related functions to load models and apply them to pandas-on-Spark dataframes. """ from typing import List, Union  from pyspark.sql.types import DataType import pandas as pd import numpy as np from typing import Any  from pyspark.pandas.\_typing import Label, Dtype from pyspark.pandas.utils import lazy\_property, default\_session from pyspark.pandas.frame import DataFrame from pyspark.pandas.series import Series, first\_series from pyspark.pandas.typedef import as\_spark\_type  \_\_all\_\_ = ["PythonModelWrapper", "load\_model"]  [[docs]](../../../reference/pyspark.pandas/api/pyspark.pandas.mlflow.PythonModelWrapper.html#pyspark.pandas.mlflow.PythonModelWrapper)class PythonModelWrapper:  """  A wrapper around MLflow's Python object model.   This wrapper acts as a predictor on pandas-on-Spark   """   def \_\_init\_\_(self, model\_uri: str, return\_type\_hint: Union[str, type, Dtype]):  self.\_model\_uri = model\_uri  self.\_return\_type\_hint = return\_type\_hint   @lazy\_property  def \_return\_type(self) -> DataType:  hint = self.\_return\_type\_hint  # The logic is simple for now, because it corresponds to the default  # case: continuous predictions  # TODO: do something smarter, for example when there is a sklearn.Classifier (it should  # return an integer or a categorical)  # We can do the same for pytorch/tensorflow/keras models by looking at the output types.  # However, this is probably better done in mlflow than here.  if hint == "infer" or not hint:  hint = np.float64  return as\_spark\_type(hint)   @lazy\_property  def \_model(self) -> Any:  """  The return object has to follow the API of mlflow.pyfunc.PythonModel.  """  from mlflow import pyfunc   return pyfunc.load\_model(model\_uri=self.\_model\_uri)   @lazy\_property  def \_model\_udf(self) -> Any:  from mlflow import pyfunc   spark = default\_session()  return pyfunc.spark\_udf(spark, model\_uri=self.\_model\_uri, result\_type=self.\_return\_type)   def \_\_str\_\_(self) -> str:  return "PythonModelWrapper({})".format(str(self.\_model))   def \_\_repr\_\_(self) -> str:  return "PythonModelWrapper({})".format(repr(self.\_model))   def predict(self, data: Union[DataFrame, pd.DataFrame]) -> Union[Series, pd.Series]:  """  Returns a prediction on the data.   If the data is a pandas-on-Spark DataFrame, the return is a pandas-on-Spark Series.   If the data is a pandas Dataframe, the return is the expected output of the underlying  pyfunc object (typically a pandas Series or a numpy array).  """  if isinstance(data, pd.DataFrame):  return self.\_model.predict(data)  elif isinstance(data, DataFrame):  return\_col = self.\_model\_udf(\*data.\_internal.data\_spark\_columns)  # TODO: the columns should be named according to the mlflow spec  # However, this is only possible with spark >= 3.0  # s = F.struct(\*data.columns)  # return\_col = self.\_model\_udf(s)  column\_labels: List[Label] = [  (col,) for col in data.\_internal.spark\_frame.select(return\_col).columns  ]  internal = data.\_internal.copy(  column\_labels=column\_labels, data\_spark\_columns=[return\_col], data\_fields=None  )  return first\_series(DataFrame(internal))  else:  raise ValueError("unknown data type: {}".format(type(data).\_\_name\_\_))  [[docs]](../../../reference/pyspark.pandas/api/pyspark.pandas.mlflow.load_model.html#pyspark.pandas.mlflow.load_model)def load\_model(  model\_uri: str, predict\_type: Union[str, type, Dtype] = "infer" ) -> PythonModelWrapper:  """  Loads an MLflow model into a wrapper that can be used both for pandas and pandas-on-Spark  DataFrame.   Parameters  ----------  model\_uri : str  URI pointing to the model. See MLflow documentation for more details.  predict\_type : a python basic type, a numpy basic type, a Spark type or 'infer'.  This is the return type that is expected when calling the predict function of the model.  If 'infer' is specified, the wrapper will attempt to automatically determine the return type  based on the model type.   Returns  -------  PythonModelWrapper  A wrapper around MLflow PythonModel objects. This wrapper is expected to adhere to the  interface of mlflow.pyfunc.PythonModel.   Examples  --------  Here is a full example that creates a model with scikit-learn and saves the model with  MLflow. The model is then loaded as a predictor that can be applied on a pandas-on-Spark  Dataframe.   We first initialize our MLflow environment:   >>> from mlflow.tracking import MlflowClient, set\_tracking\_uri  >>> import mlflow.sklearn  >>> from tempfile import mkdtemp  >>> d = mkdtemp("pandas\_on\_spark\_mlflow")  >>> set\_tracking\_uri("file:%s"%d)  >>> client = MlflowClient()  >>> exp\_id = mlflow.create\_experiment("my\_experiment")  >>> exp = mlflow.set\_experiment("my\_experiment")   We aim at learning this numerical function using a simple linear regressor.   >>> from sklearn.linear\_model import LinearRegression  >>> train = pd.DataFrame({"x1": np.arange(8), "x2": np.arange(8)\*\*2,  ... "y": np.log(2 + np.arange(8))})  >>> train\_x = train[["x1", "x2"]]  >>> train\_y = train[["y"]]  >>> with mlflow.start\_run():  ... lr = LinearRegression()  ... lr.fit(train\_x, train\_y)  ... mlflow.sklearn.log\_model(lr, "model")  LinearRegression...   Now that our model is logged using MLflow, we load it back and apply it on a pandas-on-Spark  dataframe:   >>> from pyspark.pandas.mlflow import load\_model  >>> run\_info = client.search\_runs(exp\_id)[-1].info  >>> model = load\_model("runs:/{run\_id}/model".format(run\_id=run\_info.run\_id))  >>> prediction\_df = ps.DataFrame({"x1": [2.0], "x2": [4.0]})  >>> prediction\_df["prediction"] = model.predict(prediction\_df)  >>> prediction\_df  x1 x2 prediction  0 2.0 4.0 1.355551   The model also works on pandas DataFrames as expected:   >>> model.predict(prediction\_df[["x1", "x2"]].to\_pandas())  array([[1.35555142]])   Notes  -----  Currently, the model prediction can only be merged back with the existing dataframe.  Other columns must be manually joined.  For example, this code will not work:   >>> df = ps.DataFrame({"x1": [2.0], "x2": [3.0], "z": [-1]})  >>> features = df[["x1", "x2"]]  >>> y = model.predict(features)  >>> # Works:  >>> features["y"] = y # doctest: +SKIP  >>> # Will fail with a message about dataframes not aligned.  >>> df["y"] = y # doctest: +SKIP   A current workaround is to use the .merge() function, using the feature values  as merging keys.   >>> featuresy = y  >>> everything = df.merge(features, on=x1', 'x2)  >>> everything  x1 x2 z y  0 2.0 3.0 -1 1.376932  """  return PythonModelWrapper(model\_uri, predict\_type)  def \_test() -> None:     import os     import doctest     import sys     from pyspark.sql import SparkSession     import pyspark.pandas.mlflow      os.chdir(os.environ["SPARK\_HOME"])      globs = pyspark.pandas.mlflow.\_\_dict\_\_.copy()     globs["ps"] = pyspark.pandas     spark = (         SparkSession.builder.master("local[4]").appName("pyspark.pandas.mlflow tests").getOrCreate()     )     (failure\_count, test\_count) = doctest.testmod(         pyspark.pandas.mlflow,         globs=globs,         optionflags=doctest.ELLIPSIS | doctest.NORMALIZE\_WHITESPACE,     )     spark.stop()     if failure\_count:         sys.exit(-1)  if \_\_name\_\_ == "\_\_main\_\_":     try:         import mlflow  # noqa: F401         import sklearn  # noqa: F401          \_test()     except ImportError:         pass  ```  	The context is the source code for `pyspark.pandas.mlflow`, a Python package that provides MLflow-related functions to load models and apply them to pandas-on-Spark dataframes. The package contains a class `PythonModelWrapper` that acts as a predictor on pandas-on-Spark and a function `load_model` that loads an MLflow model into a wrapper that can be used both for pandas and pandas-on-Spark DataFrame.	0	1	1
no_reaction	https://docs.databricks.com/administration-guide/workspace/delete-workspace.html	how to delete my table?	DROP TABLE ============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Deletes the table and removes the directory associated with the table from the file system if the table is not  `EXTERNAL`  table. An exception is thrown if the table does not exist. To drop a table you must be its owner.       In case of an external table, only the associated metadata information is removed from the metastore schema.       Any foreign key constraints referencing the table are also dropped.       If the table is cached, the command uncaches the table and all its dependents.       When a managed table is dropped from Unity Catalog, its underlying data is deleted from your cloud tenant within 30 days.        Syntax --------      ``` DROP TABLE [ IF EXISTS ] table\_name  ```        Parameter -----------   * **IF EXISTS**       If specified, no  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error is thrown when the table does not exist. * **[table\_name](sql-ref-names.html#table-name)**      The name of the table to be created. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the table cannot be found Databricks raises a  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error.       Examples ----------      ``` -- Assumes a table named `employeetable` exists. > DROP TABLE employeetable;  -- Assumes a table named `employeetable` exists in the `userdb` schema > DROP TABLE userdb.employeetable;  -- Assumes a table named `employeetable` does not exist. -- Throws TABLE\_OR\_VIEW\_NOT\_FOUND > DROP TABLE employeetable;  Error: TABLE\_OR\_VIEW\_NOT\_FOUND  -- Assumes a table named `employeetable` does not exist,Try with IF EXISTS -- this time it will not throw exception > DROP TABLE IF EXISTS employeetable;  ```        Related articles ------------------   * [CREATE TABLE](sql-ref-syntax-ddl-create-table.html) * [CREATE SCHEMA](sql-ref-syntax-ddl-create-schema.html) * [DROP SCHEMA](sql-ref-syntax-ddl-drop-schema.html)	To delete a table in Databricks, you can use the `DROP TABLE` command. The syntax is as follows:   ``` DROP TABLE [ IF EXISTS ] table_name ```  Where `table_name` is the name of the table you want to delete. If the table is not an `EXTERNAL` table, the command will also remove the directory associated with the table from the file system. If the table is cached, the command will also uncaches the table and all its dependents. If the table is a managed table, its underlying data is deleted from your cloud tenant within 30 days.	1	1	1
no_reaction	https://docs.databricks.com/introduction/index.html	What is databricks?	What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used to process, store, clean, share, analyze, model, and monetize datasets with solutions from BI to machine learning. The Databricks platform provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks is used by customers to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.	1	1	1
thumb_down	https://docs.databricks.com/release-notes/runtime/databricks-runtime-ver.html	Which runtime versions are LTS?	Databricks runtime support lifecycles =======================================     Databricks releases runtimes as Beta and GA versions. Databricks supports GA versions for six months, unless the runtime version is:     * [A long-term support (LTS) version](#lts)  . * [A Databricks Light runtime](#light-support)  .     Workloads on unsupported runtime versions may continue to run, but they receive no Databricks support or fixes.       For details, see the tables that follow.          Databricks Runtime versions support lifecycle -----------------------------------------------        |   Phase   |   Guarantees   | | --- | --- | |   Beta   |   Support SLAs are not applicable. For more information, see  [Databricks Runtime preview releases](../release-types.html#runtime-releases)  .   | |   Full support   |   Major stability and security fixes are backported.     Full support for Databricks Runtime versions lasts for six months, with the exception of  [long-term support (LTS)](#lts)  versions.   | |   End of support (EOS)   |   Version is unsupported:  * Workloads running on these versions receive no Databricks support * Databricks will not backport fixes    Supported releases along with their end-of-support dates are published at  [Supported releases](releases.html#supported-releases)  .     Unsupported releases are published at  [Unsupported releases](releases.html#unsupported-releases)  .   | |   End of life (EOL)   |   Databricks reserves the right to completely remove a release version from the API at any time after support ends, without prior notice.   |             Long-term support (LTS) lifecycle -----------------------------------        |   Phase   |   Guarantees   | | --- | --- | |   Beta   |   Support SLAs are not applicable. For more information, see  [Databricks Runtime preview releases](../release-types.html#runtime-releases)  .   | |   Full support for LTS version   |   Major stability and security fixes are backported.     Databricks releases LTS versions every six months and supports them for three full years.   | |   End of support (EOS)   |   Version is unsupported:  * Workloads running on these versions receive no Databricks support * Databricks does not backport fixes    The end-of-support date is three years after release.   | |   End of life (EOL)   |   Databricks reserves the right to completely remove a release version from the API at any time after support ends, without prior notice.   |         Databricks Light version support lifecycle --------------------------------------------        |   Phase   |   Guarantees   | | --- | --- | |   Beta   |   Support SLAs are not applicable. For more information, see  [Databricks Runtime preview releases](../release-types.html#runtime-releases)  .   | |   Full support   |   Major stability and security fixes are backported.     Full support lasts until either 12 months after release or two months after the next Databricks Light release, whichever comes first.   | |   End of support (EOS)   |   Version is unsupported:  * Workloads running on these versions receive no Databricks support * Databricks does not backport fixes    The end-of-support date is either 12 months after release or two months after the next Databricks Light release, whichever comes first.   | |   End of life (EOL)   |   Databricks reserves the right to completely remove a release version from the API at any time after support ends, without prior notice.   |      Important       The support policy doesn’t cover products and services provided by third parties, including those listed in the  [Technology partners](../../integrations/index.html)  .          Related docs --------------     For information about how to construct the Databricks runtime version string for REST API calls, see  [Runtime version strings](../../dev-tools/api/index.html#programmatic-version)  .	The LTS versions of Databricks runtime are supported for three full years.	0	1	1
no_reaction	https://docs.databricks.com/security/network/ip-access-list.html	How do I limit IP addresses	Title: IP address limit prevents cluster creation  Description: Learn how to fix a public IP address quota limit Cloud Provider Launch error when starting a Databricks cluster.  Answer: Problem Cluster creation fails with a message about a cloud provider error when you hover over cluster state. Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. When you view the cluster event log to get more details, you see a message about publicIPAddresses limits. ResourceQuotaExceeded Azure error message: Creating the resource of type 'Microsoft.Network/publicIPAddresses' would exceed the quota of '800' resources of type 'Microsoft.Network/publicIPAddresses' per resource group. The current resource count is '800', please delete some resources of this type before creating a new one.' Cause Azure subscriptions have a public IP address limit which restricts the number of public IP addresses you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the public IP address quota the cluster launch will fail. Solution You can either free up resources or request a quota increase for your account. Stop inactive clusters to free up public IP addresses for use. Open an Azure support case with a request to increase the public IP address quota limit for your subscription.	To limit IP addresses, you can either free up resources or request a quota increase for your account. Azure subscriptions have a public IP address limit which restricts the number of public IP addresses you can use. If you try to start a cluster that would result in your account exceeding the public IP address quota the cluster launch will fail. Stop inactive clusters to free up public IP addresses for use. Open an Azure support case with a request to increase the public IP address quota limit for your subscription.	1	1	1
no_reaction	https://kb.databricks.com/streaming/explicit-path-to-data-or-a-defined-schema-required-for-auto-loader	Cannot infer schema when the input path `s3://s3-be87-dropzone-dpc/raw/DPC/CDC_2S10_N.strcimaster__ct/` is empty. Please try to start the stream when there are files in the input path, or specify the schema. how to specify the schema	   pyspark.sql.streaming.DataStreamReader.schema  [¶](#pyspark-sql-streaming-datastreamreader-schema "Permalink to this headline") =================================================================================================================================  `DataStreamReader.`  `schema`    (   *schema     :     Union    [   [pyspark.sql.types.StructType](../../pyspark.sql/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType "pyspark.sql.types.StructType")    ,    str    ]*   )    → pyspark.sql.streaming.readwriter.DataStreamReader  [[source]](../../../_modules/pyspark/sql/streaming/readwriter.html#DataStreamReader.schema) [¶](#pyspark.sql.streaming.DataStreamReader.schema "Permalink to this definition")    Specifies the input schema.     Some data sources (e.g. JSON) can infer the input schema automatically from data. By specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading.     New in version 2.0.0.     Parameters    **schema**   [`pyspark.sql.types.StructType`](../../pyspark.sql/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType "pyspark.sql.types.StructType")  or str     a  [`pyspark.sql.types.StructType`](../../pyspark.sql/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType "pyspark.sql.types.StructType")  object or a DDL-formatted string (For example  `col0     INT,     col1     DOUBLE`  ).     Notes     This API is evolving.     Examples    ``` >>> from pyspark.sql.types import StructField, StructType, StringType >>> spark.readStream.schema(StructType([StructField("data", StringType(), True)])) <pyspark.sql.streaming.readwriter.DataStreamReader object ...> >>> spark.readStream.schema("col0 INT, col1 DOUBLE") <pyspark.sql.streaming.readwriter.DataStreamReader object ...>  ```   The example below specifies a different schema to CSV file.    ``` >>> import tempfile >>> import time >>> with tempfile.TemporaryDirectory() as d: ...     # Start a streaming query to read the CSV file. ...     spark.readStream.schema("col0 INT, col1 STRING").format("csv").load(d).printSchema() root  |-- col0: integer (nullable = true)  |-- col1: string (nullable = true)  ```  [pyspark.sql.streaming.DataStreamReader.parquet](pyspark.sql.streaming.DataStreamReader.parquet.html "previous page")  [pyspark.sql.streaming.DataStreamReader.table](pyspark.sql.streaming.DataStreamReader.table.html "next page")   	To specify the schema in PySpark, you can use the `schema` method of the `DataStreamReader` class. This method takes a `pyspark.sql.types.StructType` object or a DDL-formatted string as input. By specifying the schema, the underlying data source can skip the schema inference step and speed up data loading. Here is an example:  ``` from pyspark.sql.types import StructField, StructType, StringType  schema = StructType([     StructField("col0", StringType(), True),     StructField("col1", StringType(), True) ])  df = spark.readStream.schema(schema).csv("path/to/csv") ```  In this example, we create a `StructType` object that defines the schema of the CSV file. We then pass this schema to the `schema` method of the `DataStreamReader` object, which reads the CSV file and applies the specified schema.	1	1	1
no_reaction	https://docs.databricks.com/introduction/index.html	it's from databricks	What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. Customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. The Databricks workspace provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks can be interacted with programmatically with REST API, CLI, and Terraform. Databricks has a strong commitment to the open source community and maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use. Databricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job.	0	1	1
no_reaction	https://docs.databricks.com/getting-started/dataframes-python.html	How to load and transform data using the Apache Spark Python (PySpark) DataFrame API in Databricks?	    Tutorial: Work with PySpark DataFrames on Databricks ======================================================     This article shows you how to load and transform data using the Apache Spark Python (PySpark) DataFrame API in Databricks.       See also  [Apache Spark PySpark API reference](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrame.html#pyspark-sql-dataframe)   .        What is a DataFrame? ----------------------     A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently.       Apache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Spark DataFrames and Spark SQL use a unified planning and optimization engine, allowing you to get nearly identical performance across all supported languages on Databricks (Python, SQL, Scala, and R).         Create a DataFrame with Python --------------------------------     Most Apache Spark queries return a DataFrame. This includes reading from a table, loading data from files, and operations that transform data.       You can also create a Spark DataFrame from a list or a pandas DataFrame, such as in the following example:        ``` import pandas as pd  data = [[1, "Elia"], [2, "Teo"], [3, "Fang"]]  pdf = pd.DataFrame(data, columns=["id", "name"])  df1 = spark.createDataFrame(pdf) df2 = spark.createDataFrame(data, schema="id LONG, name STRING")  ```        Read a table into a DataFrame -------------------------------     Databricks uses Delta Lake for all tables by default. You can easily load tables to DataFrames, such as in the following example:        ``` spark.read.table("<catalog\_name>.<schema\_name>.<table\_name>")  ```        Load data into a DataFrame from files ---------------------------------------     You can load data from many supported  [file formats](../external-data/index.html)  . The following example uses a dataset available in the  `/databricks-datasets`  directory, accessible from most workspaces. See  [Sample datasets](../dbfs/databricks-datasets.html)  .        ``` df = (spark.read   .format("csv")   .option("header", "true")   .option("inferSchema", "true")   .load("/databricks-datasets/samples/population-vs-price/data\_geo.csv") )  ```        Assign transformation steps to a DataFrame --------------------------------------------     The results of most Spark transformations return a DataFrame. You can assign these results back to a DataFrame variable, similar to how you might use CTEs, temp views, or DataFrames in other systems.         Combine DataFrames with join and union ----------------------------------------     DataFrames use standard SQL semantics for join operations. A join returns the combined results of two DataFrames based on the provided matching conditions and join type. The following example is an inner join, which is the default:        ``` joined\_df = df1.join(df2, how="inner", on="id")  ```      You can add the rows of one DataFrame to another using the union operation, as in the following example:        ``` unioned\_df = df1.union(df2)  ```        Filter rows in a DataFrame ----------------------------     You can filter rows in a DataFrame using  `.filter()`  or  `.where()`  . There is no difference in performance or syntax, as seen in the following example:        ``` filtered\_df = df.filter("id > 1")  filtered\_df = df.where("id > 1")  ```      Use filtering to select a subset of rows to return or modify in a DataFrame.         Select columns from a DataFrame ---------------------------------     You can select columns by passing one or more column names to  `.select()`  , as in the following example:        ``` select\_df = df.select("id", "name")  ```      You can combine select and filter queries to limit rows and columns returned.        ``` subset\_df = df.filter("id > 1").select("name")  ```          View the DataFrame --------------------     To view this data in a tabular format, you can use the Databricks  `display()`  command, as in the following example:        ``` display(df)  ```        Print the data schema -----------------------     Spark uses the term  *schema*   to refer to the names and data types of the columns in the DataFrame.        Note       Databricks also uses the term schema to describe a collection of tables registered to a catalog.        You can print the schema using the  `.printSchema()`  method, as in the following example:        ``` df.printSchema()  ```        Save a DataFrame to a table -----------------------------     Databricks uses Delta Lake for all tables by default. You can save the contents of a DataFrame to a table using the following syntax:        ``` df.write.saveAsTable("<table\_name>")  ```        Write a DataFrame to a collection of files --------------------------------------------     Most Spark applications are designed to work on large datasets and work in a distributed fashion, and Spark writes out a directory of files rather than a single file. Many data systems are configured to read these directories of files. Databricks recommends using tables over filepaths for most applications.       The following example saves a directory of JSON files:        ``` df.write.format("json").save("/tmp/json\_data")  ```          Run SQL queries in PySpark ----------------------------     Spark DataFrames provide a number of options to combine SQL with Python.       The  `selectExpr()`  method allows you to specify each column as a SQL query, such as in the following example:        ``` display(df.selectExpr("id", "upper(name) as big\_name"))  ```      You can import the  `expr()`  function from  `pyspark.sql.functions`  to use SQL syntax anywhere a column would be specified, as in the following example:        ``` from pyspark.sql.functions import expr  display(df.select("id", expr("lower(name) as little\_name")))  ```      You can also use  `spark.sql()`  to run arbitrary SQL queries in the Python kernel, as in the following example:        ``` query\_df = spark.sql("SELECT \* FROM <table\_name>")  ```      Because logic is executed in the Python kernel and all SQL queries are passed as strings, you can use Python formatting to parameterize SQL queries, as in the following example:        ``` table\_name = "my\_table"  query\_df = spark.sql(f"SELECT \* FROM {table\_name}")  ```                                                       	To load and transform data using the Apache Spark Python (PySpark) DataFrame API in Databricks, what are the steps to follow?	0	0	0
no_reaction	https://docs.databricks.com/mlflow/migrate-mlflow-objects.html	How do I share MLFlow across 2 workspaces?	Copy MLflow objects between workspaces ========================================     To import or export MLflow objects to or from your Databricks workspace, you can use the community-driven open source project  [MLflow Export-Import](https://github.com/mlflow/mlflow-export-import#why-use-mlflow-export-import)   to migrate MLflow experiments, models, and runs between workspaces.       With these tools, you can:     * Share and collaborate with other data scientists in the same or another tracking server. For example, you can clone an experiment from another user into your workspace. * Copy a model from one workspace to another, such as from a development to a production workspace. * Copy MLflow experiments and runs from your local tracking server to your Databricks workspace. * Back up mission critical experiments and models to another Databricks workspace.	To share MLflow across 2 workspaces, you can use the MLflow Export-Import open source project to migrate MLflow experiments, models, and runs between workspaces. This allows you to copy a model from one workspace to another, such as from a development to a production workspace. You can also back up mission critical experiments and models to another Databricks workspace.	1	1	1
no_reaction	https://docs.databricks.com/delta-live-tables/cdc.html	delta live tables apply changes into from full snapshot	      Change data capture with Delta Live Tables ============================================          Preview       Delta Live Tables support for SCD type 2 is in  [Public Preview](../release-notes/release-types.html)  .        You can use change data capture (CDC) in Delta Live Tables to update tables based on changes in source data. CDC is supported in the Delta Live Tables SQL and Python interfaces. Delta Live Tables supports updating tables with slowly changing dimensions (SCD) type 1 and type 2:     * Use SCD type 1 to update records directly. History is not retained for records that are updated. * Use SCD type 2 to retain a history of records, either on all updates or on updates to a specified set of columns. See  [Track history for only specified columns with SCD type 2](#type2-track-history)     For syntax and other references, see:     * [Change data capture with Python in Delta Live Tables](python-ref.html#cdc) * [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) * [CDC table properties](properties.html#cdc)      Note       This article describes how to update tables in your Delta Live Tables pipeline based on changes in source data. To learn how to record and query row-level change information for Delta tables, see  [Use Delta Lake change data feed on Databricks](../delta/delta-change-data-feed.html)  .           How is CDC implemented with Delta Live Tables? ------------------------------------------------     You must specify a column in the source data on which to sequence records, which Delta Live Tables interprets as a monotonically increasing representation of the proper ordering of the source data. Delta Live Tables automatically handles data that arrives out of order. For SCD Type 2 changes, Delta Live Tables propagates the appropriate sequencing values to the  `\_\_START\_AT`  and  `\_\_END\_AT`  columns of the target table. There should be at most one distinct update per key at each sequencing value, and NULL sequencing values are unsupported.       To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an  `APPLY     CHANGES     INTO`  statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the  `CREATE     OR     REFRESH     STREAMING     TABLE`  statement in SQL or the  `create\_streaming\_live\_table()`  function in Python. To create the statement defining the CDC processing, use the  `APPLY     CHANGES`  statement in SQL or the  `apply\_changes()`  function in Python. For syntax details, see  [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc)  or  [Change data capture with Python in Delta Live Tables](python-ref.html#cdc)  .             What data objects are used for Delta Live Tables CDC processing? ------------------------------------------------------------------     When you declare the target table, two data structures are created in the Hive metastore:     * A view using the name assigned to the target table. * An internal backing table used by Delta Live Tables table to manage CDC processing. This table is named by prepending  `\_\_apply\_changes\_storage\_`  to the target table name.     For example, if you declare a target table named  `dlt\_cdc\_target`  , you will see a view named  `dlt\_cdc\_target`  and a table named  `\_\_apply\_changes\_storage\_dlt\_cdc\_target`  in the metastore. Creating a view allows Delta Live Tables to filter out the extra information (for example, tombstones and versions) that is required to handle out-of-order data. To view the processed data, query the target view. You can also query the raw data in the  `\_\_apply\_changes\_storage\_`  table to see deleted records and extra version columns. If you add data manually to the table, the records are assumed to come before other changes because the version columns are missing.         Limitations -------------   * Metrics for the target table, such as number of output rows, are not available. * SCD type 2 updates will add a history row for every input row, even if no columns have changed. * The target of the  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function cannot be used as a source for a streaming table. A table that reads from the target of an  `APPLY     CHANGES     INTO`  query or  `apply\_changes`  function must be a live table. * Expectations are not supported in an  `APPLY     CHANGES     INTO`  query or  `apply\_changes()`  function. To use expectations for the source or target dataset:      + Add expectations on source data by defining an intermediate table with the required expectations and use this dataset as the source for the target table.  + Add expectations on target data with a downstream table that reads input data from the target table.         SCD type 1 and SCD type 2 on Databricks -----------------------------------------     The following sections provide examples that demonstrate Delta Live Tables SCD type 1 and type 2 queries that update target tables based on source events that:     1. Create new user records. 2. Delete a user record. 3. Update user records. In the SCD type 1 example, the last  `UPDATE`  operations arrive late and are dropped from the target table, demonstrating the handling of out of order events.     All of the following examples assume familiarity with configuring and updating Delta Live Tables pipelines. See  [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html)  .       In order to run these examples, you must begin by creating a sample dataset. See  [Generate test data](#generate-data)  .       The following are the input records for these examples:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   124   |   Raul   |   Oaxaca   |   INSERT   |   1   | |   123   |   Isabel   |   Monterrey   |   INSERT   |   1   | |   125   |   Mercedes   |   Tijuana   |   INSERT   |   2   | |   126   |   Lily   |   Cancun   |   INSERT   |   2   | |   123   |   null   |   null   |   DELETE   |   6   | |   125   |   Mercedes   |   Guadalajara   |   UPDATE   |   6   | |   125   |   Mercedes   |   Mexicali   |   UPDATE   |   5   | |   123   |   Isabel   |   Chihuahua   |   UPDATE   |   5   |     If you uncomment the final row in the example data, it will insert the following record that specifies where records should be truncated:             |   userId   |   name   |   city   |   operation   |   sequenceNum   | | --- | --- | --- | --- | --- | |   null   |   null   |   null   |   TRUNCATE   |   3   |      Note       All the following examples include options to specify both  `DELETE`  and  `TRUNCATE`  operations, but each of these are optional.            Process SCD type 1 updates ----------------------------     The following code example demonstrates processing SCD type 1 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format("delta").table("cdc\_data.users")  dlt.create\_streaming\_live\_table("target")  dlt.apply\_changes(   target = "target",   source = "users",   keys = ["userId"],   sequence\_by = col("sequenceNum"),   apply\_as\_deletes = expr("operation = 'DELETE'"),   apply\_as\_truncates = expr("operation = 'TRUNCATE'"),   except\_column\_list = ["operation", "sequenceNum"],   stored\_as\_scd\_type = 1 )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = "DELETE" APPLY AS TRUNCATE WHEN  operation = "TRUNCATE" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 1;  ```       After running the SCD type 1 example, the target table contains the following records:           |   userId   |   name   |   city   | | --- | --- | --- | |   124   |   Raul   |   Oaxaca   | |   125   |   Mercedes   |   Guadalajara   | |   126   |   Lily   |   Cancun   |     After running the SCD type 1 example with the additional  `TRUNCATE`  record, records  `124`  and  `126`  are truncated because of the  `TRUNCATE`  operation at  `sequenceNum=3`  , and the target table contains the following record:           |   userId   |   name   |   city   | | --- | --- | --- | |   125   |   Mercedes   |   Guadalajara   |         Process SCD type 2 updates ----------------------------     The following code example demonstrates processing SCD type 2 updates:         ``` import dlt from pyspark.sql.functions import col, expr  @dlt.view def users():   return spark.readStream.format("delta").table("cdc\_data.users")  dlt.create\_streaming\_live\_table("target")  dlt.apply\_changes(   target = "target",   source = "users",   keys = ["userId"],   sequence\_by = col("sequenceNum"),   apply\_as\_deletes = expr("operation = 'DELETE'"),   except\_column\_list = ["operation", "sequenceNum"],   stored\_as\_scd\_type = "2" )  ```       ``` -- Create and populate the target table. CREATE OR REFRESH STREAMING TABLE target;  APPLY CHANGES INTO  live.target FROM  stream(cdc\_data.users) KEYS  (userId) APPLY AS DELETE WHEN  operation = "DELETE" SEQUENCE BY  sequenceNum COLUMNS \* EXCEPT  (operation, sequenceNum) STORED AS  SCD TYPE 2;  ```       After running the SCD type 2 example, the target table contains the following records:             |   userId   |   name   |   city   |   \_\_START\_AT   |   \_\_END\_AT   | | --- | --- | --- | --- | --- | |   123   |   Isabel   |   Monterrey   |   1   |   5   | |   123   |   Isabel   |  	Delta Live Tables support updating tables with slowly changing dimensions (SCD) type 1 and type 2. To perform CDC processing with Delta Live Tables, you first create a streaming table, and then use an `APPLY CHANGES INTO` statement to specify the source, keys, and sequencing for the change feed. To create the target streaming table, use the `CREATE OR REFRESH STREAMING TABLE` statement in SQL or the `create_streaming_live_table()` function in Python. To create the statement defining the CDC processing, use the `APPLY CHANGES` statement in SQL or the `apply_changes()` function in Python. For syntax details, see [Change data capture with SQL in Delta Live Tables](sql-ref.html#cdc) or [Change data capture with Python in Delta Live Tables](python-ref.html#cdc).	0	1	1
no_reaction	https://docs.databricks.com/data/manage-storage-credentials.html	who can add a new IAM role for storage credential in unity catalog	    Storage credentials =====================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime  ![check marked yes](../../_images/check.png)  Unity Catalog only       Unity Catalog and the built-in Databricks Hive metastore use default locations for managed tables. Unity Catalog introduces several new securable objects to grant privileges to data in cloud object storage.     * **[storage credential](#)**      A Unity Catalog object used to abstract long term credentials from cloud storage providers. * **[external location](sql-ref-external-locations.html)**      A Unity Catalog object used to associate a cloud object storage URI with a  [storage credential](#)  . * **[external table](sql-ref-external-tables.html)**      A Unity Catalog  [table](sql-ref-syntax-ddl-create-table-using.html)  created in a Unity Catalog-managed  [external location](sql-ref-external-locations.html)  .      Storage credential --------------------     A storage credential is a securable object representing an AWS IAM role.       Once a storage credential is created access to it can be granted to  [principals](sql-ref-principal.html)  (users and groups).       Storage credentials are primarily used to create  [external locations](sql-ref-external-locations.html)  , which scope access to a specific storage path.      [Storage credential names](sql-ref-names.html#credential-name)  are unqualified and must be unique within the metastore.         Graphical Representation of relationships -------------------------------------------     The following diagram describes the relationship between:     * storage credentials * external locations * external tables * storage paths * IAM entities * Azure service accounts    ![External location ER diagram](../../_images/external-location-er-diagram.png)        Examples ----------     Using CLI create a storage credential  `my\_storage\_cred`  for an AWS IAM role.        ``` databricks unity-catalog create-storage-credential --json '{"name": "my\_storage\_cred", "aws\_iam\_role": {"role\_arn": "arn:aws:iam::123456789:role/us-west-2-my-account"}}'  ```      The rest of the commands can be run within SQL.        ``` -- Grant access to the storage credential > GRANT READ FILES ON STORAGE CREDENTIAL my\_aws\_storage\_cred TO ceo;  -- ceo can directly read from any storage path using my\_aws\_storage\_cred > SELECT count(1) FROM `delta`.`s3://depts/finance/forecast/somefile` WITH (CREDENTIAL my\_aws\_storage\_cred);  100 > SELECT count(1) FROM `delta`.`s3://depts/hr/employees` WITH (CREDENTIAL my\_aws\_storage\_cred);  2017  -- Create an external location on specific path to which `my\_aws\_storage\_cred` has access > CREATE EXTERNAL LOCATION finance\_loc URL 's3://depts/finance'  WITH (CREDENTIAL my\_aws\_storage\_cred)  COMMENT 'finance';  ```        Related articles ------------------   * [Create a storage credential (CLI)](../../dev-tools/cli/unity-catalog-cli.html#create-a-storage-credential) * [ALTER STORAGE CREDENTIAL](sql-ref-syntax-ddl-alter-credential.html) * [ALTER TABLE](sql-ref-syntax-ddl-alter-table.html) * [CREATE LOCATION](sql-ref-syntax-ddl-create-location.html) * [DESCRIBE STORAGE CREDENTIAL](sql-ref-syntax-aux-describe-credential.html) * [DESCRIBE TABLE](sql-ref-syntax-aux-describe-table.html) * [DROP STORAGE CREDENTIAL](sql-ref-syntax-ddl-drop-credential.html) * [DROP TABLE](sql-ref-syntax-ddl-drop-table.html) * [SHOW STORAGE CREDENTIALS](sql-ref-syntax-aux-show-credentials.html) * [SHOW TABLES](sql-ref-syntax-aux-show-tables.html) * [GRANT](security-grant.html) * [REVOKE](security-revoke.html)                                                      	To add a new IAM role for storage credential in Unity Catalog, you can use the CLI command `databricks unity-catalog create-storage-credential` and specify the name of the credential and the AWS IAM role ARN. Once the storage credential is created, access to it can be granted to principals (users and groups) using the `GRANT` command.	1	1	1
thumb_down	https://kb.databricks.com/training/how-can-i-reset-my-password-in-databricks-academy	how to reset your Databricks password	Title: How can I reset my password in Databricks Academy?  Description: Learn step-by-step instructions for resetting your password in Databricks Academy.  Answer: Step 1: Navigate to your Academy login page. https://customer-academy.databricks.com/learn   https://partner-academy.databricks.com/learn https://microsoft-academy.databricks.com/learn Step 2: Select forgot password. Step 3: Enter the email address and click on send reset link. Step 4: Go to your email inbox. Step 5: Find the link and reset the password.  	To reset your password in Databricks Academy, you can follow these steps:  Step 1: Navigate to your Academy login page. https://customer-academy.databricks.com/learn   https://partner-academy.databricks.com/learn https://microsoft-academy.databricks.com/learn  Step 2: Select forgot password.  Step 3: Enter the email address and click on send reset link.  Step 4: Go to your email inbox.  Step 5: Find the link and reset the password.	1	1	1
no_reaction	https://kb.databricks.com/clusters/use-iptables-to-access-the-ec2-metadata-server	How do I connect my git server behind firewall to databricks?	Get a Git access token & connect a remote repo to Databricks ==============================================================     This article describes how to create a personal access token (PAT) in your Git provider so that you can connect to Databricks Repos.        Supported Git providers -------------------------     Databricks supports the following Git providers:     * [GitHub & GitHub AE](#github) * [Bitbucket Cloud](#bitbucket) * [GitLab](#gitlab) * [Azure DevOps](#devops) * [AWS CodeCommit](#codecommit)     Databricks Repos also supports Bitbucket Server, GitHub Enterprise Server, and GitLab self-managed. To integrate with a private Git server instance that is not internet-accessible, get in touch with your Databricks representative.           GitHub or GitHub AE ---------------------     In GitHub, follow these steps to create a personal access token that allows access to your repositories:     1. In the upper-right corner of any page, click your profile photo, then click  **Settings**   . 2. Click  **Developer settings**   . 3. Click the  **Personal access tokens**   tab. 4. Click the  **Generate new token**   button. 5. Enter a token description. 6. Select the  **repo**   scope and  **workflow**   scope, and click the  **Generate token**   button.  **workflow**   scope is needed in case your repository has GitHub Action workflows.      ![Generate GitHub token](../_images/github-newtoken.png) 7. Copy the token to your clipboard. You enter this token in Databricks under  **User Settings > Git integration**   .     See the  [GitHub documentation](https://help.github.com/articles/creating-an-access-token-for-command-line-use/)   to learn more about how to create personal access tokens.           GitLab --------     In GitLab, follow these steps to create a personal access token that allows access to your repositories:     1. From GitLab, click your user icon in the upper right corner of the screen and select  **Preferences**   . 2. Click  **Access Tokens**   in the sidebar.      ![Generate GitLab token](../_images/gitlab-newtoken.png) 3. Enter a name for the token. 4. Check the  `read\_repository`  and  `write\_repository`  permissions, and click  **Create personal access token**   . 5. Copy the token to your clipboard. Enter this token in Databricks under  **User Settings > Git integration**   .     See the  [GitLab documentation](https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html)   to learn more about how to create and manage personal access tokens.               AWS CodeCommit ----------------     In AWS CodeCommit, follow these steps to create a  **HTTPS Git credential**   that allows access to your repositories:     1. In AWS CodeCommit, create HTTPS Git credentials that allow access to your repositories. See the  [AWS CodeCommit](https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html)   documentation. The associated IAM user must have “read” and “write” permissions for the repository. 2. Record the password. You enter this password in Databricks under  **User Settings > Git integration**   .       Azure DevOps Services -----------------------     Authentication with  [Azure DevOps Services](https://docs.microsoft.com/azure/devops/?view=vsts)   is done automatically when you authenticate using Azure Active Directory (Azure AD). The Azure DevOps Services organization must be linked to the same Azure AD tenant as Databricks.       In Databricks, set your Git provider to Azure DevOps Services on the User Settings page:     1. Click  ![User Settings Icon](../_images/user-settings-icon.png) **Settings**   at the lower left of your screen and select  **User Settings**   . 2. Click the  **Git Integration**   tab. 3. Change your provider to Azure DevOps Services.      ![Azure DevOps Services GitHub provider](../_images/devops-provider-credentials.png)     You can also use an Azure DevOps personal access token for access. Follow the  [Azure DevOps documentation](https://learn.microsoft.com/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&tabs=Windows)   . The token should have “Full access” scope.           Bitbucket -----------     In Bitbucket, follow these steps to create an app password that allows access to your repositories:     1. Go to Bitbucket Cloud and create an app password that allows access to your repositories. See the  [Bitbucket Cloud documentation](https://confluence.atlassian.com/bitbucket/app-passwords-828781300.html)   . 2. Record the password. 3. In Databricks, enter this password under  **User Settings > Git integration**   .	To connect a private Git server instance that is not internet-accessible to Databricks Repos, you need to get in touch with your Databricks representative.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/enable-workspaces.html	How do I use unity catalog to share metastores between workspaces?	Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .	To use Unity Catalog to share metastores between workspaces, you need to create a metastore in Unity Catalog and link it to workspaces. To create a metastore, you must be a Databricks account admin, and your Databricks account must be on the Premium plan or above. You also need to configure a storage bucket and IAM role in AWS, and create the metastore in the same region as the workspaces you want to use to access the data. Once you have created the metastore, you can link workspaces to it. For more information about linking workspaces to metastores, see Enable a workspace for Unity Catalog.	1	1	1
no_reaction	https://docs.databricks.com/delta-live-tables/updates.html	update delta table based on temp view	Update (Delta Lake on Databricks) ===================================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [UPDATE](../../sql/language-manual/delta-update.html)  .         ``` UPDATE [db\_name.]table\_name [AS alias] SET col1 = value1 [, col2 = value2 ...] [WHERE predicate]  ```      Update the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows.        Note       (Delta Lake on Databricks) If a column has a  `NOT     NULL` [constraint](create-table.html#create-table-delta)  , and an  `INSERT     INTO`  statement sets a column value to  `NULL`  , a  `SparkException`  is thrown.       **`WHERE`**      Filter rows by predicate.        Example ---------      ``` UPDATE events SET eventType = 'click' WHERE eventType = 'clk'  ```     `UPDATE`  supports subqueries in the  `WHERE`  predicate, including  `IN`  ,  `NOT     IN`  ,  `EXISTS`  ,  `NOT     EXISTS`  , and scalar subqueries.         Subquery Examples -------------------      ``` UPDATE all\_events  SET session\_time = 0, ignored = true  WHERE session\_time < (SELECT min(session\_time) FROM good\_events)  UPDATE orders AS t1  SET order\_status = 'returned'  WHERE EXISTS (SELECT oid FROM returned\_orders WHERE t1.oid = oid)  UPDATE events  SET category = 'undefined'  WHERE category NOT IN (SELECT category FROM events2 WHERE date > '2001-01-01')  ```       Note       The following types of subqueries are not supported:     * Nested subqueries, that is, a subquery inside another subquery * A  `NOT     IN`  subquery inside an  `OR`  , for example,  `a     =     3     OR     b     NOT     IN     (SELECT     c     from     t)`     In most cases, you can rewrite  `NOT     IN`  subqueries using  `NOT     EXISTS`  . We recommend using  `NOT     EXISTS`  whenever possible, as  `UPDATE`  with  `NOT     IN`  subqueries can be slow.	The context is about updating Delta tables in Databricks using the `UPDATE` command with the `WHERE` clause to filter rows by predicate. It also provides examples of using subqueries in the `WHERE` predicate. However, it does not provide information on how to update Delta tables based on temp view.	1	0	0
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-credential.html	how to create storage credential	DESCRIBE STORAGE CREDENTIAL =============================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Returns the metadata of an existing storage credential. The metadata information includes credential name, comment, owner and other metadata.       You must be account or metastore admin to execute this command.        Syntax --------      ``` DESCRIBE STORAGE CREDENTIAL credential\_name  ```        Parameters ------------   * **[credential\_name](sql-ref-names.html#credential-name)**      The name of an existing storage credential in the metastore. If the name does not exist, an exception is thrown.       Examples ----------      ``` > DESCRIBE CREDENTIAL good\_cred;  name owner created\_at created\_by credential  --------- ------ ------------------------ ------------ ---------------------------------------------  good\_cred admins 2022-01-01T08:00:00.0000 jane@doe.com AwsIamRole:arn:aws:iam:123456789012:roe/us....  ```        Related articles ------------------   * [ALTER STORAGE CREDENTIAL](sql-ref-syntax-ddl-alter-credential.html) * [DROP STORAGE CREDENTIAL](sql-ref-syntax-ddl-drop-credential.html) * [SHOW STORAGE CREDENTIALS](sql-ref-syntax-aux-show-credentials.html) * [INFORMATION\_SCHEMA.STORAGE\_CREDENTIALS](information-schema/storage_credentials.html)	To create a storage credential, you can use the `CREATE STORAGE CREDENTIAL` command in Databricks. Here is an example syntax:  ``` CREATE STORAGE CREDENTIAL <credential_name>   [COMMENT '<credential_comment>   [AWS_ACCESS_KEY_ID '<access_key_id>   [AWS_SECRET_ACCESS_KEY '<secret_access_key>   [AZURE_SAS_TOKEN '<sas_token>   [AZURE_STORAGE_ACCOUNT '<storage_account>   [AZURE_STORAGE_KEY '<storage_key>   [GOOGLE_APPLICATION_CREDENTIALS '<path_to_credentials_file>   [SCOPE '<scope> ```  You can replace the placeholders with the actual values for your storage credential. Note that the available options depend on the type of storage you are using.	1	1	1
no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-use-schema.html	what is the equivalent in sql of schema() to specify the schema to use ?	USE SCHEMA ============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.2 and above       Sets the current schema. After the current schema is set, unqualified references to objects such as tables, functions, and views that are referenced by SQLs are resolved from the current schema. The default schema name is  `default`  .       While usage of  `SCHEMA`  and  `DATABASE`  is interchangeable,  `SCHEMA`  is preferred.        Syntax --------      ``` USE [SCHEMA] schema\_name  ```        Parameter -----------   * **[schema\_name](sql-ref-names.html#schema-name)**      Name of the schema to use. If  `schema\_name`  is qualified the current catalog is also set to the specified catalog name. If the schema does not exist, an exception is thrown.       Examples ----------      ``` -- Use the 'userschema' which exists. > USE SCHEMA userschema;  -- Use the 'userschema1' which doesn't exist > USE SCHEMA userschema1;  Error: Database 'userschema1' not found;  -- Setting the catalog resets the schema to `default` > USE CATALOG some\_cat; > SELECT current\_schema(), current\_catalog();  some\_cat default  -- Setting the schema within the current catalog > USE SCHEMA some\_schem; > SELECT current\_schema(), current\_catalog();  some\_cat some\_schema  -- Resetting both catalog and schema > USE SCHEMA main.my\_schema; > SELECT current\_schema(), current\_catalog();  main my\_schema  -- Setting the catalog resets the schema to `default` again > USE CATALOG some\_cat; > SELECT current\_schema(), current\_catalog();  some\_cat default  ```        Related articles ------------------   * [CREATE SCHEMA](sql-ref-syntax-ddl-create-schema.html) * [DROP SCHEMA](sql-ref-syntax-ddl-drop-schema.html)	In SQL, the equivalent of `schema()` to specify the schema to use is `USE SCHEMA schema_name`.	1	1	1
no_reaction	https://spark.apache.org/docs/latest/api/R/reference/createExternalTable-deprecated.html	create external table with other db not s3	  ![]()   (Deprecated) Create an external table =======================================  `createExternalTable-deprecated.Rd`    Creates an external table based on the dataset in a data source, Returns a SparkDataFrame associated with the external table.     Usage -------  ``` createExternalTable(tableName, path = NULL, source = NULL, schema = NULL, ...) ```   Arguments -----------   tableName     a name of the table.     path     the path of files to load.     source     the name of external data source.     schema     the schema of the data required for some data sources.     ...     additional argument(s) passed to the method.     Value -------   A SparkDataFrame.     Details ---------   The data source is specified by the  `source`   and a set of options(...). If  `source`   is not specified, the default data source configured by "spark.sql.sources.default" will be used.     Note ------   createExternalTable since 1.4.0     See also ----------  [createTable](createTable.html)    Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() df <- createExternalTable("myjson", path="path/to/json", source="json", schema) }  ```  	The `createExternalTable` function in SparkR can be used to create an external table based on the dataset in a data source other than S3. The function takes arguments such as `tableName`, `path`, `source`, and `schema` to specify the details of the external table. The `source` argument specifies the name of the external data source, and if it is not specified, the default data source configured by "spark.sql.sources.default" will be used.	0	1	1
no_reaction	https://docs.databricks.com/data-governance/unity-catalog/best-practices.html	do you need unity catalog for delta sharing?	List your data product in Databricks Marketplace ==================================================      Preview       This feature is in  [Private Preview](../release-notes/release-types.html)  . To try it, reach out to your Databricks contact.        This article describes how to create a Databricks Marketplace listing for your data product.        Before you begin ------------------     Databricks Marketplace uses  [Delta Sharing](../data-sharing/index.html)  to manage secure sharing of data products. Delta Sharing, in turn, requires that your Databricks workspace is enabled for Unity Catalog. To list data on the Marketplace exchange, you must have:     * A Databricks account on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * A Unity Catalog metastore defined in your Databricks account.       To learn how to create a Unity Catalog metastore, see  [Get started using Unity Catalog](../data-governance/unity-catalog/get-started.html)  .      *Permissions required:*      + AWS account user with permission to create or update IAM roles, IAM policies, S3 buckets, and cross-account trust relationships.  + Databricks account admin. See   [Manage users, service principals, and groups](../administration-guide/users-groups/index.html)   . * Delta Sharing enabled for your Databricks account.       To learn how to enable Delta Sharing, see  [Enable Delta Sharing for your account](../data-sharing/set-up.html#enable-feature-group)  .      *Permissions required:*   Databricks account admin * A Databricks workspace enabled for Unity Catalog.       To enable Unity Catalog on a workspace, you attach the workspace to a Unity Catalog metastore. To learn how, see  [Enable a workspace for Unity Catalog](../data-governance/unity-catalog/enable-workspaces.html)  .      *Permissions required:*   Databricks account admin       Create shares ---------------     After you have a Databricks account enabled for Delta Sharing and a Databricks workspace enabled for Unity Catalog, you can create the  *shares*   that you will use to share your data in the Marketplace.       A share is a collection of tables that are shareable and securable as a unit.     1. Add data tables to your Unity Catalog metastore.       To learn how to create tables in Unity Catalog, see  [Create tables](../data-governance/unity-catalog/create-tables.html)  . 2. Create a share and add these tables to the share.       To learn how to create and update shares, see  [Create and manage shares for Delta Sharing](../data-sharing/create-share.html)  .      *Permissions required:*      * To create a share, you must be a metastore admin or user with the   `CREATE        SHARE`   privilege for the Unity Catalog metastore where the data you want to share is registered.  * To add tables to a share, you must be either a metastore admin or the share owner. Share owners must also have the   `SELECT`   privilege on the table. You must keep that privilege in order for the table to continue to be shared. If you lose it, the recipient cannot access the table through the share. Databricks therefore recommends that you use a group as the share owner.     After your share is created, you can create or update a Marketplace listing that references it.         Create sample notebooks -------------------------     In addition to tabular data, you can also share Databricks notebooks. A notebook is a great way to demonstrate example use cases and visualize table properties.       To learn how to create a notebook, see  [Introduction to Databricks notebooks](../notebooks/index.html)  .         Create listings -----------------     Marketplace listings enable consumers to browse, select, and access your data products.      *Permission required:*   Metastore admin for the Unity Catalog metastore attached to your workspace. See  [Assign a metastore admin](../data-governance/unity-catalog/manage-privileges/index.html#assign-a-metastore-admin)  .       To create a listing:     1. Log into your Databricks workspace as a Metastore admin. 2. In the sidebar, click the  ![Marketplace icon](../_images/marketplace.png) **Marketplace**   icon. 3. On the upper-right corner of the Data Marketplace page, click  **Provider console**   . 4. On the  **Provider console**   page  **Listings**   tab, click  **Create listing**   . 5. On the  **New listing**   page, enter your listing information.       For instructions, see  [Listing field descriptions and recommendations](#listing-recommendations)  .     You can save a draft and view a preview before you publish. When you click  **Publish**   , the listing appears in the Marketplace immediately.        ###   Listing field descriptions and recommendations     The following information, organized by  **New listing**   page field, can help you create an effective listing:     * **Listing name**   : Each listing should have a unique name that helps consumers understand what it offers.       Recommendations:      + Fewer than 100 characters.  + Title case (capitalize primary words).  + Use a “- Demo” or “- Sample” suffix for free listings that include sample data.   Example       US Census 2022 * **Short description**   : A short, informative explanation of the dataset that expands on the listing name. This field appears in consumer search results.       Recommendations:      + Fewer than 100 characters. Cannot exceed 160 characters.  + Sentence case (capitalize only the first word an any proper nouns or acronyms).   Example       General information about US population count and demography in 2020 * **Provider profile**   : Your organization or company name. Select from the drop-down menu. * **Terms of service**   : A URL that links to your terms of service for the appropriate use of the shared data assets.       Must be publicly accessible and require no login. * **Share data instantly**   : Select this option to let consumers gain access to the shared data directly from the Marketplace, without requiring your approval (but requiring acceptance of terms of service). Choose a share from the drop-down menu. This option is typically used for sample and public datasets.       If you have not yet created the share that you want to include, click  **+ Create new share**   at the bottom of the drop-down menu. You are taken to the  **Create a new share**   dialog box.       If a share that you select or create here contains no data or assets, a message appears with an  **Add data**   button. Click it to go to the Data Explorer, where you can add tables to the share.       For more information about creating shares and adding tables to shares, see  [Create and manage shares for Delta Sharing](../data-sharing/create-share.html)  . * **Share data later**   : Select this option to require your approval before a consumer can access the shared data. Use this option if you require a business agreement before you make the data product available to consumers. You must manage the business agreement with consumers offline. You can initiate communications using the consumer email address.       You’ll be able to view and handle consumer requests on the  **Provider Console > Consumer requests**   tab. * **Attributes**   : Select up to five categories that consumers can use to filter listings. * **Description**   : The detailed description of your data should include a summary of the data and assets being offered in the listing.       Basic rich text formatting is supported (that is, bold, italics, bullets, and numbered lists), using Markdown syntax. To preview your content, use the buttons at the far right of the Description field toolbar.       Recommendations:      + Include a list of schemas, tables, and columns.  + Provide brief guidance about how to use the data and sample use cases.  + List data sources.  + Mention specific data boundaries, such as time period and demographic groupings.  + Use consistent punctuation and syntax.  + Add an extra line break between paragraphs.  + Check your spelling and grammar.   Example      **Description:**       The US Census of Population and Dwellings is the official count of people and houses in the US in 2020. It provides a social and economic snapshot. The 2020 Census, held on March 6, 2021, is the 23th census.      **Number of tables:**       There are 231 unique tables in this dataset.      **Information included in this dataset:**       + Population estimates  + Demographic components (births, deaths, migration)  + This data can be sorted by characteristics such as age, sex, and race, as well as by national, state, and county location  **Data source:**       The US Census Bureau * **Sample notebook**   : If you want to share sample notebooks to demonstrate how best to use the data, add up to ten notebooks.       For more information about creating notebooks, see  [Introduction to Databricks notebooks](../notebooks/index.html)  . * **Documentation**   : A URL that links to documentation that can help consumers use or understand your data set (for example, a dataset dictionary). * **Private policy**   : A URL that links to your privacy policy.       Must be publicly accessible and require no login.        Next steps ------------   * Manage consumer requests (coming soon)	Yes, you need Unity Catalog for Delta Sharing in Databricks Marketplace.	0	1	1