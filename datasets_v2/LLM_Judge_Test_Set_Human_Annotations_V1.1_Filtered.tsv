	Reaction	Sources	Question	Document	Answer	Context_Relevance_Label	Answer_Faithfulness_Label	Answer_Relevance_Label	annotated
0	no_reaction	https://docs.databricks.com/delta-live-tables/tutorial-sql.html	Give a tutorial of declaring a delta live table?	"      Delta Live Tables CLI =======================     You run Databricks Delta Live Tables CLI subcommands by appending them to  `databricks     pipelines`  . These subcommands call the  [Delta Live Tables API](../../delta-live-tables/api-guide.html)  .        ``` databricks pipelines -h  ```       ``` Usage: databricks pipelines [OPTIONS] COMMAND [ARGS]...    Utility to interact with Databricks Delta Live Tables Pipelines.  Options:   -v, --version   0.17.0   --debug         Debug Mode. Shows full stack trace on error.   --profile TEXT  CLI connection profile to use. The default profile is                   ""DEFAULT"".    -h, --help      Show this message and exit.  Commands:   create  Creates a pipeline.   delete  Deletes the pipeline and cancels any active updates.   deploy  [Deprecated] This command is deprecated, use create and edit           commands instead.  Creates or edits a pipeline specified by the           pipeline settings.    edit    Edits a pipeline.   get     Gets a pipeline's current settings and status.   list    Lists all pipelines and their statuses.   reset   [Deprecated] Use the ""start --full-refresh"" command instead. Resets           a pipeline so that data can be reprocessed from the beginning.    run     [Deprecated] Use the ""start"" command instead. Starts a pipeline           update.    start   Starts a pipeline update.   stop    Stops the pipeline by cancelling any active update.  ```       Create a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     create     --help`  .      ###   Usage      ``` databricks pipelines create --settings create-pipeline.json  ```     `create-pipeline.json`  :        ``` {  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 5  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_analysis"",  ""continuous"": false }  ```       ``` Successfully created pipeline: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003 with ID: 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Edit a pipeline -----------------     To display usage documentation, run  `databricks     pipelines     edit     --help`  .        ###   Usage      ``` databricks pipelines edit --settings edit-pipeline.json  ```     `edit-pipeline.json`  :        ``` {  ""id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 3  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_quickstart\_data"",  ""continuous"": false }  ```       ``` Successfully edited pipeline settings: https://<databricks-instance>/#joblist/pipelines/4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Delete a pipeline -------------------     To display usage documentation, run  `databricks     pipelines     delete     --help`  .        ###   Usage      ``` databricks pipelines delete --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003 deleted  ```         Get information about a pipeline ----------------------------------     To display usage documentation, run  `databricks     pipelines     get     --help`  .        ###   Usage      ``` databricks pipelines get --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` {  ""pipeline\_id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""spec"": {  ""id"": ""4ee6b5c2-7017-11ec-90d6-0242ac120003"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""storage"": ""/Users/username/data"",  ""clusters"": [  {  ""label"": ""default"",  ""autoscale"": {  ""min\_workers"": 1,  ""max\_workers"": 5  }  }  ],  ""libraries"": [  {  ""notebook"": {  ""path"": ""/Users/username/Delta Live Tables quickstart (SQL)""  }  }  ],  ""target"": ""wikipedia\_analysis"",  ""continuous"": false  },  ""state"": ""IDLE"",  ""name"": ""Wikipedia pipeline (SQL)"",  ""creator\_user\_name"": ""username@databricks.com"",  ""last\_modified"": 1641600056265 }  ```         List all pipelines and information on their status ----------------------------------------------------     To display usage documentation, run  `databricks     pipelines     list     --help`  .        ###   Usage      ``` databricks pipelines list  ```       ``` [  {  ""pipeline\_id"": ""8a93e1b4-7024-11ec-90d6-0242ac120003"",  ""state"": ""IDLE"",  ""name"": ""Retail sales pipeline (SQL)"",  ""latest\_updates"": [  {  ""update\_id"": ""9df03924-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T16:18:57.982Z""  },  {  ""update\_id"": ""a57e601c-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-28T18:19:30.371Z""  }  ],  ""creator\_user\_name"": ""user@databricks.com""  },  {  ""pipeline\_id"": ""b46e2670-7024-11ec-90d6-0242ac120003"",  ""state"": ""IDLE"",  ""name"": ""DLT quickstart (Python)"",  ""latest\_updates"": [  {  ""update\_id"": ""bcd8fa2e-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-12-16T18:19:25.827Z""  },  {  ""update\_id"": ""c2c7a2c8-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T22:22:32.586Z""  },  {  ""update\_id"": ""c88d2818-7024-11ec-90d6-0242ac120003"",  ""state"": ""COMPLETED"",  ""creation\_time"": ""2021-10-29T22:16:18.258Z""  }  ],  ""creator\_user\_name"": ""user@databricks.com""  } ]  ```         Start a pipeline update -------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Started an update c6462e2e-beb7-4c24-a9d3-962eaa19781b for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```        Start a pipeline update with full refresh -------------------------------------------     To display usage documentation, run  `databricks     pipelines     start     --help`  .        ###   Usage      ``` databricks pipelines start --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003 --full-refresh  ```       ``` Started an update ed885d3d-4fc0-44fd-b5dd-59471c789327 with full refresh for pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```         Stop a pipeline update ------------------------     To display usage documentation, run  `databricks     pipelines     stop     --help`  .        ###   Usage      ``` databricks pipelines stop --pipeline-id 4ee6b5c2-7017-11ec-90d6-0242ac120003  ```       ``` Stopped pipeline 4ee6b5c2-7017-11ec-90d6-0242ac120003.  ```                                                        "	To declare a Delta live table, you need to use the Databricks Delta Live Tables CLI subcommands by appending them to `databricks pipelines`. The subcommands call the Delta Live Tables API. You can create a pipeline, edit a pipeline, delete a pipeline, get information about a pipeline, list all pipelines and information on their status, start a pipeline update, start a pipeline update with full refresh, and stop a pipeline update. To display usage documentation, you can run `databricks pipelines -h`.	1	1	1	True
1	no_reaction	https://docs.databricks.com/archive/admin-guide/account-console.html	how do I find out who the account owner is	Access the account console (legacy) =====================================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. To view current admin documentation, see  [Manage your Databricks account](../../administration-guide/account-settings/index.html)  .        The account console is where you administer your Databricks account-level configurations. Only the account owner who initially created the Databricks account can log in to the account console. To transfer account owner rights, contact your Databricks account representative.        Note       If your account is on the E2 version of the platform, instead see  [Manage your Databricks account](../../administration-guide/account-settings/index.html)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.        To access the account console when you are viewing a workspace, click  ![User Settings Icon](../../_images/user-settings-icon.png) **Settings**   at the lower left and select  **Manage Account**   .      ![Manage account](../../_images/manage-account.png)    You can also go directly to  <https://accounts.cloud.databricks.com>  .       Enter your account owner email address and password to log in. How you get your initial account owner login ID and password depends on the way you signed up for Databricks. In most cases, you will receive a Welcome email. Follow the link in the email to verify your email address and change your temporary password. When you’ve changed your password, you can open the account console.	To find out who the account owner is, you need to log in to the account console using the account owner's email address and password. Only the account owner who initially created the Databricks account can log in to the account console. If you need to transfer account owner rights, you should contact your Databricks account representative.	1	1	1	True
3	no_reaction	https://docs.databricks.com/lakehouse/index.html	what is data lakehouse	What is the Databricks Lakehouse? ===================================     The Databricks Lakehouse combines the ACID transactions and data governance of enterprise data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data. The Databricks Lakehouse keeps your data in your massively scalable cloud object storage in open source data standards, allowing you to use your data however and wherever you want.      * [What are ACID guarantees on Databricks?](acid.html) * [What is the medallion lakehouse architecture?](medallion.html) * [What does it mean to build a single source of truth?](ssot.html) * [Data discovery and collaboration in the lakehouse](collaboration.html) * [Data objects in the Databricks Lakehouse](data-objects.html)       Components of the Databricks Lakehouse ----------------------------------------     The primary components of the Databricks Lakehouse are:     * **[Delta tables](#delta-tables)**  :      + [ACID transactions](acid.html)  + [Data versioning](../delta/history.html)  + [ETL](../ingestion/index.html)  + [Indexing](../delta/data-skipping.html) * **[Unity Catalog](../data-governance/unity-catalog/index.html)**  :      + [Data governance](../data-governance/index.html)  + [Data sharing](../data-sharing/index.html)  + [Data auditing](../data-governance/unity-catalog/audit.html)     By storing data with  [Delta Lake](../delta/index.html)  , you enable downstream data scientists, analysts, and machine learning engineers to leverage the same production data supporting your core ETL workloads as soon as data is processed.       Unity Catalog ensures that you have complete control over who gains access to which data and provides a centralized mechanism for managing all data governance and access controls without needing to replicate your data.         Delta tables --------------     Tables created on Databricks use the Delta Lake protocol by default. When you create a new Delta table:     * Metadata used to reference the table is added to the metastore in the declared schema or database. * Data and table metadata are saved to a directory in cloud object storage.     The metastore reference to a Delta table is technically optional; you can create Delta tables by directly interacting with directory paths using  [Spark APIs](../delta/tutorial.html)  . Some new features that build upon Delta Lake will store additional metadata in the table directory, but all Delta tables have:     * A directory containing table data in the Parquet file format. * A sub-directory  `/\_delta\_log`  that contains metadata about table versions in JSON and Parquet format.     Learn more about  [Data objects in the Databricks Lakehouse](data-objects.html)  .           Unity Catalog ---------------     Unity Catalog unifies data governance and discovery on Databricks. Available in notebooks, jobs, and Databricks SQL, Unity Catalog provides features and UIs that enable workloads and users designed for both data lakes and data warehouse.     * Account-level management of the Unity Catalog metastore means databases, data objects, and permissions can be shared across Databricks workspaces. * You can leverage three tier namespacing (  `<catalog>.<database>.<table>`  ) for organizing and granting access to data. * External locations and storage credentials are also securable objects with similar ACL setting to other data objects. * The  [Data Explorer](../data/index.html)  provides a graphical user interface to explore databases and manage permissions.       Data lakehouse vs. data warehouse vs. data lake -------------------------------------------------     Data warehouses have powered business intelligence (BI) decisions for about 30 years, having evolved as set of design guidelines for systems controlling the flow of data. Enterprise data warehouses optimize queries for BI reports, but can take minutes or even hours to generate results. Designed for data that is unlikely to change with high frequency, data warehouses seek to prevent conflicts between concurrently running queries. Many data warehouses rely on proprietary formats, which often limit support for machine learning. Data warehousing on Databricks leverages the capabilities of a Databricks Lakehouse and Databricks SQL. For more information, see  [What is data warehousing on Databricks?](../sql/index.html)  .       Powered by technological advances in data storage and driven by exponential increases in the types and volume of data, data lakes have come into widespread use over the last decade. Data lakes store and process data cheaply and efficiently. Data lakes are often defined in opposition to data warehouses: A data warehouse delivers clean, structured data for BI analytics, while a data lake permanently and cheaply stores data of any nature in any format. Many organizations use data lakes for data science and machine learning, but not for BI reporting due to its unvalidated nature.       The data lakehouse replaces the current dependency on data lakes and data warehouses for modern data companies that desire:     * Open, direct access to data stored in standard data formats. * Indexing protocols optimized for machine learning and data science. * Low query latency and high reliability for BI and advanced analytics.     By combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the data lakehouse allows data scientists and ML engineers to build models from the same data driving BI reports.	The Databricks Lakehouse combines the ACID transactions and data governance of enterprise data warehouses with the flexibility and cost-efficiency of data lakes to enable business intelligence (BI) and machine learning (ML) on all data. It allows open, direct access to data stored in standard data formats, indexing protocols optimized for machine learning and data science, and low query latency and high reliability for BI and advanced analytics. By combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the data lakehouse allows data scientists and ML engineers to build models from the same data driving BI reports.	1	1	1	True
5	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-use-catalog.html	is it possible to set a default catalog and schema for a user in unity catalog?	USE CATALOG =============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Sets the current catalog. After the current catalog is set, partially and unqualified identifiers for tables, functions, and views that are referenced by SQLs are resolved from the current catalog.       Setting the catalog also resets the current schema to  `default`  .        Syntax --------      ``` { USE | SET } CATALOG [ catalog\_name | ' catalog\_name ' ]  ```        Parameter -----------   * **[catalog\_name](sql-ref-names.html#catalog-name)**      Name of the catalog to use. If the catalog does not exist, an exception is thrown.       Examples ----------      ``` -- Use the 'hive\_metastore' which exists. > USE CATALOG hive\_metastore;  > USE CATALOG 'hive\_metastore';  -- Use the 'some\_catalog' which doesn't exist > USE CATALOG `some\_catalog`;  Error: Catalog 'some\_catalog' not found;  -- Setting the catalog resets the datbase to `default` > USE CATALOG some\_cat; > SELECT current\_database(), current\_catalog();  some\_cat default  -- Setting the schema within the curret catalog > USE DATABASE some\_db; > SELECT current\_database(), current\_catalog();  some\_cat some\_db  -- Resetting both catalog and schema > USE DATABASE main.my\_db; > SELECT current\_database(), current\_catalog();  main my\_db  -- Setting the catalog resets the database to `default` again > USE CATALOG some\_cat; > SELECT current\_database(), current\_catalog();  some\_cat default  ```        Related articles ------------------   * [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html) * [DROP CATALOG](sql-ref-syntax-ddl-drop-catalog.html)	Based on the context, it is not possible to set a default schema for a user in Unity Catalog. However, setting the catalog resets the current schema to `default`.	1	1	0	True
8	no_reaction	https://docs.databricks.com/external-data/parquet.html	How can I convert my files to parquet?	Parquet file ==============    [Apache Parquet](https://parquet.apache.org/)   is a columnar file format that provides optimizations to speed up queries. It is a far more efficient file format than  [CSV](csv.html)  or  [JSON](json.html)  .       For more information, see  [Parquet Files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#parquet(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#parquet(path:String):Unit)     The following notebook shows how to read and write data to Parquet files.        ###   Reading Parquet files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-parquet-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import	To convert files to Parquet format, you can use the supported read and write options in Apache Spark. For example, in Python, you can use the `pyspark.sql.DataFrameReader.parquet` method to read Parquet files and the `pyspark.sql.DataFrameWriter.parquet` method to write data to Parquet files. You can refer to the provided links for more information and examples.	1	1	1	True
10	thumb_up	https://spark.apache.org/docs/latest/spark-connect-overview.html	What is Spark Connect?	"   Spark Connect Overview ========================  **Building client-side Spark applications**    In Apache Spark 3.4, Spark Connect introduced a decoupled client-server architecture that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks and programming languages.     To get started, see  [Quickstart: Spark Connect](api/python/getting_started/quickstart_connect.html)   .    ![Spark Connect API Diagram](img/spark-connect-api.png ""Spark Connect API"")   How Spark Connect works =========================   The Spark Connect client library is designed to simplify Spark application development. It is a thin API that can be embedded everywhere: in application servers, IDEs, notebooks, and programming languages. The Spark Connect API builds on Spark’s DataFrame API using unresolved logical plans as a language-agnostic protocol between the client and the Spark driver.     The Spark Connect client translates DataFrame operations into unresolved logical query plans which are encoded using protocol buffers. These are sent to the server using the gRPC framework.     The Spark Connect endpoint embedded on the Spark Server receives and translates unresolved logical plans into Spark’s logical plan operators. This is similar to parsing a SQL query, where attributes and relations are parsed and an initial parse plan is built. From there, the standard Spark execution process kicks in, ensuring that Spark Connect leverages all of Spark’s optimizations and enhancements. Results are streamed back to the client through gRPC as Apache Arrow-encoded row batches.    ![Spark Connect communication](img/spark-connect-communication.png ""Spark Connect communication"")   Operational benefits of Spark Connect =======================================   With this new architecture, Spark Connect mitigates several multi-tenant operational issues:    **Stability**   : Applications that use too much memory will now only impact their own environment as they can run in their own processes. Users can define their own dependencies on the client and don’t need to worry about potential conflicts with the Spark driver.    **Upgradability**   : The Spark driver can now seamlessly be upgraded independently of applications, for example to benefit from performance improvements and security fixes. This means applications can be forward-compatible, as long as the server-side RPC definitions are designed to be backwards compatible.    **Debuggability and observability**   : Spark Connect enables interactive debugging during development directly from your favorite IDE. Similarly, applications can be monitored using the application’s framework native metrics and logging libraries.     How to use Spark Connect ==========================   Starting with Spark 3.4, Spark Connect is available and supports PySpark and Scala applications. We will walk through how to run an Apache Spark server with Spark Connect and connect to it from a client application using the Spark Connect client library.     Download and start Spark server with Spark Connect ----------------------------------------------------   First, download Spark from the  [Download Apache Spark](https://spark.apache.org/downloads.html)   page. Spark Connect was introduced in Apache Spark version 3.4 so make sure you choose 3.4.0 or newer in the release drop down at the top of the page. Then choose your package type, typically “Pre-built for Apache Hadoop 3.3 and later”, and click the link to download.     Now extract the Spark package you just downloaded on your computer, for example:    ``` tar -xvf spark-3.4.0-bin-hadoop3.tgz ```   In a terminal window, go to the  `spark`   folder in the location where you extracted Spark before and run the  `start-connect-server.sh`   script to start Spark server with Spark Connect, like in this example:    ``` ./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.4.0 ```   Note that we include a Spark Connect package (  `spark-connect_2.12:3.4.0`   ), when starting Spark server. This is required to use Spark Connect. Make sure to use the same version of the package as the Spark version you downloaded previously. In this example, Spark 3.4.0 with Scala 2.12.     Now Spark server is running and ready to accept Spark Connect sessions from client applications. In the next section we will walk through how to use Spark Connect when writing client applications.     Use Spark Connect in client applications ------------------------------------------   When creating a Spark session, you can specify that you want to use Spark Connect and there are a few ways to do that outlined as follows.     If you do not use one of the mechanisms outlined here, your Spark session will work just like before, without leveraging Spark Connect, and your application code will run on the Spark driver node.    ###   Set SPARK\_REMOTE environment variable   If you set the  `SPARK_REMOTE`   environment variable on the client machine where your Spark client application is running and create a new Spark Session as in the following example, the session will be a Spark Connect session. With this approach, there is no code change needed to start using Spark Connect.     In a terminal window, set the  `SPARK_REMOTE`   environment variable to point to the local Spark server you started previously on your computer:    ``` export SPARK\_REMOTE=""sc://localhost"" ```   And start the Spark shell as usual:    ``` ./bin/pyspark ```   The PySpark shell is now connected to Spark using Spark Connect as indicated in the welcome message:    ``` Client connected to the Spark Connect server at localhost ```   And if you write your own program, create a Spark session as shown in this example:    ``` from pyspark.sql import SparkSession spark = SparkSession.builder.getOrCreate() ```   This will create a Spark Connect session from your application by reading the  `SPARK_REMOTE`   environment variable we set previously.    ###   Specify Spark Connect when creating Spark session   You can also specify that you want to use Spark Connect explicitly when you create a Spark session.     For example, you can launch the PySpark shell with Spark Connect as illustrated here.     To launch the PySpark shell with Spark Connect, simply include the  `remote`   parameter and specify the location of your Spark server. We are using  `localhost`   in this example to connect to the local Spark server we started previously:    ``` ./bin/pyspark --remote ""sc://localhost"" ```   And you will notice that the PySpark shell welcome message tells you that you have connected to Spark using Spark Connect:    ``` Client connected to the Spark Connect server at localhost ```   You can also check the Spark session type. If it includes  `.connect.`   you are using Spark Connect as shown in this example:    ``` SparkSession available as 'spark'. >>> type(spark) <class 'pyspark.sql.connect.session.SparkSession'> ```   Now you can run PySpark code in the shell to see Spark Connect in action:    ``` >>> columns = [""id"",""name""] >>> data = [(1,""Sarah""),(2,""Maria"")] >>> df = spark.createDataFrame(data).toDF(\*columns) >>> df.show() +---+-----+ | id| name| +---+-----+ |  1|Sarah| |  2|Maria| +---+-----+ ```   Or, when writing your own code, include the  `remote`   function with a reference to your Spark server when you create a Spark session, as in this example:    ``` from pyspark.sql import SparkSession spark = SparkSession.builder.remote(""sc://localhost"").getOrCreate() ```  ``` import org.apache.spark.sql.SparkSession val spark = SparkSession.builder().remote(""sc://localhost"").build() ```   Client application authentication ===================================   While Spark Connect does not have built-in authentication, it is designed to work seamlessly with your existing authentication infrastructure. Its gRPC HTTP/2 interface allows for the use of authenticating proxies, which makes it possible to secure Spark Connect without having to implement authentication logic in Spark directly.     What is supported in Spark 3.4 ================================  **PySpark**   : In Spark 3.4, Spark Connect supports most PySpark APIs, including  [DataFrame](api/python/reference/pyspark.sql/dataframe.html)   ,  [Functions](api/python/reference/pyspark.sql/functions.html)   , and  [Column](api/python/reference/pyspark.sql/column.html)   . However, some APIs such as  [SparkContext](api/python/reference/api/pyspark.SparkContext.html)   and  [RDD](api/python/reference/api/pyspark.RDD.html)   are not supported. You can check which APIs are currently supported in the  [API reference](api/python/reference/index.html)   documentation. Supported APIs are labeled “Supports Spark Connect” so you can check whether the APIs you are using are available before migrating existing code to Spark Connect.    **Scala**   : In Spark 3.4, Spark Connect supports most Scala APIs, including  [Dataset](api/scala/org/apache/spark/sql/Dataset.html)   ,  [functions](api/scala/org/apache/spark/sql/functions$.html)   , and  [Column](api/scala/org/apache/spark/sql/Column.html)   .     Support for more APIs is planned for upcoming Spark releases.    "	Spark Connect is a decoupled client-server architecture introduced in Apache Spark 3.4 that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, Notebooks, and programming languages. The Spark Connect client library is designed to simplify Spark application development. It is a thin API that can be embedded everywhere: in application servers, IDEs, notebooks, and programming languages. The Spark Connect API builds on Spark’s DataFrame API using unresolved logical plans as a language-agnostic protocol between the client and the Spark driver.	1	1	1	True
13	no_reaction	https://docs.databricks.com/delta/idempotent-writes.html	how do I ensure the idempotency for streaming writes	"    Enable idempotent writes across jobs ======================================     Sometimes a job that writes data to a Delta table is restarted due to various reasons (for example, job encounters a failure). The failed job may or may not have written the data to Delta table before terminating. In the case where the data is written to the Delta table, the restarted job writes the same data to the Delta table which results in duplicate data.       To address this, Delta tables support the following  `DataFrameWriter`  options to make the writes idempotent:     * `txnAppId`  : A unique string that you can pass on each  `DataFrame`  write. For example, this can be the name of the job. * `txnVersion`  : A monotonically increasing number that acts as transaction version. This number needs to be unique for data that is being written to the Delta table(s). For example, this can be the epoch seconds of the instant when the query is attempted for the first time. Any subsequent restarts of the same job needs to have the same value for  `txnVersion`  .     The above combination of options needs to be unique for each new data that is being ingested into the Delta table and the  `txnVersion`  needs to be higher than the last data that was ingested into the Delta table. For example:     * Last successfully written data contains option values as  `dailyETL:23423`  (  `txnAppId:txnVersion`  ). * Next write of data should have  `txnAppId     =     dailyETL`  and  `txnVersion`  as at least  `23424`  (one more than the last written data  `txnVersion`  ). * Any attempt to write data with  `txnAppId     =     dailyETL`  and  `txnVersion`  as  `23422`  or less is ignored because the  `txnVersion`  is less than the last recorded  `txnVersion`  in the table. * Attempt to write data with  `txnAppId:txnVersion`  as  `anotherETL:23424`  is successful writing data to the table as it contains a different  `txnAppId`  compared to the same option value in last ingested data.      Warning       This solution assumes that the data being written to Delta table(s) in multiple retries of the job is the same. If a write attempt to a Delta table succeeds but due to some downstream failure there is a second write attempt with same txn options but different data, then that second write attempt will be ignored. This can cause unexpected results.        See the following code for an example:         ``` app\_id = ... # A unique string that is used as an application ID. version = ... # A monotonically increasing number that acts as transaction version.  dataFrame.write.option(""txnVersion"", version).option(""txnAppId"", app\_id).save(...)  ```       ``` val appId = ... // A unique string that is used as an application ID. version = ... // A monotonically increasing number that acts as transaction version.  dataFrame.write.option(""txnVersion"", version).option(""txnAppId"", appId).save(...)  ```                                                       "	To ensure idempotency for streaming writes, Delta tables support the `txnAppId` and `txnVersion` options in the `DataFrameWriter`. `txnAppId` is a unique string that you can pass on each `DataFrame` write, and `txnVersion` is a monotonically increasing number that acts as a transaction version. These options need to be unique for each new data that is being ingested into the Delta table, and the `txnVersion` needs to be higher than the last data that was ingested into the Delta table.	1	1	1	True
15	no_reaction	https://docs.databricks.com/release-notes/unity-catalog/20220825.html	How do I add a catalog to Unity Catalog?	"    Create and manage catalogs ============================     This article shows how to create and manage catalogs in Unity Catalog. A catalog contains  [schemas (databases)](create-schemas.html)  , and a schema contains tables and views.        Requirements --------------   * You must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore * Your Databricks account must be on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * You must have a Unity Catalog metastore  [linked to the workspace](create-metastore.html)  where you perform the catalog creation. * The  [compute resource](compute.html)  that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode.         Create a catalog ------------------     To create a catalog, you can use Data Explorer or a SQL command.       1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the  **Create Catalog**   button. 4. (Optional) Specify the location where data for  [managed tables](create-tables.html#managed-table)  in the catalog will be stored.       Specify a location here only if you do not want managed tables in this catalog to be stored in the default root storage location that was configured for the metastore. See  [Create a Unity Catalog metastore](create-metastore.html)  .       The path that you specify must be defined in an external location configuration, and you must have the  `CREATE     MANAGED     STORAGE`  privilege on that external location. You can also use a subpath of that path. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  . 5. Assign permissions for your catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  . 6. Click  **Save**   .     1. Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : Optional description or other comment.   ``` CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name>  [ MANAGED LOCATION '<location\_path>' ]  [ COMMENT <comment> ];  ```      For example, to create a catalog named  `example`  :        ``` CREATE CATALOG IF NOT EXISTS example;  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ```      For parameter descriptions, see  [CREATE CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-create-catalog.html)  . 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` library(SparkR)  sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` library(SparkR)  sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .       When you create a catalog, two schemas (databases) are automatically created:  `default`  and  `information\_schema`  .       You can also create a catalog by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog)   . You can retrieve information about catalogs by using  [databricks\_catalogs](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/catalogs)   .      ###   Next steps     Now you can  [add schemas (databases) to your catalog](create-schemas.html)  .          Delete a catalog ------------------     To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.         You must delete all schemas in the catalog except  `information\_schema`  before you can delete a catalog. This includes the auto-created  `default`  schema.     1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. In the  **Data**   pane, on the left, click the catalog you want to delete. 4. In the detail pane, click the three-dot menu to the left of the  **Create database**   button and select  **Delete**   . 5. On the  **Delete catalog**   dialog, click  **Delete**   .       Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` DROP CATALOG [ IF EXISTS ] <catalog\_name> [ RESTRICT | CASCADE ]  ```      For example, to delete a catalog named  `vaccine`  and its schemas:        ``` DROP CATALOG vaccine CASCADE  ```        Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` spark.sql(""DROP CATALOG [ IF EXISTS ] <catalog\_name>"	To add a catalog to Unity Catalog, you must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore. Your Databricks account must be on the Premium plan and above. You must have a Unity Catalog metastore linked to the workspace where you perform the catalog creation. The compute resource that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode. To create a catalog, you can use Data Explorer or a SQL command. To create a catalog using SQL command, you can run the following SQL command in a notebook or Databricks SQL editor: CREATE CATALOG [ IF NOT EXISTS ] <catalog_name> [ MANAGED LOCATION '<location_path>' ] [ COMMENT <comment> ]. For example, to create a catalog named `example`, you can run the following SQL command: CREATE CATALOG IF NOT EXISTS example.	1	1	1	True
19	thumb_up	https://kb.databricks.com/clusters/cluster-spark-config-not-applied	how do I optimize my spark cluster?	Optimization recommendations on Databricks ============================================     Databricks provides many optimizations supporting a variety of workloads on the lakehouse, ranging from large-scale ETL processing to ad-hoc, interactive queries. Many of these optimizations take place automatically. You get their benefits simply by using Databricks. Additionally, most Databricks Runtime features require Delta Lake, the default storage layer used to create tables in Databricks.       Databricks configures default values that optimize most workloads. But, in some cases, changing configuration settings improves performance.            Databricks Runtime performance enhancements ---------------------------------------------      Note       Use the latest Databricks Runtime to leverage the newest performance enhancements. All behaviors documented here are enabled by default in Databricks Runtime 10.4 LTS and above.      * [Disk caching](disk-cache.html)  accelerates repeated reads against Parquet data files by loading data to disk volumes attached to compute clusters. * [Dynamic file pruning](dynamic-file-pruning.html)  improves query performance by skipping directories that do not contain data files that match query predicates. * [Low shuffle merge](low-shuffle-merge.html)  reduces the number of data files rewritten by  `MERGE`  operations and reduces the need to recaculate  `ZORDER`  clusters. * Apache Spark 3.0 introduced  [adaptive query execution](aqe.html)  , which provides enhanced performance for many operations.       Databricks recommendations for enhanced performance -----------------------------------------------------   * You can  [clone](../delta/clone.html)  tables on Databricks to make deep or shallow copies of source datasets. * The  [cost-based optimizer](cbo.html)  accelerates query performance by leveraging table statistics. * You can  [auto optimize](auto-optimize.html)  Delta tables using optimized writes and automatic file compaction; this is especially useful for long-running Structured Streaming jobs. * You can use Spark SQL to interact with  [semi-structured JSON data](semi-structured.html)  without parsing strings. * [Higher order functions](higher-order-lambda-functions.html)  provide built-in, optimized performance for many operations that do not have common Spark operators. Higher order functions provide a performance benefit over user defined functions. * Databricks provides a number of built-in operators and special syntax for working with  [complex data types](complex-types.html)  , including arrays, structs, and JSON strings. * You can manually tune settings for joins that include  [ranges](range-join.html)  or contain data with substanial  [skew](skew-join.html)  .       Opt-in behaviors ------------------   * Databricks provides a write serializable isolation guarantee by default; changing the  [isolation level](isolation-level.html)  to serializable can reduce throughput for concurrent operations, but might be necessary when read serializability is required. * You can use  [bloom filter indexes](bloom-filters.html)  to reduce the likelihood of scanning data files that don’t contain records matching a given condition.	To optimize your Spark cluster, you can use the default optimizations provided by Databricks, which are automatically applied when using Databricks. Additionally, you can manually change configuration settings to improve performance. Databricks also provides a number of recommendations for enhanced performance, such as using disk caching, dynamic file pruning, and adaptive query execution. You can also clone tables, use the cost-based optimizer, auto-optimize Delta tables, use Spark SQL to interact with semi-structured JSON data, use higher order functions, and work with complex data types. Finally, you can opt-in to behaviors such as changing the isolation level to serializable and using bloom filter indexes to reduce the likelihood of scanning data files that don’t contain records matching a given condition.	1	1	1	True
21	thumb_up	https://docs.databricks.com/lakehouse/medallion.html	explain medalian architecture on databricks	What is the medallion lakehouse architecture? ===============================================     The medallion architecture describes a series of data layers that denote the quality of data stored in the lakehouse. Databricks recommends taking a multi-layered approach to building a single source of truth for enterprise data products. This architecture guarantees atomicity, consistency, isolation, and durability as data passes through multiple layers of validations and transformations before being stored in a layout optimized for efficient analytics. The terms  [bronze](#bronze)  (raw),  [silver](#silver)  (validated), and  [gold](#gold)  (enriched) describe the quality of the data in each of these layers.       It is important to note that this medallion architecture does not replace other dimensional modeling techniques. Schemas and tables within each layer can take on a variety of forms and degrees of normalization depending on the frequency and nature of data updates and the downstream use cases for the data.       Organizations can leverage the Databricks Lakehouse to create and maintain validated datasets accessible throughout the company. Adopting an organizational mindset focused on curating data-as-products is a key step in successfully building a data lakehouse.          Ingest raw data to the bronze layer -------------------------------------     The bronze layer contains unvalidated data. Data ingested in the bronze layer typically:     * Maintains the raw state of the data source. * Is appended incrementally and grows over time. * Can be any combination of streaming and batch transactions.     Retaining the full, unprocessed history of each dataset in an efficient storage format provides the ability to recreate any state of a given data system.       Additional metadata (such as source file names or recording the time data was processed) may be added to data on ingest for enhanced discoverability, description of the state of the source dataset, and optimized performance in downstream applications.           Validate and deduplicate data in the silver layer ---------------------------------------------------     Recall that while the bronze layer contains the entire data history in a nearly raw state, the silver layer represents a validated, enriched version of our data that can be trusted for downstream analytics.       While Databricks believes strongly in the lakehouse vision driven by bronze, silver, and gold tables, simply implementing a silver layer efficiently will immediately unlock many of the potential benefits of the lakehouse.       For any data pipeline, the silver layer may contain more than one table.           Power analytics with the gold layer -------------------------------------     This gold data is often highly refined and aggregated, containing data that powers analytics, machine learning, and production applications. While all tables in the lakehouse should serve an important purpose, gold tables represent data that has been transformed into knowledge, rather than just information.       Analysts largely rely on gold tables for their core responsibilities, and data shared with a customer would rarely be stored outside this level.       Updates to these tables are completed as part of regularly scheduled production workloads, which helps control costs and allows service level agreements (SLAs) for data freshness to be established.       While the lakehouse doesn’t have the same deadlock issues that you may encounter in a enterprise data warehouse, gold tables are often stored in a separate storage container to help avoid cloud limits on data requests.       In general, because aggregations, joins, and filtering are handled before data is written to the gold layer, users should see low latency query performance on data in gold tables.	The medallion architecture is a multi-layered approach to building a single source of truth for enterprise data products. It guarantees atomicity, consistency, isolation, and durability as data passes through multiple layers of validations and transformations before being stored in a layout optimized for efficient analytics. The terms bronze (raw), silver (validated), and gold (enriched) describe the quality of the data in each of these layers. The bronze layer contains unvalidated data, the silver layer represents a validated, enriched version of the data, and the gold layer contains highly refined and aggregated data that powers analytics, machine learning, and production applications.	1	1	1	True
25	no_reaction	https://docs.databricks.com/introduction/index.html	What is databricks	What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used to process, store, clean, share, analyze, model, and monetize datasets with solutions from BI to machine learning. The Databricks platform provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks can be interacted with programmatically with REST API, CLI, and Terraform. It works with AWS by configuring a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control. Databricks is used for various use cases such as building an enterprise data lakehouse, ETL and data engineering, machine learning, AI, and data science, data warehousing, analytics, and BI, data governance and secure data sharing, DevOps, CI/CD, and task orchestration, and real-time and streaming analytics.	1	1	1	True
26	no_reaction	https://docs.databricks.com/workflows/jobs/jobs.html	Can I create a job?	"    Create, run, and manage Databricks Jobs =========================================     This article details how to create, edit, run, and monitor Databricks Jobs using the Jobs UI. To learn about using the Databricks CLI to create and run jobs, see  [Jobs CLI](../../dev-tools/cli/jobs-cli.html)  . To learn about using the Jobs API, see  [Jobs API 2.1](../../dev-tools/api/latest/jobs.html)  .       To create your first workflow with a Databricks job, see the  [quickstart](jobs-quickstart.html)  .        Important     * You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. * A workspace is limited to 1000 concurrent task runs. A  `429     Too     Many     Requests`  response is returned when you request a run that cannot start immediately. * The number of jobs a workspace can create in an hour is limited to 10000 (includes “runs submit”). This limit also affects jobs created by the REST API and notebook workflows.         Create a job --------------   1. Do one of the following:      * Click   ![Jobs Icon](../../_images/jobs-icon.png)  **Workflows**    in the sidebar and click   ![Create Job Button](../../_images/create-job.png)   .  * In the sidebar, click   ![New Icon](../../_images/create-icon.png)  **New**    and select   **Job**    .  The  **Tasks**   tab appears with the create task dialog.      ![Create task screen](../../_images/create-task.png) 2. Replace  **Add a name for your job…**   with your job name. 3. Enter a name for the task in the  **Task name**   field. 4. In the  **Type**   dropdown menu, select the type of task to run. See  [Task type options](#task-types)  . 5. Configure the cluster where the task runs. In the  **Cluster**   dropdown menu, select either  **New job cluster**   or  **Existing All-Purpose Clusters**   .      * **New Job Cluster**    : Click   **Edit**    in the   **Cluster**    dropdown menu and complete the   [cluster configuration](../../clusters/configure.html)   .  * **Existing All-Purpose Cluster**    : Select an existing cluster in the   **Cluster**    dropdown menu. To open the cluster in a new page, click the   ![External Link](../../_images/external-link.png)   icon to the right of the cluster name and description.  To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  . 6. To add dependent libraries, click  **+ Add**   next to  **Dependent libraries**   . See  [Dependent libraries](#task-config-dependent-libraries)  . 7. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters.      * **Notebook**    : Click   **Add**    and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the   [Run a job with different parameters](#job-run-with-different-params)   option. Parameters set the value of the   [notebook widget](../../notebooks/widgets.html)   specified by the key of the parameter. Use   [task parameter variables](#parameter-variables)   to pass a limited set of dynamic values as part of a parameter value.  * **JAR**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See   [Configure JAR job parameters](#configure-jar-job-parameters)   .  * **Spark Submit**    task: Parameters are specified as a JSON-formatted array of strings. Conforming to the   [Apache Spark spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html)    convention, parameters after the JAR path are passed to the main method of the main class.  * **Python script**    : Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the   [argparse](https://docs.python.org/3/library/argparse.html)    module in Python.  * **Python Wheel**    : In the   **Parameters**    dropdown menu, select   **Positional arguments**    to enter parameters as a JSON-formatted array of strings, or select   **Keyword arguments > Add**    to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. 8. To optionally receive notifications for task start, success, or failure, click  **+ Add**   next to  **Emails**   . Failure notifications are sent on initial task failure and any subsequent retries. 9. To optionally configure a retry policy for the task, click  **+ Add**   next to  **Retries**   . See  [Retries](#retry-policies)  . 10. To optionally configure a timeout for the task, click  **+ Add**   next to  **Timeout in seconds**   . See  [Timeout](#timeout-setting)  . 11. Click  **Create**   .     After creating the first task, you can configure job-level settings such as notifications, job triggers, and permissions. See  [Edit a job](#job-edit)  .       To add another task, click  ![Add Task Button](../../_images/add-task.png)  in the DAG view. A shared cluster option is provided if you have configured a  **New Job Cluster**   for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see  [Cluster configuration tips](#cluster-config-tips)  .           Task type options -------------------     The following are the task types you can add to your Databricks job and available options for the different task types:     * **Notebook**   : In the  **Source**   dropdown menu, select a location for the notebook; either  **Workspace**   for a notebook located in a Databricks workspace folder or  **Git provider**   for a notebook located in a remote Git repository.      **Workspace**   : Use the file browser to find the notebook, click the notebook name, and click  **Confirm**   .      **Git provider**   : Click  **Edit**   and enter the Git repository information. See  [Use version controlled notebooks in a Databricks job](how-to/use-repos.html)  . * **JAR**   : Specify the  **Main class**   . Use the fully qualified name of the class containing the main method, for example,  `org.apache.spark.examples.SparkPi`  . Then click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. One of these libraries must contain the main class.       To learn more about JAR tasks, see  [JAR jobs](#jar-jobs)  . * **Spark Submit**   : In the  **Parameters**   text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the  `DFSReadWriteTest`  from the Apache Spark examples:        ``` [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark\_examples\_2\_12\_3\_1\_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]  ```       Important       There are several limitations for  **spark-submit**   tasks:       + You can run spark-submit tasks only on new clusters.  + Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see   [Cluster autoscaling](../../clusters/configure.html#autoscaling)   .  + Spark-submit does not support   [Databricks Utilities](../../dev-tools/databricks-utils.html)   . To use Databricks Utilities, use JAR tasks instead.  + If you are using a Unity Catalog-enabled cluster, spark-submit is supported only if the cluster uses Single User   [access mode](../../data-governance/unity-catalog/compute.html#access-mode)   . Shared access mode is not supported. * **Python script**   : In the  **Source**   drop-down, select a location for the Python script, either  **Workspace**   for a script in the local workspace, or  **DBFS / S3**   for a script located on DBFS or cloud storage. In the  **Path**   textbox, enter the path to the Python script:      **Workspace**   : In the  **Select Python File**   dialog, browse to the Python script and click  **Confirm**   . Your script must be in a  [Databricks repo](../../files/workspace.html)  .      **DBFS**   : Enter the URI of a Python script on DBFS or cloud storage; for example,  `dbfs:/FileStore/myscript.py`  . * **Delta Live Tables Pipeline**   : In the  **Pipeline**   dropdown menu, select an existing  [Delta Live Tables](../../delta-live-tables/index.html)  pipeline.        Important       You can use only triggered pipelines with the  **Pipeline**   task. Continuous pipelines are not supported as a job task. To learn more about triggered and continuous pipelines, see  [Continuous vs. triggered pipeline execution](../../delta-live-tables/updates.html#continuous-triggered)  . * **Python Wheel**   : In the  **Package name**   text box, enter the package to import, for example,  `myWheel-1.0-py2.py3-none-any.whl`  . In the  **Entry Point**   text box, enter the function to call when starting the wheel. Click  **Add**   under  **Dependent Libraries**   to add libraries required to run the task. * **SQL**   : In the  **SQL task**   dropdown menu, select  **Query**   ,  **Dashboard**   , or  **Alert**   .        Note       + The   **SQL**    task is in   [Public Preview](../../release-notes/release-types.html)   .  + The   **SQL**    task requires Databricks SQL and a   [serverless or pro SQL warehouse](../../sql/index.html#warehouse-types)   .   **Query**   : In the  **SQL query**   dropdown menu, select the query to execute when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Dashboard**   : In the  **SQL dashboard**   dropdown menu, select a dashboard to be updated when the task runs. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task.      **Alert**   : In the  **SQL alert**   dropdown menu, select an alert to trigger for evaluation. In the  **SQL warehouse**   dropdown menu, select a serverless or pro SQL warehouse to run the task. * **dbt**   : See  [Use dbt transformations in a Databricks job](how-to/use-dbt-in-workflows.html)  for a detailed example"	"Yes, you can create a job in Databricks. To create a job, you can use the Jobs UI by clicking on the ""Workflows"" icon in the sidebar and then clicking on the ""Create Job"" button. Alternatively, you can click on ""New"" in the sidebar and select ""Job"". Then, you can follow the steps to configure the task, including selecting the task type, configuring the cluster, adding dependent libraries, passing parameters, and configuring notifications, retries, and timeouts. Finally, you can click on ""Create"" to create the job."	1	1	1	True
28	no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.to_delta.html	write dataframe to delta table in unity	"   pyspark.pandas.DataFrame.to\_delta  [¶](#pyspark-pandas-dataframe-to-delta ""Permalink to this headline"") ==========================================================================================================  `DataFrame.`  `to_delta`    (   *path     :     str*  ,  *mode     :     str     =     'w'*  ,  *partition\_cols     :     Union[str, List[str], None]     =     None*  ,  *index\_col     :     Union[str, List[str], None]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → None  [[source]](../../../_modules/pyspark/pandas/frame.html#DataFrame.to_delta) [¶](#pyspark.pandas.DataFrame.to_delta ""Permalink to this definition"")    Write the DataFrame out as a Delta Lake table.     Parameters    **path**    str, required     Path to write to.    **mode**    str     Python write mode, default ‘w’.     Note     mode can accept the strings for Spark writing mode. Such as ‘append’, ‘overwrite’, ‘ignore’, ‘error’, ‘errorifexists’.    * ‘append’ (equivalent to ‘a’): Append the new data to existing data. * ‘overwrite’ (equivalent to ‘w’): Overwrite existing data. * ‘ignore’: Silently ignore this operation if data already exists. * ‘error’ or ‘errorifexists’: Throw an exception if data already exists.  **partition\_cols**    str or list of str, optional, default None     Names of partitioning columns    **index\_col: str or list of str, optional, default: None**    Column names to be used in Spark to represent pandas-on-Spark’s index. The index name in pandas-on-Spark is ignored. By default the index is always lost.    **options**    dict     All other options passed directly into Delta Lake.     See also    [`read\_delta`](pyspark.pandas.read_delta.html#pyspark.pandas.read_delta ""pyspark.pandas.read_delta"")  [`DataFrame.to\_parquet`](pyspark.pandas.DataFrame.to_parquet.html#pyspark.pandas.DataFrame.to_parquet ""pyspark.pandas.DataFrame.to_parquet"")  [`DataFrame.to\_table`](pyspark.pandas.DataFrame.to_table.html#pyspark.pandas.DataFrame.to_table ""pyspark.pandas.DataFrame.to_table"")  [`DataFrame.to\_spark\_io`](pyspark.pandas.DataFrame.to_spark_io.html#pyspark.pandas.DataFrame.to_spark_io ""pyspark.pandas.DataFrame.to_spark_io"")   Examples    ``` >>> df = ps.DataFrame(dict( ...    date=list(pd.date\_range('2012-1-1 12:00:00', periods=3, freq='M')), ...    country=KR', 'US', 'JP, ...    code=[1, 2 ,3]), columns=date', 'country', 'code) >>> df  date country code 0 2012-01-31 12:00:00 KR 1 1 2012-02-29 12:00:00 US 2 2 2012-03-31 12:00:00 JP 3  ```   Create a new Delta Lake table, partitioned by one column:    ``` >>> df.to\_delta('%s/to\_delta/foo' % path, partition\_cols='date')    ```   Partitioned by two columns:    ``` >>> df.to\_delta('%s/to\_delta/bar' % path, ...             partition\_cols=date', 'country)    ```   Overwrite an existing table’s partitions, using the ‘replaceWhere’ capability in Delta:    ``` >>> df.to\_delta('%s/to\_delta/bar' % path, ...             mode='overwrite', replaceWhere='date >= ""2012-01-01""')    ```  [pyspark.pandas.read\_delta](pyspark.pandas.read_delta.html ""previous page"")  [pyspark.pandas.read\_parquet](pyspark.pandas.read_parquet.html ""next page"")   "	The `DataFrame.to_delta` method in PySpark writes the DataFrame out as a Delta Lake table. The required parameter is the path to write to, and there are optional parameters for the write mode, partitioning columns, and index columns. The method also accepts other options passed directly into Delta Lake.	1	1	1	True
29	no_reaction	https://docs.databricks.com/administration-guide/account-settings-e2/private-access-settings.html	How to set up private link?	Manage private access settings ================================         This article describes a process that is available only for  [accounts on the E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.       This article discusses how to create private access settings objects, which are a required object as part of enabling  [AWS PrivateLink](https://aws.amazon.com/privatelink)   . This article does not contain all the information necessary to configure PrivateLink for your workspace. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see  [Enable AWS PrivateLink](../cloud-configurations/aws/privatelink.html)  .       The following related sections discuss updating existing network and configuration objects:     * [Update a running or failed workspace](../workspace/update-workspace.html)  . * [Updates of existing PrivateLink configuration objects](../cloud-configurations/aws/privatelink.html#update-related)  .      What is a private access settings object? -------------------------------------------     A private access settings object is a Databricks object that describes a workspace’s PrivateLink connectivity. Create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region.       This object serves several purposes:     * It expresses your intent to use AWS PrivateLink with your workspace. * It controls your settings for the front-end use case of AWS PrivateLink for public network access. * It controls which VPC endpoints are permitted to access your workspace.     Create a private access settings object using the account console or the  [Account API 2.0](../../dev-tools/api/latest/account.html)  . You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you  *must*   attach a private access settings object to the workspace during workspace creation.           Create a private access settings object -----------------------------------------      Note       These instructions show you how to create the private access object from the  **Cloud resources**   page in the account console before you create a new workspace. You can also create the private access settings in a similar way as part of the flow of creating a new workspace and choosing  **Add a new private access object**   from the picker instead of choosing an existing object. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .      1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. Click  **Add private access settings**   .      ![private access settings object](../../_images/privatelink-vpc-pas.png) 5. Enter a name for your new private access settings object. 6. For the region, be sure to match the region of your workspace as this is not validated immediately and workspace deployment will fail if it does not match. It is validated only during the actual creation of the workspace. 7. Set the  **Public access enabled**   field, which configures public access to the front-end connection (the web application and REST APIs) for your workspace.      * If set to   **False**    (the default), the front-end connection can be accessed only using PrivateLink connectivity and not from the public internet. When public access is disabled, the   [IP access lists for workspaces](../../security/network/ip-access-list-workspace.html)   feature is unsupported.  * If set to   **True**    , the front-end connection can be accessed either from PrivateLink connectivity or from the public internet. Any IP access lists only limit connections from the public internet but not traffic through the PrivateLink connection. 8. Set the  **Private Access Level**   field to the value that best represents which VPC endpoints to allow for your workspace.      * Set to   **Account**    to limit connections to those VPC endpoints that are registered in your Databricks account.  * Set to   **Endpoint**    to limit connections to an explicit set of VPC endpoints, which you can enter in a field that appears. It lets you select VPC endpoint registrations that you’ve already created. Be sure to include your   *front-end*    VPC endpoint registration if you created one. 9. Click  **Add**   .         Update a private access settings object -----------------------------------------     To update fields on a private access object:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Update**   . 5. Change any fields. For guidance on specific fields, see  [Create a private access settings object](#create)  .        Note       The private access access level  `ANY`  is deprecated. If the object previously had this value and you use the account console to update the private access settings for any fields, you must change the private access level to a non-deprecated value. To make changes to other fields without changing the  `ANY`  private access level at this time, use the  [Account API](../../dev-tools/api/latest/account.html)  . See  [AWS PrivateLink private access level ANY is deprecated](../../release-notes/product/2022/august.html#privatelink-private-access-level-any-deprecated)  . 6. Click  **Update private access setting**   .         Delete a private access settings object -----------------------------------------     Private access settings object cannot be edited after creation. If the configuration has incorrect data or if you no longer need it for any workspaces, delete it:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Delete**   . 5. In the confirmation dialog, click  **Confirm Delete**   .	To set up private link, you need to create a private access settings object, which is a required object as part of enabling AWS PrivateLink. This object controls your settings for the front-end use case of AWS PrivateLink for public network access and which VPC endpoints are permitted to access your workspace. You can create a private access settings object using the account console or the Account API 2.0.	1	1	1	True
30	no_reaction	https://docs.databricks.com/clusters/index.html	What is a cluster	Introduction to workspace assets ==================================     This article provides a high-level introduction to Databricks workspace assets. You can view and organize all workspace assets in the workspace browser across personas. To create a workspace asset, you must use the appropriate persona’s workspace browser.        Note       New Databricks SQL queries, dashboards, and alerts are visible in the workspace browser. To view and organize existing queries, dashboards, and articles in the workspace browser, users (or admins) must migrate them into the workspace browser. For information about migration, see  [Migrating existing queries, dashboards, and alerts](../sql/user/workspace-browser/index.html#migrating-existing-queries-dashboards-and-alerts)  .           Clusters ----------     Databricks Data Science & Engineering and Databricks Machine Learning clusters provide a unified platform for various use cases such as running production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning. A cluster is a type of Databricks  *compute resource*   . Other compute resource types include Databricks  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  .       For detailed information on managing and using clusters, see  [Clusters](../clusters/index.html)  .           Notebooks -----------     A notebook is a web-based interface to documents containing a series of runnable cells (commands) that operate on files and  [tables](../lakehouse/data-objects.html#table)  ,  [visualizations](../visualizations/index.html)  , and narrative text. Commands can be run in sequence, referring to the output of one or more previously run commands.       Notebooks are one mechanism for running code in Databricks. The other mechanism is  [jobs](../workflows/jobs/jobs.html)  .       For detailed information on managing and using notebooks, see  [Introduction to Databricks notebooks](../notebooks/index.html)  .           Jobs ------     Jobs are one mechanism for running code in Databricks. The other mechanism is  [notebooks](#ws-notebooks)  .       For detailed information on managing and using jobs, see  [Create, run, and manage Databricks Jobs](../workflows/jobs/jobs.html)  .           Libraries -----------     A library makes third-party or locally-built code available to notebooks and jobs running on your clusters.       For detailed information on managing and using libraries, see  [Libraries](../libraries/index.html)  .           Data ------     You can import data into a distributed file system mounted into a Databricks workspace and work with it in Databricks notebooks and clusters. You can also use a wide variety of Apache Spark data sources to access data.       For detailed information on loading data, see  [Load data into the Databricks Lakehouse](../ingestion/index.html)  .           Files -------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .        In Databricks Runtime 11.2 and above, you can create and use arbitrary files in the Databricks workspace. Files can be any file type. Common examples include:     * `.py`  files used in custom modules. * `.md`  files, such as  `README.md`  . * `.csv`  or other small data files. * `.txt`  files. * Log files.     For detailed information on using files, see  [How to work with files on Databricks](../files/index.html)  . For information about how to use files to modularize your code as you develop with Databricks notebooks, see  [Share code between Databricks notebooks](../notebooks/share-code.html)        Repos -------     Repos are Databricks folders whose contents are co-versioned together by syncing them to a remote Git repository. Using a Databricks repo, you can develop notebooks in Databricks and use a remote Git repository for collaboration and version control.       For detailed information on using repos, see  [Git integration with Databricks Repos](../repos/index.html)  .           Models --------    *Model*   refers to a model registered in MLflow Model Registry. Model Registry is a centralized model store that enables you to manage the full lifecycle of MLflow models. It provides chronological model lineage, model versioning, stage transitions, and model and model version annotations and descriptions.       For detailed information on managing and using models, see  [MLflow Model Registry on Databricks](../mlflow/model-registry.html)  .           Experiments -------------     An MLflow experiment is the primary unit of organization and access control for MLflow machine learning model training runs; all MLflow runs belong to an experiment. Each experiment lets you visualize, search, and compare runs, as well as download run artifacts or metadata for analysis in other tools.       For detailed information on managing and using experiments, see  [Organize training runs with MLflow experiments](../mlflow/tracking.html#mlflow-experiments)  .           Queries ---------     Queries are SQL statements that allow you to interact with your data. For more information, see  [Queries](../sql/user/queries/index.html)  .           Dashboards ------------     Dashboards are presentations of query visualizations and commentary. For more information, see  [Databricks SQL dashboards](../sql/user/dashboards/index.html)  .           Alerts --------     Alerts are notifications that a field returned by a query has reached a threshold. For more information, see  [Alerts](../sql/user/alerts/index.html)  .	A cluster is a type of Databricks compute resource that provides a unified platform for various use cases such as running production ETL pipelines, streaming analytics, ad-hoc analytics, and machine learning.	1	1	1	True
32	thumb_up	https://spark.apache.org/docs/latest/sql-error-conditions-wrong-num-args-error-class.html	How do I handle errors in SQL Code?	###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) * [Error Conditions](sql-error-conditions.html) + [SQLSTATE Codes](sql-error-conditions-sqlstates.html) + [CONNECT error class](sql-error-conditions-connect-error-class.html) + [DATATYPE\_MISMATCH error class](sql-error-conditions-datatype-mismatch-error-class.html) + [INCOMPATIBLE\_DATA\_TO\_TABLE error class](sql-error-conditions-incompatible-data-to-table-error-class.html) + [INCOMPLETE\_TYPE\_DEFINITION error class](sql-error-conditions-incomplete-type-definition-error-class.html) + [INCONSISTENT\_BEHAVIOR\_CROSS\_VERSION error class](sql-error-conditions-inconsistent-behavior-cross-version-error-class.html) + [INVALID\_FORMAT error class](sql-error-conditions-invalid-format-error-class.html) + [INVALID\_OPTIONS error class](sql-error-conditions-invalid-options-error-class.html) + [INVALID\_PARAMETER\_VALUE error class](sql-error-conditions-invalid-parameter-value-error-class.html) + [INVALID\_SCHEMA error class](sql-error-conditions-invalid-schema-error-class.html) + [INVALID\_SUBQUERY\_EXPRESSION error class](sql-error-conditions-invalid-subquery-expression-error-class.html) + [NOT\_NULL\_CONSTRAINT\_VIOLATION error class](sql-error-conditions-not-null-constraint-violation-error-class.html) + [UNRESOLVED\_COLUMN error class](sql-error-conditions-unresolved-column-error-class.html) + [UNRESOLVED\_FIELD error class](sql-error-conditions-unresolved-field-error-class.html) + [UNRESOLVED\_MAP\_KEY error class](sql-error-conditions-unresolved-map-key-error-class.html) + [UNSUPPORTED\_DESERIALIZER error class](sql-error-conditions-unsupported-deserializer-error-class.html) + [UNSUPPORTED\_FEATURE error class](sql-error-conditions-unsupported-feature-error-class.html) + [UNSUPPORTED\_GENERATOR error class](sql-error-conditions-unsupported-generator-error-class.html) + [UNSUPPORTED\_SAVE\_MODE error class](sql-error-conditions-unsupported-save-mode-error-class.html) + [UNSUPPORTED\_SUBQUERY\_EXPRESSION\_CATEGORY error class](sql-error-conditions-unsupported-subquery-expression-category-error-class.html) + [WRONG\_NUM\_ARGS error class](sql-error-conditions-wrong-num-args-error-class.html)   Error Conditions ==================   This is a list of common, named error conditions returned by Spark SQL.     Also see  [SQLSTATE Codes](sql-error-conditions-sqlstates.html)   .    ###   AMBIGUOUS\_COLUMN\_OR\_FIELD  [SQLSTATE: 42702](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Column or field  `<name>`   is ambiguous and has  `<n>`   matches.    ###   AMBIGUOUS\_LATERAL\_COLUMN\_ALIAS  [SQLSTATE: 42702](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Lateral column alias  `<name>`   is ambiguous and has  `<n>`   matches.    ###   AMBIGUOUS\_REFERENCE  [SQLSTATE: 42704](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Reference  `<name>`   is ambiguous, could be:  `<referenceNames>`   .    ###   AMBIGUOUS\_REFERENCE\_TO\_FIELDS  [SQLSTATE: 42000](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Ambiguous reference to the field  `<field>`   . It appears  `<count>`   times in the schema.    ###   ARITHMETIC\_OVERFLOW  [SQLSTATE: 22003](sql-error-conditions-sqlstates.html#class-22-data-exception)   `<message>`   .  `<alternative>`   If necessary set  `<config>`   to “false” to bypass this error.    ###   CANNOT\_CAST\_DATATYPE  [SQLSTATE: 42846](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Cannot cast  `<sourceType>`   to  `<targetType>`   .    ###   CANNOT\_CONSTRUCT\_PROTOBUF\_DESCRIPTOR   SQLSTATE: none assigned     Error constructing FileDescriptor for  `<descFilePath>`   .    ###   CANNOT\_CONVERT\_PROTOBUF\_FIELD\_TYPE\_TO\_SQL\_TYPE   SQLSTATE: none assigned     Cannot convert Protobuf  `<protobufColumn>`   to SQL  `<sqlColumn>`   because schema is incompatible (protobufType =  `<protobufType>`   , sqlType =  `<sqlType>`   ).    ###   CANNOT\_CONVERT\_PROTOBUF\_MESSAGE\_TYPE\_TO\_SQL\_TYPE   SQLSTATE: none assigned     Unable to convert  `<protobufType>`   of Protobuf to SQL type  `<toType>`   .    ###   CANNOT\_CONVERT\_SQL\_TYPE\_TO\_PROTOBUF\_ENUM\_TYPE   SQLSTATE: none assigned     Cannot convert SQL  `<sqlColumn>`   to Protobuf  `<protobufColumn>`   because  `<data>`   cannot be written since it’s not defined in ENUM  `<enumString>`   .    ###   CANNOT\_CONVERT\_SQL\_TYPE\_TO\_PROTOBUF\_FIELD\_TYPE   SQLSTATE: none assigned     Cannot convert SQL  `<sqlColumn>`   to Protobuf  `<protobufColumn>`   because schema is incompatible (protobufType =  `<protobufType>`   , sqlType =  `<sqlType>`   ).    ###   CANNOT\_DECODE\_URL  [SQLSTATE: 22546](sql-error-conditions-sqlstates.html#class-22-data-exception)    Cannot decode url :  `<url>`   .    ###   CANNOT\_LOAD\_FUNCTION\_CLASS   SQLSTATE: none assigned     Cannot load class  `<className>`   when registering the function  `<functionName>`   , please make sure it is on the classpath.    ###   CANNOT\_LOAD\_PROTOBUF\_CLASS   SQLSTATE: none assigned     Could not load Protobuf class with name  `<protobufClassName>`   .  `<explanation>`   .    ###   CANNOT\_MERGE\_INCOMPATIBLE\_DATA\_TYPE  [SQLSTATE: 42825](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Failed to merge incompatible data types  `<left>`   and  `<right>`   .    ###   CANNOT\_MODIFY\_CONFIG  [SQLSTATE: 46110](sql-error-conditions-sqlstates.html#class-46-java-ddl-1)    Cannot modify the value of the Spark config:  `<key>`   .     See also ‘  `<docroot>`   /sql-migration-guide.html#ddl-statements’.    ###   CANNOT\_PARSE\_DECIMAL  [SQLSTATE: 22018](sql-error-conditions-sqlstates.html#class-22-data-exception)    Cannot parse decimal.    ###   CANNOT\_PARSE\_JSON\_FIELD  [SQLSTATE: 2203G](sql-error-conditions-sqlstates.html#class-22-data-exception)    Cannot parse the field name  `<fieldName>`   and the value  `<fieldValue>`   of the JSON token type  `<jsonType>`   to target Spark data type  `<dataType>`   .    ###   CANNOT\_PARSE\_PROTOBUF\_DESCRIPTOR   SQLSTATE: none assigned     Error parsing file  `<descFilePath>`   descriptor byte[] into Descriptor object.    ###   CANNOT\_PARSE\_TIMESTAMP  [SQLSTATE: 22007](sql-error-conditions-sqlstates.html#class-22-data-exception)   `<message>`   . If necessary set  `<ansiConfig>`   to “false” to bypass this error.    ###   CANNOT\_READ\_FILE\_FOOTER   SQLSTATE: none assigned     Could not read footer for file:  `<file>`   .    ###   CANNOT\_RECOGNIZE\_HIVE\_TYPE  [SQLSTATE: 429BB](sql-error-conditions-sqlstates.html#class-42-syntax-error-or-access-rule-violation)    Cannot recognize hive type string:  `<fieldType>`   , column:  `<fieldName>`   .    ###   CANNOT\_RESTORE\_PERMISSIONS\_FOR\_PATH   SQLSTATE: none assigned     Failed to set permissions on created path  `<path>`   back to  `<permission>`   .    ###   CANNOT\_UP\_CAST\_DATATYPE   SQLSTATE: none assigned     Cannot up cast  `<expression>`   from  `<sourceType>`   to  `<targetType>`   .    `<details>`   ###   CAST\_INVALID\_INPUT  [SQLSTATE: 22018](sql-error-conditions-sqlstates.html#class-22-data-exception)    The value  `<expression>`   of the type  `<sourceType>`   cannot be cast to  `<targetType>`   because it is malformed. Correct the value as per the syntax, or change its target type. Use  `try_cast`	"To handle errors in SQL code, Spark SQL provides a list of common, named error conditions returned by Spark SQL. These error conditions are listed in the ""Error Conditions"" section of the Spark SQL Guide. Some of the error conditions include ""CANNOT_CAST_DATATYPE"", ""CANNOT_MODIFY_CONFIG"", ""CAST_INVALID_INPUT"", and ""ARITHMETIC_OVERFLOW"". For each error condition, Spark SQL provides a SQLSTATE code and a description of the error."	1	1	1	True
34	no_reaction	https://kb.databricks.com/streaming/readstream-is-not-whitelisted	"I get this error when I readStream with UC enabled: ""iDisplayHelper.getStreamName() is not whitelisted on class"". why?"	Title: readStream() is not whitelisted error when running a query  Description: readStream() is not whitelisted error on clusters that have table access control enabled.  Answer: Problem You have table access control (AWS | Azure | GCP) enabled on your cluster. You are trying to run a structured streaming query and get and error message. py4j.security.Py4JSecurityException: Method public org.apache.spark.sql.streaming.DataStreamReader org.apache.spark.sql.SQLContext.readStream() is not whitelisted on class class org.apache.spark.sql.SQLContext Cause Streaming is not supported on clusters that have table access control enabled. Access control allows you to set permissions for data objects on a cluster. It requires user interaction to validate and refresh credentials. Because streaming queries run continuously, it is not supported on clusters with table access control. Solution You should use a cluster that does not have table access control enabled for streaming queries.	"The error message ""iDisplayHelper.getStreamName() is not whitelisted on class"" occurs when you try to run a structured streaming query with table access control enabled on your cluster. Streaming is not supported on clusters with table access control enabled because it requires user interaction to validate and refresh credentials, which is not possible with continuously running streaming queries. The solution is to use a cluster that does not have table access control enabled for streaming queries."	1	1	1	True
35	no_reaction	https://docs.databricks.com/dbfs/root-locations.html	Two of the most common data locations on Databricks are the DBFS root storage and external object storage mounted with dbutils.fs.mount ()  Which of the following statements is correct? a- The DBFS root stores files in ephemeral block volumes attached to the driver while mounted directories will always persist saved data to external storage between sessions. b- By default, both the DBFS root and mounted data sources are only accessible to workspace administrators. c- The DBFS root is the most secure location to store data, because mounted storage volumes must have full public read and write permissions. d- DBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems. e- Neither the DBFS root nor mounted storage can be accessed when using %sh in a Databricks notebook.	What directories are in DBFS root by default? ===============================================     Each Databricks workspace has several directories configured in the DBFS root storage container by default. Some of these directories link to locations on the DBFS root, while others are virtual mounts. If you are unable to access data in any of these directories, contact your workspace administrator.     * `/FileStore` * `/databricks-datasets` * `/databricks-results` * `/databricks/init` * `/user/hive/warehouse`        What is stored in the  `/Filestore`  directory? -------------------------------------------------     Data and libraries uploaded through the Databricks UI go to the  `/Filestore`  location by default. Generated plots are also stored in this directory.           What is stored in the  `/databricks-datasets`  directory? -----------------------------------------------------------     Databricks provides a number of open source datasets in this directory. Many of the tutorials and demos provided by Databricks reference these datasets, but you can also use them to indepedently explore the functionality of Databricks.               What is stored in the  `/databricks-results`  directory? ----------------------------------------------------------    `/databricks-results`  stores files generated by downloading the  [full results](../notebooks/notebook-outputs.html#download-full-results)  of a query.         What is stored in the  `/databricks/init`  directory? -------------------------------------------------------     This directory contains global  [init scripts](../clusters/init-scripts.html)  .           What is stored in the  `/user/hive/warehouse`  directory? -----------------------------------------------------------     Databricks stores managed tables in the  `hive\_metastore`  here by default.	"The correct statement regarding the DBFS (Databricks File Store) root storage and external object storage mounted with dbutils.fs.mount() is: ""The DBFS (Databricks File Store) is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems."" This statement is not one of the options provided in the question."	1	0	0	True
37	thumb_down	https://docs.databricks.com/languages/python.html	install a python library to be available to all notebooks for all users that run on a Databricks Runtime for Machine Learning cluster	Databricks for Python developers ==================================     This section provides a guide to developing notebooks and jobs in Databricks using the Python language. The first subsection provides links to tutorials for common workflows and tasks. The second subsection provides links to APIs, libraries, and key tools.       A basic workflow for getting started is:     * [Import code](#manage-code-with-notebooks-and-databricks-repos)  : Either import your own code from files or Git repos or try a tutorial listed below. Databricks recommends learning using interactive Databricks Notebooks. * [Run your code on a cluster](#clusters-and-libraries)  : Either create a cluster of your own, or ensure you have permissions to use a shared cluster. Attach your notebook to the cluster, and run the notebook. * Beyond this, you can branch out into more specific topics:      + [Work with larger data sets](#python-apis)   using Apache Spark  + [Add visualizations](#visualizations)  + [Automate your workload](#jobs)   as a job  + [Use machine learning](#machine-learning)   to analyze your data  + [Develop in IDEs](#ides-developer-tools-and-apis)      Tutorials -----------     The below tutorials provide example code and notebooks to learn about common workflows. See  [Import a notebook](../notebooks/notebook-export-import.html#import-a-notebook)  for instructions on importing notebook examples into your workspace.      ###   Interactive data science and machine learning   * Getting started with Apache Spark DataFrames for data preparation and analytics:  [Tutorial: Work with PySpark DataFrames on Databricks](../getting-started/dataframes-python.html) * [Tutorial: End-to-end ML models on Databricks](../mlflow/end-to-end-example.html)  . For additional examples, see  [Tutorials: Get started with ML](../machine-learning/tutorial/index.html)  and the MLflow guide’s  [Quickstart Python](../mlflow/quick-start-python.html)  . * [Databricks AutoML](../machine-learning/automl/index.html)  lets you get started quickly with developing machine learning models on your own datasets. Its glass-box approach generates notebooks with the complete machine learning workflow, which you may clone, modify, and rerun.     ###   Data engineering   * [Tutorial: Work with PySpark DataFrames on Databricks](../getting-started/dataframes-python.html)  provides a walkthrough to help you learn about Apache Spark DataFrames for data preparation and analytics. * [Tutorial: Delta Lake](../delta/tutorial.html)  . * [Tutorial: Declare a data pipeline with Python in Delta Live Tables](../delta-live-tables/tutorial-python.html)  . * [Tutorial: Declare a data pipeline with SQL in Delta Live Tables](../delta-live-tables/tutorial-sql.html)  . * [Tutorial: Run your first Delta Live Tables pipeline](../delta-live-tables/tutorial-pipelines.html)  .     ###   Production machine learning and machine learning operations   * [MLflow Model Registry example](../mlflow/model-registry-example.html) * [Tutorial: End-to-end ML models on Databricks](../mlflow/end-to-end-example.html)       ###   Debug in Python notebooks     The example notebook illustrates how to use the Python debugger (pdb) in Databricks notebooks. To use the Python debugger, you must be running Databricks Runtime 11.2 or above.       With Databricks Runtime 12.1 and above, you can use  [variable explorer](../notebooks/notebooks-code.html#variable-explorer)  to track the current value of Python variables in the notebook UI. You can use variable explorer to observe the values of Python variables as you step through breakpoints.        ####   Python debugger example notebook     [Open notebook in new tab](/_extras/notebooks/source/python-debugger.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 Note      `breakpoint()`  is  [not supported in IPython](https://github.com/ipython/ipykernel/issues/897)   and thus does not work in Databricks notebooks. You can use  `import     pdb;     pdb.set\_trace()`  instead of  `breakpoint()`  .           Reference -----------     The below subsections list key features and tips to help you begin developing in Databricks with Python.      ###   Python APIs     Python code that runs outside of Databricks can generally run within Databricks, and vice versa. If you have existing code, just import it into Databricks to get started. See  [Manage code with notebooks and Databricks Repos](#manage-code-with-notebooks-and-databricks-repos)  below for details.       Databricks can run both single-machine and distributed Python workloads. For single-machine computing, you can use Python APIs and libraries as usual; for example, pandas and scikit-learn will “just work.” For distributed Python workloads, Databricks offers two popular APIs out of the box: the Pandas API on Spark and PySpark.      ####   Pandas API on Spark      Note       The  [Koalas open-source project](https://koalas.readthedocs.io/)   now recommends switching to the Pandas API on Spark. The Pandas API on Spark is available on clusters that run  [Databricks Runtime 10.0 (Unsupported)](../release-notes/runtime/10.0.html)  and above. For clusters that run  [Databricks Runtime 9.1 LTS](../release-notes/runtime/9.1.html)  and below, use  [Koalas](../archive/legacy/koalas.html)  instead.       [pandas](https://pandas.pydata.org)   is a Python package commonly used by data scientists for data analysis and manipulation. However, pandas does not scale out to big data.  [Pandas API on Spark](../pandas/pandas-on-spark.html)  fills this gap by providing pandas-equivalent APIs that work on Apache Spark. This  [open-source API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)   is an ideal choice for data scientists who are familiar with pandas but not Apache Spark.       ####   PySpark API     PySpark is the official Python API for Apache Spark. This API provides more flexibility than the Pandas API on Spark. These links provide an introduction to and reference for PySpark.     * [Introduction to DataFrames](../getting-started/dataframes-python.html) * [Introduction to Structured Streaming](../structured-streaming/examples.html) * [PySpark API reference](https://spark.apache.org/docs/latest/api/python/)      ###   Manage code with notebooks and Databricks Repos    [Databricks notebooks](../notebooks/index.html)  support Python. These notebooks provide functionality similar to that of Jupyter, but with additions such as built-in visualizations using big data, Apache Spark integrations for debugging and performance monitoring, and MLflow integrations for tracking machine learning experiments. Get started by  [importing a notebook](../notebooks/notebook-export-import.html#import-a-notebook)  . Once you have access to a cluster, you can  [attach a notebook](../notebooks/notebook-ui.html#attach)  to the cluster and  [run the notebook](../notebooks/run-notebook.html)  .        Tip       To completely reset the state of your notebook, it can be useful to restart the iPython kernel. For Jupyter users, the “restart kernel” option in Jupyter corresponds to detaching and re-attaching a notebook in Databricks. To restart the kernel in a Python notebook, click on the cluster dropdown in the upper-left and click  **Detach & Re-attach**   . This detaches the notebook from your cluster and reattaches it, which restarts the Python process.       [Databricks Repos](../repos/index.html)  allows users to synchronize notebooks and other files with Git repositories. Databricks Repos helps with code versioning and collaboration, and it can simplify importing a full repository of code into Databricks, viewing past notebook versions, and integrating with IDE development. Get started by  [cloning a remote Git repository](../repos/git-operations-with-repos.html)  . You can then open or create notebooks with the repository clone,  [attach the notebook](../notebooks/notebook-ui.html#attach)  to a cluster, and  [run the notebook](../notebooks/run-notebook.html)  .       ###   Clusters and libraries     Databricks  [Clusters](../clusters/index.html)  provide compute management for clusters of any size: from single node clusters up to large clusters. You can customize cluster hardware and libraries according to your needs. Data scientists will generally begin work either by  [creating a cluster](../clusters/configure.html)  or using an existing  [shared cluster](../clusters/clusters-manage.html#control-access-to-clusters)  . Once you have access to a cluster, you can  [attach a notebook](../notebooks/notebook-ui.html#attach)  to the cluster or  [run a job](../workflows/jobs/jobs.html#create-a-job)  on the cluster.     * For small workloads which only require single nodes, data scientists can use  [Single Node clusters](../clusters/single-node.html)  for cost savings. * For detailed tips, see  [Best practices: Cluster configuration](../clusters/cluster-config-best-practices.html) * Administrators can set up  [cluster policies](../administration-guide/clusters/policies.html)  to simplify and guide cluster creation.     Databricks clusters use a Databricks Runtime, which provides many popular libraries out-of-the-box, including Apache Spark, Delta Lake, pandas, and more. You can also install additional third-party or custom Python libraries to use with notebooks and jobs.     * Start with the default libraries in the  [Databricks Runtime](../runtime/dbr.html)  . Use the  [Introduction to Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  for machine learning workloads. For full lists of pre-installed libraries, see  [Databricks runtime releases](../release-notes/runtime/releases.html)  . * Customize your environment using  [Notebook-scoped Python libraries](../libraries/notebooks-python-libraries.html)  , which allow you to modify your notebook or job environment with libraries from PyPI or other repositories. The  `%pip     install     my\_library`  magic command installs  `my\_library`  to all nodes in your currently attached cluster, yet does not interfere with other workloads on shared clusters. * Install non-Python libraries as  [Cluster libraries](../libraries/cluster-libraries.html)  as needed. * For more details, see  [Libraries](../libraries/index.html)  .     ###   Visualizations     Databricks Python notebooks have built-in support for many types of  [visualizations](../visualizations/index.html)  . You can also use  [legacy visualizations](../visualizations/legacy-visualizations.html#visualizations-in-python)  .       You can also visualize data using third-party libraries; some are pre-installed in the Databricks Runtime, but you can install custom libraries as well. Popular options include:     * [Bokeh](../visualizations/bokeh.html) * [Matplotlib](../visualizations/matplotlib.html) * [Plotly](../visualizations/plotly.html)     ###   Jobs     You can automate Python workloads as scheduled or triggered  [Create, run, and manage Databricks Jobs](../workflows/jobs/jobs.html)  in Databricks	To install a Python library to be available to all notebooks for all users that run on a Databricks Runtime for Machine Learning cluster, you can customize your environment using Notebook-scoped Python libraries, which allow you to modify your notebook or job environment with libraries from PyPI or other repositories. The `%pip install my_library` magic command installs `my_library` to all nodes in your currently attached cluster, yet does not interfere with other workloads on shared clusters. You can also install non-Python libraries as Cluster libraries as needed. For more details, see Libraries.	1	1	1	True
38	no_reaction	https://docs.databricks.com/administration-guide/account-settings/audit-logs.html	to setup the audit logs, however, it seems a given set of settings are required to be applied to every cluster created, instead of only to the workspace. Is this understanding correct?	"      Configure audit logging =========================      Note       This feature is available on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   .        Databricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns.       There are two types of logs:     * Workspace-level audit logs with workspace-level events. * Account-level audit logs with account-level events.     For a list of each of these types of events and the associated services, see  [Audit events](#events)  .          Configure verbose audit logs ------------------------------     In addition to the default  [events](#events)  , you can configure a workspace to generate additional events by enabling  *verbose audit logs*   .       To enable verbose audit logs, your account and workspace must be on the E2 version of the platform. To confirm the version of the platform you are using, contact your Databricks representative.      ###   Enable or disable verbose audit logs   1. As an admin, go to the Databricks  [admin console](../workspace/index.html#admin-console)  . 2. Click  **Workspace settings**   . 3. Next to  **Verbose Audit Logs**   , enable or disable the feature.     When you enable or disable verbose logging, an auditable event is emitted in the category  `workspace`  with action  `workspaceConfKeys`  . The  `workspaceConfKeys`  request parameter is  `enableVerboseAuditLogs`  . The request parameter  `workspaceConfValues`  is  `true`  (feature enabled) or  `false`  (feature disabled).       ###   Additional verbose notebook action     Additional verbose action in audit log category  `notebook`  :           |   Action   |   Description   |   Request Parameters   | | --- | --- | --- | |  `runCommand`  |   Emitted after Databricks runs a command in a notebook. A command corresponds to a cell in a notebook.   |   [“notebookId”, “executionTime”, “status”, “commandId”, “commandText”]   |       ###   Additional verbose Databricks SQL actions     Additional actions in audit log category  `databrickssql`  :           |   Action   |   Description   |   Request Parameters   | | --- | --- | --- | |  `commandSubmit`  |   Runs when a command is submitted to Databricks SQL.   |   [“commandText”, “warehouseId”, “commandId”]   | |  `commandFinish`  |   Runs when a command completes or a command is cancelled.   |   [“warehouseId”, “commandId”]   |     Check the  `response`  field for additional information related to the command result:     * `statusCode`  - The HTTP response code. This will be error 400 if it is a general error.      + `errorMessage`   - Error message. In some cases for certain long-running commands, the   `errorMessage`   field may not populate on failure.  + `result`   :   *This field is empty*    .          Configure audit log delivery ------------------------------     As a Databricks account admin, you can configure low-latency delivery of audit logs in JSON file format to an AWS S3 storage bucket, where you can make the data available for  [usage analysis](#analyze)  . Databricks delivers a separate JSON file for each workspace in your account and a separate file for account-level events.       After initial setup or other log delivery configuration changes, expect a delay of up to one hour until changes take effect. After logging delivery begins, auditable events are typically logged within 15 minutes. For the file naming, delivery rules, and schema, see  [Audit delivery details and format](#details)  .       The API to configure low-latency delivery of audit logs is  [Account API 2.0](../../dev-tools/api/latest/account.html)  , which is the same API used to configure  [billable usage log delivery](billable-usage-delivery.html)  .       You can optionally deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery. This allows flexibility, for example setting up workspaces from multiple AWS accounts to deliver to the same S3 bucket. This option requires that you configure an S3 bucket policy that references a cross-account IAM role. Instructions and a policy template are provided in this article.       Access to the logs depends on how you set up the S3 bucket. Databricks delivers logs to your S3 bucket with AWS’s built-in  [BucketOwnerFullControl Canned ACL](https://docs.aws.amazon.com/AmazonS3/latest/dev/about-object-ownership.html)   so that account owners and designees can download the logs directly. To support bucket ownership for newly-created objects, you must set your bucket’s  **S3 Object Ownership**   setting to the value  **Bucket owner preferred**   .        Important       If instead you set your bucket’s  **S3 Object Ownership**   setting to  **Object writer**   , new objects such as your logs remain owned by the uploading account, which is by default the IAM role you created and specified to access your bucket. This can make it difficult to access the logs, because you cannot access them from the AWS console or automation tools that you authenticated with as the bucket owner.        Databricks recommends that you review  [Security Best Practices for S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html)   for guidance around protecting the data in your bucket from unwanted access.      ###   Configuration options     To configure audit log delivery, you have the following options.     * If you have one workspace in your Databricks account, follow the instructions in the sections that follow, creating a single configuration object with a common configuration for your workspace. * If you have multiple workspaces in the same Databricks account, you can do any of the following:      + Share the same configuration (log delivery S3 bucket and IAM role) for all workspaces in the account. This is the only configuration option that also delivers account-level audit logs. It is the default option.  + Use separate configurations for each workspace in the account.  + Use separate configurations for different groups of workspaces, each sharing a configuration. * If you have multiple workspaces, each associated with a separate Databricks account, you must create unique storage and credential configuration objects for each account, but you can reuse an S3 bucket or IAM role between these configuration objects.      Note       Even though you use the Account API to configure log delivery, you can configure log delivery for any workspace, including workspaces that were not created using the Account API.        ###   High-level flow     The high-level flow of audit log delivery:     1. [Configure storage](#configure-storage)  : In AWS, create a new AWS S3 bucket. Using Databricks APIs, call the Account API to create a storage configuration object that uses the bucket name.        Note       To deliver logs to an AWS account other than the account used for the IAM role that you create for log delivery, you need to add an S3 bucket policy. You do not add the policy in this step. 2. [Configure credentials](#configure-credentials)  : In AWS, create the appropriate AWS IAM role. Using Databricks APIs, call the Account API to create a credentials configuration object that uses the IAM role’s ARN. The role policy can specify a path prefix for log delivery within your S3 bucket. You can choose to define an IAM role to include multiple path prefixes if you want log delivery configurations for different workspaces that share the S3 bucket but use different path prefixes. 3. [Optional cross-account support](#cross-account-policy)  To deliver logs to an AWS account other than the account of the IAM role that you create for log delivery, add an S3 bucket policy. This policy references IDs for the cross-account IAM role that you created in the previous step. 4. [Call the log delivery API](#log-delivery-api)  : Call the Account API to create a log delivery configuration that uses the credential and storage configuration objects from previous steps. This step lets you specify if you want to associate the log delivery configuration for all workspaces in your account (current and future workspaces) or for a specific set of workspaces. For a list of account-level events, see  [Audit events](#events)  .     After you complete these steps, you can access the JSON files. The delivery location is:        ``` bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs\_<internal-id>.json  ```      If you configure audit log delivery for the entire account, account-level audit events that are not associated with any single workspace are delivered to the  `workspaceId=0`  partition.       New JSON files are delivered every few minutes, potentially overwriting existing files for each workspace. When you initially set up audit log delivery, it can take up to one hour for log delivery to begin. After audit log delivery begins, auditable events are typically logged within 15 minutes. Additional configuration changes typically take an hour to take effect.       For more information about accessing these files and analyzing them using Databricks, see  [Analyze audit logs](#analyze)  .        Important       There is a limit on the number of log delivery configurations available per account (each limit applies separately to each log type including billable usage and audit logs). You can create a maximum of two enabled account-level delivery configurations (configurations without a workspace filter) per type. Additionally, you can create and enable two workspace level delivery configurations per workspace for each log type, meaning the same workspace ID can occur in the workspace filter for no more than two delivery configurations per log type. You cannot delete a log delivery configuration, but you can disable it. You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.        ###   Requirements   * Account admin email address and password to authenticate with the APIs. The email address and password are both case sensitive. * Account ID. Get your account ID from  [the account console](index.html#account-id)  .     ###   How to authenticate to the APIs     The APIs described in this article are published on the  `accounts.cloud.databricks.com`  base endpoint for all AWS regional deployments.       Use the following base URL for API requests:  `https://accounts.cloud.databricks.com/api/2.0/`      This REST API requires HTTP basic authentication, which involves setting the HTTP header  `Authorization`  . In this article,  *username*   refers to your account admin email address. The email address is case sensitive. There are several ways to provide your credentials to tools such as curl.     * Pass your username and account password separately in the headers of each request in  `<username>:<password>`  syntax.       For example:        ``` curl -X GET -u `<username>:<password>` -H ""Content-Type: application/json"" \  'https://accounts.cloud.databricks.com/api/2.0/accounts/<account-id>/<endpoint>'  ``` * Apply base64 encoding to your  `<username>:<password>`  string and provide it directly in the HTTP header:        ``` curl -X GET -H ""Content-Type: application/json"" \  -H 'Authorization: Basic <base64-username-pw>'  'https://accounts.cloud.databricks.com/api/2.0/accounts/<account-id>/<endpoint>'  ``` * Create a  [.netrc](https://everything.curl.dev/usingcurl/netrc)   file with  `machine`  ,  `login`  , and  `password`  properties:        ``` machine accounts.cloud.databricks.com login <username>"	The context explains how to configure audit logging in Databricks. It provides information on the types of logs, workspace-level audit logs, and account-level audit logs. It also explains how to enable verbose audit logs and additional verbose actions in audit log categories. The context also explains how to configure audit log delivery, including the high-level flow of audit log delivery, configuration options, and requirements. However, it does not provide information on whether a given set of settings is required to be applied to every cluster created, instead of only to the workspace.	0	0	0	True
39	no_reaction	https://docs.databricks.com/delta/tune-file-size.html	what size files should I set in Delta tables?	"      Configure Delta Lake to control data file size ================================================     Delta Lake provides options for manually or automatically configuring the target file size for writes and for  `OPTIMIZE`  operations.        Limit rows written in a data file -----------------------------------     You can use the SQL session configuration  `spark.sql.files.maxRecordsPerFile`  to specify the maximum number of records to write to a single file for a Delta Lake table. Specifying a value of zero or a negative value represents no limit.       In Databricks Runtime 10.5 and above, you can also use the DataFrameWriter option  `maxRecordsPerFile`  when using the DataFrame APIs to write to a Delta Lake table. When  `maxRecordsPerFile`  is specified, the value of the SQL session configuration  `spark.sql.files.maxRecordsPerFile`  is ignored.         ``` (df.write   .mode(""append"")   .option(""maxRecordsPerFile"", ""10000"")   .save(""/tmp/delta/people10m"") )  ```       ``` df.write  .mode(""append"")  .option(""maxRecordsPerFile"", ""10000"")  .save(""/tmp/delta/people10m"")  ```           Set a target file size ------------------------      Note       Available in Databricks Runtime 8.2 and above.        If you want to tune the size of files in your Delta table, set the  [table property](table-properties.html) `delta.targetFileSize`  to the desired size. If this property is set, all data layout optimization operations will make a best-effort attempt to generate files of the specified size. Examples here include  [optimize](optimize.html)  or  [Z-order](data-skipping.html)  ,  [Auto Compaction](../optimizations/auto-optimize.html#how-auto-compaction-works)  , and  [Optimized Writes](../optimizations/auto-optimize.html#how-optimized-writes-work)  .         |   Table property   | | --- | |  **delta.targetFileSize**     Type: Size in bytes or higher units.     The target file size. For example,  `104857600`  (bytes) or  `100mb`  .     Default value: None   |     For existing tables, you can set and unset properties using the SQL command  [ALTER TABLE SET TBL PROPERTIES](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html)  . You can also set these properties automatically when creating new tables using Spark session configurations. See  [Delta table properties reference](table-properties.html)  for details.         Autotune file size based on workload --------------------------------------      Note       Available in Databricks Runtime 8.2 and above.        To minimize the need for manual tuning, Databricks can automatically tune the file size of Delta tables, based on workloads operating on the table. Databricks can automatically detect if a Delta table has frequent  `MERGE`  operations that rewrite files and may choose to reduce the size of rewritten files in anticipation of further file rewrites in the future. For example, when executing a  `MERGE`  operation, if 9 out of last 10 previous operations on the table were also MERGEs, then  [Optimized Writes](../optimizations/auto-optimize.html#delta-optimized-writes)  and  [Auto Compaction](../optimizations/auto-optimize.html#how-auto-compaction-works)  used by  `MERGE`  (if enabled) will generate smaller file sizes than it would otherwise. This helps in reducing the duration of future  `MERGE`  operations.       Autotune is activated after a few rewrite operations have occurred. However, if you anticipate a Delta table will experience frequent  `MERGE`  ,  `UPDATE`  , or  `DELETE`  operations and want this tuning immediately, you can explicitly tune file sizes for rewrites by setting the table property  `delta.tuneFileSizesForRewrites`  . Set this property to  `true`  to always use lower file sizes for all data layout optimization operations on the table. Set it to  `false`  to never tune to lower file sizes, that is, prevent auto-detection from being activated.         |   Table property   | | --- | |  **delta.tuneFileSizesForRewrites**     Type:  `Boolean`    Whether to tune file sizes for data layout optimization.     Default value: None   |     For existing tables, you can set and unset properties using the SQL command  [ALTER TABLE SET TBL PROPERTIES](../sql/language-manual/sql-ref-syntax-ddl-alter-table.html)  . You can also set these properties automatically when creating new tables using Spark session configurations. See  [Delta table properties reference](table-properties.html)  for details.         Autotune file size based on table size ----------------------------------------      Note       Available in Databricks Runtime 8.4 and above.        To minimize the need for manual tuning, Databricks automatically tunes the file size of Delta tables based on the size of the table. Databricks will use smaller file sizes for smaller tables and larger file sizes for larger tables so that the number of files in the table does not grow too large. Databricks does not autotune tables that you have tuned with a  [specific target size](#set-target-size)  or based on a workload with frequent rewrites.       The target file size is based on the current size of the Delta table. For tables smaller than 2.56 TB, the autotuned target file size is 256 MB. For tables with a size between 2.56 TB and 10 TB, the target size will grow linearly from 256 MB to 1 GB. For tables larger than 10 TB, the target file size is 1 GB.        Note       When the target file size for a table grows, existing files are not re-optimized into larger files by the  `OPTIMIZE`  command. A large table can therefore always have some files that are smaller than the target size. If it is required to optimize those smaller files into larger files as well, you can configure a fixed target file size for the table using the  `delta.targetFileSize`  table property.        When a table is written incrementally, the target file sizes and file counts will be close to the following numbers, based on table size. The file counts in this table are only an example. The actual results will be different depending on many factors.           |   Table size   |   Target file size   |   Approximate number of files in table   | | --- | --- | --- | |   10 GB   |   256 MB   |   40   | |   1 TB   |   256 MB   |   4096   | |   2.56 TB   |   256 MB   |   10240   | |   3 TB   |   307 MB   |   12108   | |   5 TB   |   512 MB   |   17339   | |   7 TB   |   716 MB   |   20784   | |   10 TB   |   1 GB   |   24437   | |   20 TB   |   1 GB   |   34437   | |   50 TB   |   1 GB   |   64437   | |   100 TB   |   1 GB   |   114437   |                                                      "	To configure the size of files in Delta tables, you can set the `delta.targetFileSize` table property to the desired size. For example, `104857600` (bytes) or `100mb`. If this property is set, all data layout optimization operations will make a best-effort attempt to generate files of the specified size. Additionally, you can use the SQL session configuration `spark.sql.files.maxRecordsPerFile` to specify the maximum number of records to write to a single file for a Delta Lake table. Specifying a value of zero or a negative value represents no limit. In Databricks Runtime 10.5 and above, you can also use the DataFrameWriter option `maxRecordsPerFile` when using the DataFrame APIs to write to a Delta Lake table.	1	1	1	True
40	no_reaction	https://docs.databricks.com/administration-guide/clusters/index.html	how do i set a cluster policy?	Configure clusters ====================      Note       These are instructions for the legacy create cluster UI, and are included only for historical accuracy. All customers should be using the  [updated create cluster UI](../../clusters/configure.html)  .        This article explains the configuration options available when you create and edit Databricks clusters. It focuses on creating and editing clusters using the UI. For other methods, see  [Clusters CLI](../../dev-tools/cli/clusters-cli.html)  ,  [Clusters API 2.0](../../dev-tools/api/latest/clusters.html)  , and  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  .       For help deciding what combination of configuration options suits your needs best, see  [cluster configuration best practices](../../clusters/cluster-config-best-practices.html)  .      ![Create cluster](../../_images/create-dialog-aws.png)       Cluster policy ----------------     A  [cluster policy](../../administration-guide/clusters/policies.html)  limits the ability to configure clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster.       To configure a cluster policy, select the cluster policy in the  **Policy**   drop-down.      ![Select cluster policy](../../_images/policy.png)     Note       If no policies have been  [created in the workspace](../../administration-guide/clusters/policies.html#create-a-cluster-policy)  , the  **Policy**   drop-down does not display.        If you have:     * [Cluster create permission](../../administration-guide/access-control/cluster-acl.html#cluster-create-permission)  , you can select the  **Unrestricted**   policy and create fully-configurable clusters. The  **Unrestricted**   policy does not limit any cluster attributes or attribute values. * Both cluster create permission and access to cluster policies, you can select the  **Unrestricted**   policy and the policies you have access to. * Access to cluster policies only, you can select the policies you have access to.       Cluster mode --------------      Note       This article describes the legacy clusters UI. For information about the new clusters UI (in preview), see  [Create a cluster](../../clusters/configure.html)  . This includes some terminology changes for cluster access types and modes. For a comparison of the new and legacy cluster types, see  [Clusters UI changes and cluster access modes](cluster-ui-preview.html)  . In the preview UI:     * *Standard mode clusters*   are now called  *No Isolation Shared access mode clusters*   . * *High Concurrency with Tables ACLs*   are now called  *Shared access mode clusters*   .      Databricks supports three cluster modes: Standard, High Concurrency, and  [Single Node](../../clusters/single-node.html)  . The default cluster mode is Standard.        Important     * If your workspace is assigned to a  [Unity Catalog](../../data-governance/unity-catalog/index.html)  metastore, High Concurrency clusters are not available. Instead, you use  [access mode](../../data-governance/unity-catalog/compute.html#access-mode)  to ensure the integrity of access controls and enforce strong isolation guarantees. See also  [Create a cluster that can access Unity Catalog](../../data-governance/unity-catalog/compute.html#create-cluster)  . * You cannot change the cluster mode after a cluster is created. If you want a different cluster mode, you must create a new cluster.      The cluster configuration includes an  [auto terminate](../../clusters/clusters-manage.html#automatic-termination)  setting whose  *default value*   depends on cluster mode:     * Standard and Single Node clusters terminate automatically after 120 minutes by default. * High Concurrency clusters  *do not*   terminate automatically by default.      ###   Standard clusters          Warning       Standard mode clusters (sometimes called No Isolation Shared clusters) can be shared by multiple users, with no isolation between users. If you use the High Concurrency cluster mode  *without additional security settings such as Table ACLs or Credential Passthrough*   , the same settings are used as Standard mode clusters. Account admins can  [prevent internal credentials from being automatically generated for Databricks workspace admins](../../administration-guide/account-settings/no-isolation-shared.html)  on these types of cluster. For more secure options, Databricks recommends alternatives such as high concurrency clusters with Table ACLs.        A Standard cluster is recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.         ###   High Concurrency clusters     A High Concurrency cluster is a managed cloud resource. The key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies.       High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.       In addition, only High Concurrency clusters support  [table access control](../../data-governance/table-acls/index.html)  .       To create a High Concurrency cluster, set  **Cluster Mode**   to  **High Concurrency**   .      ![High Concurrency cluster mode](../../_images/high-concurrency.png)        For an example of how to create a High Concurrency cluster using the Clusters API, see  [High Concurrency cluster example](../../dev-tools/api/latest/examples.html#high-concurrency-example)  .         ###   Single Node clusters     A Single Node cluster has no workers and runs Spark jobs on the driver node.       In contrast, a Standard cluster requires  *at least one*   Spark worker node in addition to the driver node to execute Spark jobs.       To create a Single Node cluster, set  **Cluster Mode**   to  **Single Node**   .      ![Single Node cluster mode](../../_images/single-node.png)    To learn more about working with Single Node clusters, see  [Single Node clusters](../../clusters/single-node.html)  .              Pools -------     To reduce cluster start time, you can attach a cluster to a predefined  [pool](../../clusters/pools.html)  of idle instances, for the driver and worker nodes. The cluster is created using instances in the pools. If a pool does not have sufficient idle resources to create the requested driver or worker nodes, the pool expands by allocating new instances from the instance provider. When an attached cluster is terminated, the instances it used are returned to the pools and can be reused by a different cluster.       If you select a pool for worker nodes but not for the driver node, the driver node inherit the pool from the worker node configuration.        Important       If you attempt to select a pool for the driver node but not for worker nodes, an error occurs and your cluster isn’t created. This requirement prevents a situation where the driver node has to wait for worker nodes to be created, or vice versa.        See  [Create a pool](../../clusters/pools.html)  to learn more about working with pools in Databricks.             Databricks Runtime --------------------     Databricks runtimes are the set of core components that run on your  [clusters](../../clusters/index.html)  . All Databricks runtimes include Apache Spark and add components and updates that improve usability, performance, and security. For details, see  [Databricks runtimes](../../runtime/index.html)  .       Databricks offers several types of runtimes and several versions of those runtime types in the  **Databricks Runtime Version**   drop-down when you create or edit a cluster.      ![Select Runtime version](../../_images/runtime-version.png)     ###   Photon acceleration    [Photon](../../runtime/photon.html)  is available for clusters running  [Databricks Runtime 9.1 LTS](../../release-notes/runtime/9.1.html)  and above.       To enable Photon acceleration, select the  **Use Photon Acceleration**   checkbox.       If desired, you can specify the instance type in the Worker Type and Driver Type drop-down.       You can view Photon activity in the  [Spark UI](../../clusters/clusters-manage.html#spark-ui)  . The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with “Photon”, for example,  `PhotonGroupingAgg`  . Second, in the DAG, Photon operators and stages are colored peach, while the non-Photon ones are blue.      ![Photon DAG](../../_images/photon-dag.png)        ###   Docker images     For some Databricks Runtime versions, you can specify a Docker image when you create a cluster. Example use cases include library customization, a golden container environment that doesn’t change, and Docker CI/CD integration.       You can also use Docker images to create custom deep learning environments on clusters with GPU devices.       For instructions, see  [Customize containers with Databricks Container Services](../../clusters/custom-containers.html)  and  [Databricks Container Services on GPU clusters](../../clusters/gpu.html#databricks-container-services-on-gpu)  .              Cluster node type -------------------     A cluster consists of one driver node and zero or more worker nodes.       You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads.      * [Driver node](#driver-node) * [Worker node](#worker-node) * [GPU instance types](#gpu-instance-types) * [AWS Graviton instance types](#aws-graviton-instance-types)     ###  [Driver node](#id1)     The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext and interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors.       The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to  `collect()`  a lot of data from Spark workers and analyze them in the notebook.        Tip       Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node.          ###  [Worker node](#id2)     Databricks worker nodes run the Spark executors and other services required for the proper functioning of the clusters. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Databricks runs one executor per worker node; therefore the terms  *executor*   and  *worker*   are used interchangeably in the context of the Databricks architecture.        Tip       To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail.         Note       Databricks launches worker nodes with two private IP addresses each. The node’s primary private IP address is used to host Databricks internal traffic. The secondary private IP address is used by the Spark container for intra-cluster communication. This model allows Databricks to provide isolation between multiple clusters in the same workspace.        ###  [GPU instance types](#id3)     For computationally challenging tasks that demand high performance, like those associated with deep learning, Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see  [GPU-enabled	To set a cluster policy in Databricks, you can select the cluster policy in the Policy drop-down. If no policies have been created in the workspace, the Policy drop-down does not display. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster. If you have access to cluster policies only, you can select the policies you have access to.	1	1	0	True
41	no_reaction	https://kb.databricks.com/streaming/sqs-stream-partition-values	how do I stream data in from kafka into an s3 bucket	"    Stream processing with Apache Kafka and Databricks ====================================================     This article describes how you can use Apache Kafka as either a source or a sink when running Structured Streaming workloads on Databricks.       For more Kafka, see the  [Kafka documentation](https://kafka.apache.org/documentation/)   .        Read data from Kafka ----------------------     The following is an example for reading data from Kafka:        ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""subscribe"", ""<topic>"")   .option(""startingOffsets"", ""latest"")   .load() )  ```        Write data to Kafka ---------------------     The following is an example for writing data to Kafka:        ``` (df.writeStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""topic"", ""<topic>"")   .option(""checkpointLocation"", ""<checkpoint\_path>"")   .start() )  ```            Configure Kafka for Apache Spark on Databricks ------------------------------------------------     Databricks provides the  `kafka`  keyword as a data format to configure connections to Kafka 0.10+.       The following are the most common configurations for Kafka:       There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:           |   Option   |   Value   |   Description   | | --- | --- | --- | |   subscribe   |   A comma-separated list of topics.   |   The topic list to subscribe to.   | |   subscribePattern   |   Java regex string.   |   The pattern used to subscribe to topic(s).   | |   assign   |   JSON string  `{""topicA"":[0,1],""topic"":[2,4]}`  .   |   Specific topicPartitions to consume.   |     Other notable configurations:            |   Option   |   Value   |   Default Value   |   Description   | | --- | --- | --- | --- | |   kafka.bootstrap.servers   |   Comma-separated list of host:port.   |   empty   |   [Required] The Kafka  `bootstrap.servers`  configuration. If you find there is no data from Kafka, check the broker address list first. If the broker address list is incorrect, there might not be any errors. This is because Kafka client assumes the brokers will become available eventually and in the event of network errors retry forever.   | |   failOnDataLoss   |  `true`  or  `false`  .   |  `true`  |   [Optional] Whether to fail the query when it’s possible that data was lost. Queries can permanently fail to read data from Kafka due to many scenarios such as deleted topics, topic truncation before processing, and so on. We try to estimate conservatively whether data was possibly lost or not. Sometimes this can cause false alarms. Set this option to  `false`  if it does not work as expected, or you want the query to continue processing despite data loss.   | |   minPartitions   |   Integer >= 0, 0 = disabled.   |   0 (disabled)   |   [Optional] Minimum number of partitions to read from Kafka. You can configure Spark to use an arbitrary minimum of partitions to read from Kafka using the  `minPartitions`  option. Normally Spark has a 1-1 mapping of Kafka topicPartitions to Spark partitions consuming from Kafka. If you set the  `minPartitions`  option to a value greater than your Kafka topicPartitions, Spark will divvy up large Kafka partitions to smaller pieces. This option can be set at times of peak loads, data skew, and as your stream is falling behind to increase processing rate. It comes at a cost of initializing Kafka consumers at each trigger, which may impact performance if you use SSL when connecting to Kafka.   | |   kafka.group.id   |   A Kafka consumer group ID.   |   not set   |   [Optional] Group ID to use while reading from Kafka. Use this with caution. By default, each query generates a unique group ID for reading data. This ensures that each query has its own consumer group that does not face interference from any other consumer, and therefore can read all of the partitions of its subscribed topics. In some scenarios (for example, Kafka group-based authorization), you may want to use specific authorized group IDs to read data. You can optionally set the group ID. However, do this with extreme caution as it can cause unexpected behavior.  * Concurrently running queries (both, batch and streaming) with the same group ID are likely interfere with each other causing each query to read only part of the data. * This may also occur when queries are started/restarted in quick succession. To minimize such issues, set the Kafka consumer configuration  `session.timeout.ms`  to be very small.  | |   startingOffsets   |   earliest , latest   |   latest   |   [Optional] The start point when a query is started, either “earliest” which is from the earliest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest.   |     See  [Structured Streaming Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)   for other optional configurations.         Schema for Kafka records --------------------------     The schema of Kafka records is:          |   Column   |   Type   | | --- | --- | |   key   |   binary   | |   value   |   binary   | |   topic   |   string   | |   partition   |   int   | |   offset   |   long   | |   timestamp   |   long   | |   timestampType   |   int   |     The  `key`  and the  `value`  are always deserialized as byte arrays with the  `ByteArrayDeserializer`  . Use DataFrame operations (such as  `cast(""string"")`  ) to explicitly deserialize the keys and values.           Retrieve Kafka metrics ------------------------      Note       Available in Databricks Runtime 8.1 and above.        You can get the average, min, and max of the number of offsets that the streaming query is behind the latest available offset among all the subscribed topics with the  `avgOffsetsBehindLatest`  ,  `maxOffsetsBehindLatest`  , and  `minOffsetsBehindLatest`  metrics. See  [Reading Metrics Interactively](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reading-metrics-interactively)   .        Note       Available in Databricks Runtime 9.1 and above.        Get the estimated total number of bytes that the query process has not consumed from the subscribed topics by examining the value of  `estimatedTotalBytesBehindLatest`  . This estimate is based on the batches that were processed in the last 300 seconds. The timeframe that the estimate is based on can be changed by setting the option  `bytesEstimateWindowLength`  to a different value. For example, to set it to 10 minutes:        ``` df = (spark.readStream   .format(""kafka"")   .option(""bytesEstimateWindowLength"", ""10m"") # m for minutes, you can also use ""600s"" for 600 seconds )  ```      If you are running the stream in a notebook, you can see these metrics under the  **Raw Data**   tab in the streaming query progress dashboard:        ``` {  ""sources"" : [ {  ""description"" : ""KafkaV2[Subscribe[topic]]"",  ""metrics"" : {  ""avgOffsetsBehindLatest"" : ""4.0"",  ""maxOffsetsBehindLatest"" : ""4"",  ""minOffsetsBehindLatest"" : ""4"",  ""estimatedTotalBytesBehindLatest"" : ""80.0""  },  } ] }  ```        Use SSL to connect Databricks to Kafka ----------------------------------------     To enable SSL connections to Kafka, follow the instructions in the Confluent documentation  [Encryption and Authentication with SSL](https://docs.confluent.io/current/kafka/authentication_ssl.html#clients)   . You can provide the configurations described there, prefixed with  `kafka.`  , as options. For example, you specify the trust store location in the property  `kafka.ssl.truststore.location`  .       Databricks recommends that you:     * Store your certificates in cloud object storage. You can restrict access to the certificates only to clusters that can access Kafka. See  [Data governance guide](../data-governance/index.html)  . * Store your certificate passwords as  [secrets](../security/secrets/secrets.html)  in a  [secret scope](../security/secrets/secret-scopes.html)  .     The following example uses object storage locations and Databricks secrets to enable an SSL connection:        ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ...)   .option(""kafka.security.protocol"", ""SASL\_SSL"")   .option(""kafka.ssl.truststore.location"", <truststore-location>)   .option(""kafka.ssl.keystore.location"", <keystore-location>)   .option(""kafka.ssl.keystore.password"", dbutils.secrets.get(scope=<certificate-scope-name>,key=<keystore-password-key-name>))   .option(""kafka.ssl.truststore.password"", dbutils.secrets.get(scope=<certificate-scope-name>,key=<truststore-password-key-name>)) )  ```                                                       "	"To stream data in from Kafka into an S3 bucket, you can use the `writeStream` method to write data to Kafka and then use the `kafka` format to read data from Kafka. Here is an example for reading data from Kafka:  ``` df = (spark.readStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""subscribe"", ""<topic>"")   .option(""startingOffsets"", ""latest"")   .load() ) ```  And here is an example for writing data to Kafka:  ``` (df.writeStream   .format(""kafka"")   .option(""kafka.bootstrap.servers"", ""<server:ip>"")   .option(""topic"", ""<topic>"")   .option(""checkpointLocation"", ""<checkpoint\_path>"")   .start() ) ```  Once you have the data in a DataFrame, you can use the `writeStream` method to write the data to an S3 bucket."	1	1	1	True
42	no_reaction	https://docs.databricks.com/administration-guide/account-settings/index.html	how do i get the account id in databricks sql	Manage your Databricks account ================================     The Databricks account configurations are managed by  [account admins](../index.html)  . The articles in this section cover tasks performed by account admins.       As an account admin, you can manage your Databricks account using the  [account console](#account-console)  , the  [Account API](../workspace/create-workspace-api.html)  , or  [Automation templates](../workspace/templates.html)  .            Manage your account using the Databricks account console ----------------------------------------------------------     The account console is where account admins manage Databricks account-level configurations.       Tasks performed through the account console include  [workspace creation](../workspace/create-workspace.html)  ,  [user management](../users-groups/users.html)  ,  [subscription management](account.html)  , and configuring cloud resources (such as  [credentials](../account-settings-e2/credentials.html)  ,  [storage](../account-settings-e2/storage.html)  , and  [networks](../account-settings-e2/networks.html)  ).      ###   Access the account console     Users can access the account console at  <https://accounts.cloud.databricks.com>  . Account admins can also access the account console from within a workspace:     1. Click your email address at the top of the Databricks workspace UI. 2. Select  **Manage Account**   .     ###   Change the account console language settings     The account console is available in multiple languages. To change the account console language, select  **Settings**   then go to the  **Language settings**   tab.            Locate your account ID ------------------------     To retrieve your account ID, go to the  [account console](#account-console)  and click the down arrow next to your username in the upper right corner. In the dropdown menu you can view and copy your  **Account ID**   .           Manage email preferences --------------------------     Databricks can occasionally send emails with personalized product and feature recommendations based on your use of Databricks. These messages may include information to help users get started with Databricks or learn about new features and previews.       You can manage whether you receive these emails in the account console:     1. Log in to the  [account console](#account-console)  and click the  **Settings**   icon in the sidebar. 2. In the  **My preferences**   section, click the  **Instructional product and feature emails**   toggle.     You can also manage your promotional email communications by clicking  **Manage**   under  **Promotional email communications**   or by going to the  [Marketing preference center](https://preferences.databricks.com/)   . Non-admin users can update this setting by clicking the  **My preferences**   link next to their workspace in the account console.	To retrieve your account ID in Databricks SQL, you can go to the account console and click the down arrow next to your username in the upper right corner. In the dropdown menu, you can view and copy your Account ID.	1	1	1	True
43	no_reaction	https://docs.databricks.com/query-federation/snowflake.html	What can Databricks do with Snowflake?	"    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format(""snowflake"")   .option(""dbtable"", table\_name)   .option(""sfUrl"", database\_host\_url)   .option(""sfUser"", username)   .option(""sfPassword"", password)   .option(""sfDatabase"", database\_name)   .option(""sfSchema"", schema\_name)   .option(""sfWarehouse"", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format(""snowflake"")  .option(""dbtable"", table\_name)  .option(""sfUrl"", database\_host\_url)  .option(""sfUser"", username)  .option(""sfPassword"", password)  .option(""sfDatabase"", database\_name)  .option(""sfSchema"", schema\_name)  .option(""sfWarehouse"", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         "	Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake. You can configure a connection to Snowflake and then query data. Databricks can also store machine learning training results in Snowflake.	1	1	1	True
45	no_reaction	https://docs.databricks.com/ingestion/index.html	how can iload a json file into databricks?	"  ![]()   Create a SparkDataFrame from a JSON file. ===========================================  `read.json.Rd`    Loads a JSON file, returning the result as a SparkDataFrame By default, (  [JSON Lines text format or newline-delimited JSON](https://jsonlines.org/)   ) is supported. For JSON (one record per file), set a named property  `multiLine`   to  `TRUE`   . It goes through the entire dataset once to determine the schema.     Usage -------  ``` read.json(path, ...) ```   Arguments -----------   path     Path of file to read. A vector of multiple paths is allowed.     ...     additional external data source specific named properties. You can find the JSON-specific options for reading JSON files in  <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>  Data Source Option in the version you use.     Value -------   SparkDataFrame     Note ------   read.json since 1.6.0     Examples ----------  ``` if (FALSE) { [sparkR.session](sparkR.session.html)() path <- ""path/to/file.json"" df <- read.json(path) df <- read.json(path, multiLine = TRUE) }  ```  "	To load a JSON file into Databricks, you can use the `read.json()` function. This function loads a JSON file and returns the result as a SparkDataFrame. You can specify the path of the file to read as an argument to the function. Additionally, you can use external data source specific named properties to specify JSON-specific options for reading JSON files.	1	1	1	True
49	thumb_down	https://docs.databricks.com/notebooks/schedule-notebook-jobs.html	how do I run a job in a notebook	Create and manage scheduled notebook jobs ===========================================     You can create and manage notebook jobs directly in the notebook UI. If a notebook is already assigned to one or more jobs, you can create and manage schedules for those jobs. If a notebook is not assigned to a job, you can create a job and a schedule to run the notebook.        Schedule a notebook job -------------------------     To schedule a notebook job to run periodically:     1. In the notebook, click  ![Notebook schedule button](../_images/schedule-button.png)  at the top right. If no jobs exist for this notebook, the Schedule dialog appears.      ![Schedule notebook dialog](../_images/schedule-dialog.png)    If jobs already exist for the notebook, the Jobs List dialog appears. To display the Schedule dialog, click  **Add a schedule**   .      ![Job list dialog](../_images/job-list-dialog.png) 2. In the Schedule dialog, optionally enter a name for the job. The default name is the name of the notebook. 3. Select  **Manual**   to run your job only when manually triggered, or  **Scheduled**   to define a schedule for running the job. If you select  **Scheduled**   , use the drop-downs to specify the frequency, time, and time zone. 4. In the  **Cluster**   drop-down, select the cluster to run the task.       If you have  **Allow Cluster Creation**   permissions, by default the job runs on a  [new job cluster](../workflows/jobs/jobs.html#choose-the-correct-cluster-type-for-your-job)  . To edit the configuration of the default job cluster, click  **Edit**   at the right of the field to display the  [cluster configuration dialog](../clusters/configure.html)  .       If you do not have  **Allow Cluster Creation**   permissions, by default the job runs on the cluster that the notebook is attached to. If the notebook is not attached to a cluster, you must select a cluster from the  **Cluster**   drop-down. 5. Optionally, enter any  **Parameters**   to pass to the job. Click  **Add**   and specify the key and value of each parameter. Parameters set the value of the  [notebook widget](widgets.html)  specified by the key of the parameter. Use  [Task parameter variables](../workflows/jobs/jobs.html#task-parameter-variables)  to pass a limited set of dynamic values as part of a parameter value. 6. Optionally, specify email addresses to receive  **Alerts**   on job events. See  [Add email and system notifications for job events](../workflows/jobs/job-notifications.html)  . 7. Click  **Submit**   .       Run a notebook job --------------------     To manually run a notebook job:     1. In the notebook, click  ![Notebook schedule button](../_images/schedule-button.png)  at the top right. 2. Click  **Run now**   . 3. To view the  [job run details](../workflows/jobs/jobs.html#job-run-details)  , click  ![New Tab Icon](../_images/open-in-new-tab.png)  .       Manage scheduled notebook jobs --------------------------------     To display jobs associated with this notebook, click the  **Schedule**   button. The jobs list dialog appears, showing all jobs currently defined for this notebook. To manage jobs, click  ![Jobs Vertical Ellipsis](../_images/jobs-vertical-ellipsis.png)  at the right of a job in the list.      ![Job list menu](../_images/job-list-menu.png)    From this menu, you can edit the schedule,  [clone](../workflows/jobs/jobs.html#clone-job)  the job, view  [job run details](../workflows/jobs/jobs.html#view-job-run-list)  , pause the job, resume the job, or delete a scheduled job.       When you clone a scheduled job, a new job is created with the same parameters as the original. The new job appears in the list with the name  `Clone     of     <initial     job     name>`  .       How you edit a job depends on the complexity of the job’s schedule. Either the Schedule dialog or the  [Job details panel](../workflows/jobs/jobs.html#job-edit)  displays, allowing you to edit the schedule, cluster, parameters, and so on.	"To run a job in a notebook, you can create and manage notebook jobs directly in the notebook UI. To schedule a notebook job to run periodically, click the ""Notebook schedule button"" at the top right of the notebook and select ""Scheduled"" to define a schedule for running the job. Then, select the cluster to run the task and optionally enter any parameters to pass to the job. Finally, click ""Submit"" to create the job. To manually run a notebook job, click the ""Notebook schedule button"" and select ""Run now"". To manage scheduled notebook jobs, click the ""Schedule"" button to display jobs associated with the notebook."	1	1	1	True
50	no_reaction	https://kb.databricks.com/sql/inner-join-drops-records-in-result	How do I perform an outer join between two tables using dataframes in python?	"   pyspark.sql.DataFrame.join  [¶](#pyspark-sql-dataframe-join ""Permalink to this headline"") ===========================================================================================  `DataFrame.`  `join`    (   *other     :     pyspark.sql.dataframe.DataFrame*  ,  *on     :     Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], None]     =     None*  ,  *how     :     Optional    [    str    ]     =     None*   )    → pyspark.sql.dataframe.DataFrame  [[source]](../../../_modules/pyspark/sql/dataframe.html#DataFrame.join) [¶](#pyspark.sql.DataFrame.join ""Permalink to this definition"")    Joins with another  [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")  , using the given join expression.     New in version 1.3.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **other**   [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")   Right side of the join    **on**    str, list or  [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")  , optional     a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If    on    is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.    **how**    str, optional     default  `inner`  . Must be one of:  `inner`  ,  `cross`  ,  `outer`  ,  `full`  ,  `fullouter`  ,  `full\_outer`  ,  `left`  ,  `leftouter`  ,  `left\_outer`  ,  `right`  ,  `rightouter`  ,  `right\_outer`  ,  `semi`  ,  `leftsemi`  ,  `left\_semi`  ,  `anti`  ,  `leftanti`  and  `left\_anti`  .     Returns    [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")   Joined DataFrame.     Examples     The following performs a full outer join between  `df1`  and  `df2`  .    ``` >>> from pyspark.sql import Row >>> from pyspark.sql.functions import desc >>> df = spark.createDataFrame([(2, ""Alice""), (5, ""Bob"")]).toDF(""age"", ""name"") >>> df2 = spark.createDataFrame([Row(height=80, name=""Tom""), Row(height=85, name=""Bob"")]) >>> df3 = spark.createDataFrame([Row(age=2, name=""Alice""), Row(age=5, name=""Bob"")]) >>> df4 = spark.createDataFrame([ ...     Row(age=10, height=80, name=""Alice""), ...     Row(age=5, height=None, name=""Bob""), ...     Row(age=None, height=None, name=""Tom""), ...     Row(age=None, height=None, name=None), ... ])  ```   Inner join on columns (default)    ``` >>> df.join(df2, 'name').select(df.name, df2.height).show() +----+------+ |name|height| +----+------+ | Bob| 85| +----+------+ >>> df.join(df4, name', 'age).select(df.name, df.age).show() +----+---+ |name|age| +----+---+ | Bob| 5| +----+---+  ```   Outer join for both DataFrames on the ‘name’ column.    ``` >>> df.join(df2, df.name == df2.name, 'outer').select( ...     df.name, df2.height).sort(desc(""name"")).show() +-----+------+ | name|height| +-----+------+ | Bob| 85| |Alice| null| | null| 80| +-----+------+ >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(""name"")).show() +-----+------+ | name|height| +-----+------+ | Tom| 80| | Bob| 85| |Alice| null| +-----+------+  ```   Outer join for both DataFrams with multiple columns.    ``` >>> df.join( ...     df3, ...     [df.name == df3.name, df.age == df3.age], ...     'outer' ... ).select(df.name, df3.age).show() +-----+---+ | name|age| +-----+---+ |Alice| 2| | Bob| 5| +-----+---+  ```  [pyspark.sql.DataFrame.isStreaming](pyspark.sql.DataFrame.isStreaming.html ""previous page"")  [pyspark.sql.DataFrame.limit](pyspark.sql.DataFrame.limit.html ""next page"")   "	To perform an outer join between two tables using dataframes in Python, you can use the `join` method of the `DataFrame` class in PySpark. The `join` method takes two parameters: `other` and `how`. `other` is the right side of the join, which is another `DataFrame`. `how` specifies the type of join to perform, and can be one of `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, and `left_anti`. To perform an outer join, you can set `how` to `outer`.	1	1	1	True
51	no_reaction	https://kb.databricks.com/delta/id-duplicate-on-append	How do I insert into a delta table and remove any duplicates	"    INSERT ========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Inserts new rows into a table and optionally truncates the table or partitions. You specify the inserted rows by value expressions or the result of a query.       Databricks SQL supports this statement only for Delta Lake tables.        Syntax --------      ``` INSERT { OVERWRITE | INTO } [ TABLE ] table\_name     [ PARTITION clause ]     [ ( column\_name [, ...] ) ]     query  INSERT INTO [ TABLE ] table\_name     REPLACE WHERE predicate     query  ```       Note       When you  `INSERT     INTO`  a Delta table, schema enforcement and evolution is supported. If a column’s data type cannot be safely cast to a Delta table’s data type, a runtime exception is thrown. If  [schema evolution](../../delta/update-schema.html)  is enabled, new columns can exist as the last columns of your schema (or nested columns) for the schema to evolve.          Parameters ------------   * **`INTO`  or  `OVERWRITE`**      If you specify  `OVERWRITE`  the following applies:      + Without a   `partition\_spec`   the table is truncated before inserting the first row.  + Otherwise, all partitions matching the   `partition\_spec`   are truncated before inserting the first row.  If you specify  `INTO`  all rows inserted are additive to the existing rows. * **[table\_name](sql-ref-names.html#table-name)**      Identifies the table to be inserted to. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the table cannot be found Databricks raises a  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error. * **[PARTITION clause](sql-ref-partition.html#partition)**      An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition.       When specifying a static partition  `column     =     value`  this column must not be repeated in the insert column list. * **(  [column\_name](sql-ref-identifiers.html)  [, …] )**       An optional list of columns in the table. The insert command may specify any particular column from the table at most once.      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  [SQL warehouse version 2022.35](../release-notes/index.html)  or higher  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.2 and above      + If this command omits a column, Databricks SQL assigns the corresponding default value instead.  + If the target table schema does not define any default value for the inserted column, Databricks SQL assigns   `NULL`   if the column is nullable. Otherwise, Databricks SQL raises an error.  Providing no column list is equivalent to specifying all columns, except for those with assigned values in the  `PARTITION`  clause, in the order defined in the table. * **REPLACE WHERE  [boolean\_expression](sql-ref-expression.html)**     **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 12.0 and above       If  `table\_name`  is a Delta Lake table, delete rows matching  `boolean\_expression`  before inserting any rows matching  `boolean-expression`  specified in  `query`  . Rows in  `query`  which do not match  `boolean\_expression`  are ignored.      `boolean\_expression`  can be any expression that evaluates to a result type  `BOOLEAN`  .       See  [Arbitrary selective overwrite with replaceWhere](../../delta/selective-overwrite.html#replace-where)  . * **[query](sql-ref-syntax-qry-query.html)**      A query that produces the rows to be inserted.       You must match the number of columns returned by the query with the specified or implied insert column list.       If a data type cannot be safely cast to the matching column data type, a runtime exception is thrown.      **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  [SQL warehouse version 2022.35](../release-notes/index.html)  or higher  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.2 and above      + If   `query`   consists of a   [VALUES](sql-ref-syntax-qry-select-values.html)   clause the   `expression`   can be   `DEFAULT`   .  + If   `query`   consists of a   [SELECT](sql-ref-syntax-qry-select.html)   clause the   `named\_expression`   can be   `DEFAULT`   .  + `DEFAULT`   will insert the explicitly defined   `DEFAULT`   expression of the corresponding column in   `table\_name`   , or   `NULL`   if none is defined.  If  [schema evolution](../../delta/update-schema.html)  is enabled, new columns can exist as the last columns of your schema (or nested columns) for the schema to evolve.       Examples ----------      In this section:     * [INSERT INTO](#insert-into) * [Insert with a column list](#insert-with-a-column-list) * [Insert with both a partition spec and a column list](#insert-with-both-a-partition-spec-and-a-column-list) * [REPLACE WHERE](#replace-where) * [INSERT OVERWRITE](#insert-overwrite)     ###  [INSERT INTO](#id3)    ####   INSERT using VALUES      ``` > CREATE TABLE students (name VARCHAR(64), address VARCHAR(64) DEFAULT 'unknown', student\_id INT)  PARTITIONED BY (student\_id);  -- Single row insert using a `VALUES` clause specifying all columns. > INSERT INTO students VALUES  ('Amy Smith', '123 Park Ave, San Jose', 111111);  -- Single row insert using an implicit default for address > INSERT INTO students(name, student\_id) VALUES('Grayson Miller', 222222);  -- Single row insert using an explicit DEFAULT keyword for address > INSERT INTO students VALUES('Youna Kim', DEFAULT, 333333);  -- Multi-row insert using a `VALUES` clause > INSERT INTO students VALUES  ('Bob Brown', '456 Taylor St, Cupertino', 444444),  ('Cathy Johnson', '789 Race Ave, Palo Alto', 555555);  -- Multi-row insert using a mix of DEFAULT and literals > INSERT INTO students VALUES  ('Gwyneth Zhao', '120 Main St, Rockport', 666666),  ('Jackson Peterson', DEFAULT, 777777);  > SELECT \* FROM students;  name address student\_id  ---------------- ------------------------ ----------  Amy Smith 123 Park Ave, San Jose 111111  Grayson Miller unknown 222222  Youna Kim unknown 333333  Bob Brown 456 Taylor St, Cupertino 444444  Cathy Johnson 789 Race Ave, Palo Alto 555555  Gwyneth Zhao 120 Main St, Rockport 666666  Jackson Peterson unknown 777777  ```      ####   Insert using a subquery      ``` -- Assuming the persons table has already been created and populated. > SELECT \* FROM persons;  name address ssn  ------------- ------------------------- ---------  Dora Williams 134 Forest Ave, Melo Park 123456789  Eddie Davis 245 Market St, Milpitas 345678901  > INSERT INTO students PARTITION (student\_id = 444444)  SELECT name, address FROM persons WHERE name = ""Dora Williams"";  > SELECT \* FROM students;  name address student\_id  ------------- ------------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111  Bob Brown 456 Taylor St, Cupertino 222222  Cathy Johnson 789 Race Ave, Palo Alto 333333  Dora Williams 134 Forest Ave, Melo Park 444444  ```      ####   Insert using a  `TABLE`  clause      ``` -- Assuming the visiting\_students table has already been created and populated. > SELECT \* FROM visiting\_students;  name address student\_id  ------------- --------------------- ----------  Fleur Laurent 345 Copper St, London 777777  Gordon Martin 779 Lake Ave, Oxford 888888  > INSERT INTO students TABLE visiting\_students;  > SELECT \* FROM students;  name address student\_id  ------------- ------------------------- ----------  Amy Smith 123 Park Ave,San Jose 111111  Bob Brown 456 Taylor St, Cupertino 222222  Cathy Johnson 789 Race Ave, Palo Alto 333333  Dora Williams 134 Forest Ave, Melo Park 444444  Fleur Laurent 345 Copper St, London 777777  Gordon Martin 779 Lake Ave, Oxford 888888  ```      ####   Insert into a directory      ``` > CREATE TABLE students (name VARCHAR(64), address VARCHAR(64), student\_id INT)  PARTITIONED BY (student\_id)  LOCATION ""/mnt/user1/students"";  > INSERT INTO delta.`/mnt/user1/students` VALUES  ('Amy Smith', '123 Park Ave, San Jose', 111111); > SELECT \* FROM students;  name address student\_id  ------------- ------------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111  ```       ###  [Insert with a column list](#id4)      ``` > INSERT INTO students (address, name, student\_id) VALUES  ('Hangzhou, China', 'Kent Yao', 11215016); > SELECT \* FROM students WHERE name = 'Kent Yao';  name address student\_id  --------- ---------------------- ----------  Kent Yao Hangzhou, China 11215016  ```      ###  [Insert with both a partition spec and a column list](#id5)      ``` > INSERT INTO students PARTITION (student\_id = 11215017) (address, name) VALUES  ('Hangzhou, China', 'Kent Yao Jr.'); > SELECT \* FROM students WHERE student\_id = 11215017;  name address student\_id  ------------ ---------------------- ----------  Kent Yao Jr. Hangzhou, China 11215017  ```      ###  [REPLACE WHERE](#id6)      ``` > CREATE TABLE sales(tx\_date DATE, amount INTEGER); > INSERT INTO sales VALUES  (DATE'2022-10-01', 1234),  (DATE'2022-10-02', 2345),  (DATE'2022-10-03', 3456),  (DATE'2022-11-01', 3214);  -- Replace any rows with a transaction date in October 2022. > INSERT INTO sales REPLACE WHERE tx\_date BETWEEN '2022-10-01' AND '2022-10-31'  VALUES (DATE'2022-10-01', 1237),  (DATE'2022-10-02', 2378),  (DATE'2022-10-04', 2456),  (DATE'2022-10-05', 6328); > SELECT \* FROM sales ORDER BY tx\_date;  tx\_date amount  ---------- ------  2022-10-01 1237  2022-10-02 2378  2022-10-04 2456  2022-10-05 6328  2022-"	To insert into a delta table and remove any duplicates, you can use the `INSERT INTO` statement with the `MERGE` clause. Here is an example:  ``` INSERT INTO delta_table SELECT DISTINCT * FROM source_table MERGE INTO delta_table ON delta_table.key_column = source_table.key_column WHEN NOT MATCHED THEN   INSERT * ```  In this example, `delta_table` is the name of the Delta table you want to insert into, and `source_table` is the name of the table you want to insert from. The `MERGE` clause is used to remove any duplicates based on the `key_column` in both tables. The `WHEN NOT MATCHED THEN INSERT *` clause is used to insert the non-duplicate rows into the Delta table.	0	0	0	True
52	no_reaction	https://docs.databricks.com/security/network/secure-cluster-connectivity.html	how do I enable secure cluster connectivity	Secure cluster connectivity =============================      Important       This feature is available only if your account is on the  [E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . Secure cluster connectivity is enabled for accounts on the E2 platform by default as of September 1, 2020.        With secure cluster connectivity enabled, customer VPCs have no open ports and Databricks Runtime cluster nodes have no public IP addresses.        Important       This article mentions the term  *data plane*   , which is the compute layer of the Databricks platform. In the context of this article, data plane refers to the Classic data plane in your AWS account. By contrast, the serverless data plane that supports  [serverless SQL warehouses (Public Preview)](../../sql/admin/serverless.html)  runs in the Databricks AWS account. To learn more, see  [Serverless compute](../../serverless-compute/index.html)  .      * At a network level, each cluster initiates a connection to the control plane secure cluster connectivity relay during cluster creation. The cluster establishes this connection using port 443 (HTTPS) and uses a different IP address than is used for the Web application and REST API. * When the control plane logically starts new Databricks Runtime jobs or performs other cluster administration tasks, these requests are sent to the cluster through this tunnel. * The data plane (the VPC) has no open ports, and Databricks Runtime cluster nodes have no public IP addresses.     Benefits:     * Easy network administration, with no need to configure ports on security groups or to configure network peering. * With enhanced security and simple network administration, information security teams can expedite approval of Databricks as a PaaS provider.    ![Secure cluster connectivity](../../_images/secure-cluster-connectivity-aws.png)     Use secure cluster connectivity ---------------------------------     To use secure cluster connectivity for a workspace, create a new workspace. You cannot add secure cluster connectivity to an existing workspace.	To enable secure cluster connectivity, you need to have an account on the E2 version of the Databricks platform. If you have an account on the E2 platform, secure cluster connectivity is enabled by default as of September 1, 2020. With secure cluster connectivity enabled, customer VPCs have no open ports and Databricks Runtime cluster nodes have no public IP addresses. At a network level, each cluster initiates a connection to the control plane secure cluster connectivity relay during cluster creation. The cluster establishes this connection using port 443 (HTTPS) and uses a different IP address than is used for the Web application and REST API. When the control plane logically starts new Databricks Runtime jobs or performs other cluster administration tasks, these requests are sent to the cluster through this tunnel. The data plane (the VPC) has no open ports, and Databricks Runtime cluster nodes have no public IP addresses. To use secure cluster connectivity for a workspace, create a new workspace. You cannot add secure cluster connectivity to an existing workspace.	1	1	1	True
53	no_reaction	https://docs.databricks.com/delta/clone.html	shallow clone table performance	"    Clone a table on Databricks =============================     You can create a copy of an existing Delta Lake table on Databricks at a specific version using the  `clone`  command. Clones can be either deep or shallow.       Clone is a Databricks-exclusive feature enabled in the Databricks Runtime by default.       Databricks also supports cloning Parquet and Iceberg tables. See  [Incrementally clone Parquet and Iceberg tables to Delta Lake](clone-parquet.html)  .          Clone types -------------   * A  *deep clone*   is a clone that copies the source table data to the clone target in addition to the metadata of the existing table. Additionally, stream metadata is also cloned such that a stream that writes to the Delta table can be stopped on a source table and continued on the target of a clone from where it left off. * A  *shallow clone*   is a clone that does not copy the data files to the clone target. The table metadata is equivalent to the source. These clones are cheaper to create.     Any changes made to either deep or shallow clones affect only the clones themselves and not the source table.       The metadata that is cloned includes: schema, partitioning information, invariants, nullability. For deep clones only, stream and  [COPY INTO](../sql/language-manual/delta-copy-into.html)  metadata are also cloned. Metadata not cloned are the table description and  [user-defined commit metadata](custom-metadata.html)  .        Important     * Shallow clones reference data files in the source directory. If you run  `vacuum`  on the source table clients will no longer be able to read the referenced data files and a  `FileNotFoundException`  will be thrown. In this case, running clone with replace over the shallow clone will repair the clone. If this occurs often, consider using a deep clone instead which does not depend on the source table. * Deep clones do not depend on the source from which they were cloned, but are expensive to create because a deep clone copies the data as well as the metadata. * Cloning with  `replace`  to a target that already has a table at that path creates a Delta log if one does not exist at that path. You can clean up any existing data by running  `vacuum`  . * If an existing Delta table exists, a new commit is created that includes the new metadata and new data from the source table. This new commit is incremental, meaning that only new changes since the last clone are committed to the table. * Cloning a table is not the same as  `Create     Table     As     Select`  or  `CTAS`  . A clone copies the metadata of the source table in addition to the data. Cloning also has simpler syntax: you don’t need to specify partitioning, format, invariants, nullability and so on as they are taken from the source table. * A cloned table has an independent history from its source table. Time travel queries on a cloned table will not work with the same inputs as they work on its source table.        ``` CREATE TABLE delta.`/data/target/` CLONE delta.`/data/source/` -- Create a deep clone of /data/source at /data/target  CREATE OR REPLACE TABLE db.target\_table CLONE db.source\_table -- Replace the target  CREATE TABLE IF NOT EXISTS delta.`/data/target/` CLONE db.source\_table -- No-op if the target table exists  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source`  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` VERSION AS OF version  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` TIMESTAMP AS OF timestamp\_expression -- timestamp can be like “2019-01-01” or like date\_sub(current\_date(), 1)  ```       ``` from delta.tables import \*  deltaTable = DeltaTable.forPath(spark, pathToTable)  # path-based tables, or deltaTable = DeltaTable.forName(spark, tableName)    # Hive metastore-based tables  deltaTable.clone(target, isShallow, replace) # clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) # clone the source at a specific version  # clone the source at a specific timestamp such as timestamp=“2019-01-01” deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace)  ```       ``` import io.delta.tables.\_  val deltaTable = DeltaTable.forPath(spark, pathToTable) val deltaTable = DeltaTable.forName(spark, tableName)  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       ``` import io.delta.tables.\*;  DeltaTable deltaTable = DeltaTable.forPath(spark, pathToTable); DeltaTable deltaTable = DeltaTable.forName(spark, tableName);  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       For syntax details, see  [CREATE TABLE CLONE](../sql/language-manual/delta-clone.html)  .         Clone metrics ---------------      Note       Available in Databricks Runtime 8.2 and above.       `CLONE`  reports the following metrics as a single row DataFrame once the operation is complete:     * `source\_table\_size`  : Size of the source table that’s being cloned in bytes. * `source\_num\_of\_files`  : The number of files in the source table. * `num\_removed\_files`  : If the table is being replaced, how many files are removed from the current table. * `num\_copied\_files`  : Number of files that were copied from the source (0 for shallow clones). * `removed\_files\_size`  : Size in bytes of the files that are being removed from the current table. * `copied\_files\_size`  : Size in bytes of the files copied to the table.    ![Clone metrics example](../_images/clone-metrics.png)      Permissions -------------     You must configure permissions for Databricks table access control and your cloud provider.      ###   Table access control     The following permissions are required for both deep and shallow clones:     * `SELECT`  permission on the source table. * If you are using  `CLONE`  to create a new table,  `CREATE`  permission on the database in which you are creating the table. * If you are using  `CLONE`  to replace a table, you must have  `MODIFY`  permission on the table.     ###   Cloud provider permissions     If you have created a deep clone, any user that reads the deep clone must have read access to the clone’s directory. To make changes to the clone, users must have write access to the clone’s directory.       If you have created a shallow clone, any user that reads the shallow clone needs permission to read the files in the original table, since the data files remain in the source table with shallow clones, as well as the clone’s directory. To make changes to the clone, users will need write access to the clone’s directory.          Use clone for data archiving ------------------------------     Data may need to be kept for longer than is feasible with time travel or for disaster recovery. In these cases, you can create a deep clone to preserve the state of a table at a certain point in time for archival. Incremental archiving is also possible to keep a continually updating state of a source table for disaster recovery.        ``` -- Every month run CREATE OR REPLACE TABLE delta.`/some/archive/path` CLONE my\_prod\_table  ```        Use clone for ML model reproduction -------------------------------------     When doing machine learning, you may want to archive a certain version of a table on which you trained an ML model. Future models can be tested using this archived data set.        ``` -- Trained model on version 15 of Delta table CREATE TABLE delta.`/model/dataset` CLONE entire\_dataset VERSION AS OF 15  ```        Use clone for short-term experiments on a production table ------------------------------------------------------------     To test a workflow on a production table without corrupting the table, you can easily create a shallow clone. This allows you to run arbitrary workflows on the cloned table that contains all the production data but does not affect any production workloads.        ``` -- Perform shallow clone CREATE OR REPLACE TABLE my\_test SHALLOW CLONE my\_prod\_table;  UPDATE my\_test WHERE user\_id is null SET invalid=true; -- Run a bunch of validations. Once happy:  -- This should leverage the update information in the clone to prune to only -- changed files in the clone if possible MERGE INTO my\_prod\_table USING my\_test ON my\_test.user\_id <=> my\_prod\_table.user\_id WHEN MATCHED AND my\_test.user\_id is null THEN UPDATE \*;  DROP TABLE my\_test;  ```        Use clone for data sharing ----------------------------     Other business units within a single organization may want to access the same data but may not require the latest updates. Instead of giving access to the source table directly, you can provide clones with different permissions for different business units. The performance of the clone can exceed that of a simple view.        ``` -- Perform deep clone CREATE OR REPLACE TABLE shared\_table CLONE my\_prod\_table;  -- Grant other users access to the shared table GRANT SELECT ON shared\_table TO `<user-name>@<user-domain>.com`;  ```        Use clone to override table properties ----------------------------------------      Note       Available in Databricks Runtime 7.5 and above.        Table property overrides are particularly useful for:     * Annotating tables with owner or user information when sharing data with different business units. * Archiving Delta tables and time travel is required. You can specify the log retention period independently for the archive table. For example:       ``` CREATE OR REPLACE TABLE archive.my\_table CLONE prod.my\_table TBLPROPERTIES ( delta.logRetentionDuration = '3650 days', delta.deletedFileRetentionDuration = '3650 days' ) LOCATION 'xx://archive/my\_table'  ```       ``` dt = DeltaTable.forName(spark, ""prod.my\_table"") tblProps = { ""delta.logRetentionDuration"": ""3650 days"", ""delta.deletedFileRetentionDuration"": ""3650 days"" } dt.clone('xx://archive/my\_table', isShallow=False, replace=True, tblProps)  ```       ``` val dt = DeltaTable.forName(spark, ""prod.my\_table"") val tblProps = Map( ""delta.logRetentionDuration"" -> ""3650 days"", ""delta.deletedFileRetentionDuration"" -> ""3650 days"" ) dt.clone(""xx://archive/my\_table"", isShallow = false, replace = true, properties = tblProps)  ```                                                        "	Shallow clone table performance.	0	0	0	True
55	no_reaction	https://docs.databricks.com/sql/index.html	why use databricks sql	###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) * [Error Conditions](sql-error-conditions.html)   Spark SQL, DataFrames and Datasets Guide ==========================================   Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.     All of the examples on this page use sample data included in the Spark distribution and can be run in the  `spark-shell`   ,  `pyspark`   shell, or  `sparkR`   shell.     SQL -----   One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. For more on how to configure this feature, please refer to the  [Hive Tables](sql-data-sources-hive-tables.html)   section. When running SQL from within another programming language the results will be returned as a  [Dataset/DataFrame](#datasets-and-dataframes)   . You can also interact with the SQL interface using the  [command-line](sql-distributed-sql-engine.html#running-the-spark-sql-cli)   or over  [JDBC/ODBC](sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server)   .     Datasets and DataFrames -------------------------   A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be  [constructed](sql-getting-started.html#creating-datasets)   from JVM objects and then manipulated using functional transformations (  `map`   ,  `flatMap`   ,  `filter`   , etc.). The Dataset API is available in  [Scala](api/scala/org/apache/spark/sql/Dataset.html)   and  [Java](api/java/index.html?org/apache/spark/sql/Dataset.html)   . Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally  `row.columnName`   ). The case for R is similar.     A DataFrame is a  *Dataset*   organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of  [sources](sql-data-sources.html)   such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java,  [Python](api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)   , and  [R](api/R/index.html)   . In Scala and Java, a DataFrame is represented by a Dataset of  `Row`   s. In  [the Scala API](api/scala/org/apache/spark/sql/Dataset.html)   ,  `DataFrame`   is simply a type alias of  `Dataset[Row]`   . While, in  [Java API](api/java/index.html?org/apache/spark/sql/Dataset.html)   , users need to use  `Dataset<Row>`   to represent a  `DataFrame`   .     Throughout this document, we will often refer to Scala/Java Datasets of  `Row`   s as DataFrames.	Databricks SQL is used for structured data processing. It provides interfaces for Spark SQL, DataFrames, and Datasets. Spark SQL uses extra information to perform extra optimizations. One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. A Dataset is a distributed collection of data. A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.	1	1	1	True
57	no_reaction	https://docs.databricks.com/archive/spark-sql-2.x-language-manual/vacuum.html	what is vacuum?	Vacuum ========      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [VACUUM](../../sql/language-manual/delta-vacuum.html)  .        Clean up files associated with a table.       This command works differently depending on whether you’re working on a Delta or Apache Spark table.          Vacuum a Delta table (Delta Lake on Databricks) -------------------------------------------------      ``` VACUUM [ [db\_name.]table\_name | path] [RETAIN num HOURS] [DRY RUN]  ```      Recursively vacuum directories associated with the Delta table and remove data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. Files are deleted according to the time they have been logically removed from Delta’s transaction log + retention hours, not their modification timestamps on the storage system. The default threshold is 7 days.       On Delta tables, Databricks  *does not*   automatically trigger  `VACUUM`  operations. See  [Remove unused data files with vacuum](../../delta/vacuum.html)  .       If you run  `VACUUM`  on a Delta table, you lose the ability to  [time travel](../../delta/history.html)  back to a version older than the specified data retention period.      **`RETAIN     num     HOURS`**      The retention threshold.      **`DRY     RUN`**      Return a list of files to be deleted.           Vacuum a Spark table (Apache Spark) -------------------------------------      ``` VACUUM [ [db\_name.]table\_name | path] [RETAIN num HOURS]  ```     **`RETAIN     num     HOURS`**      The retention threshold.       Recursively vacuum directories associated with the Spark table and remove uncommitted files older than a retention threshold. The default threshold is 7 days.       On Spark tables, Databricks automatically triggers  `VACUUM`  operations as data is written.	Vacuum is a command used to clean up files associated with a table. It works differently depending on whether you’re working on a Delta or Apache Spark table. When vacuuming a Delta table, it recursively vacuums directories associated with the table and removes data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. When vacuuming a Spark table, it recursively vacuums directories associated with the table and removes uncommitted files older than a retention threshold.	1	1	1	True
58	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/get-started.html	How to use unity data catalog?	Data discovery and collaboration in the lakehouse ===================================================     Databricks designed Unity Catalog to help organizations reduce time to insights by empowering a broader set of data users to discover and analyze data at scale. Data stewards can securely grant access to data assets for diverse teams of end users in Unity Catalog. These users can then use a variety of languages and tools, including SQL and Python, to create derivative datasets, models, and dashboards that can be shared across teams.        Manage permissions at scale -----------------------------     Unity Catalog provides administrators a unified location to assign permissions for catalogs, databases, tables, and views to groups of users. Privileges and metastores are shared across workspaces, allowing administrators to set secure permissions once against groups synced from identity providers and know that end users only have access to the proper data in any Databricks workspace they enter.       Unity Catalog also allows administrators to define storage credentials, a secure way to store and share permissions on cloud storage infrastructure. You can grant privileges on these securables to power users within the organization so they can define external locations against cloud object storage locations, allowing data engineers to self-service for new workloads without needing to provide elevated permissions in cloud account consoles.         Discover data on Databricks -----------------------------     Users can browse available data objects in Unity Catalog using the  [Data Explorer](../data/index.html)  . Data Explorer uses the privileges configured by Unity Catalog administrators to ensure that users are only able to see catalogs, databases, tables, and views that they have permissions to query. Once users find a dataset of interest, they can review field names and types, read comments on tables and individual fields, and preview a sample of the data. Users can also review the full history of the table to understand when and how data has changed, and the lineage feature allows users to track how certain datasets are derived from upstream jobs and used in downstream jobs.       Storage credentials and external locations are also displayed in Data Explorer, allowing each user to fully grasp the privileges they have to read and write data across available locations and resources.         Accelerate time to production with the lakehouse --------------------------------------------------     Databricks supports workloads in SQL, Python, Scala, and R, allowing users with diverse skill sets and technical backgrounds to leverage their knowledge to derive analytic insights. You can use all languages supported by Databricks to define production jobs, and notebooks can leverage a combination of languages. This means that you can promote queries written by SQL analysts for last mile ETL into production data engineering code with almost no effort. Queries and workloads defined by personas across the organization leverage the same datasets, so there’s no need to reconcile field names or make sure dashboards are up to date before sharing code and results with other teams. You can securely share code, notebooks, queries, and dashboards, all powered by the same scalable cloud infrastructure and defined against the same curated data sources.	To use Unity Catalog, data stewards can securely grant access to data assets for diverse teams of end users in Unity Catalog. Users can browse available data objects in Unity Catalog using the Data Explorer. Unity Catalog provides administrators a unified location to assign permissions for catalogs, databases, tables, and views to groups of users. Privileges and metastores are shared across workspaces, allowing administrators to set secure permissions once against groups synced from identity providers and know that end users only have access to the proper data in any Databricks workspace they enter. Unity Catalog also allows administrators to define storage credentials, a secure way to store and share permissions on cloud storage infrastructure.	1	1	1	True
59	no_reaction	https://docs.databricks.com/machine-learning/feature-store/time-series.html	Does FeatureStoreClient().score_batch support multidimentional predictions? I have a pyfunc model that I can use to get predictions. It takes time series data with context information at each date, and produces a string of predictions. For example:     The data is set up like below (temp/pressure/output are different than my input data columns)  date,sales,temperature,pressure,output 01-09-2020,100,101,5000,10 01-10-2020,120,91,4000,24 01-11-2020,50,110,6000,30 let's say the model is trained using a window size of 60 and prediction inverval of 14. then in the model, you provide provide 60 records, and the prediction will get 14 predictions starting the last date+1 in your prediction dataset.     the return is just of the form  date,prediction 01-12-2022,81 01-13-2022,60 01-14-2022,111 ... with N records (in our example 14). it works brilliantly if I augment the data myself and work with the predict function.     Does working with the feature store support this? score batch doesn't seem to be able to return arbitrary/different shaped data. I could try making the data wide, but then that would defeat the purpose of trying to use the feature store.     don't know if I'm making sense.	"   pyspark.ml.functions.predict\_batch\_udf  [¶](#pyspark-ml-functions-predict-batch-udf ""Permalink to this headline"") =====================================================================================================================  `pyspark.ml.functions.`  `predict_batch_udf`    (   *make\_predict\_fn     :     Callable    [     ]     ,    PredictBatchFunction    ]*  ,  *\**  ,  *return\_type     :     DataType*  ,  *batch\_size     :     int*  ,  *input\_tensor\_shapes     :     Optional    [    Union    [    List    [    Optional    [    List    [    int    ]     ]     ]     ,    Mapping    [    int    ,    List    [    int    ]     ]     ]     ]     =     None*   )    → UserDefinedFunctionLike  [[source]](../../_modules/pyspark/ml/functions.html#predict_batch_udf) [¶](#pyspark.ml.functions.predict_batch_udf ""Permalink to this definition"")    Given a function which loads a model and returns a    predict    function for inference over a batch of numpy inputs, returns a Pandas UDF wrapper for inference over a Spark DataFrame.     The returned Pandas UDF does the following on each DataFrame partition:    * calls the    make\_predict\_fn    to load the model and cache its    predict    function. * batches the input records as numpy arrays and invokes    predict    on each batch.   Note: this assumes that the    make\_predict\_fn    encapsulates all of the necessary dependencies for running the model, or the Spark executor environment already satisfies all runtime requirements.     For the conversion of the Spark DataFrame to numpy arrays, there is a one-to-one mapping between the input arguments of the    predict    function (returned by the    make\_predict\_fn    ) and the input columns sent to the Pandas UDF (returned by the    predict\_batch\_udf    ) at runtime. Each input column will be converted as follows:    * scalar column -> 1-dim np.ndarray * tensor column + tensor shape -> N-dim np.ndarray   Note that any tensor columns in the Spark DataFrame must be represented as a flattened one-dimensional array, and multiple scalar columns can be combined into a single tensor column using the standard  [`pyspark.sql.functions.array()`](../pyspark.sql/api/pyspark.sql.functions.array.html#pyspark.sql.functions.array ""pyspark.sql.functions.array"")  function.     New in version 3.4.0.     Parameters    **make\_predict\_fn**    callable     Function which is responsible for loading a model and returning a  `PredictBatchFunction`  which takes one or more numpy arrays as input and returns one of the following:    * a numpy array (for a single output) * a dictionary of named numpy arrays (for multiple outputs) * a row-oriented list of dictionaries (for multiple outputs).   For a dictionary of named numpy arrays, the arrays can only be one or two dimensional, since higher dimensional arrays are not supported. For a row-oriented list of dictionaries, each element in the dictionary must be either a scalar or one-dimensional array.    **return\_type**   [`pyspark.sql.types.DataType`](../pyspark.sql/api/pyspark.sql.types.DataType.html#pyspark.sql.types.DataType ""pyspark.sql.types.DataType"")  or str.     Spark SQL datatype for the expected output:    * Scalar (e.g. IntegerType, FloatType) –> 1-dim numpy array. * ArrayType –> 2-dim numpy array. * StructType –> dict with keys matching struct fields. * StructType –> list of dict with keys matching struct fields, for models like the  [Huggingface pipeline for sentiment analysis](https://huggingface.co/docs/transformers/quicktour#pipeline-usage)   .  **batch\_size**    int     Batch size to use for inference. This is typically a limitation of the model and/or available hardware resources and is usually smaller than the Spark partition size.    **input\_tensor\_shapes**    list, dict, optional.     A list of ints or a dictionary of ints (key) and list of ints (value). Input tensor shapes for models with tensor inputs. This can be a list of shapes, where each shape is a list of integers or None (for scalar inputs). Alternatively, this can be represented by a “sparse” dictionary, where the keys are the integer indices of the inputs, and the values are the shapes. Each tensor input value in the Spark DataFrame must be represented as a single column containing a flattened 1-D array. The provided    input\_tensor\_shapes    will be used to reshape the flattened array into the expected tensor shape. For the list form, the order of the tensor shapes must match the order of the selected DataFrame columns. The batch dimension (typically -1 or None in the first dimension) should not be included, since it will be determined by the batch\_size argument. Tabular datasets with scalar-valued columns should not provide this argument.     Returns    `UserDefinedFunctionLike`   A Pandas UDF for model inference on a Spark DataFrame.     Examples     For a pre-trained TensorFlow MNIST model with two-dimensional input images represented as a flattened tensor value stored in a single Spark DataFrame column of type    array<float>    .    ``` from pyspark.ml.functions import predict\_batch\_udf  def make\_mnist\_fn():     # load/init happens once per python worker     import tensorflow as tf     model = tf.keras.models.load\_model('/path/to/mnist\_model')      # predict on batches of tasks/partitions, using cached model     def predict(inputs: np.ndarray) -> np.ndarray:         # inputs.shape = [batch\_size, 784], see input\_tensor\_shapes         # outputs.shape = [batch\_size, 10], see return\_type         return model.predict(inputs)      return predict  mnist\_udf = predict\_batch\_udf(make\_mnist\_fn,                               return\_type=ArrayType(FloatType()),                               batch\_size=100,                               input\_tensor\_shapes=[[784]])  df = spark.read.parquet(""/path/to/mnist\_data"") df.show(5) # +--------------------+ # | data| # +--------------------+ # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # |[0.0, 0.0, 0.0, 0...| # +--------------------+  df.withColumn(""preds"", mnist\_udf(""data"")).show(5) # +--------------------+--------------------+ # | data| preds| # +--------------------+--------------------+ # |[0.0, 0.0, 0.0, 0...|[-13.511008, 8.84...| # |[0.0, 0.0, 0.0, 0...|[-5.3957458, -2.2...| # |[0.0, 0.0, 0.0, 0...|[-7.2014456, -8.8...| # |[0.0, 0.0, 0.0, 0...|[-19.466187, -13....| # |[0.0, 0.0, 0.0, 0...|[-5.7757926, -7.8...| # +--------------------+--------------------+  ```   To demonstrate usage with different combinations of input and output types, the following examples just use simple mathematical transforms as the models.    * Single scalar column     Input DataFrame has a single scalar column, which will be passed to the    predict    function as a 1-D numpy array.    ``` >>> import numpy as np >>> import pandas as pd >>> from pyspark.ml.functions import predict\_batch\_udf >>> from pyspark.sql.types import FloatType >>> >>> df = spark.createDataFrame(pd.DataFrame(np.arange(100))) >>> df.show(5) +---+ | 0| +---+ | 0| | 1| | 2| | 3| | 4| +---+ only showing top 5 rows  ```  ``` >>> def make\_times\_two\_fn(): ...     def predict(inputs: np.ndarray) -> np.ndarray: ...         # inputs.shape = [batch\_size] ...         # outputs.shape = [batch\_size] ...         return inputs \* 2 ...     return predict >>> >>> times\_two\_udf = predict\_batch\_udf(make\_times\_two\_fn, ...                                   return\_type=FloatType(), ...                                   batch\_size=10) >>> >>> df = spark.createDataFrame(pd.DataFrame(np.arange(100))) >>> df.withColumn(""x2"", times\_two\_udf(""0"")).show(5) +---+---+ | 0| x2| +---+---+ | 0|0.0| | 1|2.0| | 2|4.0| | 3|6.0| | 4|8.0| +---+---+ only showing top 5 rows  ``` * Multiple scalar columns     Input DataFrame has muliple columns of scalar values. If the user-provided    predict    function expects a single input, then the user must combine the multiple columns into a single tensor using    pyspark.sql.functions.array    .    ``` >>> import numpy as np >>> import pandas as pd >>> from pyspark.ml.functions import predict\_batch\_udf >>> from pyspark.sql.functions import array >>> >>> data = np.arange(0, 1000, dtype=np.float64).reshape(-1, 4) >>> pdf = pd.DataFrame(data, columns=a','b','c','d) >>> df = spark.create"	Based on the context, it seems that `score_batch` does not support multidimensional predictions. The user has a pyfunc model that produces a string of predictions for time series data with context information at each date. The model is trained using a window size of 60 and prediction interval of 14. The return is of the form date, prediction with N records (in the example 14). The user is wondering if working with the feature store supports this, but `score_batch` doesn't seem to be able to return arbitrary/different shaped data. The user could try making the data wide, but then that would defeat the purpose of trying to use the feature store.	0	0	0	True
60	no_reaction	https://docs.databricks.com/data/manage-permissions.html	in unity catalog, how do I grant permissions on a view?	"    Create views ==============     This article shows how to create views in Unity Catalog.       A view is a read-only object composed from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs.      [Dynamic views](#dynamic-view)  can be used to provide row- and column-level access control, in addition to data masking.       The owner of a view must have the ability to read the tables and views referenced in the view. A reader of a view does not need the ability to read the tables and views referenced in the view, unless they are using a cluster with single user access mode.        Note       To read from a view from a cluster with  **Single User**  [access mode](index.html#cluster-security-mode)  , you must have  `SELECT`  on all referenced tables and views. Dynamic views are not supported with  **Single User**   access mode.        Example syntax for creating a view:        ``` CREATE VIEW main.default.experienced\_employee  (id COMMENT 'Unique identification number', Name)  COMMENT 'View for experienced employees' AS SELECT id, name  FROM all\_employee  WHERE working\_years > 5;  ```       Requirements --------------   * You must have the  `USE     CATALOG`  permission on the parent catalog and the  `USE     SCHEMA`  and  `CREATE     TABLE`  permissions on the parent schema. A metastore admin or the catalog owner can grant you all of these privileges. A schema owner can grant you  `USE     SCHEMA`  and  `CREATE     TABLE`  privileges on the schema. * You must have the ability to read the tables and views referenced in the view (  `SELECT`  on the table or view, as well as  `USE     CATALOG`  on the catalog and  `USE     SCHEMA`  on the schema).     If a view references tables in the workspace-local Hive metastore, the view can be accessed only from the workspace that contains the workspace-local tables. For this reason, Databricks recommends creating views only from tables or views that are in the Unity Catalog metastore.         Create a view ---------------     To create a view, run the following SQL command. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. * `<schema\_name>`  : The name of the schema. * `<view\_name>`  : A name for the view. * `<query>`  : The query, columns, and tables and views used to compose the view.       ``` CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS SELECT <query>;  ```       ``` spark.sql(""CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS ""   ""SELECT <query>"")  ```       ``` library(SparkR)  sql(paste(""CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS "",  ""SELECT <query>"",  sep = """"))  ```       ``` spark.sql(""CREATE VIEW <catalog\_name>.<schema\_name>.<view\_name> AS "" +  ""SELECT <query>"")  ```       For example, to create a view named  `sales\_redacted`  from columns in the  `sales\_raw`  table:         ``` CREATE VIEW sales\_metastore.sales.sales\_redacted AS SELECT  user\_id,  email,  country,  product,  total FROM sales\_metastore.sales.sales\_raw;  ```       ``` spark.sql(""CREATE VIEW sales\_metastore.sales.sales\_redacted AS ""   ""SELECT ""   "" user\_id, ""   "" email, ""   "" country, ""   "" product, ""   "" total ""   ""FROM sales\_metastore.sales.sales\_raw"")  ```       ``` library(SparkR)  sql(paste(""CREATE VIEW sales\_metastore.sales.sales\_redacted AS "",  ""SELECT "",  "" user\_id, "",  "" email, "",  "" country, "",  "" product, "",  "" total "",  ""FROM sales\_metastore.sales.sales\_raw"",  sep = """"))  ```       ``` spark.sql(""CREATE VIEW sales\_metastore.sales.sales\_redacted AS "" +  ""SELECT "" +  "" user\_id, "" +  "" email, "" +  "" country, "" +  "" product, "" +  "" total "" +  ""FROM sales\_metastore.sales.sales\_raw"")  ```       You can also create a view by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of view full names by using  [databricks\_views](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/views)   .           Create a dynamic view -----------------------     In Unity Catalog, you can use dynamic views to configure fine-grained access control, including:     * Security at the level of columns or rows. * Data masking.      Note       Fine-grained access control using dynamic views is not available on clusters with  **Single User**  [access mode](compute.html#access-mode)  .        Unity Catalog introduces the following functions, which allow you to dynamically limit which users can access a row, column, or record in a view:     * `current\_user()`  : Returns the current user’s email address. * `is\_account\_group\_member()`  : Returns  `TRUE`  if the current user is a member of a specific account-level group. Recommended for use in dynamic views against Unity Catalog data. * `is\_member()`  : Returns  `TRUE`  if the current user is a member of a specific workspace-level group. This function is provided for compatibility with the existing Hive metastore. Avoid using it with views against Unity Catalog data, because it does not evaluate account-level group membership.     Databricks recommends that you do not grant users the ability to read the tables and views referenced in the view.       The following examples illustrate how to create dynamic views in Unity Catalog.      ###   Column-level permissions     With a dynamic view, you can limit the columns a specific user or group can access. In the following example, only members of the  `auditors`  group can access email addresses from the  `sales\_raw`  table. During query analysis, Apache Spark replaces the  `CASE`  statement with either the literal string  `REDACTED`  or the actual contents of the email address column. Other columns are returned as normal. This strategy has no negative impact on the query performance.         ``` -- Alias the field 'email' to itself (as 'email') to prevent the -- permission logic from showing up directly in the column name results. CREATE VIEW sales\_redacted AS SELECT  user\_id,  CASE WHEN  is\_account\_group\_member('auditors') THEN email  ELSE 'REDACTED'  END AS email,  country,  product,  total FROM sales\_raw  ```       ``` # Alias the field 'email' to itself (as 'email') to prevent the # permission logic from showing up directly in the column name results. spark.sql(""CREATE VIEW sales\_redacted AS ""   ""SELECT ""   "" user\_id, ""   "" CASE WHEN ""   "" is\_account\_group\_member('auditors') THEN email ""   "" ELSE 'REDACTED' ""   "" END AS email, ""   "" country, ""   "" product, ""   "" total ""   ""FROM sales\_raw"")  ```       ``` library(SparkR)  # Alias the field 'email' to itself (as 'email') to prevent the # permission logic from showing up directly in the column name results. sql(paste(""CREATE VIEW sales\_redacted AS "",  ""SELECT "",  "" user\_id, "",  "" CASE WHEN "",  "" is\_account\_group\_member('auditors') THEN email "",  "" ELSE 'REDACTED' "",  "" END AS email, "",  "" country, "",  "" product, "",  "" total "",  ""FROM sales\_raw"",  sep = """"))  ```       ``` // Alias the field 'email' to itself (as 'email') to prevent the // permission logic from showing up directly in the column name results. spark.sql(""CREATE VIEW sales\_redacted AS "" +  ""SELECT "" +  "" user\_id, "" +  "" CASE WHEN "" +  "" is\_account\_group\_member('auditors') THEN email "" +  "" ELSE 'REDACTED' "" +  "" END AS email, "" +  "" country, "" +  "" product, "" +  "" total "" +  ""FROM sales\_raw"")  ```       ###   Row-level permissions     With a dynamic view, you can specify permissions down to the row or field level. In the following example, only members of the  `managers`  group can view transaction amounts when they exceed $1,000,000. Matching results are filtered out for other users.         ```  CREATE VIEW sales\_redacted AS  SELECT  user\_id,  country,  product,  total  FROM sales\_raw  WHERE  CASE  WHEN is\_account\_group\_member('managers') THEN TRUE  ELSE total <= 1000000  END;  ```       ```  spark.sql(""CREATE VIEW sales\_redacted AS ""    ""SELECT ""    "" user\_id, ""    "" country, ""    "" product, ""    "" total ""    ""FROM sales\_raw ""    ""WHERE ""    ""CASE ""    "" WHEN is\_account\_group\_member('managers') THEN TRUE ""    "" ELSE total <= 1000000 ""    ""END"")  ```       ```  library(SparkR)   sql(paste(""CREATE VIEW sales\_redacted AS "",  ""SELECT "",  "" user\_id, "",  "" country, "",  "" product, "",  "" total "",  ""FROM sales\_raw "",  ""WHERE "",  ""CASE "",  "" WHEN is\_account\_group\_member('managers') THEN TRUE "",  "" ELSE total <= 1000000 "",  ""END"",  sep = """"))  ```       ```  spark.sql(""CREATE VIEW sales\_redacted AS "" +  ""SELECT "" +  "" user\_id, "" +  "" country, "" +  "" product, "" +  "" total "" +  ""FROM sales\_raw "" +  ""WHERE "" +  ""CASE "" +  "" WHEN is\_account\_group\_member('managers') THEN TRUE "" +  "" ELSE total <= 1000000 "" +  ""END"")  ```       ###   Data masking     Because views in Unity Catalog use Spark SQL, you can implement advanced data masking by using more complex SQL expressions and regular expressions. In the following example, all users can analyze email domains, but only members of the  `auditors`  group can view a user’s entire email address.         ``` -- The regexp\_extract function takes an email address such as -- user.x.lastname@example.com and"	To grant permissions on a view in Unity Catalog, you must have the `USE CATALOG` permission on the parent catalog and the `USE SCHEMA` and `CREATE TABLE` permissions on the parent schema. The owner of a view must have the ability to read the tables and views referenced in the view. A reader of a view does not need the ability to read the tables and views referenced in the view, unless they are using a cluster with single-user access mode. To create a view, you can use the `CREATE VIEW` command followed by the name of the view and the query that defines the view.	1	1	1	True
63	no_reaction	https://docs.databricks.com/administration-guide/account-settings-e2/private-access-settings.html	how to set up private link	Manage private access settings ================================         This article describes a process that is available only for  [accounts on the E2 version of the Databricks platform](../../getting-started/overview.html#e2-architecture)  . All new Databricks accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.       This article discusses how to create private access settings objects, which are a required object as part of enabling  [AWS PrivateLink](https://aws.amazon.com/privatelink)   . This article does not contain all the information necessary to configure PrivateLink for your workspace. For all requirements and steps, including the requirements for registering VPC endpoints and creating network configuration objects, see  [Enable AWS PrivateLink](../cloud-configurations/aws/privatelink.html)  .       The following related sections discuss updating existing network and configuration objects:     * [Update a running or failed workspace](../workspace/update-workspace.html)  . * [Updates of existing PrivateLink configuration objects](../cloud-configurations/aws/privatelink.html#update-related)  .      What is a private access settings object? -------------------------------------------     A private access settings object is a Databricks object that describes a workspace’s PrivateLink connectivity. Create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region.       This object serves several purposes:     * It expresses your intent to use AWS PrivateLink with your workspace. * It controls your settings for the front-end use case of AWS PrivateLink for public network access. * It controls which VPC endpoints are permitted to access your workspace.     Create a private access settings object using the account console or the  [Account API 2.0](../../dev-tools/api/latest/account.html)  . You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you  *must*   attach a private access settings object to the workspace during workspace creation.           Create a private access settings object -----------------------------------------      Note       These instructions show you how to create the private access object from the  **Cloud resources**   page in the account console before you create a new workspace. You can also create the private access settings in a similar way as part of the flow of creating a new workspace and choosing  **Add a new private access object**   from the picker instead of choosing an existing object. See  [Create a workspace using the account console](../workspace/create-workspace.html)  .      1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. Click  **Add private access settings**   .      ![private access settings object](../../_images/privatelink-vpc-pas.png) 5. Enter a name for your new private access settings object. 6. For the region, be sure to match the region of your workspace as this is not validated immediately and workspace deployment will fail if it does not match. It is validated only during the actual creation of the workspace. 7. Set the  **Public access enabled**   field, which configures public access to the front-end connection (the web application and REST APIs) for your workspace.      * If set to   **False**    (the default), the front-end connection can be accessed only using PrivateLink connectivity and not from the public internet. When public access is disabled, the   [IP access lists for workspaces](../../security/network/ip-access-list-workspace.html)   feature is unsupported.  * If set to   **True**    , the front-end connection can be accessed either from PrivateLink connectivity or from the public internet. Any IP access lists only limit connections from the public internet but not traffic through the PrivateLink connection. 8. Set the  **Private Access Level**   field to the value that best represents which VPC endpoints to allow for your workspace.      * Set to   **Account**    to limit connections to those VPC endpoints that are registered in your Databricks account.  * Set to   **Endpoint**    to limit connections to an explicit set of VPC endpoints, which you can enter in a field that appears. It lets you select VPC endpoint registrations that you’ve already created. Be sure to include your   *front-end*    VPC endpoint registration if you created one. 9. Click  **Add**   .         Update a private access settings object -----------------------------------------     To update fields on a private access object:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. In the horizontal tabs, click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Update**   . 5. Change any fields. For guidance on specific fields, see  [Create a private access settings object](#create)  .        Note       The private access access level  `ANY`  is deprecated. If the object previously had this value and you use the account console to update the private access settings for any fields, you must change the private access level to a non-deprecated value. To make changes to other fields without changing the  `ANY`  private access level at this time, use the  [Account API](../../dev-tools/api/latest/account.html)  . See  [AWS PrivateLink private access level ANY is deprecated](../../release-notes/product/2022/august.html#privatelink-private-access-level-any-deprecated)  . 6. Click  **Update private access setting**   .         Delete a private access settings object -----------------------------------------     Private access settings object cannot be edited after creation. If the configuration has incorrect data or if you no longer need it for any workspaces, delete it:     1. In the  [account console](../account-settings/index.html#account-console)  , click  **Cloud resources**   . 2. Click  **Network**   . 3. In the vertical tabs, click  **Private access settings**   . 4. On the row for the configuration, click the kebab menu  ![Vertical Ellipsis](../../_images/vertical-ellipsis.png)  on the right, and select  **Delete**   . 5. In the confirmation dialog, click  **Confirm Delete**   .	To set up PrivateLink in Databricks, you need to create a private access settings object, which is a Databricks object that describes a workspace's PrivateLink connectivity. You can create a new private access settings object just for this workspace, or re-use and share an existing private access setting object among multiple workspaces but they must be in the same AWS region. You can create a private access settings object using the account console or the Account API 2.0. You will reference it in the set of fields when you create a workspace. You can update a workspace to point to a different private access settings object but to use PrivateLink you must attach a private access settings object to the workspace during workspace creation.	1	1	1	True
64	no_reaction	https://kb.databricks.com/scala/jobs-fails-with-a-timeoutexception-error	I have a timeout issue when you're ingesting data, what should I do?	Title: Jobs fails with a TimeoutException error  Description: This error is usually caused by a Broadcast join that takes excessively long to complete.  Answer: Problem You are running Apache Spark SQL queries that perform join operations DataFrames, but the queries keep failing with a TimeoutException error message. Example stack trace Caused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]               at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)               at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)               at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)               at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) Cause This problem usually stems from Spark trying to perform Broadcast join and when the fetching of blocks from different executors consumes an excessive amount of time. Spark performs Broadcast join using the BitTorrent protocol. The driver splits the data to be broadcasted into small chunks and stores the chunks in the block manager of the driver. The driver also sends the chunks of data to the executors. Each executors keeps copies of the chunks of data in its own block manager.  When a specific executor is not able to fetch the chunks of data from its local block manager (say the executor died and re-launched) that executor tries to fetch the broadcast data from the driver as well as other executors. This avoids driver being the bottleneck in serving the remote requests.  Even with this distributed approach, there are some scenarios where the broadcast can take an excessive amount of time, resulting in a TimeoutException error. Busy driver or busy executor: If the Spark driver and executors are extremely busy, it can introduce delay in the broadcast process. If the broadcast process exceeds the threshold limits, it can result in a broadcast timeout. Large broadcast data size: Trying to broadcast a large amount of data can also result in a broadcast timeout. Spark has a default limit of 8GB for broadcast data.  Solution You need to identify the query that is causing resource bottleneck on the cluster. Open the Spark UI (AWS | Azure | GCP) and review any failed stages to locate the SQL query causing the failure. Review the Spark SQL plan to see if it uses BroadcastNestedLoopJoin. If the Spark SQL plan uses BroadcastNestedLoopJoin, you need to follow the instructions in the Disable broadcast when query plan has BroadcastNestedLoopJoin article. If the Spark SQL plan does not use BroadcastNestedLoopJoin, you can disable the Broadcast join by setting Spark config values right before the problematic query. You can then revert these changes after the problematic query. Making the change query specific allows other queries, which can benefit from the Broadcast join, to still leverage the benefits. SET spark.sql.autoBroadcastJoinThreshold=-1  This disables Broadcast join. SET spark.databricks.adaptive.autoBroadcastJoinThreshold=-1 This particular configuration disables adaptive Broadcast join. Another option is to increase spark.sql.broadcastTimeout to a value above 300 seconds, which is the default value. Increasing spark.sql.broadcastTimeout allows more time for the broadcasting process to finish before it generates a failure. The downside to this approach, is that it may result in longer query times. For example, setting the value to 600 doubles the amount of time for the Broadcast join to complete. SET spark.sql.broadcastTimeout=600 This value can be set at the cluster level or the notebook level.	If you are experiencing a timeout issue when ingesting data, it may be caused by a Broadcast join that takes excessively long to complete. You can try to identify the query causing the resource bottleneck on the cluster and review the Spark SQL plan to see if it uses BroadcastNestedLoopJoin. If it does, you can disable the Broadcast join by setting Spark config values right before the problematic query. Alternatively, you can increase the spark.sql.broadcastTimeout to a value above 300 seconds, which is the default value.	1	1	1	True
65	no_reaction	https://docs.databricks.com/data-sharing/grant-access.html	How do I grant access to my Delta share	"      Grant and manage access to Delta Sharing data shares ======================================================     This article explains how to grant a data recipient access to a Delta Sharing share. It also explains how to view, update, and revoke access.        Requirements --------------     To share data with recipients:     * You must be a metastore admin, have the  `CREATE\_SHARE`  privilege for the Unity Catalog metastore where the data you want to share is registered, or be the owner of both the share and recipient objects. * You must use a Databricks workspace that has a Unity Catalog metastore attached. * You must use a SQL warehouse or cluster that uses a Unity-Catalog-capable cluster access mode. * Shares and recipients must already be defined.       Grant recipient access to share ---------------------------------     To grant share access to recipients, you can use Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or share object owner. The share object owner must also be the owner of any recipient objects being added.         To add recipients to a share (starting at the share):     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. Click  **Add recipient**   . 5. On the  **Add recipient**   dialog, start typing the recipient name or click the drop-down menu to select the recipients you want to add to the share. 6. Click  **Add**   .     To grant share access to a recipient (starting at the recipient):     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. Click  **Grant share**   . 5. On the  **Grant share**   dialog, start typing the share name or click the drop-down menu to select the shares you want to grant. 6. Click  **Grant**   .       Run the following command in a notebook or the Databricks SQL query editor.        ``` GRANT SELECT ON SHARE <share\_name> TO RECIPIENT <recipient\_name>;  ```     `SELECT`  is the only privilege that you can grant a recipient on a share.         Create a JSON file that includes the following, replacing  `recipient-name`  .  `SELECT`  is the only privilege that you can grant on a share.        ``` {  ""privilege\_assignments"": [{  ""principal"": ""recipient-name"",  ""privileges"": [  ""SELECT""  ]  }] }  ```      Then run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  . Replace  `<share\_name>`  with the name of the share you want to grant to the recipient, and replace  `update-share-perms.json`  with the filename of the JSON file.        ``` databricks unity-catalog shares update-permissions --name <share_name> \  --json-file update-share-perms.json  ```            Revoke recipient access to a share ------------------------------------     To revoke a recipient’s access to a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `REVOKE     ON     SHARE`  SQL command in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : Metastore admin or share object owner.         To revoke a recipient’s access to a share, starting at the share:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. On the  **Recipients**   tab, find the recipient. 5. Click the  ![Kebab menu](../_images/kebab-menu.png)  kebab menu (also known as the three-dot menu) and select  **Revoke**   . 6. On the confirmation dialog, click  **Revoke**   .     To revoke a recipient’s access to a share, starting at the recipient:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. On the  **Shares**   tab, find the share. 5. Click the  ![Kebab menu](../_images/kebab-menu.png)  kebab menu (also known as the three-dot menu) on the share row and select  **Revoke**   . 6. On the confirmation dialog, click  **Revoke**   .       Run the following command in a notebook or the Databricks SQL query editor.        ``` REVOKE SELECT ON SHARE <share\_name> FROM RECIPIENT <recipient\_name>;  ```        Create a JSON file that includes the following, replacing  `recipient-name`  :        ``` {  ""changes"": [{  ""principal"": ""recipient-name"",  ""remove"": [""SELECT""]  }] }  ```      Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  . Replace  `<share\_name>`  with the name of the share you want to grant to the recipient, and replace  `update-share-perms.json`  with the filename of the JSON file.        ``` databricks unity-catalog shares update-permissions --name <share_name> --json-file update-share-perms.json  ```          View grants on a share or grants possessed by a recipient -----------------------------------------------------------     To view the current grants on a share, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `SHOW     GRANTS     ON     SHARE`  SQL command in a Databricks notebook or the Databricks SQL query editor.      **Permissions required**   : If you are viewing recipients granted access to a share, you must be a metastore admin or the share object owner. If you are viewing shares granted to a recipient, you must be a metastore admin or the recipient object owner.         To view recipients with access to a share:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Shares**   tab, find and select the share. 4. Go to the  **Recipients**   tab to view all recipients who have access to the share.       Run the following command in a notebook or the Databricks SQL query editor.        ``` SHOW GRANT ON SHARE <share\_name>;  ```        Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog shares list-permissions --name <share_name>  ```        To view the current share grants possessed by a recipient, you can use Data Explorer, the Databricks Unity Catalog CLI, or the  `SHOW     GRANTS     TO     RECIPIENT`  SQL command in a Databricks notebook or the Databricks SQL query editor.         To view shares granted to a recipient:     1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   . 2. In the left pane, expand the  **Delta Sharing**   menu and select  **Shared by me**   . 3. On the  **Recipients**   tab, find and select the recipient. 4. Go to the  **Shares**   tab to view all shares that the recipient has access to.       Run the following command in a notebook or the Databricks SQL query editor.        ``` SHOW GRANTS TO RECIPIENT <recipient\_name>;  ```        Run the following command using the  [Databricks CLI](../dev-tools/cli/index.html)  .        ``` databricks unity-catalog recipients list-permissions --name <recipient_name>  ```                                                         "	To grant access to a Delta share, you can use Data Explorer, the Databricks Unity Catalog CLI, or SQL commands in a Databricks notebook or the Databricks SQL query editor. To add recipients to a share, you need to be a metastore admin, have the CREATE_SHARE privilege for the Unity Catalog metastore where the data you want to share is registered, or be the owner of both the share and recipient objects. You also need to use a Databricks workspace that has a Unity Catalog metastore attached and use a SQL warehouse or cluster that uses a Unity-Catalog-capable cluster access mode. Once you have met these requirements, you can follow the steps outlined in the context to grant access to your Delta share.	1	1	1	True
68	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/index.html	Does unity catalog work with Informatica?	What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	Based on the context, there is no clear information on whether Unity Catalog works with Informatica.	0	0	0	True
71	no_reaction	https://kb.databricks.com/clusters/calculate-number-of-cores	how to get the number of total cores in a databricks cluster?	Title: How to calculate the number of cores in a cluster  Description: Learn how to calculate the number of cores in a Databricks cluster.  Answer: You can view the number of cores in a Databricks cluster in the Workspace UI using the Metrics tab on the cluster details page. Note Azure Databricks cluster nodes must have a metrics service installed. If the driver and executors are of the same node type, you can also determine the number of cores available in a cluster programmatically, using Scala utility code: Use sc.statusTracker.getExecutorInfos.length to get the total number of nodes. The result includes the driver node, so subtract 1. Use java.lang.Runtime.getRuntime.availableProcessors to get the number of cores per node. Multiply both results (subtracting 1 from the total number of nodes) to get the total number of cores available. Scala example code: java.lang.Runtime.getRuntime.availableProcessors * (sc.statusTracker.getExecutorInfos.length -1)	To get the number of total cores in a Databricks cluster, you can view the Metrics tab on the cluster details page in the Workspace UI. If the driver and executors are of the same node type, you can also determine the number of cores available in a cluster programmatically using Scala utility code. You can use `sc.statusTracker.getExecutorInfos.length` to get the total number of nodes and `java.lang.Runtime.getRuntime.availableProcessors` to get the number of cores per node. Then, you can multiply both results (subtracting 1 from the total number of nodes) to get the total number of cores available. An example of Scala code to calculate this is `java.lang.Runtime.getRuntime.availableProcessors * (sc.statusTracker.getExecutorInfos.length -1)`.	1	1	1	True
72	no_reaction	https://docs.databricks.com/archive/spark-sql-2.x-language-manual/clone.html	What's the different between shadow clone and deep clone	"    Clone a table on Databricks =============================     You can create a copy of an existing Delta Lake table on Databricks at a specific version using the  `clone`  command. Clones can be either deep or shallow.       Clone is a Databricks-exclusive feature enabled in the Databricks Runtime by default.       Databricks also supports cloning Parquet and Iceberg tables. See  [Incrementally clone Parquet and Iceberg tables to Delta Lake](clone-parquet.html)  .          Clone types -------------   * A  *deep clone*   is a clone that copies the source table data to the clone target in addition to the metadata of the existing table. Additionally, stream metadata is also cloned such that a stream that writes to the Delta table can be stopped on a source table and continued on the target of a clone from where it left off. * A  *shallow clone*   is a clone that does not copy the data files to the clone target. The table metadata is equivalent to the source. These clones are cheaper to create.     Any changes made to either deep or shallow clones affect only the clones themselves and not the source table.       The metadata that is cloned includes: schema, partitioning information, invariants, nullability. For deep clones only, stream and  [COPY INTO](../sql/language-manual/delta-copy-into.html)  metadata are also cloned. Metadata not cloned are the table description and  [user-defined commit metadata](custom-metadata.html)  .        Important     * Shallow clones reference data files in the source directory. If you run  `vacuum`  on the source table clients will no longer be able to read the referenced data files and a  `FileNotFoundException`  will be thrown. In this case, running clone with replace over the shallow clone will repair the clone. If this occurs often, consider using a deep clone instead which does not depend on the source table. * Deep clones do not depend on the source from which they were cloned, but are expensive to create because a deep clone copies the data as well as the metadata. * Cloning with  `replace`  to a target that already has a table at that path creates a Delta log if one does not exist at that path. You can clean up any existing data by running  `vacuum`  . * If an existing Delta table exists, a new commit is created that includes the new metadata and new data from the source table. This new commit is incremental, meaning that only new changes since the last clone are committed to the table. * Cloning a table is not the same as  `Create     Table     As     Select`  or  `CTAS`  . A clone copies the metadata of the source table in addition to the data. Cloning also has simpler syntax: you don’t need to specify partitioning, format, invariants, nullability and so on as they are taken from the source table. * A cloned table has an independent history from its source table. Time travel queries on a cloned table will not work with the same inputs as they work on its source table.        ``` CREATE TABLE delta.`/data/target/` CLONE delta.`/data/source/` -- Create a deep clone of /data/source at /data/target  CREATE OR REPLACE TABLE db.target\_table CLONE db.source\_table -- Replace the target  CREATE TABLE IF NOT EXISTS delta.`/data/target/` CLONE db.source\_table -- No-op if the target table exists  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source`  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` VERSION AS OF version  CREATE TABLE db.target\_table SHALLOW CLONE delta.`/data/source` TIMESTAMP AS OF timestamp\_expression -- timestamp can be like “2019-01-01” or like date\_sub(current\_date(), 1)  ```       ``` from delta.tables import \*  deltaTable = DeltaTable.forPath(spark, pathToTable)  # path-based tables, or deltaTable = DeltaTable.forName(spark, tableName)    # Hive metastore-based tables  deltaTable.clone(target, isShallow, replace) # clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) # clone the source at a specific version  # clone the source at a specific timestamp such as timestamp=“2019-01-01” deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace)  ```       ``` import io.delta.tables.\_  val deltaTable = DeltaTable.forPath(spark, pathToTable) val deltaTable = DeltaTable.forName(spark, tableName)  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       ``` import io.delta.tables.\*;  DeltaTable deltaTable = DeltaTable.forPath(spark, pathToTable); DeltaTable deltaTable = DeltaTable.forName(spark, tableName);  deltaTable.clone(target, isShallow, replace) // clone the source at latest version  deltaTable.cloneAtVersion(version, target, isShallow, replace) // clone the source at a specific version  deltaTable.cloneAtTimestamp(timestamp, target, isShallow, replace) // clone the source at a specific timestamp  ```       For syntax details, see  [CREATE TABLE CLONE](../sql/language-manual/delta-clone.html)  .         Clone metrics ---------------      Note       Available in Databricks Runtime 8.2 and above.       `CLONE`  reports the following metrics as a single row DataFrame once the operation is complete:     * `source\_table\_size`  : Size of the source table that’s being cloned in bytes. * `source\_num\_of\_files`  : The number of files in the source table. * `num\_removed\_files`  : If the table is being replaced, how many files are removed from the current table. * `num\_copied\_files`  : Number of files that were copied from the source (0 for shallow clones). * `removed\_files\_size`  : Size in bytes of the files that are being removed from the current table. * `copied\_files\_size`  : Size in bytes of the files copied to the table.    ![Clone metrics example](../_images/clone-metrics.png)      Permissions -------------     You must configure permissions for Databricks table access control and your cloud provider.      ###   Table access control     The following permissions are required for both deep and shallow clones:     * `SELECT`  permission on the source table. * If you are using  `CLONE`  to create a new table,  `CREATE`  permission on the database in which you are creating the table. * If you are using  `CLONE`  to replace a table, you must have  `MODIFY`  permission on the table.     ###   Cloud provider permissions     If you have created a deep clone, any user that reads the deep clone must have read access to the clone’s directory. To make changes to the clone, users must have write access to the clone’s directory.       If you have created a shallow clone, any user that reads the shallow clone needs permission to read the files in the original table, since the data files remain in the source table with shallow clones, as well as the clone’s directory. To make changes to the clone, users will need write access to the clone’s directory.          Use clone for data archiving ------------------------------     Data may need to be kept for longer than is feasible with time travel or for disaster recovery. In these cases, you can create a deep clone to preserve the state of a table at a certain point in time for archival. Incremental archiving is also possible to keep a continually updating state of a source table for disaster recovery.        ``` -- Every month run CREATE OR REPLACE TABLE delta.`/some/archive/path` CLONE my\_prod\_table  ```        Use clone for ML model reproduction -------------------------------------     When doing machine learning, you may want to archive a certain version of a table on which you trained an ML model. Future models can be tested using this archived data set.        ``` -- Trained model on version 15 of Delta table CREATE TABLE delta.`/model/dataset` CLONE entire\_dataset VERSION AS OF 15  ```        Use clone for short-term experiments on a production table ------------------------------------------------------------     To test a workflow on a production table without corrupting the table, you can easily create a shallow clone. This allows you to run arbitrary workflows on the cloned table that contains all the production data but does not affect any production workloads.        ``` -- Perform shallow clone CREATE OR REPLACE TABLE my\_test SHALLOW CLONE my\_prod\_table;  UPDATE my\_test WHERE user\_id is null SET invalid=true; -- Run a bunch of validations. Once happy:  -- This should leverage the update information in the clone to prune to only -- changed files in the clone if possible MERGE INTO my\_prod\_table USING my\_test ON my\_test.user\_id <=> my\_prod\_table.user\_id WHEN MATCHED AND my\_test.user\_id is null THEN UPDATE \*;  DROP TABLE my\_test;  ```        Use clone for data sharing ----------------------------     Other business units within a single organization may want to access the same data but may not require the latest updates. Instead of giving access to the source table directly, you can provide clones with different permissions for different business units. The performance of the clone can exceed that of a simple view.        ``` -- Perform deep clone CREATE OR REPLACE TABLE shared\_table CLONE my\_prod\_table;  -- Grant other users access to the shared table GRANT SELECT ON shared\_table TO `<user-name>@<user-domain>.com`;  ```        Use clone to override table properties ----------------------------------------      Note       Available in Databricks Runtime 7.5 and above.        Table property overrides are particularly useful for:     * Annotating tables with owner or user information when sharing data with different business units. * Archiving Delta tables and time travel is required. You can specify the log retention period independently for the archive table. For example:       ``` CREATE OR REPLACE TABLE archive.my\_table CLONE prod.my\_table TBLPROPERTIES ( delta.logRetentionDuration = '3650 days', delta.deletedFileRetentionDuration = '3650 days' ) LOCATION 'xx://archive/my\_table'  ```       ``` dt = DeltaTable.forName(spark, ""prod.my\_table"") tblProps = { ""delta.logRetentionDuration"": ""3650 days"", ""delta.deletedFileRetentionDuration"": ""3650 days"" } dt.clone('xx://archive/my\_table', isShallow=False, replace=True, tblProps)  ```       ``` val dt = DeltaTable.forName(spark, ""prod.my\_table"") val tblProps = Map( ""delta.logRetentionDuration"" -> ""3650 days"", ""delta.deletedFileRetentionDuration"" -> ""3650 days"" ) dt.clone(""xx://archive/my\_table"", isShallow = false, replace = true, properties = tblProps)  ```                                                        "	A deep clone is a clone that copies the source table data to the clone target in addition to the metadata of the existing table. Additionally, stream metadata is also cloned such that a stream that writes to the Delta table can be stopped on a source table and continued on the target of a clone from where it left off. On the other hand, a shallow clone is a clone that does not copy the data files to the clone target. The table metadata is equivalent to the source. These clones are cheaper to create.	1	1	1	True
73	no_reaction	https://docs.databricks.com/structured-streaming/synapse.html	how to apply deletes in structured streaming, bronze and silver layers?	DELETE FROM =============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Deletes the rows that match a predicate. When no predicate is provided, deletes all rows.       This statement is only supported for Delta Lake tables.        Syntax --------      ``` DELETE FROM table\_name [table\_alias] [WHERE predicate]  ```        Parameters ------------   * [table\_name](sql-ref-names.html#table-name)      Identifies an existing table. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . * [table\_alias](sql-ref-names.html#table-alias)      Define an alias for the table. The alias must not include a column list. * **[WHERE](sql-ref-syntax-qry-select-where.html)**      Filter rows by predicate.       The  `WHERE`  predicate supports subqueries, including  `IN`  ,  `NOT     IN`  ,  `EXISTS`  ,  `NOT     EXISTS`  , and scalar subqueries. The following types of subqueries are not supported:      + Nested subqueries, that is, an subquery inside another subquery  + `NOT        IN`   subquery inside an   `OR`   , for example,   `a        =        3        OR        b        NOT        IN        (SELECT        c        from        t)`  In most cases, you can rewrite  `NOT     IN`  subqueries using  `NOT     EXISTS`  . We recommend using  `NOT     EXISTS`  whenever possible, as  `DELETE`  with  `NOT     IN`  subqueries can be slow.       Examples ----------      ``` > DELETE FROM events WHERE date < '2017-01-01'  > DELETE FROM all\_events  WHERE session\_time < (SELECT min(session\_time) FROM good\_events)  > DELETE FROM orders AS t1  WHERE EXISTS (SELECT oid FROM returned\_orders WHERE t1.oid = oid)  > DELETE FROM events  WHERE category NOT IN (SELECT category FROM events2 WHERE date > '2001-01-01')  ```    * [COPY](delta-copy-into.html) * [INSERT](sql-ref-syntax-dml-insert-into.html) * [MERGE](delta-merge-into.html) * [PARTITION](sql-ref-partition.html#partition) * [query](sql-ref-syntax-qry-query.html) * [UPDATE](delta-update.html)	To apply deletes in structured streaming, bronze and silver layers, you can use the `DELETE FROM` statement in Databricks SQL. However, this statement is only supported for Delta Lake tables. The syntax for the statement is `DELETE FROM table_name [table_alias] [WHERE predicate]`. The `WHERE` predicate can be used to filter rows by a condition.	1	1	1	True
74	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-metastore.html	Is the Unity Catalog metastore in the data plane?	Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .	Based on the context, it is not explicitly stated whether the Unity Catalog metastore is in the data plane or not.	1	0	0	True
75	no_reaction	https://docs.databricks.com/sql/language-manual/functions/current_date.html	databricks sql get current date	"   pyspark.sql.functions.current\_date  [¶](#pyspark-sql-functions-current-date ""Permalink to this headline"") ============================================================================================================  `pyspark.sql.functions.`  `current_date`    (     )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#current_date) [¶](#pyspark.sql.functions.current_date ""Permalink to this definition"")    Returns the current date at the start of query evaluation as a  `DateType`  column. All calls of current\_date within the same query return the same value.     New in version 1.5.0.     Changed in version 3.4.0:    Supports Spark Connect.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")   current date.     Examples    ``` >>> df = spark.range(1) >>> df.select(current\_date()).show()  +--------------+ |current\_date()| +--------------+ | 2022-08-26| +--------------+  ```  [pyspark.sql.functions.add\_months](pyspark.sql.functions.add_months.html ""previous page"")  [pyspark.sql.functions.current\_timestamp](pyspark.sql.functions.current_timestamp.html ""next page"")   "	To get the current date in Databricks SQL, you can use the `current_date()` function from the `pyspark.sql.functions` module. It returns the current date at the start of query evaluation as a `DateType` column.	1	1	1	True
80	no_reaction	https://docs.databricks.com/runtime/dbr.html	runtime versioning	Databricks Runtime ====================     Databricks Runtime includes Apache Spark but also adds a number of components and updates that substantially improve the usability, performance, and security of big data analytics:     * [Delta Lake](../delta/index.html)  , a next-generation storage layer built on top of Apache Spark that provides ACID transactions, optimized layouts and indexes, and execution engine improvements for building data pipelines. * Installed Java, Scala, Python, and R libraries * Ubuntu and its accompanying system libraries * GPU libraries for GPU-enabled clusters * Databricks services that integrate with other components of the platform, such as notebooks, jobs, and cluster manager     For information about the contents of each runtime version, see the  [release notes](../release-notes/runtime/releases.html)  .        Runtime versioning --------------------     Databricks Runtime versions are released on a regular basis:     * **Major**   versions are represented by an increment to the version number that precedes the decimal point (the jump from 3.5 to 4.0, for example). They are released when there are major changes, some of which may not be backwards-compatible. * **Feature**   versions are represented by an increment to the version number that follows the decimal point (the jump from 3.4 to 3.5, for example). Each major release includes multiple feature releases. Feature releases are always backwards compatible with previous releases within their major release. * **Long Term Support**   versions are represented by an  **LTS**   qualifier (for example,  **3.5 LTS**   ). For each major release, we declare a “canonical” feature version, for which we provide two full years of support. See  [Databricks runtime support lifecycles](../release-notes/runtime/databricks-runtime-ver.html#runtime-support)  for more information.	The context is about Databricks Runtime, which includes Apache Spark and additional components and updates to improve the usability, performance, and security of big data analytics. Runtime versions are released regularly, with major versions representing major changes that may not be backwards-compatible, feature versions being backwards-compatible with previous releases within their major release, and Long Term Support versions providing two full years of support for a canonical feature version within each major release.	1	1	1	True
81	no_reaction	https://docs.databricks.com/clusters/graviton.html	Not all metrics are available when using Graviton instances	"   p    [org](../../../index.html)   .  [apache](../../index.html)   .  [spark](../index.html)    metrics   [**](../../../../org/apache/spark/metrics/index.html ""Permalink"") ==============================================================================  ####    package     metrics  **   **    Ordering   1. Alphabetic   Visibility   1. Public 2. All  ###   Type Members  1. [**](../../../../org/apache/spark/metrics/ExecutorMetricType.html ""Permalink"")   sealed     trait    [ExecutorMetricType](ExecutorMetricType.html ""Executor metric types for executor-level metrics stored in ExecutorMetrics."")   extends  [AnyRef](../../../../scala/index.html#AnyRef=Object)    Executor metric types for executor-level metrics stored in ExecutorMetrics. 2. [**](../../../../org/apache/spark/metrics/SingleValueExecutorMetricType.html ""Permalink"")   sealed     trait    [SingleValueExecutorMetricType](SingleValueExecutorMetricType.html)   extends  [ExecutorMetricType](ExecutorMetricType.html)  ###   Value Members  1. [**](../../../../org/apache/spark/metrics/DirectPoolMemory$.html ""Permalink"")   object    [DirectPoolMemory](DirectPoolMemory$.html)   extends    MBeanExecutorMetricType    with    Product    with    Serializable 2. [**](../../../../org/apache/spark/metrics/GarbageCollectionMetrics$.html ""Permalink"")   object    [GarbageCollectionMetrics](GarbageCollectionMetrics$.html)   extends  [ExecutorMetricType](ExecutorMetricType.html)   with    Logging    with    Product    with    Serializable 3. [**](../../../../org/apache/spark/metrics/JVMHeapMemory$.html ""Permalink"")   object    [JVMHeapMemory](JVMHeapMemory$.html)   extends  [SingleValueExecutorMetricType](SingleValueExecutorMetricType.html)   with    Product    with    Serializable 4. [**](../../../../org/apache/spark/metrics/JVMOffHeapMemory$.html ""Permalink"")   object    [JVMOffHeapMemory](JVMOffHeapMemory$.html)   extends  [SingleValueExecutorMetricType](SingleValueExecutorMetricType.html)   with    Product    with    Serializable 5. [**](../../../../org/apache/spark/metrics/MappedPoolMemory$.html ""Permalink"")   object    [MappedPoolMemory](MappedPoolMemory$.html)   extends    MBeanExecutorMetricType    with    Product    with    Serializable 6. [**](../../../../org/apache/spark/metrics/OffHeapExecutionMemory$.html ""Permalink"")   object    [OffHeapExecutionMemory](OffHeapExecutionMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 7. [**](../../../../org/apache/spark/metrics/OffHeapStorageMemory$.html ""Permalink"")   object    [OffHeapStorageMemory](OffHeapStorageMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 8. [**](../../../../org/apache/spark/metrics/OffHeapUnifiedMemory$.html ""Permalink"")   object    [OffHeapUnifiedMemory](OffHeapUnifiedMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 9. [**](../../../../org/apache/spark/metrics/OnHeapExecutionMemory$.html ""Permalink"")   object    [OnHeapExecutionMemory](OnHeapExecutionMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 10. [**](../../../../org/apache/spark/metrics/OnHeapStorageMemory$.html ""Permalink"")   object    [OnHeapStorageMemory](OnHeapStorageMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 11. [**](../../../../org/apache/spark/metrics/OnHeapUnifiedMemory$.html ""Permalink"")   object    [OnHeapUnifiedMemory](OnHeapUnifiedMemory$.html)   extends    MemoryManagerExecutorMetricType    with    Product    with    Serializable 12. [**](../../../../org/apache/spark/metrics/ProcessTreeMetrics$.html ""Permalink"")   object    [ProcessTreeMetrics](ProcessTreeMetrics$.html)   extends  [ExecutorMetricType](ExecutorMetricType.html)   with    Product    with    Serializable  ###   Ungrouped  "	"Based on the context provided, there is no direct answer to the question ""Not all metrics are available when using Graviton instances"". The context seems to be a documentation page for the Apache Spark Metrics package, which lists various types of metrics and their corresponding objects. It does not provide any information about Graviton instances or their compatibility with metrics."	0	0	0	True
82	thumb_down	https://kb.databricks.com/sql/to_json-results-in-cannot-use-null-as-map-key-error	Hi Assitant. how do I make to_json SQL function to preserve null fields from passed struct?	"Title: to_json() results in Cannot use null as map key error  Description: You must filter or replace null values in your input data before using to_json().  Answer: Problem You are using to_json() to convert data to JSON and you get a Cannot use null as map key error: RuntimeException: Cannot use null as map key. Cause to_json() function does not support using null values as the input map keys. This example code causes the Cannot use null as map key error when run, because of the null value used as a map key in the fourth line. %sql select   to_json(     map(       1, 'Databricks',       2, 'Map',       3, 'Error',       null, 'Data'     )   ) as json; Solution  You should filter out any null values present in the input data before running to_json(), or use nvl() to replace all of the null values with non-null values. Filter null values Consider this example DataFrame: +---+----------+-------+ | Id|     Value|address| +---+----------+-------+ |  1|Databricks|   null| |  2|       Map|   null| |  3|     Error|    xyz| +---+----------+-------+ There are two null values in the example. Attempting to use to_json() on this DataFrame will return an error. We can filter the null data by showing only the rows that have non-null values. For example, filtering with df.filter(""address is not null"").show() returns: +---+-----+-------+ | Id|Value|address| +---+-----+-------+ |  3|Error|    xyz| +---+-----+-------+ This filtered DataFrame does not contain any null values, so it can now be used as an input with to_json(). Replace null values with replacements If you cannot filter out the null values, you can use nvl() to replace the null values with non-null values.  The sample code originally had a null value as the map key for the fourth line. Since that results in an error, nvl() is used in this updated example to substitute 4 for the null value. %sql select   to_json(     map(       1, 'Databricks',       2, 'Map',       3, 'Error', nvl(null, 4), 'Data'     ) ) as JSON;"	To preserve null fields from a passed struct in the to_json SQL function, you should filter out any null values present in the input data before running to_json(), or use nvl() to replace all of the null values with non-null values. If you cannot filter out the null values, you can use nvl() to replace the null values with non-null values.	1	1	1	True
85	no_reaction	https://docs.databricks.com/delta-live-tables/import-workspace-files.html	dlt with python	Python programming for Delta Live Tables ==========================================     The articles linked here provide guidance for using Python to perform programming tasks with Delta Live Tables. These recommendations assume familiarity with concepts and patterns in the following articles:     * [Tutorial: Declare a data pipeline with Python in Delta Live Tables](tutorial-python.html) * [Tutorial: Run your first Delta Live Tables pipeline](tutorial-pipelines.html) * [How to develop and test Delta Live Tables pipelines](testing.html)      Note       Patterns shown in this article cannot be easily completed with only SQL. Because Python datasets can be defined against any query that returns a DataFrame, you can use  `spark.sql()`  as necessary to leverage SQL syntax within Python functions.       You can use Python user-defined functions (UDFs) in your SQL queries, but you must define these UDFs in Python files in the same pipeline before calling them in SQL source files. See  [User-defined scalar functions - Python](../udf/python.html)  .       * [Programmatically create tables](create-multiple-tables.html) * [Import modules from workspace files](import-workspace-files.html)	The context is about using Python to perform programming tasks with Delta Live Tables. It provides guidance for using Python to declare a data pipeline, run a Delta Live Tables pipeline, and develop and test Delta Live Tables pipelines. The article also mentions that Python datasets can be defined against any query that returns a DataFrame, and that you can use `spark.sql()` as necessary to leverage SQL syntax within Python functions. Additionally, the article mentions that you can use Python user-defined functions (UDFs) in your SQL queries, but you must define these UDFs in Python files in the same pipeline before calling them in SQL source files. Finally, the article provides links to other articles that cover topics such as programmatically creating tables and importing modules from workspace files.	1	0	1	True
87	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-metastore.html	How do I create a unity catalog metastore	Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .	To create a Unity Catalog metastore, you need to configure a storage bucket and IAM role in AWS, then log in to the Databricks account console, click on Data, and then click on Create Metastore. You will need to enter a name for the metastore, the region where the metastore will be deployed, the S3 bucket path, and the IAM role name that you created in step 1. Finally, you will need to select workspaces to link to the metastore.	1	1	1	True
90	no_reaction	https://docs.databricks.com/marketplace/index.html	What are the key concepts in the databricks marketplace	What is Databricks Marketplace? =================================      Preview       This feature is in  [Private Preview](../release-notes/release-types.html)  . To try it, reach out to your Databricks contact.        This article introduces Databricks Marketplace, an open forum for exchanging data products. Databricks Marketplace takes advantage of  [Delta Sharing](../data-sharing/index.html)  to give data providers the tools to share data products securely and data consumers the power to explore and expand their access to the data and data services they need.          How do consumers use Databricks Marketplace? ----------------------------------------------     To find a data product you want on the Databricks Marketplace, simply browse or search provider listings and request access to the data products that you want. Some data products are available immediately, as soon as you request them and agree to terms. Others might require provider approval and transaction completion using provider interfaces. In either case, the Delta Sharing protocol that powers the Marketplace ensures that you can access shared data securely, regardless of whether you use Databricks to work with your data.      ###   Get started accessing data products     To access data products on Databricks Marketplace, you must have a Databricks account and workspace. To learn how to get started, see  [Access data products in Databricks Marketplace](get-started-consumer.html)  .          How do providers use Databricks Marketplace? ----------------------------------------------     Databricks Marketplace gives data providers a secure platform for sharing data products that data scientists and analysts need to help their organizations succeed. Databricks Marketplace uses Delta Sharing to provide security and control over your shared data. You can share public data, free sample data, and commercialized data offerings. In addition to datasets, you can also share Databricks notebooks and other content to demonstrate use cases and show customers how to take full advantage of your data products.      ###   Get started listing data products     To list your data products on Databricks Marketplace, you must have a Databricks account and workspace. To learn how to get started, see  [List your data product in Databricks Marketplace](get-started-provider.html)  .	The key concepts in the Databricks Marketplace are an open forum for exchanging data products, Delta Sharing protocol for secure data sharing, data providers sharing data products, and data consumers accessing data products by browsing or searching provider listings and requesting access.	1	1	1	True
92	no_reaction	https://kb.databricks.com/data-sources/sql-access-control-error-when-using-snowflake-as-a-data-source	How do I connect from snowflake	"    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format(""snowflake"")   .option(""dbtable"", table\_name)   .option(""sfUrl"", database\_host\_url)   .option(""sfUser"", username)   .option(""sfPassword"", password)   .option(""sfDatabase"", database\_name)   .option(""sfSchema"", schema\_name)   .option(""sfWarehouse"", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format(""snowflake"")  .option(""dbtable"", table\_name)  .option(""sfUrl"", database\_host\_url)  .option(""sfUser"", username)  .option(""sfPassword"", password)  .option(""sfDatabase"", database\_name)  .option(""sfSchema"", schema\_name)  .option(""sfWarehouse"", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         "	"To connect from Snowflake, you can configure a connection to Snowflake and then query data using the Snowflake connector provided in the Databricks Runtime. The connector supports reading and writing data from Snowflake. You can use the provided example syntax in Python, SQL, and Scala to query a Snowflake table in Databricks. See the ""Using the Spark Connector"" and ""Setting Configuration Options for the Connector"" sections in the Snowflake Connector for Spark documentation for more details and all configuration options."	1	1	1	True
93	no_reaction	https://kb.databricks.com/clusters/azure-ssh-cluster-driver-node	How do I ssh into a cluster?	"Title: SSH to the cluster driver node  Description: How to SSH to the Apache Spark cluster driver node in an Azure virtual network  Answer: This article explains how to use SSH to connect to an Apache Spark driver node for advanced troubleshooting and installing custom software. Warning You can only use SSH if your workspace is deployed in an Azure Virtual Network (VNet) under your control. If your workspace is NOT VNet injected, the SSH option will not appear. Additionally, NPIP workspaces do not support SSH. Configure an Azure network security group The network security group associated with your VNet must allow SSH traffic. The default port for SSH is 2200. If you are using a custom port, you should make note of it before proceeding. You also have to identify a traffic source. This can be a single IP address, or it can be an IP range that represents your entire office. In the Azure portal, find the network security group. The network security group name can be found in the public subnet. Edit the inbound security rules to allow connections to the SSH port. In this example, we are using the default port. Info Make sure that your computer and office firewall rules allow you to send TCP traffic on the port you are using for SSH. If the SSH port is blocked at your computer or office firewall, you cannot connect to the Azure VNet via SSH. Generate SSH key pair Open a local terminal. Create an SSH key pair by running this command:ssh-keygen -t rsa -b 4096 -C Info You must provide the path to the directory where you want to save the public and private key. The public key is saved with the extension .pub. Configure a new cluster with your public key Copy the ENTIRE contents of the public key file. Open the cluster configuration page. Click Advanced Options. Click the SSH tab. Paste the ENTIRE contents of the public key into the Public key field. Continue with cluster configuration as normal. Configure an existing cluster with your public key If you have an existing cluster and did not provide the public key during cluster creation, you can inject the public key from a notebook. Open any notebook that is attached to the cluster. Copy the following code into the notebook, updating it with your public key as noted:%scala val publicKey = ""&lt;put your public key here&gt;"" def addAuthorizedPublicKey(key: String): Unit = {   val fw = new java.io.FileWriter(""/home/ubuntu/.ssh/authorized_keys"", /* append */ true)   fw.write(""\n"" + key)   fw.close() } addAuthorizedPublicKey(publicKey) Run the code block to inject the public key. SSH into the Spark driver Open the cluster configuration page. Click Advanced Options. Click the SSH tab. Note the Driver Hostname. Open a local terminal. Run the following command, replacing the hostname and private key file path:ssh ubuntu@&lt;hostname&gt; -p 2200 -i &lt;private-key-file-path&gt;"	To SSH into an Apache Spark cluster driver node, you need to configure an Azure network security group to allow SSH traffic, generate an SSH key pair, and configure the cluster with your public key. Once configured, you can SSH into the Spark driver by opening the cluster configuration page, noting the Driver Hostname, and running the following command in a local terminal: ssh ubuntu@<hostname> -p 2200 -i <private-key-file-path>.	1	1	1	True
94	no_reaction	https://docs.databricks.com/spark/index.html	Why use Apache Spark on Databricks?	Apache Spark on Databricks ============================     This article describes the how Apache Spark is related to Databricks and the Databricks Lakehouse Platform.       Apache Spark is at the heart of the Databricks Lakehouse Platform and is the technology powering compute clusters and SQL warehouses on the platform. Databricks is an optimized platform for Apache Spark, providing an efficient and simple platform for running Apache Spark workloads.            What is the relationship of Apache Spark to Databricks? ---------------------------------------------------------     The Databricks company was founded by the orginal creators of Apache Spark. As an open source software project, Apache Spark has  [committers from many top companies](https://spark.apache.org/committers.html)   , including Databricks.       Databricks continues to develop and release features to Apache Spark. The Databricks Runtime includes additional optimizations and proprietary features that build upon and extend Apache Spark, including  [Photon](../runtime/photon.html)  , an optimized version of Apache Spark rewritten in C++.           How does Apache Spark work on Databricks? -------------------------------------------     Whenever you deploy a compute cluster or SQL warehouse on Databricks, Apache Spark is configured and deployed to virtual machines. You don’t need to worry about configuring or initializing a Spark context or Spark session, as these are managed for you by Databricks.           Can I use Databricks without using Apache Spark? --------------------------------------------------     Databricks supports a variety of workloads and includes a number of other open source libraries in the Databricks Runtime. Databricks SQL uses Apache Spark under the hood, but end users use standard SQL syntax to create and query database objects.       Databricks Runtime for Machine Learning is optimized for ML workloads, and many data scientists use primary open source libraries like TensorFlow and SciKit Learn while working on Databricks. You can use  [workflows](../workflows/index.html)  to schedule arbitrary workloads against compute resources deployed and managed by Databricks.           Why use Apache Spark on Databricks? -------------------------------------     The Databricks Lakehouse Platform provides a secure, collaborative environment for developing and deploying enterprise solutions that scale with your business. Databricks employees representative many of the most knowledgeable Apache Spark maintainers and users in the world, and the company continuously develops and releases new optimizations to ensure that user have access to the fastest environment for running Apache Spark.	You would use Apache Spark on Databricks because Databricks is an optimized platform for Apache Spark, providing an efficient and simple platform for running Apache Spark workloads. The Databricks Lakehouse Platform provides a secure, collaborative environment for developing and deploying enterprise solutions that scale with your business. Additionally, Databricks employees represent many of the most knowledgeable Apache Spark maintainers and users in the world, and the company continuously develops and releases new optimizations to ensure that users have access to the fastest environment for running Apache Spark.	1	1	1	True
95	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-alter-table.html	alter table change datatype of column	"   DataType  [¶](#datatype ""Permalink to this headline"") =======================================================  *class*  `pyspark.sql.types.`  `DataType`  [[source]](../../../_modules/pyspark/sql/types.html#DataType) [¶](#pyspark.sql.types.DataType ""Permalink to this definition"")    Base class for data types.     Methods    |  [`fromInternal`](#pyspark.sql.types.DataType.fromInternal ""pyspark.sql.types.DataType.fromInternal"")  (obj)   |   Converts an internal SQL object into a native Python object.   | |  [`json`](#pyspark.sql.types.DataType.json ""pyspark.sql.types.DataType.json"")  ()   |   | |  [`jsonValue`](#pyspark.sql.types.DataType.jsonValue ""pyspark.sql.types.DataType.jsonValue"")  ()   |   | |  [`needConversion`](#pyspark.sql.types.DataType.needConversion ""pyspark.sql.types.DataType.needConversion"")  ()   |   Does this type needs conversion between Python object and internal SQL object.   | |  [`simpleString`](#pyspark.sql.types.DataType.simpleString ""pyspark.sql.types.DataType.simpleString"")  ()   |   | |  [`toInternal`](#pyspark.sql.types.DataType.toInternal ""pyspark.sql.types.DataType.toInternal"")  (obj)   |   Converts a Python object into an internal SQL object.   | |  [`typeName`](#pyspark.sql.types.DataType.typeName ""pyspark.sql.types.DataType.typeName"")  ()   |   |   Methods Documentation    `fromInternal`    (   *obj     :     Any*   )    → Any  [[source]](../../../_modules/pyspark/sql/types.html#DataType.fromInternal) [¶](#pyspark.sql.types.DataType.fromInternal ""Permalink to this definition"")    Converts an internal SQL object into a native Python object.    `json`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#DataType.json) [¶](#pyspark.sql.types.DataType.json ""Permalink to this definition"")   `jsonValue`    (     )    → Union    [    str    ,    Dict    [    str    ,    Any    ]     ]   [[source]](../../../_modules/pyspark/sql/types.html#DataType.jsonValue) [¶](#pyspark.sql.types.DataType.jsonValue ""Permalink to this definition"")   `needConversion`    (     )    → bool  [[source]](../../../_modules/pyspark/sql/types.html#DataType.needConversion) [¶](#pyspark.sql.types.DataType.needConversion ""Permalink to this definition"")    Does this type needs conversion between Python object and internal SQL object.     This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.    `simpleString`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#DataType.simpleString) [¶](#pyspark.sql.types.DataType.simpleString ""Permalink to this definition"")   `toInternal`    (   *obj     :     Any*   )    → Any  [[source]](../../../_modules/pyspark/sql/types.html#DataType.toInternal) [¶](#pyspark.sql.types.DataType.toInternal ""Permalink to this definition"")    Converts a Python object into an internal SQL object.    *classmethod*  `typeName`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#DataType.typeName) [¶](#pyspark.sql.types.DataType.typeName ""Permalink to this definition"")   [ByteType](pyspark.sql.types.ByteType.html ""previous page"")  [DateType](pyspark.sql.types.DateType.html ""next page"")   "	The context you provided is about the `DataType` class in PySpark, which is the base class for data types. It provides several methods such as `fromInternal`, `toInternal`, `simpleString`, `json`, `jsonValue`, `needConversion`, and `typeName`. However, it doesn't provide any information about altering the table or changing the datatype of a column.	0	0	0	True
96	no_reaction	https://docs.databricks.com/serverless-compute/index.html	How can I create a serverless cluster on databricks?	Serverless compute ====================     With the serverless compute version of the Databricks platform architecture, the compute layer exists in your Databricks account rather than your AWS account.            Databricks SQL Serverless ---------------------------    [Databricks SQL Serverless](../sql/admin/index.html)  supports serverless compute. Admins can create serverless SQL warehouses (formerly SQL endpoints) that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute clusters in your Databricks account. Use them with Databricks SQL queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.       Databricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it’s connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.       If serverless SQL warehouses are enabled for your account, note the following:     * New SQL warehouses are serverless by default when you create them from the  [UI](../sql/admin/create-sql-warehouse.html#create-warehouse)  . New SQL warehouses are not serverless by default when you create them using the  [API](../sql/api/sql-endpoints.html#create-warehouse)  , which requires that you explicitly specify serverless. You can also create new pro or classic SQL warehouses using either method. For more information about warehouse type defaults, see  [What are the warehouse type defaults?](../sql/index.html#warehouse-type-defaults)  . * You can  [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](../sql/admin/create-sql-warehouse.html#convert-to-serverless)  or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic. * This feature only affects Databricks SQL. It does not affect how Databricks Runtime clusters work with notebooks and jobs in the Data Science & Engineering or Databricks Machine Learning workspace environments. Databricks Runtime clusters always run in the classic data plane in your AWS account. See  [Serverless quotas](#compare)  .     If your account needs updated  [terms of use](../sql/admin/serverless.html#accept-terms)  , workspace admins are prompted in the Databricks SQL UI.       If your workspace has an AWS instance profile,  [you might need to update the trust relationship](../sql/admin/serverless.html#aws-instance-profile-setup)  to support serverless compute, depending on how and when it was created.       For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .         Model Serving ---------------     Model Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your Databricks account. The <ServerlessCompute> resources run as Databricks AWS resources in what is known as the serverless data plane.       In contrast, the legacy model serving architecture is a single-node cluster that runs in your AWS account within the classic data plane.     * **Easy configuration and compute resource management**   : Databricks automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration. * **High availability and scalability**   : Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests. * **Dashboards**   : Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.     For regional support, see  [Databricks clouds and regions](../resources/supported-regions.html)  .       Before you can use serverless Model Serving, you must  [enable serverless compute for your account](../machine-learning/model-serving/migrate-model-serving.html)  .                   Serverless quotas -------------------     Serverless quotas are a safety measure for serverless compute. Serverless quotas restrict how many serverless compute resources a customer can have at any given time. The quota is enforced at the regional level for all workspaces in your account. Quotas are enforced only for  [serverless SQL warehouses](../sql/admin/serverless.html)  . See  [Serverless quotas](serverless-quotas.html)  .         Compare serverless compute to other Databricks architectures --------------------------------------------------------------     Databricks operates out of a  *control plane*   and a  *data plane*   :     * The control plane includes the backend services that Databricks manages in its own AWS account. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest. * The data plane is where data is processed by clusters of compute resources.     There are important differences between the  *classic data plane*   (the original Databricks platform architecture) and the  *serverless data plane*   :     * For a classic data plane, Databricks compute resources run in your AWS account. Clusters perform distributed data analysis using queries (in Databricks SQL) or notebooks (in the Data Science & Engineering or Databricks Machine Learning environments):      + New clusters are created within each workspace’s virtual network in the customer’s AWS account.  + A classic data plane has natural isolation because it runs in each customer’s own AWS account. * For a serverless data plane, Databricks compute resources run in a compute layer within your Databricks account:      + The serverless data plane is used for serverless SQL warehouses and Model Serving. Enabling serverless compute does not change how Databricks Runtime clusters work in the Data Science & Engineering or Databricks Machine Learning environments.  + To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different Databricks customer workspaces and additional network controls between clusters of the same customer.     Databricks creates a serverless data plane in the same AWS region as your workspace’s classic data plane.       Worker nodes are private, which means they do not have public IP addresses.       For communication between the Databricks control plane and the serverless data plane:     * For Databricks SQL Serverless, the communication uses private connectivity. * For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.     When reading or writing to AWS S3 buckets in the same region as your workspace,  [serverless SQL warehouses](../sql/admin/serverless.html)  now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace’s root S3 bucket in your AWS account and to other S3 data sources in the same region.       The following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.      ![Compare classic and serverless data plane for Databricks SQL](../_images/serverless-compute-data-plane-comparison-dbsql-aws.png)   ![Compare classic and serverless data plane for Model Serving](../_images/serverless-compute-data-plane-comparison-models-aws.png)    For more information about secure cluster connectivity, which is mentioned in the diagram, see  [Secure cluster connectivity](../security/network/secure-cluster-connectivity.html)  .       The table below summarizes differences between serverless compute and the classic data plane architecture of Databricks, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about serverless compute security, or if you have questions about items in this table, contact your Databricks representative.               |   Item   |   Serverless data plane (AWS only)   |   Classic data plane (AWS and Azure)   | | --- | --- | --- | |  **Location of control plane resources**   |   Databricks cloud account   |   Databricks cloud account   | |  **Location of data plane compute resources**   |   Serverless data plane (VPC in the Databricks AWS account)   |   Classic data plane (VPC in the customer’s cloud provider account)   | |  **Data plane compute resources**   |   Databricks-managed Kubernetes (EKS) clusters   |   Databricks-managed standalone VMs   | |  **Customer access to data plane**   |   Access through Databricks control plane   | * **AWS**   : Direct access in customer’s AWS account. Additional indirect access through Databricks control plane. * **Azure**   : Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.  | |  **Who pays for unassigned VMs for Databricks SQL?**   |   Databricks   |   Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.   | |  **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**   |   Customer pays based on DBUs until Auto Stop stops the SQL warehouse.   |   Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.   | |  **Virtual private network (VPC) for data plane**   |   VPC in the customer’s Databricks account, with network boundaries between workspaces and between clusters.   | * **AWS**   : Exclusive, the VPC is in the customer’s account. * **Azure**   : Exclusive, the VNet is in the customer’s account.  | |  **OS image**   |   Databricks-modified cloud-managed Amazon-linux2   |   Databricks-managed Ubuntu or CentOS   | |  **Technology that manages default egress from the VPC**   |   Databricks-created AWS internet gateway   |   Default internet gateway or load balancer provided by the cloud   | |  **Customize VPC and firewall settings**   |   No   |   Yes   | |  **Customize CIDR ranges**   |   No   |   Yes   | |  **Public IPs**   |   No   |   Secure cluster connectivity  * When disabled, one public IP for each VM. * When enabled (the default), no public IPs for VMs.  | |  **Container-level network isolation for Databricks Runtime clusters**   |   Uses Kubernetes network policy   |   Uses Databricks-managed  `iptable`  rules   | |  **VM-level network isolation for Databricks Runtime clusters**   |   Security group isolation   |   Security group and isolation of VPC (AWS) or VNet (Azure)   | |  **VM isolation**   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   |   VMs in a cluster can communicate among themselves, but no ingress traffic is allowed from other clusters.   | |  **Communication between control plane and data plane**   |   For Databricks SQL Serverless, communication uses private connectivity. For Model Serving, communication uses direct mTLS encrypted communication with the connection initiated from the control plane with access limited to control plane IP addresses.   |   Secure cluster connectivity  * When enabled (the default for AWS E2 and Azure): Individual VMs connect to the SCC relay in the control plane during cluster creation * When disabled: Control plane connects to individual VMs using public IPs.  | |  **Credential for initial deployment**   |   Databricks internal IAM roles   | * **AWS**   : IAM roles provided by customers. * **Azure**   : None required.  | |  **Credential for regular data plane operations**   |   Databricks invokes  `sts:AssumeRole`  on customer-provided IAM role.   | * **AWS**   : VMs run with instance profiles that are provided by	To create a serverless cluster on Databricks, you can use the serverless compute version of the Databricks platform architecture, where the compute layer exists in your Databricks account rather than your AWS account. Databricks SQL Serverless supports serverless compute, and admins can create serverless SQL warehouses that enable instant compute and are managed by Databricks. Serverless SQL warehouses use compute clusters in your Databricks account. You can create new SQL warehouses from the UI, which are serverless by default, or from the API, which requires that you explicitly specify serverless. For more information, you can refer to the Databricks documentation on serverless compute.	1	1	1	True
98	no_reaction	https://docs.databricks.com/dev-tools/cli/index.html	configure datatbricks cli	Use the command line ======================     Databricks provides the following command-line tools.          |   Name   |   Use this tool when you want to…   | | --- | --- | |  [Databricks CLI setup & documentation](cli/index.html)  |   Use the command line to work with Databricks workspace assets such as cluster policies, clusters, file systems, groups, pools, jobs, libraries, runs, secrets, and tokens.   | |  [Databricks SQL CLI](databricks-sql-cli.html)  |   Use the command line to run SQL commands and scripts on a Databricks SQL warehouse.   |	To configure Databricks CLI, you can follow the setup and documentation provided in the Databricks CLI setup & documentation tool. This tool allows you to work with Databricks workspace assets such as cluster policies, clusters, file systems, groups, pools, jobs, libraries, runs, secrets, and tokens through the command line.	1	1	1	True
99	no_reaction	https://kb.databricks.com/clusters/cluster-failed-launch	what does CLOUD_PROVIDER_LAUNCH_FAILURE mean	Title: Cluster failed to launch  Description: Learn how to resolve <Databricks> cluster launch failures.  Answer: This article describes several scenarios in which a cluster fails to launch, and provides troubleshooting steps for each scenario based on error messages found in logs. Cluster timeout Error messages: Driver failed to start in time INTERNAL_ERROR: The Spark driver failed to start within 300 seconds Cluster failed to be healthy within 200 seconds Cause The cluster can fail to launch if it has a connection to an external Hive metastore and it tries to download all the Hive metastore libraries from a Maven repo. A cluster downloads almost 200 JAR files, including dependencies. If the Databricks cluster manager cannot confirm that the driver is ready within 5 minutes, then cluster launch fails. This can occur because JAR downloading is taking too much time. Solution Store the Hive libraries in DBFS and access them locally from the DBFS location. See Spark Options. Global or cluster-specific init scripts Error message: The cluster could not be started in 50 minutes. Cause: Timed out with exception after &lt;xxx&gt; attempts Cause Init scripts that run during the cluster spin-up stage send an RPC (remote procedure call) to each worker machine to run the scripts locally. All RPCs must return their status before the process continues. If any RPC hits an issue and doesn’t respond back (due to a transient networking issue, for example), then the 1-hour timeout can be hit, causing the cluster setup job to fail. Solution Use a cluster-scoped init script instead of global or cluster-named init scripts. With cluster-scoped init scripts, Databricks does not use synchronous blocking of RPCs to fetch init script execution status. Too many libraries installed in cluster UI Error message: Library installation timed out after 1800 seconds. Libraries that are not yet installed: Cause This is usually an intermittent problem due to network problems. Solution Usually you can fix this problem by re-running the job or restarting the cluster. The library installer is configured to time out after 3 minutes. While fetching and installing jars, a timeout can occur due to network problems. To mitigate this issue, you can download the libraries from Maven to a DBFS location and install it from there. Cloud provider limit Error message: Cluster terminated. Reason: Cloud Provider Limit Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Cloud provider shutdown Error message: Cluster terminated. Reason: Cloud Provider Shutdown Cause This error is usually returned by the cloud provider. Solution See the cloud provider error information in cluster unexpected termination. Instances unreachable (Azure) Error message: Cluster terminated. Reason: Instances Unreachable An unexpected error was encountered while setting up the cluster. Please retry and contact Azure Databricks if the problem persists. Internal error message: Timeout while placing node Cause This error is usually returned by the cloud provider. Typically, it occurs when you have an Azure Databricks workspace deployed to your own virtual network (VNet) (as opposed to the default VNet created when you launch a new Azure Databricks workspace). If the virtual network where the workspace is deployed is already peered or has an ExpressRoute connection to on-premises resources, the virtual network cannot make an ssh connection to the cluster node when Azure Databricks is attempting to create a cluster. Solution Add a user-defined route (UDR) to give the Azure Databricks control plane ssh access to the cluster instances, Blob Storage instances, and artifact resources. This custom UDR allows outbound connections and does not interfere with cluster creation. For detailed UDR instructions, see Step 3: Create user-defined routes and associate them with your Azure Databricks virtual network subnets. For more VNet-related troubleshooting information, see Troubleshooting.	CLOUD_PROVIDER_LAUNCH_FAILURE is not explicitly mentioned in the context. However, the context describes several scenarios in which a cluster fails to launch, and provides troubleshooting steps for each scenario based on error messages found in logs. If the error message is related to cloud provider limit or cloud provider shutdown, the solution is to see the cloud provider error information in cluster unexpected termination.	1	0	0	True
100	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-tables.html	When should I use a managed table vs external table?	"    Create tables ===============     This article introduces the concept of  *managed*   and  *external*   tables in Unity Catalog and describes how to create tables in Unity Catalog.        Note       When you create a table, be sure to reference a catalog that is governed by Unity Catalog. The catalog  `hive\_metastore`  appears in Data Explorer but is not considered governed by Unity Catalog. It is managed by your Databricks workspace’s Hive metastore. All other catalogs listed are governed by Unity Catalog.       You can use the Unity Catalog table upgrade interface to upgrade existing tables registered in the Hive metastore to Unity Catalog. See  [Upgrade tables and views to Unity Catalog](migrate.html)  .           Managed tables ----------------     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.           External tables -----------------     External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.       When you run  `DROP     TABLE`  on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a  `LOCATION`  path in your  `CREATE     TABLE`  statement. External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     To manage access to the underlying cloud storage for an external table, you must set up  [storage credentials and external locations](manage-external-locations-and-credentials.html)  .       To learn more, see  [Create an external table](#create-an-external-table)  .         Requirements --------------     You must have the  `CREATE     TABLE`  privilege on the schema in which you want to create the table, as well as the  `USE     SCHEMA`  privilege on the schema and the  `USE     CATALOG`  privilege on the parent catalog.       If you are creating an external table, see  [Create an external table](#create-an-external-table)  for additional requirements.         Create a managed table ------------------------     To create a managed table, run the following SQL command. You can also use the  [example notebook](#example-notebook-managed-table)  to create a table. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. This cannot be the  `hive\_metastore`  catalog that is created automatically for the Hive metastore associated with your Databricks workspace. * `<schema\_name>`  : The name of the schema. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column.       ``` CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> (  <column\_specification> );  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> ""   ""(""   "" <column\_specification>""   "")"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "",  ""("",  "" <column\_specification>"",  "")"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "" +  ""("" +  "" <column\_specification>"" +  "")"")  ```       You can also create a managed table by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of table full names by using  [databricks\_tables](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/tables)   .       For example, to create the table  `main.default.department`  and insert five rows into it:         ``` CREATE TABLE main.default.department (  deptcode INT,  deptname STRING,  location STRING );  INSERT INTO main.default.department VALUES  (10, 'FINANCE', 'EDINBURGH'),  (20, 'SOFTWARE', 'PADDINGTON'),  (30, 'SALES', 'MAIDSTONE'),  (40, 'MARKETING', 'DARLINGTON'),  (50, 'ADMIN', 'BIRMINGHAM');  ```       ``` spark.sql(""CREATE TABLE main.default.department ""   ""(""   "" deptcode INT,""   "" deptname STRING,""   "" location STRING""   "")""   ""INSERT INTO main.default.department VALUES ""   "" (10, 'FINANCE', 'EDINBURGH'),""   "" (20, 'SOFTWARE', 'PADDINGTON'),""   "" (30, 'SALES', 'MAIDSTONE'),""   "" (40, 'MARKETING', 'DARLINGTON'),""   "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE main.default.department "",  ""("",  "" deptcode INT,"",  "" deptname STRING,"",  "" location STRING"",  "")"",  ""INSERT INTO main.default.department VALUES "",  "" (10, 'FINANCE', 'EDINBURGH'),"",  "" (20, 'SOFTWARE', 'PADDINGTON'),"",  "" (30, 'SALES', 'MAIDSTONE'),"",  "" (40, 'MARKETING', 'DARLINGTON'),"",  "" (50, 'ADMIN', 'BIRMINGHAM')"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE main.default.department "" +  ""("" +  "" deptcode INT,"" +  "" deptname STRING,"" +  "" location STRING"" +  "")"" +  ""INSERT INTO main.default.department VALUES "" +  "" (10, 'FINANCE', 'EDINBURGH'),"" +  "" (20, 'SOFTWARE', 'PADDINGTON'),"" +  "" (30, 'SALES', 'MAIDSTONE'),"" +  "" (40, 'MARKETING', 'DARLINGTON'),"" +  "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```        ###   Example notebooks for creating managed tables     You can use the following example notebooks to create a catalog, schema, and managed table, and to manage permissions on them.        ####   Create and manage a table in Unity Catalog with SQL     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-example-notebook.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Create and manage a table in Unity Catalog with Python     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-quickstart-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                   Drop a managed table ----------------------     You must be the table’s owner to drop a table. To drop a managed table, run the following SQL command:        ``` DROP TABLE IF EXISTS catalog\_name.schema\_name.table\_name;  ```      When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.         Create an external table --------------------------     The data in an external table is stored in a path on your cloud tenant. To work with external tables, Unity Catalog introduces two objects to access and work with external cloud storage:     * A  *storage credential*   contains an authentication method for accessing a cloud storage location. The storage credential does not contain a mapping to the path to which it grants access. Storage credentials are access-controlled to determine which users can use the credential. * An  *external location*   maps a storage credential with a cloud storage path to which it grants access. The external location grants access only to that cloud storage path and its contents. External locations are access-controlled to determine which users can use them. An external location is used automatically when your SQL command contains a  `LOCATION`  clause.      ###   Requirements     To create an external table, you must have:     * The  `CREATE     EXTERNAL     TABLE`  privilege on an external location or storage credential, that grants access to the  `LOCATION`  accessed by the external table. * The  `USE     SCHEMA`  permission on the table’s parent schema. * The  `USE     CATALOG`  permission on the table’s parent catalog. * The  `CREATE     TABLE`  permission on the table’s parent schema.     External locations and storage credentials are stored in the top level of the metastore, rather than in a catalog. To create a storage credential or an external location, you must be the metastore admin or an account-level admin. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .       ###   Create a table     Use one of the following command examples in a notebook or the SQL query editor to create an external table.       You can also use an  [example notebook](#example-notebook-external-table)  to create the storage credential, external location, and external table, and also manage permissions for them.       In the following examples, replace the placeholder values:     * `<catalog>`  : The name of the catalog that will contain the table. * `<schema>`  : The name of the schema that will contain the table. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column. * `<bucket\_path>`  : The path on your cloud tenant where the table will be created. * `<table\_directory>`  : A directory where the table will be created. Use a unique directory for each table.      Important       Once a table is created in a path, users can no longer directly access the files in that path from Databricks even if they have been given privileges on an external location or storage credential to do so. This is to ensure that users cannot circumvent access controls applied to tables by reading files from your cloud tenant directly.          ``` CREATE TABLE <catalog>.<schema>.<table\_name> (  <column\_specification> ) LOCATION 's3://<bucket\_path>/<table\_directory>';  ```       ``` spark.sql(""CREATE TABLE <catalog>.<schema>.<table\_name> ""   ""(""   "" <column\_specification>""   "") ""   ""LOCATION 's"	You should use a managed table when you want Unity Catalog to manage the lifecycle and file layout of the table. Managed tables are stored in the root storage location that you configure when you create a metastore. On the other hand, you should use an external table when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses. External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. When you run `DROP TABLE` on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner.	1	1	1	True
101	no_reaction	https://docs.databricks.com/dev-tools/python-api.html	should i use python api or cli	"    Databricks CLI setup & documentation ======================================         The Databricks command-line interface (CLI) provides an easy-to-use interface to the Databricks platform. The open source project is hosted on  [GitHub](https://github.com/databricks/databricks-cli)   . The CLI is built on top of the  [Databricks REST API](../api/index.html)  and is organized into command groups based on primary endpoints.       You can use the Databricks CLI to do things such as:     * Provision compute resources in Databricks workspaces. * Run data processing and data analysis tasks. * List, import, and export notebooks and folders in workspaces.      Experimental       This CLI is under active development and is released as an  [Experimental](../../release-notes/release-types.html)  client. This means that interfaces are still subject to change.         Set up the CLI ----------------     This section lists CLI requirements and describes how to install and configure your environment to run the CLI.      ###   Requirements   * **Python 3**   - 3.6 and above * **Python 2**   - 2.7.9 and above        Important       On macOS, the default Python 2 installation does not implement the TLSv1\_2 protocol and running the CLI with this Python installation results in the error:  `AttributeError:     'module'     object     has     no     attribute     'PROTOCOL\_TLSv1\_2'`  . Use  [Homebrew](https://brew.sh/)   to install a version of Python that has  `ssl.PROTOCOL\_TLSv1\_2`  .     ###   Install the CLI     Run  `pip     install     databricks-cli`  using the appropriate version of  `pip`  for your Python installation:        ``` pip install databricks-cli  ```      ###   Update the CLI     Run  `pip     install     databricks-cli     --upgrade`  using the appropriate version of  `pip`  for your Python installation:        ``` pip install databricks-cli --upgrade  ```      To list the version of the CLI that is currently installed, run  `databricks     --version`  (or  `databricks     -v`  ):        ``` databricks --version  # Or...  databricks -v  ```        ###   Set up authentication      Note       As a security best practice, when authenticating with automated tools, systems, scripts, and apps, Databricks recommends you use access tokens belonging to  [service principals](../../administration-guide/users-groups/service-principals.html)  instead of workspace users. To create access tokens for service principals, see  [Manage access tokens for a service principal](../../administration-guide/users-groups/service-principals.html#personal-access-tokens)  .        Before you can run CLI commands, you must set up authentication. To authenticate to the CLI, you can use a  [Databricks personal access token](../auth.html#pat)  . A Databricks username and password are also supported but not recommended.       To configure the CLI to use a personal access token, run the following command:        ``` databricks configure --token  ```      The command begins by issuing the prompt:        ``` Databricks Host (should begin with https://):  ```      Enter your workspace URL, with the format  `https://<instance-name>.cloud.databricks.com`  . To get your workspace URL, see  [Workspace instance names, URLs, and IDs](../../workspace/workspace-details.html#workspace-url)  .       The command continues by issuing the prompt to enter your personal access token:        ``` Token:  ```      After you complete the prompts, your access credentials are stored in the file  `~/.databrickscfg`  on Unix, Linux, or macOS, or  `%USERPROFILE%\.databrickscfg`  on Windows. The file contains a default profile entry:        ``` [DEFAULT] host = <workspace-URL> token = <personal-access-token>  ```      For CLI 0.8.1 and above, you can change the path of this file by setting the environment variable  `DATABRICKS\_CONFIG\_FILE`  .          ``` export DATABRICKS\_CONFIG\_FILE=<path-to-file>  ```         ``` setx DATABRICKS_CONFIG_FILE ""<path-to-file>"" /M  ```        Although not recommended, it is possible to use your Databricks username and password instead of a Databricks personal access token to authenticate. Run  `databricks     configure`  and follow the prompts. The  `.databrickscfg`  file contains a default profile entry:        ``` [DEFAULT] host = <workspace-URL> username = <username> password = <password>  ```       Important       Beginning with CLI 0.17.2, the CLI does not work with a  [.netrc file](../api/latest/authentication.html#netrc)  . You can have a  `.netrc`  file in your environment for other purposes, but the CLI will not use that  `.netrc`  file.        CLI 0.8.0 and above supports the following Databricks  [environment variables](../auth.html#env-vars)  :     * `DATABRICKS\_HOST` * `DATABRICKS\_USERNAME` * `DATABRICKS\_PASSWORD` * `DATABRICKS\_TOKEN`     An environment variable setting takes precedence over the setting in the configuration file.      ####   Test your authentication setup     To check whether you set up authentication correctly, you can run a command such as the following, replacing  `<someone@example.com>`  with your Databricks workspace username:        ``` databricks workspace ls /Users/<someone@example.com>  ```      If successful, this command  [lists the objects in the specified workspace path](workspace-cli.html#list-objects-in-a-workspace)  .          ###   Connection profiles     The Databricks CLI configuration supports multiple connection profiles. The same installation of Databricks CLI can be used to make API calls on multiple Databricks workspaces.       To add a connection profile, specify a unique name for the profile:        ``` databricks configure --token --profile <profile-name>  ```      The  `.databrickscfg`  file contains a corresponding profile entry:        ``` [<profile-name>] host = <workspace-URL> token = <token>  ```      To use the connection profile:        ``` databricks <group> <command> --profile <profile-name>  ```      If  `--profile     <profile-name>`  is not specified, the default profile is used. If a default profile is not found, you are prompted to configure the CLI with a default profile.       ###   Test your connection profiles     To check whether you set up your connection profiles correctly, you can run a command such as the following, replacing  `<someone@example.com>`  with your Databricks workspace username and  `<DEFAULT>`  with one of your connection profile names:        ``` databricks workspace ls /Users/<someone@example.com> --profile <DEFAULT>  ```      If successful, this command  [lists the objects in the specified workspace path](workspace-cli.html#list-objects-in-a-workspace)  in the workspace for the specified connection profile. Run this command for each connection profile that you want to test.         ###   Alias command groups     Sometimes it can be inconvenient to prefix each CLI invocation with the name of a command group, for example  `databricks     workspace     ls`  . To make the CLI easier to use, you can alias command groups to shorter commands. For example, to shorten  `databricks     workspace     ls`  to  `dw     ls`  in the Bourne again shell, you can add  `alias     dw=""databricks     workspace""`  to the appropriate bash profile. Typically, this file is located at  `~/.bash\_profile`  .        Tip       Databricks already aliases  `databricks     fs`  to  `dbfs`  ;  `databricks     fs     ls`  and  `dbfs     ls`  are equivalent.           Use the CLI -------------     This section shows you how to get CLI help, parse CLI output, and invoke commands in each command group.      ###   Display CLI command group help     You list the subcommands for any command group by running  `databricks     <group>     --help`  (or  `databricks     <group>     -h`  ). For example, you list the DBFS CLI subcommands by running  `databricks     fs     -h`  :        ``` databricks fs -h  ```      ###   Display CLI subcommand help     You list the help for a subcommand by running  `databricks     <group>     <subcommand>     --help`  (or  `databricks     <group>     <subcommand>     -h`  ). For example, you list the help for the DBFS copy files subcommand by running  `databricks     fs     cp     -h`  :        ``` databricks fs cp -h  ```        ###   Use  `jq`  to parse CLI output     Some Databricks CLI commands output the JSON response from the API endpoint. Sometimes it can be useful to parse out parts of the JSON to pipe into other commands. For example, to copy a job definition, you must take the  `settings`  field of a  `databricks     jobs     get`  command and use that as an argument to the  `databricks     jobs     create`  command. In these cases, we recommend you to use the utility  `jq`  .       For example, the following command prints the settings of the job with the ID of 233.        ``` databricks jobs list --output JSON | jq '.jobs[] | select(.job\_id == 233) | .settings'  ```       ``` {  ""name"": ""Quickstart"",  ""new\_cluster"": {  ""spark\_version"": ""7.5.x-scala2.12"",  ""spark\_env\_vars"": {  ""PYSPARK\_PYTHON"": ""/databricks/python3/bin/python3""  },  ""num\_workers"": 8,  ...  },  ""email\_notifications"": {},  ""timeout\_seconds"": 0,  ""notebook\_task"": {  ""notebook\_path"": ""/Quickstart""  },  ""max\_concurrent\_runs"": 1 }  ```      As another example, the following command prints the names and IDs of all available clusters in the workspace:        ``` databricks clusters list --output JSON | jq '[ .clusters[] | { name: .cluster\_name, id: .cluster\_id } ]'  ```       ``` [  {  ""name"": ""My Cluster 1"",  ""id"": ""1234-5"	Based on the context, you can use both Python API and CLI to interact with the Databricks platform. The CLI is built on top of the Databricks REST API and provides an easy-to-use interface to the Databricks platform. You can use the Databricks CLI to provision compute resources in Databricks workspaces, run data processing and data analysis tasks, list, import, and export notebooks and folders in workspaces. On the other hand, the Python API provides a programmatic way to interact with the Databricks platform using Python code. The choice between using the Python API or CLI depends on your use case and personal preference.	1	1	1	True
102	no_reaction	https://docs.databricks.com/dev-tools/cli/dbfs-cli.html	how to delete file on dbfs?	DBFS CLI ==========     You run Databricks DBFS CLI subcommands appending them to  `databricks     fs`  (or the alias  `dbfs`  ), prefixing all DBFS paths with  `dbfs:/`  . These subcommands call the  [DBFS API 2.0](../api/latest/dbfs.html)  .        ``` databricks fs -h  ```       ``` Usage: databricks fs [OPTIONS] COMMAND [ARGS]...    Utility to interact with DBFS. DBFS paths are all prefixed   with dbfs:/. Local paths can be absolute or local.  Options:   -v, --version   -h, --help     Show this message and exit.  Commands:   cat        Shows the contents of a file. Does not work for directories.   configure   cp         Copies files to and from DBFS.     Options:       -r, --recursive       --overwrite     Overwrites files that exist already.   ls         Lists files in DBFS.     Options:       --absolute      Displays absolute paths.       -l              Displays full information including size and file type.   mkdirs     Makes directories in DBFS.   mv         Moves a file between two DBFS paths.   rm         Removes files from DBFS.     Options:       -r, --recursive  ```      For operations that list, move, or delete more than 10k files, we strongly discourage using the DBFS CLI.     * The  `list`  operation (  `databricks     fs     ls`  ) will time out after approximately 60s. * The  `move`  operation (  `databricks     fs     mv`  ) will time out after approximately 60s, potentially resulting in partially moved data. * The  `delete`  operation (  `databricks     fs     rm`  ) will incrementally delete batches of files.     We recommend that you perform such operations in the context of a cluster, using  [File system utility (dbutils.fs)](../databricks-utils.html#dbutils-fs)  .  `dbutils.fs`  covers the functional scope of the DBFS REST API, but from notebooks. Running such operations using notebooks provides better control, such as selective deletes, manageability, and the possibility to automate periodic jobs.        List the contents of a file -----------------------------     To display usage documentation, run  `databricks     fs     cat     --help`  .        ``` databricks fs cat dbfs:/tmp/my-file.txt  ```       ``` Apache Spark is awesome!  ```        Copy a file -------------     To display usage documentation, run  `databricks     fs     cp     --help`  .        ``` databricks fs cp dbfs:/tmp/your_file.txt dbfs:/parent/child/grandchild/my_file.txt --overwrite  ```      On success, this command displays nothing.         List information about files and directories ----------------------------------------------     To display usage documentation, run  `databricks     fs     ls     --help`  .        ``` databricks fs ls dbfs:/tmp --absolute -l  ```       ``` file 42408084 dbfs:/tmp/LoanStats.csv 1590005159000 file 40 dbfs:/tmp/file\_b.txt 1603991038000 dir 0 dbfs:/tmp/hive 0 dir 0 dbfs:/tmp/mlflow 0 file 385 dbfs:/tmp/multi-line.json 1597770632000 dir 0 dbfs:/tmp/new 0 dir 0 dbfs:/tmp/parent 0 file 243 dbfs:/tmp/test.json 1597770628000 file 40 dbfs:/tmp/test\_dbfs.txt 1603989162000  ```        Create a directory --------------------     To display usage documentation, run  `databricks     fs     mkdirs     --help`  .        ``` databricks fs mkdirs dbfs:/tmp/new-dir  ```      On success, this command displays nothing.         Move a file -------------     To display usage documentation, run  `databricks     fs     mv     --help`  .        ``` databricks fs mv dbfs:/tmp/my-file.txt dbfs:/parent/child/grandchild/my-file.txt  ```      On success, this command displays nothing.         Delete a file ---------------     To display usage documentation, run  `databricks     fs     rm     --help`  .        ``` databricks fs rm dbfs:/tmp/parent/child/grandchild/my-file.txt  ```       ``` Delete finished successfully.  ```	To delete a file on DBFS, you can use the `databricks fs rm` command followed by the path of the file you want to delete, prefixed with `dbfs:/`. For example, to delete a file named `my_file.txt` located in the `/tmp` directory, you can run the command `databricks fs rm dbfs:/tmp/my_file.txt`.	1	1	1	True
104	no_reaction	https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-table.html	how to create a table in Python?	"   pyspark.sql.Catalog.createTable  [¶](#pyspark-sql-catalog-createtable ""Permalink to this headline"") =====================================================================================================  `Catalog.`  `createTable`    (   *tableName     :     str*  ,  *path     :     Optional    [    str    ]     =     None*  ,  *source     :     Optional    [    str    ]     =     None*  ,  *schema     :     Optional    [   [pyspark.sql.types.StructType](pyspark.sql.types.StructType.html#pyspark.sql.types.StructType ""pyspark.sql.types.StructType"")    ]     =     None*  ,  *description     :     Optional    [    str    ]     =     None*  ,  *\*\*     options     :     str*   )    → pyspark.sql.dataframe.DataFrame  [[source]](../../../_modules/pyspark/sql/catalog.html#Catalog.createTable) [¶](#pyspark.sql.Catalog.createTable ""Permalink to this definition"")    Creates a table based on the dataset in a data source.     New in version 2.2.0.     Parameters    **tableName**    str     name of the table to create.     Changed in version 3.4.0:    Allow  `tableName`  to be qualified with catalog name.    **path**    str, optional     the path in which the data for this table exists. When  `path`  is specified, an external table is created from the data at the given path. Otherwise a managed table is created.    **source**    str, optional     the source of this table such as ‘parquet, ‘orc’, etc. If  `source`  is not specified, the default data source configured by  `spark.sql.sources.default`  will be used.    **schema**    class:    StructType    , optional     the schema for this table.    **description**    str, optional     the description of this table.     Changed in version 3.1.0:    Added the  `description`  parameter.    **\*\*options**    dict, optional     extra options to specify in the table.     Returns    [`DataFrame`](pyspark.sql.DataFrame.html#pyspark.sql.DataFrame ""pyspark.sql.DataFrame"")   The DataFrame associated with the table.     Examples     Creating a managed table.    ``` >>> \_ = spark.catalog.createTable(""tbl1"", schema=spark.range(1).schema, source='parquet') >>> \_ = spark.sql(""DROP TABLE tbl1"")  ```   Creating an external table    ``` >>> import tempfile >>> with tempfile.TemporaryDirectory() as d: ...     \_ = spark.catalog.createTable( ...         ""tbl2"", schema=spark.range(1).schema, path=d, source='parquet') >>> \_ = spark.sql(""DROP TABLE tbl2"")  ```  [pyspark.sql.Catalog.createExternalTable](pyspark.sql.Catalog.createExternalTable.html ""previous page"")  [pyspark.sql.Catalog.currentCatalog](pyspark.sql.Catalog.currentCatalog.html ""next page"")   "	To create a table in Python, you can use the `Catalog.createTable` method from the `pyspark.sql` module. This method allows you to create a table based on the dataset in a data source. You need to provide the name of the table to create, and optionally the path in which the data for this table exists, the source of this table such as ‘parquet, ‘orc’, etc., the schema for this table, the description of this table, and extra options to specify in the table.	1	1	1	True
105	no_reaction	https://docs.databricks.com/data/manage-permissions.html	how do I use SQL to revoke access to a catalog?	"Title: Revoke all user privileges  Description: Use a regex and a series of for loops to revoke all privileges for a single user.  Answer: When user permissions are explicitly granted for individual tables and views, the selected user can access those tables and views even if they don’t have permission to access the underlying database. If you want to revoke a user’s access, you can do so with the REVOKE command. However, the REVOKE command is explicit, and is strictly scoped to the object specified in the command. For example: %sql REVOKE ALL PRIVILEGES ON DATABASE &lt;database-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;` REVOKE SELECT ON &lt;table-name&gt; FROM `&lt;user&gt;@&lt;domain-name&gt;` If you want to revoke all privileges for a single user you can do it with a series of multiple commands, or you can use a regular expression and a series of for loops to automate the process. Example code This example code matches the &lt;search-string&gt; pattern to the database name and the table name and then revokes the user’s privileges. The search is recursive. %python from re import search databaseQuery = sqlContext.sql(""show databases"") databaseList = databaseQuery.collect() # This loop revokes at the database level. for db in databaseList:   listTables = sqlContext.sql(""show tables from ""+dbdatabaseName)   tableRows = listTables.collect()   if search(&lt;search-string&gt;, dbdatabaseName):     revokeDatabase=sqlContext.sql(""REVOKE ALL PRIVILAGES ON DATABASE ""+dbdatabaseName+"" to `&lt;username&gt;`"")     display(revokeDatabase)     print(""Ran the REVOKE query on ""+dbdatabaseName+"" for &lt;username&gt;"")   # This loop revokes at the table level.   for table in tableRows:     if search(&lt;search-string&gt;,tabletableName):       revokeCommand=sqlContext.sql(""REVOKE SELECT ON ""+tabledatabase+"".""+tabletableName+"" FROM `&lt;username&gt;`"")       display(revokeCommand)       print(""Revoked the SELECT permissions on ""+tabledatabase+"".""+tabletableName+"" for &lt;username&gt;"") Info These commands only work if you have enabled table access control for the cluster (AWS | Azure | GCP)."	To revoke access to a catalog using SQL, you can use the REVOKE command. However, the REVOKE command is explicit and is strictly scoped to the object specified in the command. If you want to revoke all privileges for a single user, you can use a regular expression and a series of for loops to automate the process. The example code provided in the context matches the search string pattern to the database name and the table name and then revokes the user's privileges. These commands only work if you have enabled table access control for the cluster.	1	1	1	True
109	no_reaction	https://kb.databricks.com/python/convert-datetime-to-string	how do i get a timestamp into a dataframe	"   pyspark.sql.functions.to\_timestamp  [¶](#pyspark-sql-functions-to-timestamp ""Permalink to this headline"") ============================================================================================================  `pyspark.sql.functions.`  `to_timestamp`    (   *col     :     ColumnOrName*  ,  *format     :     Optional    [    str    ]     =     None*   )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#to_timestamp) [¶](#pyspark.sql.functions.to_timestamp ""Permalink to this definition"")    Converts a  [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")  into  [`pyspark.sql.types.TimestampType`](pyspark.sql.types.TimestampType.html#pyspark.sql.types.TimestampType ""pyspark.sql.types.TimestampType"")  using the optionally specified format. Specify formats according to  [datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)   . By default, it follows casting rules to  [`pyspark.sql.types.TimestampType`](pyspark.sql.types.TimestampType.html#pyspark.sql.types.TimestampType ""pyspark.sql.types.TimestampType"")  if the format is omitted. Equivalent to  `col.cast(""timestamp"")`  .     New in version 2.2.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **col**   [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")  or str     column values to convert.    **format: str, optional**    format to use to convert timestamp values.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")   timestamp value as  [`pyspark.sql.types.TimestampType`](pyspark.sql.types.TimestampType.html#pyspark.sql.types.TimestampType ""pyspark.sql.types.TimestampType"")  type.     Examples    ``` >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], t) >>> df.select(to\_timestamp(df.t).alias('dt')).collect() [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]  ```  ``` >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], t) >>> df.select(to\_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect() [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]  ```  [pyspark.sql.functions.unix\_timestamp](pyspark.sql.functions.unix_timestamp.html ""previous page"")  [pyspark.sql.functions.to\_date](pyspark.sql.functions.to_date.html ""next page"")   "	To get a timestamp into a dataframe, you can use the `to_timestamp` function from the `pyspark.sql.functions` module. This function converts a column into a `pyspark.sql.types.TimestampType` type using the specified format. If the format is omitted, it follows casting rules to `pyspark.sql.types.TimestampType`. Here is an example:  ``` from pyspark.sql.functions import to_timestamp  df = spark.createDataFrame([('1997-02-28 10:30:00',)], t) df.select(to_timestamp(df.t).alias('dt')).show() ```  This will output a dataframe with a new column `dt` containing the timestamp value.	1	1	1	True
118	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-functions-builtin-alpha.html	What is the list of databricks functions?	Alphabetical list of built-in functions =========================================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       This article provides an alphabetically-ordered list of built-in functions and operators in Databricks.      * [`abs`  function](functions/abs.html) * [`acos`  function](functions/acos.html) * [`acosh`  function](functions/acosh.html) * [`add\_months`  function](functions/add_months.html) * [`aes\_decrypt`  function](functions/aes_decrypt.html) * [`aes\_encrypt`  function](functions/aes_encrypt.html) * [`aggregate`  function](functions/aggregate.html) * [`&`  (ampersand sign) operator](functions/ampersandsign.html) * [`and`  predicate](functions/and.html) * [`any`  aggregate function](functions/any.html) * [`any\_value`  aggregate function](functions/any_value.html) * [`approx\_count\_distinct`  aggregate function](functions/approx_count_distinct.html) * [`approx\_percentile`  aggregate function](functions/approx_percentile.html) * [`approx\_top\_k`  aggregate function](functions/approx_top_k.html) * [`array`  function](functions/array.html) * [`array\_agg`  aggregate function](functions/array_agg.html) * [`array\_append`  function](functions/array_append.html) * [`array\_compact`  function](functions/array_compact.html) * [`array\_contains`  function](functions/array_contains.html) * [`array\_distinct`  function](functions/array_distinct.html) * [`array\_except`  function](functions/array_except.html) * [`array\_intersect`  function](functions/array_intersect.html) * [`array\_join`  function](functions/array_join.html) * [`array\_max`  function](functions/array_max.html) * [`array\_min`  function](functions/array_min.html) * [`array\_position`  function](functions/array_position.html) * [`array\_remove`  function](functions/array_remove.html) * [`array\_repeat`  function](functions/array_repeat.html) * [`array\_size`  function](functions/array_size.html) * [`array\_sort`  function](functions/array_sort.html) * [`array\_union`  function](functions/array_union.html) * [`arrays\_overlap`  function](functions/arrays_overlap.html) * [`arrays\_zip`  function](functions/arrays_zip.html) * [`ascii`  function](functions/ascii.html) * [`asin`  function](functions/asin.html) * [`asinh`  function](functions/asinh.html) * [`assert\_true`  function](functions/assert_true.html) * [`\*`  (asterisk sign) operator](functions/asterisksign.html) * [`atan`  function](functions/atan.html) * [`atan2`  function](functions/atan2.html) * [`atanh`  function](functions/atanh.html) * [`avg`  aggregate function](functions/avg.html) * [`!=`  (bangeq sign) operator](functions/bangeqsign.html) * [`!`  (bang sign) operator](functions/bangsign.html) * [`base64`  function](functions/base64.html) * [`between`  predicate](functions/between.html) * [`bigint`  function](functions/bigint.html) * [`bin`  function](functions/bin.html) * [`binary`  function](functions/binary.html) * [`bit\_and`  aggregate function](functions/bit_and.html) * [`bit\_count`  function](functions/bit_count.html) * [`bit\_get`  function](functions/bit_get.html) * [`bit\_length`  function](functions/bit_length.html) * [`bit\_or`  aggregate function](functions/bit_or.html) * [`bit\_reverse`  function](functions/bit_reverse.html) * [`bit\_xor`  aggregate function](functions/bit_xor.html) * [`bool\_and`  aggregate function](functions/bool_and.html) * [`bool\_or`  aggregate function](functions/bool_or.html) * [`boolean`  function](functions/boolean.html) * [`[     ]`  (bracket sign) operator](functions/bracketsign.html) * [`bround`  function](functions/bround.html) * [`btrim`  function](functions/btrim.html) * [`cardinality`  function](functions/cardinality.html) * [`^`  (caret sign) operator](functions/caretsign.html) * [`case`  expression](functions/case.html) * [`cast`  function](functions/cast.html) * [`cbrt`  function](functions/cbrt.html) * [`ceil`  function](functions/ceil.html) * [`ceiling`  function](functions/ceiling.html) * [`char`  function](functions/char.html) * [`char\_length`  function](functions/char_length.html) * [`character\_length`  function](functions/character_length.html) * [`charindex`  function](functions/charindex.html) * [`chr`  function](functions/chr.html) * [`cloud\_files\_state`  table-valued function](functions/cloud_files_state.html) * [`coalesce`  function](functions/coalesce.html) * [`collect\_list`  aggregate function](functions/collect_list.html) * [`collect\_set`  aggregate function](functions/collect_set.html) * [`::`  (colon colon sign) operator](functions/coloncolonsign.html) * [`:`  (colon sign) operator](functions/colonsign.html) * [`concat`  function](functions/concat.html) * [`concat\_ws`  function](functions/concat_ws.html) * [`contains`  function](functions/contains.html) * [`conv`  function](functions/conv.html) * [`corr`  aggregate function](functions/corr.html) * [`cos`  function](functions/cos.html) * [`cosh`  function](functions/cosh.html) * [`cot`  function](functions/cot.html) * [`count`  aggregate function](functions/count.html) * [`count\_if`  aggregate function](functions/count_if.html) * [`count\_min\_sketch`  aggregate function](functions/count_min_sketch.html) * [`covar\_pop`  aggregate function](functions/covar_pop.html) * [`covar\_samp`  aggregate function](functions/covar_samp.html) * [`crc32`  function](functions/crc32.html) * [`csc`  function](functions/csc.html) * [`cube`  function](functions/cube.html) * [`cume\_dist`  analytic window function](functions/cume_dist.html) * [`curdate`  function](functions/curdate.html) * [`current\_catalog`  function](functions/current_catalog.html) * [`current\_database`  function](functions/current_database.html) * [`current\_date`  function](functions/current_date.html) * [`current\_metastore`  function](functions/current_metastore.html) * [`current\_schema`  function](functions/current_schema.html) * [`current\_timestamp`  function](functions/current_timestamp.html) * [`current\_timezone`  function](functions/current_timezone.html) * [`current\_user`  function](functions/current_user.html) * [`current\_version`  function](functions/current_version.html) * [`date`  function](functions/date.html) * [`date\_add`  function](functions/date_add.html) * [`date\_format`  function](functions/date_format.html) * [`date\_from\_unix\_date`  function](functions/date_from_unix_date.html) * [`date\_part`  function](functions/date_part.html) * [`date\_sub`  function](functions/date_sub.html) * [`date\_trunc`  function](functions/date_trunc.html) * [`dateadd`  function](functions/dateadd.html) * [`dateadd`  (days) function](functions/dateadd2.html) * [`datediff`  function](functions/datediff.html) * [`datediff`  (timestamp) function](functions/datediff3.html) * [`day`  function](functions/day.html) * [`dayofmonth`  function](functions/dayofmonth.html) * [`dayofweek`  function](functions/dayofweek.html) * [`dayofyear`  function](functions/dayofyear.html) * [`decimal`  function](functions/decimal.html) * [`decode`  function](functions/decode.html) * [`decode`  (character set) function](functions/decode_cs.html) * [`degrees`  function](functions/degrees.html) * [`dense\_rank`  ranking window function](functions/dense_rank.html) * [`div`  operator](functions/div.html) * [`.`  (dot sign) operator](functions/dotsign.html) * [`double`  function](functions/double.html) * [`e`  function](functions/e.html) * [`element\_at`  function](functions/element_at.html) * [`elt`  function](functions/elt.html) * [`encode`  function](functions/encode.html) * [`endswith`  function](functions/endswith.html) * [`==`  (eq eq sign) operator](functions/eqeqsign.html) * [`=`  (eq sign) operator](functions/eqsign.html) * [`equal\_null`  function](functions/equal_null.html) * [`every`  aggregate function](functions/every.html) * [`exists`  function](functions/exists.html) * [`exp`  function](functions/exp.	The list of Databricks functions can be found in an alphabetically-ordered list in the Databricks documentation. The list includes functions such as `abs`, `acos`, `add_months`, `aggregate`, `array`, `ascii`, `avg`, `base64`, `ceil`, `concat`, `count`, `date`, `decode`, `double`, `element_at`, `endswith`, `exp`, and many more.	1	1	1	True
123	thumb_down	https://docs.databricks.com/ingestion/index.html	1. How do I load files into Databricks?	LOAD DATA ===========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Loads the data into a Hive SerDe table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the  `LOAD     DATA`  statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table.       If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the table or the dependents are accessed the next time.        Syntax --------      ``` LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table\_name [ PARTITION clause ]  ```        Parameters ------------   * **path**       Path of the file system. It can be either an absolute or a relative path. * **[table\_name](sql-ref-names.html#table-name)**      Identifies the table to be inserted to. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the table cannot be found Databricks raises a  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error. * **[PARTITION clause](sql-ref-partition.html#partition)**      An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition. * **LOCAL**       If specified, it causes the  `INPATH`  to be resolved against the local file system, instead of the default file system, which is typically a distributed storage. * **OVERWRITE**       By default, new data is appended to the table. If  `OVERWRITE`  is used, the table is instead overwritten with new data.       Examples ----------      ``` -- Example without partition specification. -- Assuming the students table has already been created and populated. > SELECT \* FROM students;  name address student\_id  --------- ---------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111   > CREATE TABLE test\_load (name VARCHAR(64), address VARCHAR(64), student\_id INT) USING HIVE;  -- Assuming the students table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test\_load;  > SELECT \* FROM test\_load;  name address student\_id  --------- ---------------------- ----------  Amy Smith 123 Park Ave, San Jose 111111  -- Example with partition specification. > CREATE TABLE test\_partition (c1 INT, c2 INT, c3 INT) PARTITIONED BY (c2, c3);  > INSERT INTO test\_partition PARTITION (c2 = 2, c3 = 3) VALUES (1);  > INSERT INTO test\_partition PARTITION (c2 = 5, c3 = 6) VALUES (4);  > INSERT INTO test\_partition PARTITION (c2 = 8, c3 = 9) VALUES (7);  > SELECT \* FROM test\_partition;  c1 c2 c3  --- --- ---  1 2 3  4 5 6  7 8 9  > CREATE TABLE test\_load\_partition (c1 INT, c2 INT, c3 INT) USING HIVE PARTITIONED BY (c2, c3);  -- Assuming the test\_partition table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/test\_partition/c2=2/c3=3'  OVERWRITE INTO TABLE test\_load\_partition PARTITION (c2=2, c3=3);  > SELECT \* FROM test\_load\_partition;  c1 c2 c3  --- --- ---  1 2 3  ```        Related articles ------------------   * [INSERT INTO](sql-ref-syntax-dml-insert-into.html) * [COPY INTO](delta-copy-into.html)	To load files into Databricks, you can use the `LOAD DATA` statement to load data into a Hive SerDe table from the user-specified directory or file. If a directory is specified, then all the files from the directory are loaded. If a file is specified, then only the single file is loaded. Additionally, the `LOAD DATA` statement takes an optional partition specification. When a partition is specified, the data files (when the input source is a directory) or the single file (when the input source is a file) are loaded into the partition of the target table.	1	1	1	True
126	no_reaction	https://docs.databricks.com/administration-guide/workspace/create-workspace.html	create new workspace	Manage your Databricks workspaces ===================================     This article is an overview of your options for creating and managing workspaces. It also includes guidance on accessing the admin console and workspace settings.          What is a workspace? ----------------------     A workspace is a Databricks deployment in a cloud service account. It provides a unified environment for working with Databricks assets for a specified set of users.         Create a workspace --------------------     There are a few different methods you can use to create new workspaces. These are:     * Using the Databricks  [account console](create-workspace.html)  UI * Calling the Databricks  [Account API](create-workspace-api.html) * Using the Databricks  [Terraform Provider](../../dev-tools/terraform/e2-workspace.html) * [AWS Quick Start](templates.html#quickstart)  (not formally supported by Databricks)         Access the admin console --------------------------     The admin console is where you can manage features and settings for your Databricks workspace. To access the admin console, click your username in the top bar of the Databricks workspace and select  **Admin Console**   .         Manage workspace settings ---------------------------     The admin console includes a tab for  **Workspace Settings**   where you can manage workspace-specific behavior. For more information, see  [Manage workspace settings](settings/index.html)  .	To create a new workspace, there are a few different methods you can use, including using the Databricks account console UI, calling the Databricks Account API, using the Databricks Terraform Provider, or AWS Quick Start (not formally supported by Databricks).	1	1	1	True
127	no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.merge.html	pyspark merge	"   pyspark.pandas.merge  [¶](#pyspark-pandas-merge ""Permalink to this headline"") ===============================================================================  `pyspark.pandas.`  `merge`    (   *obj     :     pyspark.pandas.frame.DataFrame*  ,  *right     :     pyspark.pandas.frame.DataFrame*  ,  *how     :     str     =     'inner'*  ,  *on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *left\_on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *right\_on     :     Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]], None]     =     None*  ,  *left\_index     :     bool     =     False*  ,  *right\_index     :     bool     =     False*  ,  *suffixes     :     Tuple    [    str    ,    str    ]     =     '\_x', '\_y'*   )    → pyspark.pandas.frame.DataFrame  [[source]](../../../_modules/pyspark/pandas/namespace.html#merge) [¶](#pyspark.pandas.merge ""Permalink to this definition"")    Merge DataFrame objects with a database-style join.     The index of the resulting DataFrame will be one of the following:    * 0…n if no index is used for merging * Index of the left DataFrame if merged only on the index of the right DataFrame * Index of the right DataFrame if merged only on the index of the left DataFrame * All involved indices if merged using the indices of both DataFrames     e.g. if    left    with indices (a, x) and    right    with indices (b, x), the result will be an index (x, a, b)   Parameters    **right: Object to merge with.**   **how: Type of merge to be performed.**    {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’     left: use only keys from left frame, like a SQL left outer join; preserve key     order.     right: use only keys from right frame, like a SQL right outer join; preserve key     order.     outer: use union of keys from both frames, like a SQL full outer join; sort keys     lexicographically.     inner: use intersection of keys from both frames, like a SQL inner join;     preserve the order of the left keys.    **on: Column or index level names to join on. These must be found in both DataFrames. If on**    is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.    **left\_on: Column or index level names to join on in the left DataFrame. Can also**    be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.    **right\_on: Column or index level names to join on in the right DataFrame. Can also**    be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.    **left\_index: Use the index from the left DataFrame as the join key(s). If it is a**    MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.    **right\_index: Use the index from the right DataFrame as the join key. Same caveats as**    left\_index.    **suffixes: Suffix to apply to overlapping column names in the left and right side,**    respectively.     Returns     DataFrame     A DataFrame of the two merged objects.     Notes     As described in #263, joining string columns currently returns None for missing values     instead of NaN.     Examples    ``` >>> df1 = ps.DataFrame({'lkey': foo', 'bar', 'baz', 'foo, ...                     'value': [1, 2, 3, 5]}, ...                    columns=lkey', 'value) >>> df2 = ps.DataFrame({'rkey': foo', 'bar', 'baz', 'foo, ...                     'value': [5, 6, 7, 8]}, ...                    columns=rkey', 'value) >>> df1  lkey value 0 foo 1 1 bar 2 2 baz 3 3 foo 5 >>> df2  rkey value 0 foo 5 1 bar 6 2 baz 7 3 foo 8  ```   Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, \_x and \_y, appended.    ``` >>> merged = ps.merge(df1, df2, left\_on='lkey', right\_on='rkey') >>> merged.sort\_values(by=lkey', 'value\_x', 'rkey', 'value\_y)    lkey value\_x rkey value\_y ...bar 2 bar 6 ...baz 3 baz 7 ...foo 1 foo 5 ...foo 1 foo 8 ...foo 5 foo 5 ...foo 5 foo 8  ```  ``` >>> left\_psdf = ps.DataFrame({'A': [1, 2]}) >>> right\_psdf = ps.DataFrame({'B': x', 'y}, index=[1, 2])  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True).sort\_index()  A B 1 2 x  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='left').sort\_index()  A B 0 1 None 1 2 x  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='right').sort\_index()  A B 1 2.0 x 2 NaN y  ```  ``` >>> ps.merge(left\_psdf, right\_psdf, left\_index=True, right\_index=True, how='outer').sort\_index()  A B 0 1.0 None 1 2.0 x 2 NaN y  ```  [pyspark.pandas.melt](pyspark.pandas.melt.html ""previous page"")  [pyspark.pandas.merge\_asof](pyspark.pandas.merge_asof.html ""next page"")   "	The `pyspark.pandas.merge` function is used to merge DataFrame objects with a database-style join. It takes in two DataFrames, `left` and `right`, and performs a merge based on the specified `how` parameter, which can be 'left', 'right', 'outer', or 'inner'. The resulting DataFrame will have an index that depends on the type of merge performed. The `on` parameter specifies the column or index level names to join on, and if it is None and not merging on indexes, it defaults to the intersection of the columns in both DataFrames. The `left_on` and `right_on` parameters specify the column or index level names to join on in the left and right DataFrames, respectively. The `left_index` and `right_index` parameters specify whether to use the index from the left or right DataFrame as the join key(s). The `suffixes` parameter specifies the suffix to apply to overlapping column names in the left and right side, respectively.	1	1	1	True
128	no_reaction	https://docs.databricks.com/delta-live-tables/python-ref.html	how to write create or refresh streaming live table in python?	"   pyspark.sql.streaming.DataStreamWriter.toTable  [¶](#pyspark-sql-streaming-datastreamwriter-totable ""Permalink to this headline"") ===================================================================================================================================  `DataStreamWriter.`  `toTable`    (   *tableName     :     str*  ,  *format     :     Optional    [    str    ]     =     None*  ,  *outputMode     :     Optional    [    str    ]     =     None*  ,  *partitionBy     :     Union[str, List[str], None]     =     None*  ,  *queryName     :     Optional    [    str    ]     =     None*  ,  *\*\*     options     :     OptionalPrimitiveType*   )    → pyspark.sql.streaming.query.StreamingQuery  [[source]](../../../_modules/pyspark/sql/streaming/readwriter.html#DataStreamWriter.toTable) [¶](#pyspark.sql.streaming.DataStreamWriter.toTable ""Permalink to this definition"")    Starts the execution of the streaming query, which will continually output results to the given table as new data arrives.     The returned  [`StreamingQuery`](pyspark.sql.streaming.StreamingQuery.html#pyspark.sql.streaming.StreamingQuery ""pyspark.sql.streaming.StreamingQuery"")  object can be used to interact with the stream.     New in version 3.1.0.     Parameters    **tableName**    str     string, for the name of the table.    **format**    str, optional     the format used to save.    **outputMode**    str, optional     specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.    * append    : Only the new rows in the streaming DataFrame/Dataset will be written to the sink * complete    : All the rows in the streaming DataFrame/Dataset will be written to the sink every time these are some updates * update    : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn’t contain aggregations, it will be equivalent to    append    mode.  **partitionBy**    str or list, optional     names of partitioning columns    **queryName**    str, optional     unique name for the query    **\*\*options**    dict     All other string options. You may want to provide a    checkpointLocation    .     Notes     This API is evolving.     For v1 table, partitioning columns provided by    partitionBy    will be respected no matter the table exists or not. A new table will be created if the table not exists.     For v2 table,    partitionBy    will be ignored if the table already exists.    partitionBy    will be respected only if the v2 table does not exist. Besides, the v2 table created by this API lacks some functionalities (e.g., customized properties, options, and serde info). If you need them, please create the v2 table manually before the execution to avoid creating a table with incomplete information.     Examples     Save a data stream to a table.    ``` >>> import tempfile >>> import time >>> \_ = spark.sql(""DROP TABLE IF EXISTS my\_table2"") >>> with tempfile.TemporaryDirectory() as d: ...     # Create a table with Rate source. ...     q = spark.readStream.format(""rate"").option( ...         ""rowsPerSecond"", 10).load().writeStream.toTable( ...             ""my\_table2"", ...             queryName='that\_query', ...             outputMode=""append"", ...             format='parquet', ...             checkpointLocation=d) ...     time.sleep(3) ...     q.stop() ...     spark.read.table(""my\_table2"").show() ...     \_ = spark.sql(""DROP TABLE my\_table2"") +...---------+-----+ |...timestamp|value| +...---------+-----+ ...  ```  [pyspark.sql.streaming.DataStreamWriter.start](pyspark.sql.streaming.DataStreamWriter.start.html ""previous page"")  [pyspark.sql.streaming.DataStreamWriter.trigger](pyspark.sql.streaming.DataStreamWriter.trigger.html ""next page"")   "	"To create or refresh a streaming live table in Python, you can use the `toTable` method of the `DataStreamWriter` class in PySpark. This method starts the execution of the streaming query, which will continually output results to the given table as new data arrives. The returned `StreamingQuery` object can be used to interact with the stream. You can specify the table name, format, output mode, partitioning columns, query name, and other options using the method parameters. Here is an example code snippet:  ``` from pyspark.sql.streaming import DataStreamWriter  streamingDF.writeStream \     .format(""memory"") \     .queryName(""my_table"") \     .outputMode(""append"") \     .start() \     .awaitTermination() ```  Note that this API is evolving and the behavior may differ for v1 and v2 tables."	1	1	1	True
129	no_reaction	https://docs.databricks.com/migration/index.html	What's the best way to move data from an on-premise database to Databricks?	Migrate data applications to Databricks =========================================     This article provides an introduction to migrating existing data applications to Databricks. Databricks provides a unified approach that lets you work with data from many source systems on a single platform.       For an overview of platform capabilities, see  [What is Databricks?](../introduction/index.html)  .          Migrate ETL jobs to Databricks --------------------------------     You can migrate Apache Spark jobs used to extract, transform, and load data from on-premises or cloud-native implementations to Databricks with just a few steps. See  [Adapt your exisiting Apache Spark code for Databricks](spark.html)  .       Databricks extends the functionality of Spark SQL with pre-configured open source integrations, partner integrations, and enterprise product offerings. If your ETL workloads are written in SQL or Hive, you can migrate to Databricks with minimal refactoring. Learn more about Databricks SQL offerings:     * [What is data warehousing on Databricks?](../sql/index.html) * [What is Delta Live Tables?](../delta-live-tables/index.html) * [What is Databricks Partner Connect?](../partner-connect/index.html)     For specific instructions on migrating from various source systems to Databricks, see  [Migrate ETL pipelines to Databricks](etl.html)  .         Replace your enterprise data warehouse with a lakehouse ---------------------------------------------------------     Databricks provides optimal value and performance when workloads align around data stored in the lakehouse. Many enterprise data stacks include both a data lake and an enterprise data warehouse, and organizations create complex ETL workflows to try to keep these systems and data in sync. The lakehouse allows you to use the same data, stored in the data lake, across queries and systems that usually rely on a separate data warehouse. For more on the lakehouse, see  [What is the Databricks Lakehouse?](../lakehouse/index.html)  . For more on data warehousing on Databricks, see  [What is data warehousing on Databricks?](../sql/index.html)  .       Migrating from an enterprise data warehouse to the lakehouse generally involves reducing the complexity of your data architecture and workflows, but there are some caveats and best practices to keep in mind while completing this work. See  [Migrate your data warehouse to the Databricks Lakehouse](warehouse-to-lakehouse.html)  .         Unify your ML, data science, and analytics workloads ------------------------------------------------------     Because the lakehouse provides optimized access to cloud-based data files through table queries or file paths, you can do ML, data science, and analytics on a single copy of your data. Databricks makes it easy to move workloads from both open source and proprietary tools, and maintains updated versions of many of open source libraries used by analysts and data scientists.       Pandas workloads in Jupyter notebooks can be synced and run using  [Databricks Repos](../repos/index.html)  . Databricks provides  [native support for pandas](../pandas/index.html)  in all Databricks Runtime versions, and configures many popular ML and deep learning libraries in the  [Databricks ML Runtime](../runtime/mlruntime.html)  . If you sync your local workloads using Git and  [Files in Repos](../files/workspace.html)  , you can use the same relative paths for data and custom libaries present in your local environment.        Note       By default, Databricks maintains  `.ipynb`  extensions for Jupyter notebooks synced with Databricks Repos, but automatically converts Jupyter notebooks to Databricks notebooks when  [imported with the UI](../notebooks/notebook-export-import.html#import-notebook)  . Databricks notebooks save with a  `.py`  extension, and so can live side-by-side with Jupyter notebooks in a Git repository.	To move data from an on-premise database to Databricks, you can migrate Apache Spark jobs used to extract, transform, and load data from on-premises or cloud-native implementations to Databricks with just a few steps. For specific instructions on migrating from various source systems to Databricks, see [Migrate ETL pipelines to Databricks](etl.html).	1	1	1	True
130	no_reaction	https://docs.databricks.com/query-federation/snowflake.html	Is Databricks or Snowflake better	"    Read and write data from Snowflake ====================================     Databricks provides a Snowflake connector in the Databricks Runtime to support reading and writing data from Snowflake.        Query a Snowflake table in Databricks ---------------------------------------     You can configure a connection to Snowflake and then query data. The following code provides example syntax in Python, SQL, and Scala:         ``` snowflake\_table = (spark.read   .format(""snowflake"")   .option(""dbtable"", table\_name)   .option(""sfUrl"", database\_host\_url)   .option(""sfUser"", username)   .option(""sfPassword"", password)   .option(""sfDatabase"", database\_name)   .option(""sfSchema"", schema\_name)   .option(""sfWarehouse"", warehouse\_name)   .load() )  ```       ``` DROP TABLE IF EXISTS snowflake\_table; CREATE TABLE snowflake\_table USING snowflake OPTIONS (  dbtable '<table-name>',  sfUrl '<database-host-url>',  sfUser '<username>',  sfPassword '<password>',  sfDatabase '<database-name>',  sfSchema '<schema-name>',  sfWarehouse '<warehouse-name>' ); SELECT \* FROM snowflake\_table;  ```       ``` val snowflake\_table = spark.read  .format(""snowflake"")  .option(""dbtable"", table\_name)  .option(""sfUrl"", database\_host\_url)  .option(""sfUser"", username)  .option(""sfPassword"", password)  .option(""sfDatabase"", database\_name)  .option(""sfSchema"", schema\_name)  .option(""sfWarehouse"", warehouse\_name)  .load()  ```         Notebook example: Snowflake Connector for Spark -------------------------------------------------     The following notebooks provide simple examples of how to write data to and read data from Snowflake. See  [Using the Spark Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html)   for more details. In particular, see  [Setting Configuration Options for the Connector](https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   for all configuration options.        Tip       Avoid exposing your Snowflake username and password in notebooks by using  [Secrets](../security/secrets/index.html)  , which are demonstrated in the notebooks.         ###   Snowflake Python notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Notebook example: Save model training results to Snowflake ------------------------------------------------------------     The following notebook walks through best practices for using the Snowflake Connector for Spark. It writes data to Snowflake, uses Snowflake for some basic data manipulation, trains a machine learning model in Databricks, and writes the results back to Snowflake.        ###   Store ML training results in Snowflake notebook     [Open notebook in new tab](/_extras/notebooks/source/snowflake-ml.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                  Frequently asked questions (FAQ) ----------------------------------    ###   Why don’t my Spark DataFrame columns appear in the same order in Snowflake?     The Snowflake Connector for Spark doesn’t respect the order of the columns in the table being written to; you must explicitly specify the mapping between DataFrame and Snowflake columns. To specify this mapping, use the  [columnmap parameter](https://docs.snowflake.net/manuals/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)   .       ###   Why is  `INTEGER`  data written to Snowflake read back as  `DECIMAL`  ?     Snowflake represents all  `INTEGER`  types as  `NUMBER`  , which can cause a change in data type when you write data to and read data from Snowflake. For example,  `INTEGER`  data can be converted to  `DECIMAL`  when writing to Snowflake, because  `INTEGER`  and  `DECIMAL`  are semantically equivalent in Snowflake (see  [Snowflake Numeric Data Types](https://docs.snowflake.net/manuals/sql-reference/data-types-numeric.html#int-integer-bigint-smallint-tinyint-byteint)   ).       ###   Why are the fields in my Snowflake table schema always uppercase?     Snowflake uses uppercase fields by default, which means that the table schema is converted to uppercase.                                                         "	Based on the context provided, the document explains how to read and write data from Snowflake using Databricks. It provides examples of syntax in Python, SQL, and Scala. It also includes notebooks that demonstrate how to use the Snowflake Connector for Spark to write data to Snowflake, use Snowflake for basic data manipulation, train a machine learning model in Databricks, and write the results back to Snowflake. The document also includes a FAQ section that answers common questions about using the Snowflake Connector for Spark. However, it does not provide a direct comparison between Databricks and Snowflake to determine which one is better.	1	0	0	True
133	no_reaction	https://docs.databricks.com/ingestion/file-metadata-column.html	How do I get file metadata in databricks	"    File metadata column ======================     You can get metadata information for input files with the  `\_metadata`  column. The  `\_metadata`  column is a  *hidden*   column, and is available for all input file formats. To include the  `\_metadata`  column in the returned DataFrame, you must explicitly reference it in your query.       If the data source contains a column named  `\_metadata`  , queries return the column from the data source, and not the file metadata.        Warning       New fields might be added to the  `\_metadata`  column in future releases. To prevent schema evolution errors if the  `\_metadata`  column is updated, Databricks recommends selecting specific fields from the column in your queries. See  [examples](#metadata-examples)  .         Supported metadata --------------------     The  `\_metadata`  column is a  `STRUCT`  containing the following fields:             |   Name   |   Type   |   Description   |   Example   |   Minimum Databricks Runtime release   | | --- | --- | --- | --- | --- | |  **file\_path**   |  `STRING`  |   File path of the input file.   |  `file:/tmp/f0.csv`  |   10.5   | |  **file\_name**   |  `STRING`  |   Name of the input file along with its extension.   |  `f0.csv`  |   10.5   | |  **file\_size**   |  `LONG`  |   Length of the input file, in bytes.   |   628   |   10.5   | |  **file\_modification\_time**   |  `TIMESTAMP`  |   Last modification timestamp of the input file.   |  `2021-12-20     20:05:21`  |   10.5   | |  **file\_block\_start**   |  `LONG`  |   Start offset of the block being read, in bytes.   |   0   |   13.0   | |  **file\_block\_length**   |  `LONG`  |   Length of the block being read, in bytes.   |   628   |   13.0   |         Examples ----------    ###   Use in a basic file-based data source reader       ``` df = spark.read \   .format(""csv"") \   .schema(schema) \   .load(""dbfs:/tmp/\*"") \   .select(""\*"", ""\_metadata"")  display(df)  ''' Result: +---------+-----+----------------------------------------------------+ | name | age | \_metadata | +=========+=====+====================================================+ | | | { | | | | ""file\_path"": ""dbfs:/tmp/f0.csv"", | | Debbie | 18 | ""file\_name"": ""f0.csv"", | | | | ""file\_size"": 12, | | | | ""file\_block\_start"": 0, | | | | ""file\_block\_length"": 12, | | | | ""file\_modification\_time"": ""2021-07-02 01:05:21"" | | | | } | +---------+-----+----------------------------------------------------+ | | | { | | | | ""file\_path"": ""dbfs:/tmp/f1.csv"", | | Frank | 24 | ""file\_name"": ""f1.csv"", | | | | ""file\_size"": 12, | | | | ""file\_block\_start"": 0, | | | | ""file\_block\_length"": 12, | | | | ""file\_modification\_time"": ""2021-12-20 02:06:21"" | | | | } | +---------+-----+----------------------------------------------------+ '''  ```       ``` val df = spark.read  .format(""csv"")  .schema(schema)  .load(""dbfs:/tmp/\*"")  .select(""\*"", ""\_metadata"")  display(df\_population)  /\* Result: +---------+-----+----------------------------------------------------+ | name | age | \_metadata | +=========+=====+====================================================+ | | | { | | | | ""file\_path"": ""dbfs:/tmp/f0.csv"", | | Debbie | 18 | ""file\_name"": ""f0.csv"", | | | | ""file\_size"": 12, | | | | ""file\_block\_start"": 0, | | | | ""file\_block\_length"": 12, | | | | ""file\_modification\_time"": ""2021-07-02 01:05:21"" | | | | } | +---------+-----+----------------------------------------------------+ | | | { | | | | ""file\_path"": ""dbfs:/tmp/f1.csv"", | | Frank | 24 | ""file\_name"": ""f1.csv"", | | | | ""file\_size"": 10, | | | | ""file\_block\_start"": 0, | | | | ""file\_block\_length"": 12, | | | | ""file\_modification\_time"": ""2021-12-20 02:06:21"" | | | | } | +---------+-----+----------------------------------------------------+ \*/  ```       ###   Select specific fields       ``` spark.read \   .format(""csv"") \   .schema(schema) \   .load(""dbfs:/tmp/\*"") \   .select(""\_metadata.file\_name"", ""\_metadata.file\_size"")  ```       ``` spark.read  .format(""csv"")  .schema(schema)  .load(""dbfs:/tmp/\*"")  .select(""\_metadata.file\_name"", ""\_metadata.file\_size"")  ```       ###   Use in filters       ``` spark.read \   .format(""csv"") \   .schema(schema) \   .load(""dbfs:/tmp/\*"") \   .select(""\*"") \   .filter(col(""\_metadata.file\_name"") == lit(""test.csv""))  ```       ``` spark.read  .format(""csv"")  .schema(schema)  .load(""dbfs:/tmp/\*"")  .select(""\*"")  .filter(col(""\_metadata.file\_name"") === lit(""test.csv""))  ```       ###   Use in COPY INTO      ``` COPY INTO my\_delta\_table FROM (  SELECT \*, \_metadata FROM 's3://my-bucket/csvData' ) FILEFORMAT = CSV  ```      ###   Use in Auto Loader      Note       When writing the  `\_metadata`  column, we rename it to  `source\_metadata`  . Writing it as  `\_metadata`  would make it impossible to access the metadata column in the target table, because if the data source contains a column named  `\_metadata`  , queries will return the column from the data source, and not the file metadata.          ``` spark.readStream \   .format(""cloudFiles"") \   .option(""cloudFiles.format"", ""csv"") \   .schema(schema) \   .load(""s3://my-bucket/csvData"") \   .selectExpr(""\*"", ""\_metadata as source\_metadata"") \   .writeStream \   .format(""delta"") \   .option(""checkpointLocation"", checkpointLocation) \   .start(targetTable)  ```       ``` spark.readStream  .format(""cloudFiles"")  .option(""cloudFiles.format"", ""csv"")  .schema(schema)  .load(""s3://my-bucket/csvData"")  .selectExpr(""\*"", ""\_metadata as source\_metadata"")  .writeStream  .format(""delta"")  .option(""checkpointLocation"", checkpointLocation)  .start(targetTable)  ```          Related articles ------------------   * [COPY INTO](../sql/language-manual/delta-copy-into.html) * [Auto Loader](auto-loader/index.html) * [Structured Streaming](../structured-streaming/index.html)                                                      "	To get metadata information for input files in Databricks, you can use the `_metadata` column. The `_metadata` column is a hidden column that is available for all input file formats. To include the `_metadata` column in the returned DataFrame, you must explicitly reference it in your query. The `_metadata` column is a `STRUCT` containing fields such as `file_path`, `file_name`, `file_size`, `file_modification_time`, `file_block_start`, and `file_block_length`. To prevent schema evolution errors if the `_metadata` column is updated, Databricks recommends selecting specific fields from the column in your queries.	1	1	1	True
136	no_reaction	https://docs.databricks.com/dev-tools/python-sql-connector.html	How do i get Row samples with Databricks SQL Connector for Python	###  [Spark SQL Guide](sql-programming-guide.html)  * [Getting Started](sql-getting-started.html) * [Data Sources](sql-data-sources.html) * [Performance Tuning](sql-performance-tuning.html) * [Distributed SQL Engine](sql-distributed-sql-engine.html) * [PySpark Usage Guide for Pandas with Apache Arrow](sql-pyspark-pandas-with-arrow.html) * [Migration Guide](sql-migration-old.html) * [SQL Reference](sql-ref.html) + [ANSI Compliance](sql-ref-ansi-compliance.html) + [Data Types](sql-ref-datatypes.html) + [Datetime Pattern](sql-ref-datetime-pattern.html) + [Number Pattern](sql-ref-number-pattern.html) + [Functions](sql-ref-functions.html) + [Identifiers](sql-ref-identifier.html) + [Literals](sql-ref-literals.html) + [Null Semantics](sql-ref-null-semantics.html) + [SQL Syntax](sql-ref-syntax.html) - [Data Definition Statements](sql-ref-syntax.html#ddl-statements) - [Data Manipulation Statements](sql-ref-syntax.html#dml-statements) - [Data Retrieval(Queries)](sql-ref-syntax.html#data-retrieval-statements) - [Auxiliary Statements](sql-ref-syntax.html#auxiliary-statements)  * [Error Conditions](sql-error-conditions.html)   Sampling Queries ==================  ###   Description   The  `TABLESAMPLE`   statement is used to sample the table. It supports the following sampling methods:    * `TABLESAMPLE`   (x  `ROWS`   ): Sample the table down to the given number of rows. * `TABLESAMPLE`   (x  `PERCENT`   ): Sample the table down to the given percentage. Note that percentages are defined as a number between 0 and 100. * `TABLESAMPLE`   (  `BUCKET`   x  `OUT OF`   y): Sample the table down to a  `x`   out of  `y`   fraction.  **Note:**  `TABLESAMPLE`   returns the approximate number of rows or fraction requested.    ###   Syntax  ``` TABLESAMPLE ({ integer\_expression | decimal\_expression } PERCENT)     | TABLESAMPLE ( integer\_expression ROWS )     | TABLESAMPLE ( BUCKET integer\_expression OUT OF integer\_expression )  ```  ###   Examples  ``` SELECT \* FROM test; +--+----+ |id|name| +--+----+ | 5|Alex| | 8|Lucy| | 2|Mary| | 4|Fred| | 1|Lisa| | 9|Eric| |10|Adam| | 6|Mark| | 7|Lily| | 3|Evan| +--+----+  SELECT \* FROM test TABLESAMPLE (50 PERCENT); +--+----+ |id|name| +--+----+ | 5|Alex| | 2|Mary| | 4|Fred| | 9|Eric| |10|Adam| | 3|Evan| +--+----+  SELECT \* FROM test TABLESAMPLE (5 ROWS); +--+----+ |id|name| +--+----+ | 5|Alex| | 8|Lucy| | 2|Mary| | 4|Fred| | 1|Lisa| +--+----+  SELECT \* FROM test TABLESAMPLE (BUCKET 4 OUT OF 10); +--+----+ |id|name| +--+----+ | 8|Lucy| | 2|Mary| | 9|Eric| | 6|Mark| +--+----+  ```  ###   Related Statements  * [SELECT](sql-ref-syntax-qry-select.html)	To get row samples with Databricks SQL Connector for Python, you can use the `TABLESAMPLE` statement. It supports three sampling methods: `TABLESAMPLE (x ROWS)`, `TABLESAMPLE (x PERCENT)`, and `TABLESAMPLE (BUCKET x OUT OF y)`. You can use these methods to sample the table down to the given number of rows, percentage, or fraction. Note that `TABLESAMPLE` returns the approximate number of rows or fraction requested.	1	1	1	True
137	no_reaction	https://docs.databricks.com/ingestion/auto-loader/schema.html	databricks autoloader schema inference	"      Configure schema inference and evolution in Auto Loader =========================================================     You can configure Auto Loader to automatically detect the schema of loaded data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time.       Auto Loader can also “rescue” data that was unexpected (for example, of differing data types) in a JSON blob column, that you can choose to access later using the  [semi-structured data access APIs](../../optimizations/semi-structured.html)  .       The following formats are supported for schema inference and evolution:          |   File format   |   Supported versions   | | --- | --- | |  `JSON`  |   Databricks Runtime 8.2 and above   | |  `CSV`  |   Databricks Runtime 8.3 and above   | |  `Avro`  |   Databricks Runtime 10.2 and above   | |  `Parquet`  |   Databricks Runtime 11.1 and above   | |  `ORC`  |   Unsupported   | |  `Text`  |   Not applicable (fixed-schema)   | |  `Binaryfile`  |   Not applicable (fixed-schema)   |      Syntax for schema inference and evolution -------------------------------------------     Specifying a target directory for the option  `cloudFiles.schemaLocation`  enables schema inference and evolution. You can choose to use the same directory you specify for the  `checkpointLocation`  . If you use  [Delta Live Tables](../../delta-live-tables/index.html)  , Databricks manages schema location and other checkpoint information automatically.        Note       If you have more than one source data location being loaded into the target table, each Auto Loader ingestion workload requires a separate streaming checkpoint.        The following example uses  `parquet`  for the  `cloudFiles.format`  . Use  `csv`  ,  `avro`  , or  `json`  for other file sources. All other settings for read and write stay the same for the default behaviors for each format.         ``` (spark.readStream.format(""cloudFiles"")   .option(""cloudFiles.format"", ""parquet"")   # The schema location directory keeps track of your data schema over time   .option(""cloudFiles.schemaLocation"", ""<path\_to\_checkpoint>"")   .load(""<path\_to\_source\_data>"")   .writeStream   .option(""checkpointLocation"", ""<path\_to\_checkpoint>"")   .start(""<path\_to\_target"") )  ```       ``` spark.readStream.format(""cloudFiles"")  .option(""cloudFiles.format"", ""parquet"")  // The schema location directory keeps track of your data schema over time  .option(""cloudFiles.schemaLocation"", ""<path\_to\_checkpoint>"")  .load(""<path\_to\_source\_data>"")  .writeStream  .option(""checkpointLocation"", ""<path\_to\_checkpoint>"")  .start(""<path\_to\_target"")  ```             How does Auto Loader schema inference work? ---------------------------------------------     To infer the schema when first reading data, Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first. Auto Loader stores the schema information in a directory  `\_schemas`  at the configured  `cloudfFiles.schemaLocation`  to track schema changes to the input data over time.        Note       To change the size of the sample that’s used you can set the SQL configurations:        ``` spark.databricks.cloudFiles.schemaInference.sampleSize.numBytes  ```      (byte string, for example  `10gb`  )       and        ``` spark.databricks.cloudFiles.schemaInference.sampleSize.numFiles  ```      (integer)        By default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don’t encode data types (JSON and CSV), Auto Loader infers all columns as strings (including nested fields in JSON files). For formats with typed schema (Parquet and Avro), Auto Loader samples a subset of files and merges the schemas of individual files. This behavior is summarized in the following table:          |   File format   |   Default inferred data type   | | --- | --- | |  `JSON`  |   String   | |  `CSV`  |   String   | |  `Avro`  |   Types encoded in Avro schema   | |  `Parquet`  |   Types encoded in Parquet schema   |     The Apache Spark DataFrameReader uses different behavior for schema inference, selecting data types for columns in JSON and CSV sources based on sample data. To enable this behavior with Auto Loader, set the option  `cloudFiles.inferColumnTypes`  to  `true`  .        Note       When inferring schema for CSV data, Auto Loader assumes that the files contain headers. If your CSV files do not contain headers, provide the option  `.option(""header"",     ""false"")`  . In addition, Auto Loader merges the schemas of all the files in the sample to come up with a global schema. Auto Loader can then read each file according to its header and parse the CSV correctly.         Note       When a column has different data types in two Parquet files, Auto Loader attempts to  `upcast`  one type to the other. If upcasting is not possible, data inference fails. See the following table for examples:           |   Type 1   |   Type 2   |   Upcast type   | | --- | --- | --- | |  `int`  |  `double`  |  `double`  | |  `double`  |  `string`  |  `string`  | |  `int`  |  `struct`  |   Inference fails   |     After merging data types on inference, files containing records of the unselected type are loaded to the  [rescued data column](#rescue)  , because the data type is different from the inferred schema.              How does Auto Loader schema evolution work? ---------------------------------------------     Auto Loader detects the addition of new columns as it processes your data. When Auto Loader detects a new column, the stream stops with an  `UnknownFieldException`  . Before your stream throws this error, Auto Loader performs schema inference on the latest micro-batch of data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged.       Databricks recommends configuring Auto Loader streams with  [workflows](../../workflows/index.html)  to restart automatically after such schema changes.       Auto Loader supports the following modes for schema evolution, which you set in the option  `cloudFiles.schemaEvolutionMode`  :          |   Mode   |   Behavior on reading new column   | | --- | --- | |  `addNewColumns`  (default)   |   Stream fails. New columns are added to the schema. Existing columns do not evolve data types.   | |  `rescue`  |   Schema is never evolved and stream does not fail due to schema changes. All new columns are recorded in the  [rescued data column](#rescue)  .   | |  `failOnNewColumns`  |   Stream fails. Stream does not restart unless the provided schema is updated, or the offending data file is removed.   | |  `none`  |   Does not evolve the schema, new columns are ignored, and data is not rescued unless the  `rescuedDataColumn`  option is set. Stream does not fail due to schema changes.   |         How do partitions work with Auto Loader? ------------------------------------------     Auto Loader attempts to infer partition columns from the underlying directory structure of the data if the data is laid out in Hive style partitioning. For example, the file path  `base\_path/event=click/date=2021-04-01/f0.json`  results in the inference of  `date`  and  `event`  as partition columns. If the underlying directory structure contains conflicting Hive partitions or doesn’t contain Hive style partitioning, partition columns are ignored.       Binary file (  `binaryFile`  ) and  `text`  file formats have fixed data schemas, but support partition column inference. Databricks recommends setting  `cloudFiles.schemaLocation`  for these file formats. This avoids any potential errors or information loss and prevents inference of partitions columns each time an Auto Loader begins.       Partition columns are not considered for schema evolution. If you had an initial directory structure like  `base\_path/event=click/date=2021-04-01/f0.json`  , and then start receiving new files as  `base\_path/event=click/date=2021-04-01/hour=01/f1.json`  , Auto Loader ignores the hour column. To capture information for new partition columns, set  `cloudFiles.partitionColumns`  to  `event,date,hour`  .        Note       The option  `cloudFiles.partitionColumns`  takes a comma-separated list of column names. Only columns that exist as  `key=value`  pairs in your directory structure are parsed.            What is the rescued data column? ----------------------------------     When Auto Loader infers the schema, a rescued data column is automatically added to your schema as  `\_rescued\_data`  . You can rename the column or include it in cases where you provide a schema by setting the option  `rescuedDataColumn`  .       The rescued data column ensures that columns that don’t match with the schema are rescued instead of being dropped. The rescued data column contains any data that isn’t parsed for the following reasons:     * The column is missing from the schema. * Type mismatches. * Case mismatches.     The rescued data column contains a JSON containing the rescued columns and the source file path of the record.        Note       The JSON and CSV parsers support three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records are dropped or throw errors, such as incomplete or malformed JSON or CSV. If you use  `badRecordsPath`  when parsing JSON or CSV, data type mismatches are not considered as bad records when using the  `rescuedDataColumn`  . Only incomplete and malformed JSON or CSV records are stored in  `badRecordsPath`  .            Change case-sensitive behavior --------------------------------     Unless case sensitivity is enabled, the columns  `abc`  ,  `Abc`  , and  `ABC`  are considered the same column for the purposes of schema inference. The case that is chosen is arbitrary and depends on the sampled data. You can use  [schema hints](#schema-hints)  to enforce which case should be used. Once a selection has been made and the schema is inferred, Auto Loader does not consider the casing variants that were not selected consistent with the schema.       When  [rescued data column](#rescue)  is enabled, fields named in a case other than that of the schema are loaded to the  `\_rescued\_data`  column. Change this behavior by setting the option  `readerCaseSensitive`  to false, in which case Auto Loader reads data in a case-insensitive way.           Override schema inference with schema hints ---------------------------------------------     You can use schema hints to enforce the schema information that you know and expect on an inferred schema. When you know that a column is of a specific data type, or if you want to choose a more general data type (for example,"	Auto Loader can be configured to automatically detect the schema of loaded data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time. Auto Loader supports schema inference and evolution for JSON, CSV, Avro, Parquet, Binaryfile, and Text file formats. To enable schema inference and evolution, you need to specify a target directory for the option `cloudFiles.schemaLocation`. When first reading data, Auto Loader samples the first 50 GB or 1000 files that it discovers, whichever limit is crossed first, to infer the schema. Auto Loader detects the addition of new columns as it processes your data and updates the schema location with the latest schema by merging new columns to the end of the schema. The data types of existing columns remain unchanged. Auto Loader supports different modes for schema evolution, which you set in the option `cloudFiles.schemaEvolutionMode`. You can also use schema hints to enforce the schema information that you know and expect on an inferred schema.	1	1	1	True
138	no_reaction	https://docs.databricks.com/data-sharing/index.html	what's delta sharing?	Delta Sharing ===============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data.       There are three components to Delta Sharing:     * **[Providers](#providers)**      A provider is an entity which has made data available for sharing. * **[Shares](#shares)**      A share defines a logical grouping for the tables you intend to share. * **[Recipients](#recipients)**      A recipient identifies an organization with which you want to share any number of shares.     For a detailed guide on how to use Delta Sharing see  [Share data securely using Delta Sharing](../../data-sharing/index.html)  .        Providers -----------    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 11.1 and above       A data provider is an object representing the organization in the real world who shares the data. A provider contains shares which further contain the shared data. Once a provider has added you as a  **[Recipients](#recipients)**  you can     * Customize the local name of the provider using  [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html)  . * List the sets of data shared with you with  [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html)  . * Create a catalog using shares of interest to you with  [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html)  .    ###   Examples      ``` -- Change the data provider name locally. > ALTER PROVIDER `Center for Disease Control` RENAME TO cdc;  -- List the shares the provider has granted you access too. > SHOW SHARES IN PROVIDER cdc;  vaccinedata  -- Make the share accessible locally as a catalog. > CREATE CATALOG cdcdata USING cdc.vaccinedata;  -- Use the data. > USE CATALOG cdcdata; > SELECT COUNT(\*) FROM information\_schema.tables;  10  ```      ###   Related articles   * [DESCRIBE PROVIDER](sql-ref-syntax-aux-describe-provider.html) * [ALTER PROVIDER](sql-ref-syntax-ddl-alter-provider.html) * [DROP PROVIDER](sql-ref-syntax-ddl-drop-provider.html) * [SHOW PROVIDERS](sql-ref-syntax-aux-show-providers.html) * [COMMENT ON PROVIDER](sql-ref-syntax-ddl-comment.html) * [SHOW SHARES IN PROVIDER](sql-ref-syntax-aux-show-shares-in-provider.html) * [CREATE CATALOG](sql-ref-syntax-ddl-create-catalog.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Shares --------     A share is a container instantiated with the  [CREATE SHARE](sql-ref-syntax-ddl-create-share.html)  command. Once created you can iteratively register a collection of existing tables defined within the metastore using the  [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html)  command. You can register tables under their original name, qualified by their original schema, or provide alternate exposed names.       You must be a metastore admin or account admin to create, alter, and drop shares.        ###   Examples      ``` -- Create share `customer\_share` only if share with same name doesn't exist, with a comment. > CREATE SHARE IF NOT EXISTS customer\_share COMMENT 'This is customer share';  -- Add 2 tables to the share. -- Expose my\_schema.tab1 a different name. -- Expose only two partitions of other\_schema.tab2 > ALTER SHARE customer\_share ADD TABLE my\_schema.tab1 AS their\_schema.tab1; > ALTER SHARE customer\_share ADD TABLE other\_schema.tab2 PARTITION (c1 = 5), (c1 = 7);  -- List the content of the share > SHOW ALL IN SHARE customer\_share;  name type shared\_object added\_at added\_by comment partitions  ----------------- ---- ---------------------- ---------------------------- -------------------------- ------- -----------------  other\_schema.tab2 TABLE main.other\_schema.tab2 2022-01-01T00:00:01.000+0000 alwaysworks@databricks.com NULL  their\_schema.tab1 TABLE main.myschema.tab2 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com NULL (c1 = 5), (c1 = 7)  ```        ###   Related articles   * [Change data feed](../../delta/delta-change-data-feed.html) * [CREATE SHARE](sql-ref-syntax-ddl-create-share.html) * [DESCRIBE SHARE](sql-ref-syntax-aux-describe-share.html) * [ALTER SHARE](sql-ref-syntax-ddl-alter-share.html) * [DROP SHARE](sql-ref-syntax-ddl-drop-share.html) * [SHOW SHARES](sql-ref-syntax-aux-show-shares.html) * [SHOW ALL IN SHARE](sql-ref-syntax-aux-show-all-in-share.html) * [COMMENT ON SHARE](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)        Recipients ------------     A recipient is an object you create using  [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html)  to represent an organization which you want to allow access shares. When you create a recipient Databricks SQL generates an activation link you can send to the organization. To retrieve the activation link after creation you use  [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html)  .       Once a recipient has been created you can give it  `SELECT`  privileges on shares of your choice using  [GRANT ON SHARE](security-grant-share.html)  .       You must be a metastore administrator to create recipients, drop recipients, and grant access to shares.        ###   Examples      ``` -- Create a recipient. > CREATE RECIPIENT IF NOT EXISTS other\_org COMMENT 'other.org';  -- Retrieve the activation link to send to other.org > DESCRIBE RECIPIENT other\_org;  name created\_at created\_by comment activation\_link active\_token\_id active\_token\_expiration\_time rotated\_token\_id rotated\_token\_expiration\_time  --------- ---------------------------- -------------------------- --------- --------------- ------------------------------------ ---------------------------- ---------------- -----------------------------  other\_org 2022-01-01T00:00:00.000+0000 alwaysworks@databricks.com other.org https://.... 0160c81f-5262-40bb-9b03-3ee12e6d98d7 9999-12-31T23:59:59.999+0000 NULL NULL  -- Choose shares that other.org has access to > GRANT SELECT ON SHARE customer\_share TO RECIPIENT other\_org;  ```        ###   Related articles   * [CREATE RECIPIENT](sql-ref-syntax-ddl-create-recipient.html) * [DESCRIBE RECIPIENT](sql-ref-syntax-aux-describe-recipient.html) * [ALTER RECIPIENT](sql-ref-syntax-ddl-alter-recipient.html) * [DROP RECIPIENT](sql-ref-syntax-ddl-drop-recipient.html) * [SHOW RECIPIENTS](sql-ref-syntax-aux-show-recipients.html) * [COMMENT ON RECIPIENT](sql-ref-syntax-ddl-comment.html) * [Share data securely using Delta Sharing](../../data-sharing/index.html)	Delta Sharing is an open protocol for secure data sharing with other organizations regardless of which computing platforms they use. It can share collections of tables in a Unity Catalog metastore in real time without copying them, so that data recipients can immediately begin working with the latest version of the shared data.	1	1	1	True
141	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-tables.html	can i convert managed tables to external tables	"    Create tables ===============     This article introduces the concept of  *managed*   and  *external*   tables in Unity Catalog and describes how to create tables in Unity Catalog.        Note       When you create a table, be sure to reference a catalog that is governed by Unity Catalog. The catalog  `hive\_metastore`  appears in Data Explorer but is not considered governed by Unity Catalog. It is managed by your Databricks workspace’s Hive metastore. All other catalogs listed are governed by Unity Catalog.       You can use the Unity Catalog table upgrade interface to upgrade existing tables registered in the Hive metastore to Unity Catalog. See  [Upgrade tables and views to Unity Catalog](migrate.html)  .           Managed tables ----------------     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.           External tables -----------------     External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.       When you run  `DROP     TABLE`  on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a  `LOCATION`  path in your  `CREATE     TABLE`  statement. External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     To manage access to the underlying cloud storage for an external table, you must set up  [storage credentials and external locations](manage-external-locations-and-credentials.html)  .       To learn more, see  [Create an external table](#create-an-external-table)  .         Requirements --------------     You must have the  `CREATE     TABLE`  privilege on the schema in which you want to create the table, as well as the  `USE     SCHEMA`  privilege on the schema and the  `USE     CATALOG`  privilege on the parent catalog.       If you are creating an external table, see  [Create an external table](#create-an-external-table)  for additional requirements.         Create a managed table ------------------------     To create a managed table, run the following SQL command. You can also use the  [example notebook](#example-notebook-managed-table)  to create a table. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. This cannot be the  `hive\_metastore`  catalog that is created automatically for the Hive metastore associated with your Databricks workspace. * `<schema\_name>`  : The name of the schema. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column.       ``` CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> (  <column\_specification> );  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> ""   ""(""   "" <column\_specification>""   "")"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "",  ""("",  "" <column\_specification>"",  "")"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "" +  ""("" +  "" <column\_specification>"" +  "")"")  ```       You can also create a managed table by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of table full names by using  [databricks\_tables](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/tables)   .       For example, to create the table  `main.default.department`  and insert five rows into it:         ``` CREATE TABLE main.default.department (  deptcode INT,  deptname STRING,  location STRING );  INSERT INTO main.default.department VALUES  (10, 'FINANCE', 'EDINBURGH'),  (20, 'SOFTWARE', 'PADDINGTON'),  (30, 'SALES', 'MAIDSTONE'),  (40, 'MARKETING', 'DARLINGTON'),  (50, 'ADMIN', 'BIRMINGHAM');  ```       ``` spark.sql(""CREATE TABLE main.default.department ""   ""(""   "" deptcode INT,""   "" deptname STRING,""   "" location STRING""   "")""   ""INSERT INTO main.default.department VALUES ""   "" (10, 'FINANCE', 'EDINBURGH'),""   "" (20, 'SOFTWARE', 'PADDINGTON'),""   "" (30, 'SALES', 'MAIDSTONE'),""   "" (40, 'MARKETING', 'DARLINGTON'),""   "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE main.default.department "",  ""("",  "" deptcode INT,"",  "" deptname STRING,"",  "" location STRING"",  "")"",  ""INSERT INTO main.default.department VALUES "",  "" (10, 'FINANCE', 'EDINBURGH'),"",  "" (20, 'SOFTWARE', 'PADDINGTON'),"",  "" (30, 'SALES', 'MAIDSTONE'),"",  "" (40, 'MARKETING', 'DARLINGTON'),"",  "" (50, 'ADMIN', 'BIRMINGHAM')"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE main.default.department "" +  ""("" +  "" deptcode INT,"" +  "" deptname STRING,"" +  "" location STRING"" +  "")"" +  ""INSERT INTO main.default.department VALUES "" +  "" (10, 'FINANCE', 'EDINBURGH'),"" +  "" (20, 'SOFTWARE', 'PADDINGTON'),"" +  "" (30, 'SALES', 'MAIDSTONE'),"" +  "" (40, 'MARKETING', 'DARLINGTON'),"" +  "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```        ###   Example notebooks for creating managed tables     You can use the following example notebooks to create a catalog, schema, and managed table, and to manage permissions on them.        ####   Create and manage a table in Unity Catalog with SQL     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-example-notebook.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Create and manage a table in Unity Catalog with Python     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-quickstart-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                   Drop a managed table ----------------------     You must be the table’s owner to drop a table. To drop a managed table, run the following SQL command:        ``` DROP TABLE IF EXISTS catalog\_name.schema\_name.table\_name;  ```      When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.         Create an external table --------------------------     The data in an external table is stored in a path on your cloud tenant. To work with external tables, Unity Catalog introduces two objects to access and work with external cloud storage:     * A  *storage credential*   contains an authentication method for accessing a cloud storage location. The storage credential does not contain a mapping to the path to which it grants access. Storage credentials are access-controlled to determine which users can use the credential. * An  *external location*   maps a storage credential with a cloud storage path to which it grants access. The external location grants access only to that cloud storage path and its contents. External locations are access-controlled to determine which users can use them. An external location is used automatically when your SQL command contains a  `LOCATION`  clause.      ###   Requirements     To create an external table, you must have:     * The  `CREATE     EXTERNAL     TABLE`  privilege on an external location or storage credential, that grants access to the  `LOCATION`  accessed by the external table. * The  `USE     SCHEMA`  permission on the table’s parent schema. * The  `USE     CATALOG`  permission on the table’s parent catalog. * The  `CREATE     TABLE`  permission on the table’s parent schema.     External locations and storage credentials are stored in the top level of the metastore, rather than in a catalog. To create a storage credential or an external location, you must be the metastore admin or an account-level admin. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .       ###   Create a table     Use one of the following command examples in a notebook or the SQL query editor to create an external table.       You can also use an  [example notebook](#example-notebook-external-table)  to create the storage credential, external location, and external table, and also manage permissions for them.       In the following examples, replace the placeholder values:     * `<catalog>`  : The name of the catalog that will contain the table. * `<schema>`  : The name of the schema that will contain the table. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column. * `<bucket\_path>`  : The path on your cloud tenant where the table will be created. * `<table\_directory>`  : A directory where the table will be created. Use a unique directory for each table.      Important       Once a table is created in a path, users can no longer directly access the files in that path from Databricks even if they have been given privileges on an external location or storage credential to do so. This is to ensure that users cannot circumvent access controls applied to tables by reading files from your cloud tenant directly.          ``` CREATE TABLE <catalog>.<schema>.<table\_name> (  <column\_specification> ) LOCATION 's3://<bucket\_path>/<table\_directory>';  ```       ``` spark.sql(""CREATE TABLE <catalog>.<schema>.<table\_name> ""   ""(""   "" <column\_specification>""   "") ""   ""LOCATION 's"	Yes, you can convert managed tables to external tables. To do so, you can create an external table with the same schema as the managed table and then insert the data from the managed table into the external table. Alternatively, you can use the CTAS (Create Table As Select) statement to create an external table from a managed table.	1	1	1	True
142	no_reaction	https://kb.databricks.com/scala/spark-job-fail-parquet-column-convert	Possible cause: Parquet column cannot be converted.	"Title: Apache Spark job fails with Parquet column cannot be converted error  Description: Parquet column cannot be converted error appears when you are reading decimal data in Parquet format and writing to a Delta table.  Answer: Problem You are reading data in Parquet format and writing to a Delta table when you get a Parquet column cannot be converted error message. The cluster is running Databricks Runtime 7.3 LTS or above. org.apache.spark.SparkException: Task failed while writing rows. Caused by: com.databricks.sql.io.FileReadException: Error while reading file s3://bucket-name/landing/edw/xxx/part-xxxx-tid-c00.snappy.parquet. Parquet column cannot be converted. Column: [Col1], Expected: DecimalType(10,0), Found: FIXED_LEN_BYTE_ARRAY Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException. Cause The vectorized Parquet reader is decoding the decimal type column to a binary format. The vectorized Parquet reader is enabled by default in Databricks Runtime 7.3 and above for reading datasets in Parquet files. The read schema uses atomic data types: binary, boolean, date, string, and timestamp. Info This error only occurs if you have decimal type columns in the source data. Solution If you have decimal type columns in your source data, you should disable the vectorized Parquet reader. Set spark.sql.parquet.enableVectorizedReader to false in the cluster’s Spark configuration to disable the vectorized Parquet reader at the cluster level. You can also disable the vectorized Parquet reader at the notebook level by running: %scala spark.conf.set(""spark.sql.parquet.enableVectorizedReader"",""false"") Info The vectorized Parquet reader enables native record-level filtering using push-down filters, improving memory locality, and cache utilization. If you disable the vectorized Parquet reader, there may be a minor performance impact. You should only disable it, if you have decimal type columns in your source data."	"Possible cause for the Parquet column cannot be converted error is when you are reading decimal data in Parquet format and writing to a Delta table. This error occurs if you have decimal type columns in the source data and the vectorized Parquet reader is enabled by default in Databricks Runtime 7.3 and above for reading datasets in Parquet files. To solve this issue, you can disable the vectorized Parquet reader at the cluster or notebook level by setting spark.sql.parquet.enableVectorizedReader to false or running %scala spark.conf.set(""spark.sql.parquet.enableVectorizedReader"",""false""). However, disabling the vectorized Parquet reader may have a minor performance impact."	1	1	1	True
143	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-ansi-compliance.html	does databricks support ansi sql	ANSI compliance in Databricks Runtime =======================================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       This article describes ANSI compliance in Databricks Runtime. For ANSI mode in Databricks SQL, see  [ANSI\_MODE](parameters/ansi_mode.html)  .       Spark SQL has two options to support compliance with the ANSI SQL standard:  `spark.sql.ansi.enabled`  and  `spark.sql.storeAssignmentPolicy`  .       When  `spark.sql.ansi.enabled`  is set to  `true`  , Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results if the inputs to a SQL operator/function are invalid. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL’s style.       Moreover, Spark SQL has an independent option to control implicit casting behaviours when inserting rows in a table. The casting behaviours are defined as store assignment rules in the standard.       When  `spark.sql.storeAssignmentPolicy`  is set to  `ANSI`  , Spark SQL complies with the ANSI store assignment rules. This is a separate configuration because its default value is  `ANSI`  , while the configuration  `spark.sql.ansi.enabled`  is disabled by default.       The following table summarizes the behavior:           |   Property Name   |   Default   |   Meaning   | | --- | --- | --- | |  `spark.sql.ansi.enabled`  |   false   |   When true, Spark attempts to conform to the ANSI SQL specification:  * Throws a runtime exception if an overflow occurs in any operation on an integer or decimal field. * Forbids using the reserved keywords of ANSI SQL as identifiers in the SQL parser.  | |  `spark.sql.storeAssignmentPolicy`  |   ANSI   |   When inserting a value into a column with a different data type, Spark performs type conversion. There are three policies for the type coercion rules:  `ANSI`  ,  `legacy`  , and  `strict`  .  * `ANSI`  : Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting string to int or double to boolean. * `legacy`  : Spark allows the type coercion as long as it is a valid Cast, which is very loose. For example, converting string to int or double to boolean is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. * `strict`  : Spark doesn’t allow any possible precision loss or data truncation in type coercion, for example, converting double to int or decimal to double is not allowed.  |     The following subsections present behavior changes in arithmetic operations, type conversions, and SQL parsing when ANSI mode is enabled. For type conversions in Spark SQL, there are three kinds of them and this article will introduce them one by one: cast, store assignment and type coercion.        Arithmetic operations -----------------------     In Spark SQL, arithmetic operations performed on numeric types (with the exception of decimal) are not checked for overflows by default. This means that in case an operation causes overflows, the result is the same with the corresponding operation in a Java or Scala program (For example, if the sum of 2 integers is higher than the maximum value representable, the result is a negative number). On the other hand, Spark SQL returns null for decimal overflows. When  `spark.sql.ansi.enabled`  is set to  `true`  and an overflow occurs in numeric and interval arithmetic operations, it throws an arithmetic exception at runtime.        ``` -- `spark.sql.ansi.enabled=true` > SELECT 2147483647 + 1;  error: integer overflow  -- `spark.sql.ansi.enabled=false` > SELECT 2147483647 + 1;  -2147483648  ```        Cast ------     When  `spark.sql.ansi.enabled`  is set to  `true`  , explicit casting by  `CAST`  syntax throws a runtime exception for illegal cast patterns defined in the standard, such as casts from a string to an integer.       The  `CAST`  clause of Spark ANSI mode follows the syntax rules of section 6.13 “cast specification” in  [ISO/IEC 9075-2:2011 Information technology — Database languages - SQL — Part 2: Foundation (SQL/Foundation)](https://www.iso.org/standard/53682.html)   , except it specially allows the following straightforward type conversions which are disallowed as per the ANSI standard:     * NumericType <=> BooleanType * StringType <=> BinaryType     The valid combinations of source and target data type in a  `CAST`  expression are given by the following table. “Y” indicates that the combination is syntactically valid without restriction and “N” indicates that the combination is not valid.                   |   SourceTarget   |   Numeric   |   String   |   Date   |   Timestamp   |   Interval   |   Boolean   |   Binary   |   Array   |   Map   |   Struct   | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | |   Numeric   |   Y   |   Y   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   | |   String   |   Y   |   Y   |   Y   |   Y   |   Y   |   Y   |   Y   |   N   |   N   |   N   | |   Date   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Timestamp   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Interval   |   N   |   Y   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   |   N   | |   Boolean   |   Y   |   Y   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   | |   Binary   |   Y   |   N   |   N   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   | |   Array   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   Y   |   N   |   N   | |   Map   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   Y   |   N   | |   Struct   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   Y   |      ``` -- Examples of explicit casting  -- `spark.sql.ansi.enabled=true` > SELECT CAST('a' AS INT);  error: invalid input syntax for type numeric: a  > SELECT CAST(2147483648L AS INT);  error: Casting 2147483648 to int causes overflow  > SELECT CAST(DATE'2020-01-01' AS INT)  error: cannot resolve 'CAST(DATE '2020-01-01' AS INT)' due to data type mismatch: cannot cast date to int. To convert values from date to int, you can use function UNIX\_DATE instead.  -- `spark.sql.ansi.enabled=false` (This is a default behavior) > SELECT cast('a' AS INT);  null  > SELECT CAST(2147483648L AS INT);  -2147483648  > SELECT CAST(DATE'2020-01-01' AS INT);  null  -- Examples of store assignment rules > CREATE TABLE t (v INT);  -- `spark.sql.storeAssignmentPolicy=ANSI` > INSERT INTO t VALUES ('1');  error: Cannot write incompatible data to table '`default`.`t`': - Cannot safely cast 'v': string to int;  -- `spark.sql.storeAssignmentPolicy=LEGACY` (This is a legacy behavior until Spark 2.x) > INSERT INTO t VALUES ('1'); > SELECT \* FROM t;  1  ```        Store assignment ------------------     As mentioned at the beginning, when  `spark.sql.storeAssignmentPolicy`  is set to  `ANSI`  (which is the default value), Spark SQL complies with the ANSI store assignment rules on table insertions. The valid combinations of source and target data type in table insertions are given by the following table.                   |   SourceTarget   |   Numeric   |   String   |   Date   |   Timestamp   |   Interval   |   Boolean   |   Binary   |   Array   |   Map   |   Struct   | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | |   Numeric   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   | |   String   |   N   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   |   N   |   N   | |   Date   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Timestamp   |   N   |   Y   |   Y   |   Y   |   N   |   N   |   N   |   N   |   N   |   N   | |   Interval   |   N   |   Y   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   |   N   | |   Boolean   |   N   |   Y   |   N   |   N   |   N   |   Y   |   N   |   N   |   N   |   N   | |   Binary   |   N   |   Y	Yes, Databricks supports ANSI SQL. Databricks Runtime has two options to support compliance with the ANSI SQL standard: `spark.sql.ansi.enabled` and `spark.sql.storeAssignmentPolicy`. When `spark.sql.ansi.enabled` is set to `true`, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results if the inputs to a SQL operator/function are invalid.	1	1	1	True
144	no_reaction	https://docs.databricks.com/ingestion/auto-loader/options.html	how to declare schema with cloudfiles	Title: Generate schema from case class  Description: Learn how to generate a schema from a Scala case class.  Answer: Spark provides an easy way to generate a schema from a Scala case class. For case class A, use the method ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]. For example: %scala import org.apache.spark.sql.types.StructType import org.apache.spark.sql.catalyst.ScalaReflection case class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]]) val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] schema.printTreeString	To declare schema with cloud files, you can use the method ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] where A is the case class that defines the schema. This method generates a schema from a Scala case class and can be used with cloud files.	0	0	0	True
145	no_reaction	https://docs.databricks.com/files/download-internet-files.html	how to download a csv file from the dbfs to a local machine?	"      CSV file ==========     This article provides examples for reading and writing to CSV files with Databricks using Python, Scala, R, and SQL.        Note       You can use SQL to read CSV data directly or by using a temporary view. Databricks recommends using a temporary view. Reading the CSV file directly has the following drawbacks:     * You can’t  [specify data source options](#options)  . * You can’t  [specify the schema](#specify-schema)  for the data.     See  [Examples](#examples)  .         Options ---------     You can configure several options for CSV file data sources. See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#csv(path:String):Unit) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html#pyspark.sql.DataFrameWriter.csv)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#csv(path:String):Unit)         Work with malformed CSV records ---------------------------------     When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:     * `PERMISSIVE`  (default): nulls are inserted for fields that could not be parsed correctly * `DROPMALFORMED`  : drops lines that contain fields that could not be parsed * `FAILFAST`  : aborts the reading if any malformed data is found     To set the mode, use the  `mode`  option.        ``` diamonds\_df = (spark.read   .format(""csv"")   .option(""mode"", ""PERMISSIVE"")   .load(""/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv"") )  ```      In the  `PERMISSIVE`  mode it is possible to inspect the rows that could not be parsed correctly using one of the following methods:     * You can provide a custom path to the option  `badRecordsPath`  to record corrupt records to a file. * You can add the column  `\_corrupt\_record`  to the schema provided to the DataFrameReader to review corrupt records in the resultant DataFrame.      Note       The  `badRecordsPath`  option takes precedence over  `\_corrupt\_record`  , meaning that malformed rows written to the provided path do not appear in the resultant DataFrame.       Default behavior for malformed records changes when using the  [rescued data column](#rescued-data)  .         ###   Find malformed rows notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-corrupt-record.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                    Rescued data column ---------------------      Note       This feature is supported in  [Databricks Runtime 8.3 (Unsupported)](../release-notes/runtime/8.3.html)  and above.        When using the  `PERMISSIVE`  mode, you can enable the rescued data column to capture any data that wasn’t parsed because one or more fields in a record have one of the following issues:     * Absent from the provided schema. * Does not match the data type of the provided schema. * Has a case mismatch with the field names in the provided schema.     The rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record (the source file path is available in Databricks Runtime 8.3 and above). To remove the source file path from the rescued data column, you can set the SQL configuration  `spark.conf.set(""spark.databricks.sql.rescuedDataColumn.filePath.enabled"",     ""false"")`  . You can enable the rescued data column by setting the option  `rescuedDataColumn`  to a column name when reading data, such as  `\_rescued\_data`  with  `spark.read.option(""rescuedDataColumn"",     ""\_rescued\_data"").format(""csv"").load(<path>)`  .       The CSV parser supports three modes when parsing records:  `PERMISSIVE`  ,  `DROPMALFORMED`  , and  `FAILFAST`  . When used together with  `rescuedDataColumn`  , data type mismatches do not cause records to be dropped in  `DROPMALFORMED`  mode or throw an error in  `FAILFAST`  mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors.       When  `rescuedDataColumn`  is used in  `PERMISSIVE`  mode, the following rules apply to  [corrupt records](#corrupt-records)  :     * The first row of the file (either a header row or a data row) sets the expected row length. * A row with a different number of columns is considered incomplete. * Data type mismatches are not considered corrupt records. * Only incomplete and malformed CSV records are considered corrupt and recorded to the  `\_corrupt\_record`  column or  `badRecordsPath`  .       Examples ----------     These examples use the  [diamonds dataset](../dbfs/databricks-datasets.html)  . Specify the path to the dataset as well as any options that you would like.        In this section:     * [Read file in any language](#read-file-in-any-language) * [Specify schema](#specify-schema) * [Pitfalls of reading a subset of columns](#pitfalls-of-reading-a-subset-of-columns)     ###  [Read file in any language](#id1)     This notebook shows how to read a file, display sample data, and print the data schema using Scala, R, Python, and SQL.        ####   Read CSV files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Specify schema](#id2)     When the schema of the CSV file is known, you can specify the desired schema to the CSV reader with the  `schema`  option.        ####   Read CSV files with schema notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-schema.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                ###  [Pitfalls of reading a subset of columns](#id3)     The behavior of the CSV parser depends on the set of columns that are read. If the specified schema is incorrect, the results might differ considerably depending on the subset of columns that is accessed. The following notebook presents the most common pitfalls.        ####   Caveats of reading a subset of columns of a CSV file notebook     [Open notebook in new tab](/_extras/notebooks/source/read-csv-column-subset.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                                                                  "	To download a CSV file from the DBFS to a local machine, you can use the Databricks CLI command `databricks fs cp` followed by the path of the file in the DBFS and the path of the destination on the local machine. For example, to download a file named `example.csv` from the DBFS to the current directory on the local machine, you can run the command `databricks fs cp dbfs:/path/to/example.csv ./example.csv`.	1	0	1	True
149	no_reaction	https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html	Transition a model’s stage using the MLflow Client API.	"      Log, load, register, and deploy MLflow models ===============================================     An MLflow  [Model](https://mlflow.org/docs/latest/models.html)   is a standard format for packaging machine learning models that can be used in a variety of downstream tools—for example, batch inference on Apache Spark or real-time serving through a REST API. The format defines a convention that lets you save a model in different  [flavors](https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors)   (python-function, pytorch, sklearn, and so on), that can be understood by different model  [serving and inference platforms](https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools)   .          Log and load models ---------------------     When you log a model, MLflow automatically logs  `requirements.txt`  and  `conda.yaml`  files. You can use these files to recreate the model development environment and reinstall dependencies using  `virtualenv`  (recommended) or  `conda`  .        Important       Anaconda Inc. updated their  [terms of service](https://www.anaconda.com/terms-of-service)   for anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anaconda’s packaging and distribution. See  [Anaconda Commercial Edition FAQ](https://www.anaconda.com/blog/anaconda-commercial-edition-faq)   for more information. Your use of any Anaconda channels is governed by their terms of service.       MLflow models logged before  [v1.18](https://mlflow.org/news/2021/06/18/1.18.0-release/index.html)   (Databricks Runtime 8.3 ML or earlier) were by default logged with the conda  `defaults`  channel (  <https://repo.anaconda.com/pkgs/>  ) as a dependency. Because of this license change, Databricks has stopped the use of the  `defaults`  channel for models logged using MLflow v1.18 and above. The default channel logged is now  `conda-forge`  , which points at the community managed  <https://conda-forge.org/>  .       If you logged a model before MLflow v1.18 without excluding the  `defaults`  channel from the conda environment for the model, that model may have a dependency on the  `defaults`  channel that you may not have intended. To manually confirm whether a model has this dependency, you can examine  `channel`  value in the  `conda.yaml`  file that is packaged with the logged model. For example, a model’s  `conda.yaml`  with a  `defaults`  channel dependency may look like this:        ``` channels: - defaults dependencies: - python=3.8.8 - pip - pip:  - mlflow  - scikit-learn==0.23.2  - cloudpickle==1.6.0  name: mlflow-env  ```      Because Databricks can not determine whether your use of the Anaconda repository to interact with your models is permitted under your relationship with Anaconda, Databricks is not forcing its customers to make any changes. If your use of the Anaconda.com repo through the use of Databricks is permitted under Anaconda’s terms, you do not need to take any action.       If you would like to change the channel used in a model’s environment, you can re-register the model to the model registry with a new  `conda.yaml`  . You can do this by specifying the channel in the  `conda\_env`  parameter of  `log\_model()`  .       For more information on the  `log\_model()`  API, see the MLflow documentation for the model flavor you are working with, for example,  [log\_model for scikit-learn](https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html#mlflow.sklearn.log_model)   .       For more information on  `conda.yaml`  files, see the  [MLflow documentation](https://www.mlflow.org/docs/latest/models.html#additional-logged-files)   .       ###   API commands     To log a model to the MLflow  [tracking server](tracking.html)  , use  `mlflow.<model-type>.log\_model(model,     ...)`  .       To load a previously logged model for inference or further development, use  `mlflow.<model-type>.load\_model(modelpath)`  , where  `modelpath`  is one of the following:     * a run-relative path (such as  `runs:/{run\_id}/{model-path}`  ) * a DBFS path * a  [registered model](model-registry.html)  path (such as  `models:/{model\_name}/{model\_stage}`  ).     For a complete list of options for loading MLflow models, see  [Referencing Artifacts in the MLflow documentation](https://www.mlflow.org/docs/latest/concepts.html#artifact-locations)   .       For Python MLflow models, an additional option is to use  `mlflow.pyfunc.load\_model()`  to load the model as a generic Python function. You can use the following code snippet to load the model and score data points.        ``` model = mlflow.pyfunc.load\_model(model\_path) model.predict(model\_input)  ```      As an alternative, you can export the model as an Apache Spark UDF to use for scoring on a Spark cluster, either as a batch job or as a real-time  [Spark Streaming](../structured-streaming/index.html)  job.        ``` # load input data table as a Spark DataFrame input\_data = spark.table(input\_table\_name) model\_udf = mlflow.pyfunc.spark\_udf(model\_path) df = input\_data.withColumn(""prediction"", model\_udf())  ```      ###   Log model dependencies     To accurately load a model, you should make sure the model dependencies are loaded with the correct versions into the notebook environment. In Databricks Runtime 10.5 ML and above, MLflow warns you if a mismatch is detected between the current environment and the model’s dependencies.       Additional functionality to simplify restoring model dependencies is included in Databricks Runtime 11.0 ML and above. In Databricks Runtime 11.0 ML and above, for  `pyfunc`  flavor models, you can call  `mlflow.pyfunc.get\_model\_dependencies`  to retrieve and download the model dependencies. This function returns a path to the dependencies file which you can then install by using  `%pip     install     <file-path>`  . When you load a model as a PySpark UDF, specify  `env\_manager=""virtualenv""`  in the  `mlflow.pyfunc.spark\_udf`  call. This restores model dependencies in the context of the PySpark UDF and does not affect the outside environment.       You can also use this functionality in Databricks Runtime 10.5 or below by manually installing  [MLflow version 1.25.0 or above](https://www.mlflow.org/docs/latest/index.html)   :        ``` %pip install ""mlflow>=1.25.0""  ```      For additional information on how to log model dependencies (Python and non-Python) and artifacts, see  [Log model dependencies](log-model-dependencies.html)  .       Learn how to log model dependencies and custom artifacts for model serving:     * [Deploy models with dependencies](log-model-dependencies.html#deploy-dependencies) * [Use custom Python libraries with Model Serving](../machine-learning/model-serving/private-libraries-model-serving.html) * [Package custom artifacts for Model Serving](../machine-learning/model-serving/model-serving-custom-artifacts.html)    * [Log model dependencies](log-model-dependencies.html)      ###   Automatically generated code snippets in the MLflow UI     When you log a model in a Databricks notebook, Databricks automatically generates code snippets that you can copy and use to load and run the model. To view these code snippets:     1. Navigate to the Runs screen for the run that generated the model. (See  [View notebook experiment](tracking.html#view-notebook-experiment)  for how to display the Runs screen.) 2. Scroll to the  **Artifacts**   section. 3. Click the name of the logged model. A panel opens to the right showing code you can use to load the logged model and make predictions on Spark or pandas DataFrames.    ![Artifact panel code snippets](../_images/code-snippets.png)    ###   Examples     For examples of logging models, see the examples in  [Track machine learning training runs examples](tracking.html#tracking-examples)  . For an example of loading a logged model for inference, see the following example.      * [Model inference example](model-example.html)         Register models in the Model Registry ---------------------------------------     You can register models in the  [MLflow Model Registry](model-registry.html)  , a centralized model store that provides a UI and set of APIs to manage the full lifecycle of MLflow Models. For general information about the Model Registry, see  [MLflow Model Registry on Databricks](model-registry.html)  . For instructions on how to use the Model Registry to manage models in Databricks, see  [Manage model lifecycle](../machine-learning/manage-model-lifecycle/index.html)  .       To register a model using the API, use  `mlflow.register\_model(""runs:/{run\_id}/{model-path}"",     ""{registered-model-name}"")`  .         Save models to DBFS ---------------------     To save a model locally, use  `mlflow.<model-type>.save\_model(model,     modelpath)`  .  `modelpath`  must be a  [DBFS](../dbfs/index.html)  path. For example, if you use a DBFS location  `dbfs:/my\_project\_models`  to store your project work, you must use the model path  `/dbfs/my\_project\_models`  :        ```   modelpath = ""/dbfs/my\_project\_models/model-%f-%f"" % (alpha, l1\_ratio)   mlflow.sklearn.save\_model(lr, modelpath)  ```      For MLlib models, use  [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-persistence-saving-and-loading-pipelines)   .         Download model artifacts --------------------------     You can download the logged model artifacts (such as model files, plots, and metrics) for a registered model with various APIs.      [Python API](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_model_version_download_uri)   example:        ``` from mlflow.store.artifact.models\_artifact\_repo import ModelsArtifactRepository  model\_uri = MlflowClient.get\_model\_version\_download\_uri(model\_name, model\_version) ModelsArtifactRepository(model\_uri).download\_artifacts(artifact\_path="""")  ```     [Java API](https://mlflow.org/docs/latest/java_api/org/mlflow/tracking/MlflowClient.html#downloadModelVersion-java.lang"	"To transition a model's stage using the MLflow Client API, you can use the `transition_model_version_stage` method of the `MlflowClient` class. This method allows you to move a registered model version from one stage to another. The stages available are `None`, `Staging`, and `Production`. Here is an example code snippet:  ``` import mlflow  # Set the tracking URI to the Databricks workspace mlflow.set_tracking_uri(""databricks"")  # Set the Databricks host and token mlflow_databricks = {     ""host"": ""<databricks-instance>"",     ""token"": ""<databricks-token>"" }  # Create an MLflow client client = mlflow.tracking.MlflowClient(     tracking_uri=mlflow.get_tracking_uri(),     registry_uri=f""databricks://{mlflow_databrickshost}"",     artifact_uri=mlflow.get_artifact_uri(),     backend_store_uri=mlflow.get_tracking_uri(),     default_artifact_root=mlflow.get_artifact_uri(),     store_secret=None, )  # Get the registered model version model_version = client.get_model_version(""<model-name>"", <model-version>)  # Transition the model version to the Staging stage client.transition_model_version_stage(     name=""<model-name>"",     version=model_version.version,     stage=""Staging"" ) ```  Note that you need to set the tracking URI to the Databricks workspace and provide the Databricks host and token to create the MLflow client. Also, you need to replace `<model-name>` and `<model-version>` with the actual name and version of the registered model version you want to transition."	0	0	0	True
152	no_reaction	https://kb.databricks.com/streaming/ss-read-from-last-offset	How do you use offset in streaming query	"Title: How to restart a structured streaming query from last written offset  Description: Learn how to restart a structured streaming query from the last written offset.  Answer: Scenario You have a stream, running a windowed aggregation query, that reads from Apache Kafka and writes files in Append mode. You want to upgrade the application and restart the query with the offset equal to the last written offset. You want to discard all state information that hasn’t been written to the sink, start processing from the earliest offsets that contributed to the discarded state, and modify the checkpoint directory accordingly. However, if you use existing checkpoints after upgrading the application code, old states and objects from the previous application version are re-used, which results in unexpected output such as reading from old sources or processing with old application code. Solution Apache Spark maintains state across the execution and binary objects on checkpoints. Therefore you cannot modify the checkpoint directory. As an alternative, copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. Make sure to delete the checkpoint directory. You can get the current offsets by using asynchronous APIs: %scala spark.streams.addListener(new StreamingQueryListener() {     override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {         println(""Query started:"" + queryStarted.id)     }     override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {         println(""Query terminated"" + queryTerminated.id)     }     override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {      println(""Query made progress"")         println(""Starting offset:"" + queryProgress.progress.sources(0).startOffset)         println(""Ending offset:"" + queryProgress.progress.sources(0).endOffset)         //Logic to save these offsets     } }) You can use readStream with the latest offset written by the process shown above: %scala option(""startingOffsets"",  """""" {""articleA"":{""0"":23,""1"":-1},""articleB"":{""0"":-2}} """""") The input schema for streaming records is: root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- article: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) Also, you can implement logic to save and update the offset to a database and read it at the next restart."	To use offset in streaming query, you can copy and update the offset with the input records and store this in a file or a database. Read it during the initialization of the next restart and use the same value in readStream. You can get the current offsets by using asynchronous APIs and use readStream with the latest offset written by the process shown above.	1	1	1	True
155	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-location.html	How do I create an External Location on the UI?	CREATE EXTERNAL LOCATION ==========================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Creates an external location with the specified name. If a location with the same name already exists, an exception is thrown.        Syntax --------      ``` CREATE EXTERNAL LOCATION [IF NOT EXISTS] location\_name     URL url     WITH (STORAGE CREDENTIAL credential\_name)     [COMMENT comment]  ```        Parameters ------------   * **[location\_name](sql-ref-names.html#location-name)**      The name of the location to be created. * **IF NOT EXISTS**       Creates a location with the given name if it does not exist. If a location with the same name already exists, nothing will happen. * **url**       A  `STRING`  literal with the location of the cloud storage described as an absolute URL. * **[credential\_name](sql-ref-names.html#credential-name)**      The named credential used to connect to this location. * **comment**       An optional description for the location, or  `NULL`  . The default is  `NULL`  .       Examples ----------      ``` -- Create a location accessed using the s3\_remote\_cred credential > CREATE EXTERNAL LOCATION s3\_remote URL 's3://us-east-1/location'  WITH (STORAGE CREDENTIAL s3\_remote\_cred)  COMMENT 'Default source for AWS exernal data';  ```        Related articles ------------------   * [ALTER EXTERNAL LOCATION](sql-ref-syntax-ddl-alter-location.html) * [DESCRIBE EXTERNAL LOCATION](sql-ref-syntax-aux-describe-location.html) * [DROP EXTERNAL LOCATION](sql-ref-syntax-ddl-drop-location.html) * [External locations and storage credentials](sql-ref-external-locations.html) * [SHOW EXTERNAL LOCATIONS](sql-ref-syntax-aux-show-locations.html)	To create an External Location on the UI, you can use the `CREATE EXTERNAL LOCATION` command with the specified name, URL, and storage credential. The syntax for the command is as follows:  ``` CREATE EXTERNAL LOCATION [IF NOT EXISTS] location_name     URL url     WITH (STORAGE CREDENTIAL credential_name)     [COMMENT comment] ```  You can replace `location_name` with the name you want to give to the location, `url` with the absolute URL of the cloud storage, and `credential_name` with the named credential used to connect to this location. The `COMMENT` parameter is optional.	1	1	1	True
156	thumb_up	https://docs.databricks.com/data-governance/unity-catalog/index.html	What is Unity Catalog?	What is Unity Catalog? ========================     This article introduces Unity Catalog, a unified governance solution for data and AI assets on the Lakehouse.                Overview of Unity Catalog ---------------------------     Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces.      ![Unity Catalog diagram](../../_images/with-unity-catalog.png)    Key features of Unity Catalog include:     * **Define once, secure everywhere**   : Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. * **Standards-compliant security model**   : Unity Catalog’s security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. * **Built-in auditing and lineage**   : Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog also captures lineage data that tracks how data assets are created and used across all languages and personas. * **Data discovery**   : Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data.           The Unity Catalog object model --------------------------------     In Unity Catalog, the hierarchy of primary data objects flows from metastore to table:     * **Metastore**   : The top-level container for metadata. Each metastore exposes a three-level namespace (  `catalog`  .  `schema`  .  `table`  ) that organizes your data. * **Catalog**   : The first layer of the object hierarchy, used to organize your data assets. * **Schema**   : Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views. * **Table**   : At the lowest level in the object hierarchy are tables and views.    ![Unity Catalog object model diagram](../../_images/object-model.png)    This is a simplified view of securable Unity Catalog objects. For more details, see  [Securable objects in Unity Catalog](manage-privileges/privileges.html#securable-objects)  .       You reference all data in Unity Catalog using a  [three-level namespace](queries.html#three-level-namespace-notation)  .        ###   Metastores     A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create a metastore for each region in which they operate and assign them to Databricks workspaces in the same region. For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.       Each metastore is configured with a root storage location in an S3 bucket in your AWS account. This storage location is used by default for storing data for  [managed tables](#managed-tables)  .        Note       This metastore is distinct from the Hive metastore included in Databricks workspaces that have not been enabled for Unity Catalog. If your workspace includes a legacy Hive metastore, the data in that metastore will still be available alongside data defined in Unity Catalog, in a catalog named  `hive\_metastore`  . Note that the  `hive\_metastore`  catalog is not managed by Unity Catalog and does not benefit from the same feature set as catalogs defined in Unity Catalog.        See  [Create a Unity Catalog metastore](create-metastore.html)  .         ###   Catalogs     A catalog is the first layer of Unity Catalog’s three-level namespace. It’s used to organize your data assets. Users can see all catalogs on which they have been assigned the  `USE     CATALOG` [data permission](manage-privileges/index.html)  .       See  [Create and manage catalogs](create-catalogs.html)  .         ###   Schemas     A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace. A schema organizes tables and views. To access (or list) a table or view in a schema, users must have the  `USE     SCEHMA`  data permission on the schema and its parent catalog, and they must have the  `SELECT`  permission on the table or view.       See  [Create and manage schemas (databases)](create-schemas.html)  .       ###   Tables     A table resides in the third layer of Unity Catalog’s three-level namespace. It contains rows of data. To create a table, users must have  `CREATE`  and  `USE     SCHEMA`  permissions on the schema, and they must have the  `USE     CATALOG`  permission on its parent catalog. To query a table, users must have the  `SELECT`  permission on the table, the  `USE     SCHEMA`  permission on its parent schema, and the  `USE     CATALOG`  permission on its parent catalog.       A table can be  *managed*   or  *external*   .        ####   Managed tables     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.       See  [Managed tables](create-tables.html#managed-table)  .         ####   External tables     External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.       When you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.       External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     See  [External tables](create-tables.html#external-table)  .       ####   Storage credentials and external locations     To manage access to the underlying cloud storage for an external table, Unity Catalog introduces the following object types:     * **Storage credentials**   encapsulate a long-term cloud credential that provides access to cloud storage. For example, an IAM role that can access S3 buckets. * **External locations**   contain a reference to a storage credential and a cloud storage path.     See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .          ###   Views     A view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog’s  [three-level namespace](queries.html#three-level-namespace-notation)  . A view can be created from tables and other views in multiple schemas and catalogs. You can create  [dynamic views](#)   to enable row- and column-level permissions.       See  [Create a dynamic view](create-views.html#dynamic-view)  .              Identity management for Unity Catalog ---------------------------------------     Unity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.       To configure identities in the account, follow the instructions in  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  . Refer to those users, service principals, and groups when you create  [access-control policies](manage-privileges/index.html)  in Unity Catalog.       Unity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Data Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called  *identity federation*   .       All workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.      ###   Special considerations for groups     Any groups that already exist in the workspace are labeled  **Workspace local**   in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.       See  [Manage groups](../../administration-guide/users-groups/groups.html)  .              Admin roles for Unity Catalog -------------------------------     The following admin roles are required for managing Unity Catalog:     * **Account admins**   can manage identities, cloud resources and the creation of workspaces and Unity Catalog metastores.       Account admins can enable workspaces for Unity Catalog. They can grant both workspace and metastore admin permissions. * **Metastore admins**   can manage privileges and ownership for all securable objects within a metastore, such as who can create catalogs or query a table.       The account admin who creates the Unity Catalog metastore becomes the initial metastore admin. The metastore admin can also choose to delegate this role to another user or group. We recommend assigning the metastore admin to a group, in which case any member of the group receives the privileges of the metastore admin. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  . * **Workspace admins**   can add users to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters and change job ownership.     See  [Manage users, service principals, and groups](../../administration-guide/users-groups/index.html)  .           Data permissions in Unity Catalog -----------------------------------     In Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.       You can assign and revoke permissions using Data Explorer, SQL commands, or REST APIs.       See  [Manage privileges in Unity Catalog](manage-privileges/index.html)  .             Cluster access modes for Unity Catalog ----------------------------------------     To access data in Unity Catalog, clusters must be configured with the correct  *access mode*   . Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or single user), the cluster can’t access data in Unity Catalog.       See  [Create clusters & SQL warehouses with Unity Catalog access](compute.html)  .           Data lineage for Unity Catalog --------------------------------     You can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see  [Capture and view data lineage with Unity Catalog](data-lineage.html)  .           How do I set up Unity Catalog for my organization? ----------------------------------------------------     To set up Unity Catalog for your organization, you do the following:     1. Configure an S3 bucket and IAM role that Unity Catalog can use to store and access data in your AWS account. 2. Create a metastore for each region in which your organization operates. 3. Attach workspaces to the metastore. Each workspace will have the same view of the data you manage in Unity Catalog. 4. If you have a new account, add users, groups, and service principals to your Databricks account.     Next, you create and grant access to catalogs, schemas, and tables.       For complete setup instructions,	Unity Catalog is a unified governance solution for data and AI assets on the Lakehouse. It provides centralized access control, auditing, lineage, and data discovery capabilities across Databricks workspaces. Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. Its security model is based on standard ANSI SQL and allows administrators to grant permissions in their existing data lake using familiar syntax, at the level of catalogs, databases (also called schemas), tables, and views. Unity Catalog lets you tag and document data assets, and provides a search interface to help data consumers find data. The hierarchy of primary data objects in Unity Catalog flows from metastore to table: Metastore, Catalog, Schema, and Table. You reference all data in Unity Catalog using a three-level namespace.	1	1	1	True
157	no_reaction	https://docs.databricks.com/sql/language-manual/delta-describe-history.html	Databricks SQL how long does query history remain?	Query history ===============     The query history shows SQL queries performed using  [SQL warehouses](create-sql-warehouse.html)  .       You can use the information available through this screen to help you debug issues with queries.       This section describes how to work with query history using the UI. To work with query history using the API, see  [Query History API 2.0](../api/query-history.html)  .        Important       The time recorded in query history for a SQL query is only the time the SQL warehouse spends actually executing the query. It does not record any additional overhead associated with getting ready to execute the query, such as internal queuing, or additional time related to the data upload and download process.         View query history --------------------     To view the history of all executions of a query:     1. Click  ![History Icon](../../_images/history-icon.png) **Query History**   in the sidebar. 2. Optionally, click  **Duration**   to sort the list by duration. By default, the list is sorted by start time. 3. Click the name of a query to see more details, such as the SQL command and the  [execution details](query-profile.html)  .     You can filter the list by user, date range, SQL warehouse, and query status.       If you’re a non-admin user without  `CAN\_MANAGE`  permissions, you can only view your own queries in  **Query History**   .        Note       Queries shared by a user with  **Run as Owner**   permissions to another user with  **Can Run**   permissions appear in the query history of the user executing the query and not the user that shared the query.          View query details --------------------     To view details about a query, such as its duration, SQL command, number of rows returned, and I/O performance:     1. View  [query history](#view-query-history)  . 2. Click the name of a query.      ![Query history details](../../_images/query-details.png)      Brief information about a query’s performance appears, such as time spent in each task, rows returned, and I/O performance. 3. For more detailed information about the query’s performance,including its execution plan, click  **View Query Profile**   at the bottom of the page. For more details, see  [Query profile](query-profile.html)  .       Terminate an executing query ------------------------------     To terminate a long-running query started by you or another user:     1. View  [query history](#view-query-history)  . 2. Click the name of a query. 3. Next to  **Status**   , click  **Cancel**   .        Note      **Cancel**   only appears when a query is running.     The query is terminated and its status changes to  **Canceled**   .	The query history in Databricks SQL shows SQL queries performed using SQL warehouses. The time recorded in query history for a SQL query is only the time the SQL warehouse spends actually executing the query. It does not record any additional overhead associated with getting ready to execute the query, such as internal queuing, or additional time related to the data upload and download process. The duration of query history in Databricks SQL is not mentioned in the context.	0	0	0	True
158	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-external-tables.html	"use SQL ""select *"" for a parquet file in an UC external storage"	Parquet file ==============    [Apache Parquet](https://parquet.apache.org/)   is a columnar file format that provides optimizations to speed up queries. It is a far more efficient file format than  [CSV](csv.html)  or  [JSON](json.html)  .       For more information, see  [Parquet Files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)   .        Options ---------     See the following Apache Spark reference articles for supported read and write options.     * Read      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameReader.html#parquet(paths:String*):org.apache.spark.sql.DataFrame) * Write      + [Python](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)  + [Scala](https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/DataFrameWriter.html#parquet(path:String):Unit)     The following notebook shows how to read and write data to Parquet files.        ###   Reading Parquet files notebook     [Open notebook in new tab](/_extras/notebooks/source/read-parquet-files.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import	"Based on the context, we can use SQL ""select *"" to read all columns from a Parquet file stored in an external storage system using the supported read options provided by Apache Spark."	1	1	1	True
159	no_reaction	https://docs.databricks.com/sql/language-manual/data-types/struct-type.html	"i got this error ""AnalysisException: Can't extract value from name#478: need struct type but got string"""	"   StructField  [¶](#structfield ""Permalink to this headline"") =============================================================  *class*  `pyspark.sql.types.`  `StructField`    (   *name     :     str*  ,  *dataType     :    [pyspark.sql.types.DataType](pyspark.sql.types.DataType.html#pyspark.sql.types.DataType ""pyspark.sql.types.DataType"")*  ,  *nullable     :     bool     =     True*  ,  *metadata     :     Optional    [    Dict    [    str    ,    Any    ]     ]     =     None*   )   [[source]](../../../_modules/pyspark/sql/types.html#StructField) [¶](#pyspark.sql.types.StructField ""Permalink to this definition"")    A field in  [`StructType`](pyspark.sql.types.StructType.html#pyspark.sql.types.StructType ""pyspark.sql.types.StructType"")  .     Parameters    **name**    str     name of the field.    **dataType**   [`DataType`](pyspark.sql.types.DataType.html#pyspark.sql.types.DataType ""pyspark.sql.types.DataType"")  [`DataType`](pyspark.sql.types.DataType.html#pyspark.sql.types.DataType ""pyspark.sql.types.DataType"")  of the field.    **nullable**    bool, optional     whether the field can be null (None) or not.    **metadata**    dict, optional     a dict from string to simple type that can be toInternald to JSON automatically     Examples    ``` >>> from pyspark.sql.types import StringType, StructField >>> (StructField(""f1"", StringType(), True) ...      == StructField(""f1"", StringType(), True)) True >>> (StructField(""f1"", StringType(), True) ...      == StructField(""f2"", StringType(), True)) False  ```   Methods    |  [`fromInternal`](#pyspark.sql.types.StructField.fromInternal ""pyspark.sql.types.StructField.fromInternal"")  (obj)   |   Converts an internal SQL object into a native Python object.   | |  [`fromJson`](#pyspark.sql.types.StructField.fromJson ""pyspark.sql.types.StructField.fromJson"")  (json)   |   | |  [`json`](#pyspark.sql.types.StructField.json ""pyspark.sql.types.StructField.json"")  ()   |   | |  [`jsonValue`](#pyspark.sql.types.StructField.jsonValue ""pyspark.sql.types.StructField.jsonValue"")  ()   |   | |  [`needConversion`](#pyspark.sql.types.StructField.needConversion ""pyspark.sql.types.StructField.needConversion"")  ()   |   Does this type needs conversion between Python object and internal SQL object.   | |  [`simpleString`](#pyspark.sql.types.StructField.simpleString ""pyspark.sql.types.StructField.simpleString"")  ()   |   | |  [`toInternal`](#pyspark.sql.types.StructField.toInternal ""pyspark.sql.types.StructField.toInternal"")  (obj)   |   Converts a Python object into an internal SQL object.   | |  [`typeName`](#pyspark.sql.types.StructField.typeName ""pyspark.sql.types.StructField.typeName"")  ()   |   |   Methods Documentation    `fromInternal`    (   *obj     :     T*   )    → T  [[source]](../../../_modules/pyspark/sql/types.html#StructField.fromInternal) [¶](#pyspark.sql.types.StructField.fromInternal ""Permalink to this definition"")    Converts an internal SQL object into a native Python object.    *classmethod*  `fromJson`    (   *json     :     Dict    [    str    ,    Any    ]*   )    →  [pyspark.sql.types.StructField](#pyspark.sql.types.StructField ""pyspark.sql.types.StructField"")  [[source]](../../../_modules/pyspark/sql/types.html#StructField.fromJson) [¶](#pyspark.sql.types.StructField.fromJson ""Permalink to this definition"")   `json`    (     )    → str  [¶](#pyspark.sql.types.StructField.json ""Permalink to this definition"")   `jsonValue`    (     )    → Dict    [    str    ,    Any    ]   [[source]](../../../_modules/pyspark/sql/types.html#StructField.jsonValue) [¶](#pyspark.sql.types.StructField.jsonValue ""Permalink to this definition"")   `needConversion`    (     )    → bool  [[source]](../../../_modules/pyspark/sql/types.html#StructField.needConversion) [¶](#pyspark.sql.types.StructField.needConversion ""Permalink to this definition"")    Does this type needs conversion between Python object and internal SQL object.     This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.    `simpleString`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#StructField.simpleString) [¶](#pyspark.sql.types.StructField.simpleString ""Permalink to this definition"")   `toInternal`    (   *obj     :     T*   )    → T  [[source]](../../../_modules/pyspark/sql/types.html#StructField.toInternal) [¶](#pyspark.sql.types.StructField.toInternal ""Permalink to this definition"")    Converts a Python object into an internal SQL object.    `typeName`    (     )    → str  [[source]](../../../_modules/pyspark/sql/types.html#StructField.typeName) [¶](#pyspark.sql.types.StructField.typeName ""Permalink to this definition"")   [VarcharType](pyspark.sql.types.VarcharType.html ""previous page"")  [StructType](pyspark.sql.types.StructType.html ""next page"")   "	"The error ""AnalysisException: Can't extract value from name#478: need struct type but got string"" suggests that there is an issue with extracting a value from a field named ""name"" with field type string, when a struct type is expected. The context is a documentation for the `StructField` class in PySpark, which defines a field in a `StructType`. The documentation provides information on the parameters and methods of the `StructField` class."	1	1	1	True
160	no_reaction	https://docs.databricks.com/mlflow/index.html	Can we set authority in mlflow.	MLflow API 2.0 ================	Yes, MLflow allows you to set authority for tracking and managing experiments, models, and other artifacts. You can set permissions for users and groups using access control lists (ACLs) in the MLflow tracking server. This allows you to control who can view, modify, or delete experiments and models.	1	1	1	True
161	no_reaction	https://docs.databricks.com/dev-tools/databricks-utils.html	DBUtils	"    Databricks Utilities ======================     Databricks Utilities (  `dbutils`  ) make it easy to perform powerful combinations of tasks. You can use the utilities to work with object storage efficiently, to chain and parameterize notebooks, and to work with secrets.  `dbutils`  are not supported outside of notebooks.        Important       Calling  `dbutils`  inside of executors can produce unexpected results. To learn more about limitations of  `dbutils`  and alternatives that could be used instead, see  [Limitations](#limitations)  .       `dbutils`  utilities are available in Python, R, and Scala notebooks.      **How to**   :  [List utilities](#dbutils-utilities)  ,  [list commands](#dbutils-commands)  ,  [display command help](#dbutils-help)     **Utilities**   :  [credentials](#dbutils-credentials)  ,  [data](#dbutils-data)  ,  [fs](#dbutils-fs)  ,  [jobs](#dbutils-jobs)  ,  [library](#dbutils-library)  ,  [notebook](#dbutils-workflow)  ,  [secrets](#dbutils-secrets)  ,  [widgets](#dbutils-widgets)  ,  [Utilities API library](#dbutils-api)         List available utilities --------------------------     To list available utilities along with a short description for each utility, run  `dbutils.help()`  for Python or Scala.       This example lists available commands for the Databricks Utilities.         ``` dbutils.help()  ```       ``` dbutils.help()  ```        ``` This module provides various utilities for users to interact with the rest of Databricks.  credentials: DatabricksCredentialUtils -> Utilities for interacting with credentials within notebooks fs: DbfsUtils -> Manipulates the Databricks filesystem (DBFS) from the console jobs: JobsUtils -> Utilities for leveraging jobs features library: LibraryUtils -> Utilities for session isolated libraries notebook: NotebookUtils -> Utilities for the control flow of a notebook (EXPERIMENTAL) secrets: SecretUtils -> Provides utilities for leveraging secrets within notebooks widgets: WidgetsUtils -> Methods to create and get bound value of input widgets inside notebooks  ```          List available commands for a utility ---------------------------------------     To list available commands for a utility along with a short description of each command, run  `.help()`  after the programmatic name for the utility.       This example lists available commands for the Databricks File System (DBFS) utility.         ``` dbutils.fs.help()  ```       ``` dbutils.fs.help()  ```       ``` dbutils.fs.help()  ```        ``` dbutils.fs provides utilities for working with FileSystems. Most methods in this package can take either a DBFS path (e.g., ""/foo"" or ""dbfs:/foo""), or another FileSystem URI. For more info about a method, use dbutils.fs.help(""methodName""). In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps straightforwardly onto dbutils calls. For example, ""%fs head --maxBytes=10000 /file/path"" translates into ""dbutils.fs.head(""/file/path"", maxBytes = 10000)"".  fsutils  cp(from: String, to: String, recurse: boolean = false): boolean -> Copies a file or directory, possibly across FileSystems head(file: String, maxBytes: int = 65536): String -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8 ls(dir: String): Seq -> Lists the contents of a directory mkdirs(dir: String): boolean -> Creates the given directory if it does not exist, also creating any necessary parent directories mv(from: String, to: String, recurse: boolean = false): boolean -> Moves a file or directory, possibly across FileSystems put(file: String, contents: String, overwrite: boolean = false): boolean -> Writes the given String out to a file, encoded in UTF-8 rm(dir: String, recurse: boolean = false): boolean -> Removes a file or directory  mount  mount(source: String, mountPoint: String, encryptionType: String = """", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Mounts the given source directory into DBFS at the given mount point mounts: Seq -> Displays information about what is mounted within DBFS refreshMounts: boolean -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information unmount(mountPoint: String): boolean -> Deletes a DBFS mount point updateMount(source: String, mountPoint: String, encryptionType: String = """", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean -> Similar to mount(), but updates an existing mount point instead of creating a new one  ```          Display help for a command ----------------------------     To display help for a command, run  `.help(""<command-name>"")`  after the command name.       This example displays help for the DBFS copy command.         ``` dbutils.fs.help(""cp"")  ```       ``` dbutils.fs.help(""cp"")  ```       ``` dbutils.fs.help(""cp"")  ```        ``` /** * Copies a file or directory, possibly across FileSystems. * * Example: cp(""/mnt/my-folder/a"", ""dbfs:/a/b"") * * @param from FileSystem URI of the source file or directory * @param to FileSystem URI of the destination file or directory * @param recurse if true, all files and directories will be recursively copied * @return true if all files were successfully copied */ cp(from: java.lang.String, to: java.lang.String, recurse: boolean = false): boolean  ```                  Credentials utility (dbutils.credentials) -------------------------------------------    **Commands**   :  [assumeRole](#dbutils-credentials-assumerole)  ,  [showCurrentRole](#dbutils-credentials-showcurrentrole)  ,  [showRoles](#dbutils-credentials-showroles)      The credentials utility allows you to interact with credentials within notebooks. This utility is usable only on clusters with  [credential passthrough](../data-governance/credential-passthrough/iam-passthrough.html)  enabled. To list the available commands, run  `dbutils.credentials.help()`  .        ``` assumeRole(role: String): boolean -> Sets the role ARN to assume when looking for credentials to authenticate with S3 showCurrentRole: List -> Shows the currently set role showRoles: List -> Shows the set of possible assumed roles  ```       ###   assumeRole command (dbutils.credentials.assumeRole)     Sets the Amazon Resource Name (ARN) for the AWS Identity and Access Management (IAM) role to assume when looking for credentials to authenticate with Amazon S3. After you run this command, you can run S3 access commands, such as  `sc.textFile(""s3a://my-bucket/my-file.csv"")`  to access an object.       To display help for this command, run  `dbutils.credentials.help(""assumeRole"")`  .         ``` dbutils.credentials.assumeRole(""arn:aws:iam::123456789012:roles/my-role"")  # Out[1]: True  ```       ``` dbutils.credentials.assumeRole(""arn:aws:iam::123456789012:roles/my-role"")  # TRUE  ```       ``` dbutils.credentials.assumeRole(""arn:aws:iam::123456789012:roles/my-role"")  // res0: Boolean = true  ```         ###   showCurrentRole command (dbutils.credentials.showCurrentRole)     Lists the currently set AWS Identity and Access Management (IAM) role.       To display help for this command, run  `dbutils.credentials.help(""showCurrentRole"")`  .         ``` dbutils.credentials.showCurrentRole()  # Out[1]: arn:aws:iam::123456789012:role/my-role-a  ```       ``` dbutils.credentials.showCurrentRole()  # [[1]] # [1] ""arn:aws:iam::123456789012:role/my-role-a""  ```       ``` dbutils.credentials.showCurrentRole()  // res0: java.util.List[String] = [arn:aws:iam::123456789012:role/my-role-a]  ```         ###   showRoles command (dbutils.credentials.showRoles)     Lists the set of possible assumed AWS Identity and Access Management (IAM) roles.       To display help for this command, run  `dbutils.credentials.help(""showRoles"")`  .         ``` dbutils.credentials.showRoles()  # Out[1]: arn:aws:iam::123456789012:role/my-role-a', 'arn:aws:iam::123456789012:role/my-role-b  ```       ``` dbutils.credentials.showRoles()  # [[1]] # [1] ""arn:aws:iam::123456789012:role/my-role-a"" # # [[2]] # [1] ""arn:aws:iam::123456789012:role/my-role-b""  ```       ``` dbutils.credentials.showRoles()  // res0: java.util.List[String] = [arn:aws:iam::123456789012:role/my-role-a, arn:aws:iam::123456789012:role/my-role-b]  ```          Data utility (dbutils.data) -----------------------------      Preview       This feature is in  [Public Preview](../release-notes/release-types.html)  .         Note       Available in Databricks Runtime 9.0 and above.       **Commands**   :  [summarize](#dbutils-data-summarize)      The data utility allows you to understand and interpret datasets. To list the available commands, run  `dbutils.data.help()`  .        ``` dbutils.data provides utilities for understanding and interpreting datasets. This module is currently in preview and may be unstable. For more info about a method, use dbutils.data.help(""methodName"").  summarize(df: Object, precise: boolean): void -> Summarize a Spark DataFrame and visualize the statistics to get quick insights  ```       ###   summarize command (dbutils.data.summarize)     Calculates and displays summary statistics of an Apache Spark DataFrame or pandas DataFrame. This command is available for Python, Scala and R.       To display help for this command, run  `dbutils.data.help(""summarize"")`  .       In Databricks Runtime 10.1 and above, you can use the additional  `precise`  parameter to adjust the precision of the computed statistics.        Note       This feature is in  [Public Preview](../release-notes/release-types.html)  .      * When  `precise`  is set to false (the default), some"	The context is about Databricks Utilities, which are a set of tools that make it easy to perform powerful combinations of tasks. The utilities can be used to work with object storage efficiently, to chain and parameterize notebooks, and to work with secrets. `dbutils` are not supported outside of notebooks. The `dbutils` utilities are available in Python, R, and Scala notebooks. There are several commands available for the utilities, including `credentials`, `data`, `fs`, `jobs`, `library`, `notebook`, `secrets`, and `widgets`. The `dbutils.help()` command can be used to list available utilities and commands, and the `.help()` command can be used to display help for a specific command. The `dbutils.data` utility allows you to understand and interpret datasets, and the `summarize` command can be used to calculate and display summary statistics of a Spark DataFrame or pandas DataFrame.	0	0	0	True
165	no_reaction	https://docs.databricks.com/dev-tools/index-ci-cd.html	how do i do CI/CD on databricks	Use CI/CD ===========     To manage the lifecycle of Databricks assets and data, you can use continuous integration and continuous delivery (CI/CD) and data pipeline tools.          |   Area   |   Use these tools when you want to…   | | --- | --- | |  [CI/CD workflows with Git and Databricks Repos](../repos/ci-cd-techniques-with-repos.html)  |   Use GitHub and Databricks Repos for source control and CI/CD workflows.   | |  [Continuous integration and delivery using GitHub Actions](ci-cd/ci-cd-github.html)  |   Build a CI/CD workflow on GitHub that uses GitHub Actions developed for Databricks.   | |  [CI/CD with Jenkins on Databricks](ci-cd/ci-cd-jenkins.html)  |   Develop a CI/CD pipeline for Databricks that uses Jenkins.   | |  [Orchestrate Databricks jobs with Apache Airflow](../workflows/jobs/how-to/use-airflow-with-jobs.html)  |   Manage and schedule a data pipeline that uses Apache Airflow.   | |  [Service principals for CI/CD](ci-cd/ci-cd-sp.html)  |   Use service principals, instead of users, with CI/CD systems.   |	To do CI/CD on Databricks, you can use continuous integration and continuous delivery (CI/CD) and data pipeline tools such as CI/CD workflows with Git and Databricks Repos, Continuous integration and delivery using GitHub Actions, CI/CD with Jenkins on Databricks, Orchestrate Databricks jobs with Apache Airflow, and Service principals for CI/CD. These tools can be used for source control and CI/CD workflows, building a CI/CD workflow on GitHub that uses GitHub Actions developed for Databricks, developing a CI/CD pipeline for Databricks that uses Jenkins, managing and scheduling a data pipeline that uses Apache Airflow, and using service principals, instead of users, with CI/CD systems.	1	1	1	True
166	no_reaction	https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.date_format.html	how do I format date columns in pyspark	"   pyspark.sql.functions.date\_format  [¶](#pyspark-sql-functions-date-format ""Permalink to this headline"") ==========================================================================================================  `pyspark.sql.functions.`  `date_format`    (   *date     :     ColumnOrName*  ,  *format     :     str*   )    → pyspark.sql.column.Column  [[source]](../../../_modules/pyspark/sql/functions.html#date_format) [¶](#pyspark.sql.functions.date_format ""Permalink to this definition"")    Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.     A pattern could be for instance    dd.MM.yyyy    and could return a string like ‘18.03.1993’. All pattern letters of  [datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)   . can be used.     New in version 1.5.0.     Changed in version 3.4.0:    Supports Spark Connect.     Parameters    **date**   [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")  or str     input column of values to format.    **format: str**    format to use to represent datetime values.     Returns    [`Column`](pyspark.sql.Column.html#pyspark.sql.Column ""pyspark.sql.Column"")   string value representing formatted datetime.     Notes     Whenever possible, use specialized functions like    year    .     Examples    ``` >>> df = spark.createDataFrame([('2015-04-08',)], dt) >>> df.select(date\_format('dt', 'MM/dd/yyy').alias('date')).collect() [Row(date='04/08/2015')]  ```  [pyspark.sql.functions.date\_add](pyspark.sql.functions.date_add.html ""previous page"")  [pyspark.sql.functions.date\_sub](pyspark.sql.functions.date_sub.html ""next page"")   "	To format date columns in PySpark, you can use the `date_format` function from the `pyspark.sql.functions` module. This function takes two arguments: the date column to format and the format string to use. The format string specifies the pattern to use for the date format, and can include pattern letters from the [datetime pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html) documentation. Here is an example usage:  ``` from pyspark.sql.functions import date_format  df = spark.createDataFrame([('2015-04-08',)], dt) df.select(date_format('dt', 'MM/dd/yyyy').alias('date')).show() ```  This will output a DataFrame with a new column named `date`, where the date values are formatted in the `MM/dd/yyyy` format.	1	1	1	True
167	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/index.html	how do I set up unity catalog?	"    Create and manage catalogs ============================     This article shows how to create and manage catalogs in Unity Catalog. A catalog contains  [schemas (databases)](create-schemas.html)  , and a schema contains tables and views.        Requirements --------------   * You must be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore * Your Databricks account must be on the  [Premium plan and above](https://databricks.com/product/aws-pricing)   . * You must have a Unity Catalog metastore  [linked to the workspace](create-metastore.html)  where you perform the catalog creation. * The  [compute resource](compute.html)  that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode.         Create a catalog ------------------     To create a catalog, you can use Data Explorer or a SQL command.       1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the  **Create Catalog**   button. 4. (Optional) Specify the location where data for  [managed tables](create-tables.html#managed-table)  in the catalog will be stored.       Specify a location here only if you do not want managed tables in this catalog to be stored in the default root storage location that was configured for the metastore. See  [Create a Unity Catalog metastore](create-metastore.html)  .       The path that you specify must be defined in an external location configuration, and you must have the  `CREATE     MANAGED     STORAGE`  privilege on that external location. You can also use a subpath of that path. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  . 5. Assign permissions for your catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  . 6. Click  **Save**   .     1. Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : Optional description or other comment.   ``` CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name>  [ MANAGED LOCATION '<location\_path>' ]  [ COMMENT <comment> ];  ```      For example, to create a catalog named  `example`  :        ``` CREATE CATALOG IF NOT EXISTS example;  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ```      For parameter descriptions, see  [CREATE CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-create-catalog.html)  . 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` library(SparkR)  sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` library(SparkR)  sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .     1. Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder values:      * `<catalog\_name>`   : A name for the catalog.  * `<location\_path>`   : Optional. Provide a storage location path if you want managed tables in this catalog to be stored in a location that is different than the default that was configured for the metastore. This path must be defined in an   [external location configuration](manage-external-locations-and-credentials.html)   , and you must have the   `CREATE        MANAGED        STORAGE`   privilege on the external location configuration. You can use the path that is defined in the external location configuration or a subpath (in other words,   `'s3://depts/finance'`   or   `'s3://depts/finance/product'`   ). Requires Databricks Runtime 11.3 and above.  * `<comment>`   : An optional comment.   ``` spark.sql(""CREATE CATALOG [ IF NOT EXISTS ] <catalog\_name> [ MANAGED LOCATION '<location\_path>' ] [ COMMENT <comment> ]"")  ```      For example, to create a catalog named  `example`  :        ``` spark.sql(""CREATE CATALOG IF NOT EXISTS example"")  ``` 2. Assign privileges to the catalog. See  [Unity Catalog privileges and securable objects](manage-privileges/privileges.html)  .       When you create a catalog, two schemas (databases) are automatically created:  `default`  and  `information\_schema`  .       You can also create a catalog by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog)   . You can retrieve information about catalogs by using  [databricks\_catalogs](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/catalogs)   .      ###   Next steps     Now you can  [add schemas (databases) to your catalog](create-schemas.html)  .          Delete a catalog ------------------     To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.         You must delete all schemas in the catalog except  `information\_schema`  before you can delete a catalog. This includes the auto-created  `default`  schema.     1. Log in to a workspace that is linked to the metastore. 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. In the  **Data**   pane, on the left, click the catalog you want to delete. 4. In the detail pane, click the three-dot menu to the left of the  **Create database**   button and select  **Delete**   . 5. On the  **Delete catalog**   dialog, click  **Delete**   .       Run the following SQL command in a notebook or Databricks SQL editor. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` DROP CATALOG [ IF EXISTS ] <catalog\_name> [ RESTRICT | CASCADE ]  ```      For example, to delete a catalog named  `vaccine`  and its schemas:        ``` DROP CATALOG vaccine CASCADE  ```        Run the following SQL command in a notebook. Items in brackets are optional. Replace the placeholder  `<catalog\_name>`  .       For parameter descriptions, see  [DROP CATALOG](../../sql/language-manual/sql-ref-syntax-ddl-drop-catalog.html)  .       If you use  `DROP     CATALOG`  without the  `CASCADE`  option, you must delete all schemas in the catalog except  `information\_schema`  before you can delete the catalog. This includes the auto-created  `default`  schema.        ``` spark.sql(""DROP CATALOG [ IF EXISTS ] <catalog\_name>"	To set up Unity Catalog, you need to be a Databricks metastore admin or have been granted the CREATE CATALOG privilege on the metastore. Your Databricks account must be on the Premium plan and above. You must have a Unity Catalog metastore linked to the workspace where you perform the catalog creation. The compute resource that you use to run the notebook or Databricks SQL to create the catalog must be using a Unity Catalog compliant access mode. To create a catalog, you can use Data Explorer or a SQL command. There are different SQL commands to create a catalog in a notebook or Databricks SQL editor. Once you create a catalog, two schemas (databases) are automatically created: default and information_schema. To delete (or drop) a catalog, you can use Data Explorer or a SQL command. To drop a catalog you must be its owner.	1	1	1	True
169	no_reaction	https://kb.databricks.com/data-sources/delete-table-if-s3-bucket-deleted	When I drop a table does the underlying data get deleted?	Drop Table ============      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [DROP TABLE](../../sql/language-manual/sql-ref-syntax-ddl-drop-table.html)  .         ``` DROP TABLE [IF EXISTS] [db\_name.]table\_name  ```      Drop a table and delete the directory associated with the table from the file system if this is not an  `EXTERNAL`  table. If the table to drop does not exist, an exception is thrown.      **`IF     EXISTS`**      If the table does not exist, nothing happens.	Yes, when you drop a table, the underlying data gets deleted as well, but only if it is not an external table. If it is an external table, only the metadata is dropped, and the data remains in the external location.	1	1	1	True
171	no_reaction	https://docs.databricks.com/error-messages/user-defined-functions-error-class.html	can I trigger a user defined function based upon an update to a table	Functions ===========    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks Runtime       Spark SQL provides two function features to meet a wide range of needs: built-in functions and user-defined functions (UDFs).        Built-in functions --------------------     This article presents the usages and descriptions of categories of frequently used built-in functions for aggregation, arrays and maps, dates and timestamps, and JSON data.      * [Built-in functions](sql-ref-functions-builtin.html)        SQL user-defined functions ----------------------------     SQL user-defined functions (UDFs) are functions you can define yourself which can return scalar values or result sets.       See  [CREATE FUNCTION (SQL)](sql-ref-syntax-ddl-create-sql-function.html)  for more information.      ###   User-defined functions     UDFs allow you to define your own functions when the system’s built-in functions are not enough to perform the desired task. To use UDFs, you first define the function, then register the function with Spark, and finally call the registered function. A UDF can act on a single row or act on multiple rows at once. Spark SQL also supports integration of existing Hive implementations of UDFs, user defined aggregate functions (UDAF), and user defined table functions (UDTF).      * [User-defined aggregate functions (UDAFs)](sql-ref-functions-udf-aggregate.html) * [Integration with Hive UDFs, UDAFs, and UDTFs](sql-ref-functions-udf-hive.html) * [User-defined scalar functions (UDFs)](sql-ref-functions-udf-scalar.html)	Yes, you can trigger a user-defined function based upon an update to a table. However, the process for doing so may vary depending on the specific database or platform being used. The context provides information on user-defined functions (UDFs) in Spark SQL, which can be defined and registered with Spark to act on single or multiple rows.	1	1	1	True
172	no_reaction	https://docs.databricks.com/structured-streaming/tutorial.html	When to use delta live tables vs. structured streaming?	"      Run your first Structured Streaming workload ==============================================     This article provides code examples and explanation of basic concepts necessary to run your first Structured Streaming queries on Databricks. You can use Structured Streaming for near real-time and incremental processing workloads.       Structured Streaming is one of several technologies that power streaming tables in Delta Live Tables. Databricks recommends using Delta Live Tables for all new ETL, ingestion, and Structured Streaming workloads. See  [What is Delta Live Tables?](../delta-live-tables/index.html)  .        Note       While Delta Live Tables provides a slightly modified syntax for declaring streaming tables, the general syntax for configuring streaming reads and transformations applies to all streaming use cases on Databricks. Delta Live Tables also simplifies streaming by managing state information, metadata, and numerous configurations.         Read from a data stream -------------------------     You can use Structured Streaming to incrementally ingest data from supported data sources. Some of the most common data sources used in Databricks Structured Streaming workloads include the following:     * Data files in cloud object storage * Message buses and queues * Delta Lake     Databricks recommends using Auto Loader for streaming ingestion from cloud object storage. Auto Loader supports most file formats supported by Structured Streaming. See  [What is Auto Loader?](../ingestion/auto-loader/index.html)  .       Each data source provides a number of options to specify how to load batches of data. During reader configuration, the main options you might need to set fall into the following categories:     * Options that specify the data source or format (for example, file type, delimiters, and schema). * Options that configure access to source systems (for example, port settings and credentials). * Options that specify where to start in a stream (for example, Kafka offsets or reading all existing files). * Options that control how much data is processed in each batch (for example, max offsets, files, or bytes per batch).         Use Auto Loader to read streaming data from object storage ------------------------------------------------------------     The following example demonstrates loading JSON data with Auto Loader, which uses  `cloudFiles`  to denote format and options. The  `schemaLocation`  option enables schema inference and evolution. Paste the following code in a Databricks notebook cell and run the cell to create a streaming DataFrame named  `raw\_df`  :        ``` file\_path = ""/databricks-datasets/structured-streaming/events"" checkpoint\_path = ""/tmp/ss-tutorial/\_checkpoint""  raw\_df = (spark.readStream     .format(""cloudFiles"")     .option(""cloudFiles.format"", ""json"")     .option(""cloudFiles.schemaLocation"", checkpoint\_path)     .load(file\_path) )  ```      Like other read operations on Databricks, configuring a streaming read does not actually load data. You must trigger an action on the data before the stream begins.        Note       Calling  `display()`  on a streaming DataFrame starts a streaming job. For most Structured Streaming use cases, the action that triggers a stream should be writing data to a sink. See  [Preparing your Structured Streaming code for production](#production)  .          Perform a streaming transformation ------------------------------------     Structured Streaming supports most transformations that are available in Databricks and Spark SQL. You can even load MLflow models as UDFs and make streaming predictions as a transformation.       The following code example completes a simple transformation to enrich the ingested JSON data with additional information using Spark SQL functions:        ``` from pyspark.sql.functions import input\_file\_name, current\_timestamp  transformed\_df = (raw\_df.select(     ""\*"",     input\_file\_name().alias(""source\_file""),     current\_timestamp().alias(""processing\_time"")     ) )  ```      The resulting  `transformed\_df`  contains query instructions to load and transform each record as it arrives in the data source.        Note       Structured Streaming treats data sources as unbounded or infinite datasets. As such, some transformations are not supported in Structured Streaming workloads because they would require sorting an infinite number of items.       Most aggregations and many joins require managing state information with watermarks, windows, and output mode. See  [Apply watermarks to control data processing thresholds](watermarks.html)  .          Write to a data sink ----------------------     A data sink is the target of a streaming write operation. Common sinks used in Databricks streaming workloads include the following:     * Delta Lake * Message buses and queues * Key-value databases     As with data sources, most data sinks provide a number of options to control how data is written to the target system. During writer configuration, the main options you might need to set fall into the following categories:     * Output mode (append by default). * A checkpoint location (required for each  **writer**   ). * Trigger intervals; see  [Configure Structured Streaming trigger intervals](triggers.html)  . * Options that specify the data sink or format (for example, file type, delimiters, and schema). * Options that configure access to target systems (for example, port settings and credentials).         Perform an incremental batch write to Delta Lake --------------------------------------------------     The following example writes to Delta Lake using a specified file path and checkpoint.        Important       Always make sure you specify a unique checkpoint location for each streaming writer you configure. The checkpoint provides the unique identity for your stream, tracking all records processed and state information associated with your streaming query.        The  `availableNow`  setting for the trigger instructs Structured Streaming to process all previously unprocessed records from the source dataset and then shut down, so you can safely execute the following code without worrying about leaving a stream running:        ``` target\_path = ""/tmp/ss-tutorial/"" checkpoint\_path = ""/tmp/ss-tutorial/\_checkpoint""  transformed\_df.writeStream     .trigger(availableNow=True)     .option(""checkpointLocation"", checkpoint\_path)     .option(""path"", target\_path)     .start()  ```      In this example, no new records arrive in our data source, so repeat execution of this code does not ingest new records.        Warning       Structured Streaming execution can prevent auto termination from shutting down compute resources. To avoid unexpected costs, be sure to terminate streaming queries.              Preparing your Structured Streaming code for production ---------------------------------------------------------     Databricks recommends using Delta Live Tables for most Structured Streaming workloads. The following recommendations provide a starting point for preparing Structured Streaming workloads for production:     * Remove unnecessary code from notebooks that would return results, such as  `display`  and  `count`  . * Do not run Structured Streaming workloads on interactive clusters; always schedule streams as jobs. * To help streaming jobs recover automatically, configure jobs with infinite retries. * Do not use auto-scaling for workloads with Structured Streaming.     For more recommendations, see  [Production considerations for Structured Streaming](production.html)  .             Read data from Delta Lake, transform, and write to Delta Lake ---------------------------------------------------------------     Delta Lake has extensive support for working with Structured Streaming as both a source and a sink. See  [Table streaming reads and writes](delta-lake.html)  .       The following example shows example syntax to incrementally load all new records from a Delta table, join them with a snapshot of another Delta table, and write them to a Delta table:        ``` (spark.readStream     .table(""<table\_name1>"")     .join(spark.read.table(""<table\_name2>""), on=""<id>"", how=""left"")     .writeStream     .trigger(availableNow=True)     .option(""checkpointLocation"", ""<checkpoint\_path>"")     .toTable(""<table\_name3>"") )  ```      You must have proper permissions configured to read source tables and write to target tables and the specified checkpoint location. Fill in all parameters denoted with angle brackets (  `<>`  ) using the relevant values for your data sources and sinks.        Note       Delta Live Tables provides a fully declarative syntax for creating Delta Lake pipelines and manages properties like triggers and checkpoints automatically. See  [What is Delta Live Tables?](../delta-live-tables/index.html)  .            Read data from Kafka, transform, and write to Kafka -----------------------------------------------------     Apache Kafka and other messaging buses provide some of the lowest latency available for large datasets. You can use Databricks to apply transformations to data ingested from Kafka and then write data back to Kafka.        Note       Writing data to cloud object storage adds additional latency overhead. If you wish to store data from a messaging bus in Delta Lake but require the lowest latency possible for streaming workloads, Databricks recommends configuring separate streaming jobs to ingest data to the lakehouse and apply near real-time transformations for downstream messaging bus sinks.        The following code example demonstrates a simple pattern to enrich data from Kafka by joining it with data in a Delta table and then writing back to Kafka:        ``` (spark.readStream     .format(""kafka"")     .option(""kafka.bootstrap.servers"", ""<server:ip>"")     .option(""subscribe"", ""<topic>"")     .option(""startingOffsets"", ""latest"")     .load()     .join(spark.read.table(""<table\_name>""), on=""<id>"", how=""left"")     .writeStream     .format(""kafka"")     .option(""kafka.bootstrap.servers"", ""<server:ip>"")     .option(""topic"", ""<topic>"")     .option(""checkpointLocation"", ""<checkpoint\_path>"")     .start() )  ```      You must have proper permissions configured for access to your Kafka service. Fill in all parameters denoted with angle brackets (  `<>`  ) using the relevant values for your data sources and sinks. See  [Stream processing with Apache Kafka and Databricks](kafka.html)  .                                                        "	When to use delta live tables vs. structured streaming?	0	0	0	True
174	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/create-tables.html	when should you use an external table instead of managed table	"    Create tables ===============     This article introduces the concept of  *managed*   and  *external*   tables in Unity Catalog and describes how to create tables in Unity Catalog.        Note       When you create a table, be sure to reference a catalog that is governed by Unity Catalog. The catalog  `hive\_metastore`  appears in Data Explorer but is not considered governed by Unity Catalog. It is managed by your Databricks workspace’s Hive metastore. All other catalogs listed are governed by Unity Catalog.       You can use the Unity Catalog table upgrade interface to upgrade existing tables registered in the Hive metastore to Unity Catalog. See  [Upgrade tables and views to Unity Catalog](migrate.html)  .           Managed tables ----------------     Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly.       By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the  [Delta](../../delta/index.html)  table format.       When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.           External tables -----------------     External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses.       When you run  `DROP     TABLE`  on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a  `LOCATION`  path in your  `CREATE     TABLE`  statement. External tables can use the following file formats:     * DELTA * CSV * JSON * AVRO * PARQUET * ORC * TEXT     To manage access to the underlying cloud storage for an external table, you must set up  [storage credentials and external locations](manage-external-locations-and-credentials.html)  .       To learn more, see  [Create an external table](#create-an-external-table)  .         Requirements --------------     You must have the  `CREATE     TABLE`  privilege on the schema in which you want to create the table, as well as the  `USE     SCHEMA`  privilege on the schema and the  `USE     CATALOG`  privilege on the parent catalog.       If you are creating an external table, see  [Create an external table](#create-an-external-table)  for additional requirements.         Create a managed table ------------------------     To create a managed table, run the following SQL command. You can also use the  [example notebook](#example-notebook-managed-table)  to create a table. Items in brackets are optional. Replace the placeholder values:     * `<catalog\_name>`  : The name of the catalog. This cannot be the  `hive\_metastore`  catalog that is created automatically for the Hive metastore associated with your Databricks workspace. * `<schema\_name>`  : The name of the schema. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column.       ``` CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> (  <column\_specification> );  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> ""   ""(""   "" <column\_specification>""   "")"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "",  ""("",  "" <column\_specification>"",  "")"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE <catalog\_name>.<schema\_name>.<table\_name> "" +  ""("" +  "" <column\_specification>"" +  "")"")  ```       You can also create a managed table by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  and  [databricks\_table](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/table)   . You can retrieve a list of table full names by using  [databricks\_tables](https://registry.terraform.io/providers/databricks/databricks/latest/docs/data-sources/tables)   .       For example, to create the table  `main.default.department`  and insert five rows into it:         ``` CREATE TABLE main.default.department (  deptcode INT,  deptname STRING,  location STRING );  INSERT INTO main.default.department VALUES  (10, 'FINANCE', 'EDINBURGH'),  (20, 'SOFTWARE', 'PADDINGTON'),  (30, 'SALES', 'MAIDSTONE'),  (40, 'MARKETING', 'DARLINGTON'),  (50, 'ADMIN', 'BIRMINGHAM');  ```       ``` spark.sql(""CREATE TABLE main.default.department ""   ""(""   "" deptcode INT,""   "" deptname STRING,""   "" location STRING""   "")""   ""INSERT INTO main.default.department VALUES ""   "" (10, 'FINANCE', 'EDINBURGH'),""   "" (20, 'SOFTWARE', 'PADDINGTON'),""   "" (30, 'SALES', 'MAIDSTONE'),""   "" (40, 'MARKETING', 'DARLINGTON'),""   "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```       ``` library(SparkR)  sql(paste(""CREATE TABLE main.default.department "",  ""("",  "" deptcode INT,"",  "" deptname STRING,"",  "" location STRING"",  "")"",  ""INSERT INTO main.default.department VALUES "",  "" (10, 'FINANCE', 'EDINBURGH'),"",  "" (20, 'SOFTWARE', 'PADDINGTON'),"",  "" (30, 'SALES', 'MAIDSTONE'),"",  "" (40, 'MARKETING', 'DARLINGTON'),"",  "" (50, 'ADMIN', 'BIRMINGHAM')"",  sep = """"))  ```       ``` spark.sql(""CREATE TABLE main.default.department "" +  ""("" +  "" deptcode INT,"" +  "" deptname STRING,"" +  "" location STRING"" +  "")"" +  ""INSERT INTO main.default.department VALUES "" +  "" (10, 'FINANCE', 'EDINBURGH'),"" +  "" (20, 'SOFTWARE', 'PADDINGTON'),"" +  "" (30, 'SALES', 'MAIDSTONE'),"" +  "" (40, 'MARKETING', 'DARLINGTON'),"" +  "" (50, 'ADMIN', 'BIRMINGHAM')"")  ```        ###   Example notebooks for creating managed tables     You can use the following example notebooks to create a catalog, schema, and managed table, and to manage permissions on them.        ####   Create and manage a table in Unity Catalog with SQL     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-example-notebook.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                 ####   Create and manage a table in Unity Catalog with Python     [Open notebook in new tab](/_extras/notebooks/source/unity-catalog-quickstart-python.html)   ![Copy to clipboard](/_static/clippy.svg)  Copy link for import                   Drop a managed table ----------------------     You must be the table’s owner to drop a table. To drop a managed table, run the following SQL command:        ``` DROP TABLE IF EXISTS catalog\_name.schema\_name.table\_name;  ```      When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.         Create an external table --------------------------     The data in an external table is stored in a path on your cloud tenant. To work with external tables, Unity Catalog introduces two objects to access and work with external cloud storage:     * A  *storage credential*   contains an authentication method for accessing a cloud storage location. The storage credential does not contain a mapping to the path to which it grants access. Storage credentials are access-controlled to determine which users can use the credential. * An  *external location*   maps a storage credential with a cloud storage path to which it grants access. The external location grants access only to that cloud storage path and its contents. External locations are access-controlled to determine which users can use them. An external location is used automatically when your SQL command contains a  `LOCATION`  clause.      ###   Requirements     To create an external table, you must have:     * The  `CREATE     EXTERNAL     TABLE`  privilege on an external location or storage credential, that grants access to the  `LOCATION`  accessed by the external table. * The  `USE     SCHEMA`  permission on the table’s parent schema. * The  `USE     CATALOG`  permission on the table’s parent catalog. * The  `CREATE     TABLE`  permission on the table’s parent schema.     External locations and storage credentials are stored in the top level of the metastore, rather than in a catalog. To create a storage credential or an external location, you must be the metastore admin or an account-level admin. See  [Manage external locations and storage credentials](manage-external-locations-and-credentials.html)  .       ###   Create a table     Use one of the following command examples in a notebook or the SQL query editor to create an external table.       You can also use an  [example notebook](#example-notebook-external-table)  to create the storage credential, external location, and external table, and also manage permissions for them.       In the following examples, replace the placeholder values:     * `<catalog>`  : The name of the catalog that will contain the table. * `<schema>`  : The name of the schema that will contain the table. * `<table\_name>`  : A name for the table. * `<column\_specification>`  : The name and data type for each column. * `<bucket\_path>`  : The path on your cloud tenant where the table will be created. * `<table\_directory>`  : A directory where the table will be created. Use a unique directory for each table.      Important       Once a table is created in a path, users can no longer directly access the files in that path from Databricks even if they have been given privileges on an external location or storage credential to do so. This is to ensure that users cannot circumvent access controls applied to tables by reading files from your cloud tenant directly.          ``` CREATE TABLE <catalog>.<schema>.<table\_name> (  <column\_specification> ) LOCATION 's3://<bucket\_path>/<table\_directory>';  ```       ``` spark.sql(""CREATE TABLE <catalog>.<schema>.<table\_name> ""   ""(""   "" <column\_specification>""   "") ""   ""LOCATION 's"	You should use an external table instead of a managed table when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses. External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. When you run `DROP TABLE` on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. External tables can use various file formats such as DELTA, CSV, JSON, AVRO, PARQUET, ORC, and TEXT. To manage access to the underlying cloud storage for an external table, you must set up storage credentials and external locations.	1	1	1	True
175	no_reaction	https://docs.databricks.com/repos/index.html	repo git provider	Git integration with Databricks Repos =======================================     Databricks Repos is a visual Git client in Databricks. It supports common Git operations such a cloning a repository, committing and pushing, pulling, branch management, and visual comparison of diffs when committing.       Within Repos you can develop code in notebooks or other files and follow data science and engineering code development best practices using Git for version control, collaboration, and CI/CD.        What can you do with Databricks Repos? ----------------------------------------     Databricks Repos provides source control for data and AI projects by integrating with Git providers.       In Databricks Repos, you can use Git functionality to:     * Clone, push to, and pull from a remote Git repository. * Create and manage branches for development work. * Create notebooks, and edit notebooks and other files. * Visually compare differences upon commit.     For step-by-step instructions, see  [Clone a Git repo & other common Git operations](git-operations-with-repos.html)  . Databricks Repos also has an  [API](../dev-tools/api/latest/repos.html)  that you can integrate with your CI/CD pipeline. For example, you can programmatically update a Databricks repo so that it always has the most recent version of the code. For information about best practices for code development using Databricks Repos, see  [CI/CD workflows with Git integration and Databricks Repos](ci-cd-techniques-with-repos.html)  .       For following tasks, work in your Git provider:     * Create a pull request. * Resolve merge conflicts. * Merge or delete branches. * Rebase a branch.       Supported Git providers -------------------------     Databricks supports the following Git providers:     * GitHub * Bitbucket Cloud * GitLab * Azure DevOps * AWS CodeCommit * GitHub AE     See  [Get a Git access token & connect a remote repo to Databricks](get-access-tokens-from-git-provider.html)  .       Databricks Repos also supports Bitbucket Server, GitHub Enterprise Server, and GitLab self-managed integration, if the server is internet accessible. To integrate with a private Git server instance that is not internet-accessible, get in touch with your Databricks representative.       Support for arbitrary files in Databricks Repos is available in Databricks Runtime 8.4 and above. See  [What are workspace files?](../files/workspace.html)  .         Next step -----------    [Set up Databricks Repos](repos-setup.html)	Databricks Repos is a visual Git client that integrates with Git providers such as GitHub, Bitbucket Cloud, GitLab, Azure DevOps, AWS CodeCommit, and GitHub AE. It supports common Git operations such as cloning a repository, committing and pushing, pulling, branch management, and visual comparison of diffs when committing. Within Repos, you can develop code in notebooks or other files and follow data science and engineering code development best practices using Git for version control, collaboration, and CI/CD. Databricks Repos also has an API that you can integrate with your CI/CD pipeline. For more information about best practices for code development using Databricks Repos, see CI/CD workflows with Git integration and Databricks Repos.	1	1	1	True
177	no_reaction	https://docs.databricks.com/sql/language-manual/functions/current_metastore.html	how do i get the metastore id in databricks sql	`current\_metastore`  function ===============================    **Applies to:**  ![check marked yes](../../../_images/check.png)  Databricks SQL  ![check marked yes](../../../_images/check.png)  Databricks Runtime 11.1 and above       Returns the current metastore id.        Syntax --------      ``` current\_metastore()  ```        Arguments -----------     This function takes no arguments.         Returns ---------     A STRING.       The metastore id is of the form:  `<cloud>:<region>:<uuid>`        Examples ----------      ``` > SELECT current\_metastore();  azure:westus:f64dcb39-5046-4d4c-9588-c6ac84f9b38a  ```        Related functions -------------------   * [CREATE RECIPIENT](../sql-ref-syntax-ddl-create-recipient.html)	To get the metastore id in Databricks SQL, you can use the `current_metastore()` function, which returns the current metastore id in the form of `<cloud>:<region>:<uuid>`.	1	1	1	True
180	no_reaction	https://docs.databricks.com/files/download-internet-files.html	how to download a spark dataframe into a local machine in a csv format?	"   pyspark.pandas.Series.to\_csv  [¶](#pyspark-pandas-series-to-csv ""Permalink to this headline"") ================================================================================================  `Series.`  `to_csv`    (   *path     :     Optional    [    str    ]     =     None*  ,  *sep     :     str     =     ','*  ,  *na\_rep     :     str     =     ''*  ,  *columns     :     Optional[List[Union[Any, Tuple[Any, …]]]]     =     None*  ,  *header     :     bool     =     True*  ,  *quotechar     :     str     =     '""'*  ,  *date\_format     :     Optional    [    str    ]     =     None*  ,  *escapechar     :     Optional    [    str    ]     =     None*  ,  *num\_files     :     Optional    [    int    ]     =     None*  ,  *mode     :     str     =     'w'*  ,  *partition\_cols     :     Union[str, List[str], None]     =     None*  ,  *index\_col     :     Union[str, List[str], None]     =     None*  ,  *\*\*     options     :     Any*   )    → Optional    [    str    ]   [¶](#pyspark.pandas.Series.to_csv ""Permalink to this definition"")    Write object to a comma-separated values (csv) file.     Note     pandas-on-Spark    to\_csv    writes files to a path or URI. Unlike pandas’, pandas-on-Spark respects HDFS’s property such as ‘fs.default.name’.     Note     pandas-on-Spark writes CSV files into the directory,    path    , and writes multiple    part-…    files in the directory when    path    is specified. This behavior was inherited from Apache Spark. The number of partitions can be controlled by    num\_files    . This is deprecated. Use    DataFrame.spark.repartition    instead.     Parameters    **path: str, default None**    File path. If None is provided the result is returned as a string.    **sep: str, default ‘,’**    String of length 1. Field delimiter for the output file.    **na\_rep: str, default ‘’**    Missing data representation.    **columns: sequence, optional**    Columns to write.    **header: bool or list of str, default True**    Write out the column names. If a list of strings is given it is assumed to be aliases for the column names.    **quotechar: str, default ‘”’**    String of length 1. Character used to quote fields.    **date\_format: str, default None**    Format string for datetime objects.    **escapechar: str, default None**    String of length 1. Character used to escape    sep    and    quotechar    when appropriate.    **num\_files: the number of partitions to be written in `path` directory when**    this is a path. This is deprecated. Use    DataFrame.spark.repartition    instead.    **mode: str**    Python write mode, default ‘w’.     Note     mode can accept the strings for Spark writing mode. Such as ‘append’, ‘overwrite’, ‘ignore’, ‘error’, ‘errorifexists’.    * ‘append’ (equivalent to ‘a’): Append the new data to existing data. * ‘overwrite’ (equivalent to ‘w’): Overwrite existing data. * ‘ignore’: Silently ignore this operation if data already exists. * ‘error’ or ‘errorifexists’: Throw an exception if data already exists.  **partition\_cols: str or list of str, optional, default None**    Names of partitioning columns    **index\_col: str or list of str, optional, default: None**    Column names to be used in Spark to represent pandas-on-Spark’s index. The index name in pandas-on-Spark is ignored. By default, the index is always lost.    **options: keyword arguments for additional options specific to PySpark.**    These kwargs are specific to PySpark’s CSV options to pass. Check the options in PySpark’s API documentation for spark.write.csv(…). It has higher priority and overwrites all other options. This parameter only works when    path    is specified.     Returns     str or None     See also    [`read\_csv`](pyspark.pandas.read_csv.html#pyspark.pandas.read_csv ""pyspark.pandas.read_csv"")  [`DataFrame.to\_delta`](pyspark.pandas.DataFrame.to_delta.html#pyspark.pandas.DataFrame.to_delta ""pyspark.pandas.DataFrame.to_delta"")  [`DataFrame.to\_table`](pyspark.pandas.DataFrame.to_table.html#pyspark.pandas.DataFrame.to_table ""pyspark.pandas.DataFrame.to_table"")  [`DataFrame.to\_parquet`](pyspark.pandas.DataFrame.to_parquet.html#pyspark.pandas.DataFrame.to_parquet ""pyspark.pandas.DataFrame.to_parquet"")  [`DataFrame.to\_spark\_io`](pyspark.pandas.DataFrame.to_spark_io.html#pyspark.pandas.DataFrame.to_spark_io ""pyspark.pandas.DataFrame.to_spark_io"")   Examples    ``` >>> df = ps.DataFrame(dict( ...    date=list(pd.date\_range('2012-1-1 12:00:00', periods=3, freq='M')), ...    country=KR', 'US', 'JP, ...    code=[1, 2 ,3]), columns=date', 'country', 'code) >>> df.sort\_values(by=""date"")    date country code ... 2012-01-31 12:00:00      KR     1 ... 2012-02-29 12:00:00      US     2 ... 2012-03-31 12:00:00      JP     3  ```  ``` >>> print(df.to\_csv())   date,country,code 2012-01-31 12:00:00,KR,1 2012-02-29 12:00:00,US,2 2012-03-31 12:00:00,JP,3  ```  ``` >>> df.cummax().to\_csv(path=r'%s/to\_csv/foo.csv' % path, num\_files=1) >>> ps.read\_csv( ...    path=r'%s/to\_csv/foo.csv' % path ... ).sort\_values(by=""date"")    date country code ... 2012-01-31 12:00:00      KR     1 ... 2012-02-29 12:00:00      US     2 ... 2012-03-31 12:00:00      US     3  ```   In case of Series,    ``` >>> print(df.date.to\_csv())   date 2012-01-31 12:00:00 2012-02-29 12:00:00 2012-03-31 12:00:00  ```  ``` >>> df.date.to\_csv(path=r'%s/to\_csv/foo.csv' % path, num\_files=1) >>> ps.read\_csv( ...     path=r'%s/to\_csv/foo.csv' % path ... ).sort\_values(by=""date"")    date ... 2012-01-31 12:00:00 ... 2012-02-29 12:00:00 ... 2012-03-31 12:00:00  ```   You can preserve the index in the roundtrip as below.    ``` >>> df.set\_index(""country"", append=True, inplace=True) >>> df.date.to\_csv( ...     path=r'%s/to\_csv/bar.csv' % path, ...     num\_files=1, ...     index\_col=[""index1"", ""index2""]) >>> ps.read\_csv( ...     path=r'%s/to\_csv/bar.csv' % path, index\_col=[""index1"", ""index2""] ... ).sort\_values(by=""date"")    date index1 index2 ...    ...    2012-01-31 12:00:00 ...    ...    2012-02-29 12:00:00 ...    ...    2012-03-31 12:00:00  ```  [pyspark.pandas.Series.to\_json](pyspark.pandas.Series.to_json.html ""previous page"")  [pyspark.pandas.Series.to\_excel](pyspark.pandas.Series.to_excel.html ""next page"")   "	You can use the `to_csv` method of a Spark DataFrame to write it to a CSV file. The method takes a file path as an argument, and if you provide `None` as the file path, the result will be returned as a string. Here is an example:  ``` df.write.csv('/path/to/file.csv') ```  This will write the DataFrame to a CSV file at the specified path. If you want to write the DataFrame to a local file on your machine, you can use a file path that starts with `file://`. For example:  ``` df.write.csv('file:///path/to/file.csv') ```  This will write the DataFrame to a CSV file at `/path/to/file.csv` on your local machine.	1	1	1	True
182	no_reaction	https://docs.databricks.com/administration-guide/workspace/delete-workspace.html	how to delete my table?	DROP TABLE ============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime       Deletes the table and removes the directory associated with the table from the file system if the table is not  `EXTERNAL`  table. An exception is thrown if the table does not exist. To drop a table you must be its owner.       In case of an external table, only the associated metadata information is removed from the metastore schema.       Any foreign key constraints referencing the table are also dropped.       If the table is cached, the command uncaches the table and all its dependents.       When a managed table is dropped from Unity Catalog, its underlying data is deleted from your cloud tenant within 30 days.        Syntax --------      ``` DROP TABLE [ IF EXISTS ] table\_name  ```        Parameter -----------   * **IF EXISTS**       If specified, no  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error is thrown when the table does not exist. * **[table\_name](sql-ref-names.html#table-name)**      The name of the table to be created. The name must not include a  [temporal specification](sql-ref-names.html#table-name)  . If the table cannot be found Databricks raises a  [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html)  error.       Examples ----------      ``` -- Assumes a table named `employeetable` exists. > DROP TABLE employeetable;  -- Assumes a table named `employeetable` exists in the `userdb` schema > DROP TABLE userdb.employeetable;  -- Assumes a table named `employeetable` does not exist. -- Throws TABLE\_OR\_VIEW\_NOT\_FOUND > DROP TABLE employeetable;  Error: TABLE\_OR\_VIEW\_NOT\_FOUND  -- Assumes a table named `employeetable` does not exist,Try with IF EXISTS -- this time it will not throw exception > DROP TABLE IF EXISTS employeetable;  ```        Related articles ------------------   * [CREATE TABLE](sql-ref-syntax-ddl-create-table.html) * [CREATE SCHEMA](sql-ref-syntax-ddl-create-schema.html) * [DROP SCHEMA](sql-ref-syntax-ddl-drop-schema.html)	To delete a table in Databricks, you can use the `DROP TABLE` command. The syntax is as follows:   ``` DROP TABLE [ IF EXISTS ] table_name ```  Where `table_name` is the name of the table you want to delete. If the table is not an `EXTERNAL` table, the command will also remove the directory associated with the table from the file system. If the table is cached, the command will also uncaches the table and all its dependents. If the table is a managed table, its underlying data is deleted from your cloud tenant within 30 days.	1	1	1	True
183	no_reaction	https://docs.databricks.com/introduction/index.html	What is databricks?	What is Databricks? =====================     Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.        What is Databricks used for? ------------------------------     Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.       The Databricks workspace provides a unified interface and tools for most data tasks, including:     * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git     In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:     * REST API * CLI * Terraform       Managed integration with open source --------------------------------------     Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:     * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)   and  [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)     Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:     * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)         How does Databricks work with AWS? ------------------------------------     The Databricks platform architecture comprises two primary parts:     * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.     Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.       Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.       Databricks workspaces meet the security and networking requirements of  [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)   . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.         What are common use cases for Databricks? -------------------------------------------     Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.         Build an enterprise data lakehouse ------------------------------------     The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See  [What is the Databricks Lakehouse?](../lakehouse/index.html)  .         ETL and data engineering --------------------------     Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.      [Delta Live Tables](../delta-live-tables/index.html)  simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.       Databricks provides a number of custom tools for  [data ingestion](../ingestion/index.html)  , including  [Auto Loader](../ingestion/auto-loader/index.html)  , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.         Machine learning, AI, and data science ----------------------------------------     Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including  [MLflow](../mlflow/index.html)  and the  [Databricks Runtime for Machine Learning](../runtime/mlruntime.html)  . See  [Introduction to Databricks Machine Learning](../machine-learning/index.html)  .         Data warehousing, analytics, and BI -------------------------------------     Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as  [SQL warehouses](../sql/admin/create-sql-warehouse.html)  , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the  [SQL query editor](../sql/user/queries/queries.html)  or in notebooks.  [Notebooks](../notebooks/index.html)  support Python, R, and Scala in addition to SQL, and allow users to embed the same  [visualizations](../visualizations/index.html)  available in  [dashboards](../sql/user/dashboards/index.html)  alongside links, images, and commentary written in markdown.         Data governance and secure data sharing -----------------------------------------     Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.       Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See  [What is Unity Catalog?](../data-governance/unity-catalog/index.html)  .       The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of  [Delta Sharing](../data-sharing/index.html)  .         DevOps, CI/CD, and task orchestration ---------------------------------------     The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations.  [Workflows](../workflows/index.html)  schedule Databricks notebooks, SQL queries, and other arbitrary code.  [Repos](../repos/index.html)  let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see  [Developer tools and guidance](../dev-tools/index.html)  .         Real-time and streaming analytics -----------------------------------     Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See  [What is Apache Spark Structured Streaming?](../structured-streaming/index.html)  .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used to process, store, clean, share, analyze, model, and monetize datasets with solutions from BI to machine learning. The Databricks platform provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks is used by customers to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.	1	1	1	True
185	no_reaction	https://docs.databricks.com/security/network/ip-access-list.html	How do I limit IP addresses	Title: IP address limit prevents cluster creation  Description: Learn how to fix a public IP address quota limit Cloud Provider Launch error when starting a Databricks cluster.  Answer: Problem Cluster creation fails with a message about a cloud provider error when you hover over cluster state. Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. When you view the cluster event log to get more details, you see a message about publicIPAddresses limits. ResourceQuotaExceeded Azure error message: Creating the resource of type 'Microsoft.Network/publicIPAddresses' would exceed the quota of '800' resources of type 'Microsoft.Network/publicIPAddresses' per resource group. The current resource count is '800', please delete some resources of this type before creating a new one.' Cause Azure subscriptions have a public IP address limit which restricts the number of public IP addresses you can use. This is a hard limit. If you try to start a cluster that would result in your account exceeding the public IP address quota the cluster launch will fail. Solution You can either free up resources or request a quota increase for your account. Stop inactive clusters to free up public IP addresses for use. Open an Azure support case with a request to increase the public IP address quota limit for your subscription.	To limit IP addresses, you can either free up resources or request a quota increase for your account. Azure subscriptions have a public IP address limit which restricts the number of public IP addresses you can use. If you try to start a cluster that would result in your account exceeding the public IP address quota the cluster launch will fail. Stop inactive clusters to free up public IP addresses for use. Open an Azure support case with a request to increase the public IP address quota limit for your subscription.	1	1	1	True
186	no_reaction	https://kb.databricks.com/streaming/explicit-path-to-data-or-a-defined-schema-required-for-auto-loader	Cannot infer schema when the input path `s3://s3-be87-dropzone-dpc/raw/DPC/CDC_2S10_N.strcimaster__ct/` is empty. Please try to start the stream when there are files in the input path, or specify the schema. how to specify the schema	"   pyspark.sql.streaming.DataStreamReader.schema  [¶](#pyspark-sql-streaming-datastreamreader-schema ""Permalink to this headline"") =================================================================================================================================  `DataStreamReader.`  `schema`    (   *schema     :     Union    [   [pyspark.sql.types.StructType](../../pyspark.sql/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType ""pyspark.sql.types.StructType"")    ,    str    ]*   )    → pyspark.sql.streaming.readwriter.DataStreamReader  [[source]](../../../_modules/pyspark/sql/streaming/readwriter.html#DataStreamReader.schema) [¶](#pyspark.sql.streaming.DataStreamReader.schema ""Permalink to this definition"")    Specifies the input schema.     Some data sources (e.g. JSON) can infer the input schema automatically from data. By specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading.     New in version 2.0.0.     Parameters    **schema**   [`pyspark.sql.types.StructType`](../../pyspark.sql/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType ""pyspark.sql.types.StructType"")  or str     a  [`pyspark.sql.types.StructType`](../../pyspark.sql/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType ""pyspark.sql.types.StructType"")  object or a DDL-formatted string (For example  `col0     INT,     col1     DOUBLE`  ).     Notes     This API is evolving.     Examples    ``` >>> from pyspark.sql.types import StructField, StructType, StringType >>> spark.readStream.schema(StructType([StructField(""data"", StringType(), True)])) <pyspark.sql.streaming.readwriter.DataStreamReader object ...> >>> spark.readStream.schema(""col0 INT, col1 DOUBLE"") <pyspark.sql.streaming.readwriter.DataStreamReader object ...>  ```   The example below specifies a different schema to CSV file.    ``` >>> import tempfile >>> import time >>> with tempfile.TemporaryDirectory() as d: ...     # Start a streaming query to read the CSV file. ...     spark.readStream.schema(""col0 INT, col1 STRING"").format(""csv"").load(d).printSchema() root  |-- col0: integer (nullable = true)  |-- col1: string (nullable = true)  ```  [pyspark.sql.streaming.DataStreamReader.parquet](pyspark.sql.streaming.DataStreamReader.parquet.html ""previous page"")  [pyspark.sql.streaming.DataStreamReader.table](pyspark.sql.streaming.DataStreamReader.table.html ""next page"")   "	"To specify the schema in PySpark, you can use the `schema` method of the `DataStreamReader` class. This method takes a `pyspark.sql.types.StructType` object or a DDL-formatted string as input. By specifying the schema, the underlying data source can skip the schema inference step and speed up data loading. Here is an example:  ``` from pyspark.sql.types import StructField, StructType, StringType  schema = StructType([     StructField(""col0"", StringType(), True),     StructField(""col1"", StringType(), True) ])  df = spark.readStream.schema(schema).csv(""path/to/csv"") ```  In this example, we create a `StructType` object that defines the schema of the CSV file. We then pass this schema to the `schema` method of the `DataStreamReader` object, which reads the CSV file and applies the specified schema."	1	1	1	True
188	no_reaction	https://docs.databricks.com/getting-started/dataframes-python.html	How to load and transform data using the Apache Spark Python (PySpark) DataFrame API in Databricks?	"    Tutorial: Work with PySpark DataFrames on Databricks ======================================================     This article shows you how to load and transform data using the Apache Spark Python (PySpark) DataFrame API in Databricks.       See also  [Apache Spark PySpark API reference](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrame.html#pyspark-sql-dataframe)   .        What is a DataFrame? ----------------------     A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently.       Apache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Spark DataFrames and Spark SQL use a unified planning and optimization engine, allowing you to get nearly identical performance across all supported languages on Databricks (Python, SQL, Scala, and R).         Create a DataFrame with Python --------------------------------     Most Apache Spark queries return a DataFrame. This includes reading from a table, loading data from files, and operations that transform data.       You can also create a Spark DataFrame from a list or a pandas DataFrame, such as in the following example:        ``` import pandas as pd  data = [[1, ""Elia""], [2, ""Teo""], [3, ""Fang""]]  pdf = pd.DataFrame(data, columns=[""id"", ""name""])  df1 = spark.createDataFrame(pdf) df2 = spark.createDataFrame(data, schema=""id LONG, name STRING"")  ```        Read a table into a DataFrame -------------------------------     Databricks uses Delta Lake for all tables by default. You can easily load tables to DataFrames, such as in the following example:        ``` spark.read.table(""<catalog\_name>.<schema\_name>.<table\_name>"")  ```        Load data into a DataFrame from files ---------------------------------------     You can load data from many supported  [file formats](../external-data/index.html)  . The following example uses a dataset available in the  `/databricks-datasets`  directory, accessible from most workspaces. See  [Sample datasets](../dbfs/databricks-datasets.html)  .        ``` df = (spark.read   .format(""csv"")   .option(""header"", ""true"")   .option(""inferSchema"", ""true"")   .load(""/databricks-datasets/samples/population-vs-price/data\_geo.csv"") )  ```        Assign transformation steps to a DataFrame --------------------------------------------     The results of most Spark transformations return a DataFrame. You can assign these results back to a DataFrame variable, similar to how you might use CTEs, temp views, or DataFrames in other systems.         Combine DataFrames with join and union ----------------------------------------     DataFrames use standard SQL semantics for join operations. A join returns the combined results of two DataFrames based on the provided matching conditions and join type. The following example is an inner join, which is the default:        ``` joined\_df = df1.join(df2, how=""inner"", on=""id"")  ```      You can add the rows of one DataFrame to another using the union operation, as in the following example:        ``` unioned\_df = df1.union(df2)  ```        Filter rows in a DataFrame ----------------------------     You can filter rows in a DataFrame using  `.filter()`  or  `.where()`  . There is no difference in performance or syntax, as seen in the following example:        ``` filtered\_df = df.filter(""id > 1"")  filtered\_df = df.where(""id > 1"")  ```      Use filtering to select a subset of rows to return or modify in a DataFrame.         Select columns from a DataFrame ---------------------------------     You can select columns by passing one or more column names to  `.select()`  , as in the following example:        ``` select\_df = df.select(""id"", ""name"")  ```      You can combine select and filter queries to limit rows and columns returned.        ``` subset\_df = df.filter(""id > 1"").select(""name"")  ```          View the DataFrame --------------------     To view this data in a tabular format, you can use the Databricks  `display()`  command, as in the following example:        ``` display(df)  ```        Print the data schema -----------------------     Spark uses the term  *schema*   to refer to the names and data types of the columns in the DataFrame.        Note       Databricks also uses the term schema to describe a collection of tables registered to a catalog.        You can print the schema using the  `.printSchema()`  method, as in the following example:        ``` df.printSchema()  ```        Save a DataFrame to a table -----------------------------     Databricks uses Delta Lake for all tables by default. You can save the contents of a DataFrame to a table using the following syntax:        ``` df.write.saveAsTable(""<table\_name>"")  ```        Write a DataFrame to a collection of files --------------------------------------------     Most Spark applications are designed to work on large datasets and work in a distributed fashion, and Spark writes out a directory of files rather than a single file. Many data systems are configured to read these directories of files. Databricks recommends using tables over filepaths for most applications.       The following example saves a directory of JSON files:        ``` df.write.format(""json"").save(""/tmp/json\_data"")  ```          Run SQL queries in PySpark ----------------------------     Spark DataFrames provide a number of options to combine SQL with Python.       The  `selectExpr()`  method allows you to specify each column as a SQL query, such as in the following example:        ``` display(df.selectExpr(""id"", ""upper(name) as big\_name""))  ```      You can import the  `expr()`  function from  `pyspark.sql.functions`  to use SQL syntax anywhere a column would be specified, as in the following example:        ``` from pyspark.sql.functions import expr  display(df.select(""id"", expr(""lower(name) as little\_name"")))  ```      You can also use  `spark.sql()`  to run arbitrary SQL queries in the Python kernel, as in the following example:        ``` query\_df = spark.sql(""SELECT \* FROM <table\_name>"")  ```      Because logic is executed in the Python kernel and all SQL queries are passed as strings, you can use Python formatting to parameterize SQL queries, as in the following example:        ``` table\_name = ""my\_table""  query\_df = spark.sql(f""SELECT \* FROM {table\_name}"")  ```                                                       "	To load and transform data using the Apache Spark Python (PySpark) DataFrame API in Databricks, what are the steps to follow?	0	0	0	True
189	no_reaction	https://docs.databricks.com/mlflow/migrate-mlflow-objects.html	How do I share MLFlow across 2 workspaces?	Copy MLflow objects between workspaces ========================================     To import or export MLflow objects to or from your Databricks workspace, you can use the community-driven open source project  [MLflow Export-Import](https://github.com/mlflow/mlflow-export-import#why-use-mlflow-export-import)   to migrate MLflow experiments, models, and runs between workspaces.       With these tools, you can:     * Share and collaborate with other data scientists in the same or another tracking server. For example, you can clone an experiment from another user into your workspace. * Copy a model from one workspace to another, such as from a development to a production workspace. * Copy MLflow experiments and runs from your local tracking server to your Databricks workspace. * Back up mission critical experiments and models to another Databricks workspace.	To share MLflow across 2 workspaces, you can use the MLflow Export-Import open source project to migrate MLflow experiments, models, and runs between workspaces. This allows you to copy a model from one workspace to another, such as from a development to a production workspace. You can also back up mission critical experiments and models to another Databricks workspace.	1	1	1	True
191	no_reaction	https://docs.databricks.com/data/manage-storage-credentials.html	who can add a new IAM role for storage credential in unity catalog	"    Storage credentials =====================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime  ![check marked yes](../../_images/check.png)  Unity Catalog only       Unity Catalog and the built-in Databricks Hive metastore use default locations for managed tables. Unity Catalog introduces several new securable objects to grant privileges to data in cloud object storage.     * **[storage credential](#)**      A Unity Catalog object used to abstract long term credentials from cloud storage providers. * **[external location](sql-ref-external-locations.html)**      A Unity Catalog object used to associate a cloud object storage URI with a  [storage credential](#)  . * **[external table](sql-ref-external-tables.html)**      A Unity Catalog  [table](sql-ref-syntax-ddl-create-table-using.html)  created in a Unity Catalog-managed  [external location](sql-ref-external-locations.html)  .      Storage credential --------------------     A storage credential is a securable object representing an AWS IAM role.       Once a storage credential is created access to it can be granted to  [principals](sql-ref-principal.html)  (users and groups).       Storage credentials are primarily used to create  [external locations](sql-ref-external-locations.html)  , which scope access to a specific storage path.      [Storage credential names](sql-ref-names.html#credential-name)  are unqualified and must be unique within the metastore.         Graphical Representation of relationships -------------------------------------------     The following diagram describes the relationship between:     * storage credentials * external locations * external tables * storage paths * IAM entities * Azure service accounts    ![External location ER diagram](../../_images/external-location-er-diagram.png)        Examples ----------     Using CLI create a storage credential  `my\_storage\_cred`  for an AWS IAM role.        ``` databricks unity-catalog create-storage-credential --json '{""name"": ""my\_storage\_cred"", ""aws\_iam\_role"": {""role\_arn"": ""arn:aws:iam::123456789:role/us-west-2-my-account""}}'  ```      The rest of the commands can be run within SQL.        ``` -- Grant access to the storage credential > GRANT READ FILES ON STORAGE CREDENTIAL my\_aws\_storage\_cred TO ceo;  -- ceo can directly read from any storage path using my\_aws\_storage\_cred > SELECT count(1) FROM `delta`.`s3://depts/finance/forecast/somefile` WITH (CREDENTIAL my\_aws\_storage\_cred);  100 > SELECT count(1) FROM `delta`.`s3://depts/hr/employees` WITH (CREDENTIAL my\_aws\_storage\_cred);  2017  -- Create an external location on specific path to which `my\_aws\_storage\_cred` has access > CREATE EXTERNAL LOCATION finance\_loc URL 's3://depts/finance'  WITH (CREDENTIAL my\_aws\_storage\_cred)  COMMENT 'finance';  ```        Related articles ------------------   * [Create a storage credential (CLI)](../../dev-tools/cli/unity-catalog-cli.html#create-a-storage-credential) * [ALTER STORAGE CREDENTIAL](sql-ref-syntax-ddl-alter-credential.html) * [ALTER TABLE](sql-ref-syntax-ddl-alter-table.html) * [CREATE LOCATION](sql-ref-syntax-ddl-create-location.html) * [DESCRIBE STORAGE CREDENTIAL](sql-ref-syntax-aux-describe-credential.html) * [DESCRIBE TABLE](sql-ref-syntax-aux-describe-table.html) * [DROP STORAGE CREDENTIAL](sql-ref-syntax-ddl-drop-credential.html) * [DROP TABLE](sql-ref-syntax-ddl-drop-table.html) * [SHOW STORAGE CREDENTIALS](sql-ref-syntax-aux-show-credentials.html) * [SHOW TABLES](sql-ref-syntax-aux-show-tables.html) * [GRANT](security-grant.html) * [REVOKE](security-revoke.html)                                                      "	To add a new IAM role for storage credential in Unity Catalog, you can use the CLI command `databricks unity-catalog create-storage-credential` and specify the name of the credential and the AWS IAM role ARN. Once the storage credential is created, access to it can be granted to principals (users and groups) using the `GRANT` command.	1	1	1	True
192	thumb_down	https://kb.databricks.com/training/how-can-i-reset-my-password-in-databricks-academy	how to reset your Databricks password	Title: How can I reset my password in Databricks Academy?  Description: Learn step-by-step instructions for resetting your password in Databricks Academy.  Answer: Step 1: Navigate to your Academy login page. https://customer-academy.databricks.com/learn   https://partner-academy.databricks.com/learn https://microsoft-academy.databricks.com/learn Step 2: Select forgot password. Step 3: Enter the email address and click on send reset link. Step 4: Go to your email inbox. Step 5: Find the link and reset the password.  	To reset your password in Databricks Academy, you can follow these steps:  Step 1: Navigate to your Academy login page. https://customer-academy.databricks.com/learn   https://partner-academy.databricks.com/learn https://microsoft-academy.databricks.com/learn  Step 2: Select forgot password.  Step 3: Enter the email address and click on send reset link.  Step 4: Go to your email inbox.  Step 5: Find the link and reset the password.	1	1	1	True
194	no_reaction	https://docs.databricks.com/data-governance/unity-catalog/enable-workspaces.html	How do I use unity catalog to share metastores between workspaces?	Create a Unity Catalog metastore ==================================     This article shows how to create a metastore in Unity Catalog and link it to workspaces. A metastore is the top-level container of objects in Unity Catalog. It stores metadata about data assets (tables and views) and the permissions that govern access to them. You must create a metastore for each region in which your organization operates.        Note       In addition to the approaches described in this article, you can also create a metastore by using the  [Databricks Terraform provider](../../dev-tools/terraform/index.html)  , specifically the  [databricks\_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore)   resource. To enable Unity Catalog to access the metastore, use  [databricks\_metastore\_data\_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access)   . To link workspaces to a metastore, use  [databricks\_metastore\_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment)   .         Requirements --------------   * You must be a Databricks account admin. * Your Databricks account must be on the  [Premium plan or above](https://databricks.com/product/aws-pricing)   . * In AWS, you must have the ability to create S3 buckets, IAM roles, IAM policies, and cross-account trust relationships.       Create the metastore ----------------------     To create a Unity Catalog metastore:     1. Configure a storage bucket and IAM role in AWS.       This bucket will store all of the metastore’s  [managed tables](create-tables.html#managed-table)  , except those that are in a catalog or schema with their own managed storage location.       When you create the bucket:      * Create it in the same region as the workspaces you will to use to access the data.  * Use a dedicated S3 bucket for each metastore that you create.  * Do not allow direct user access to the bucket.  For instructions, see  [Configure a storage bucket and IAM role in AWS](get-started.html#cloud-tenant-setup-aws)  . 2. Log in to the Databricks  [account console](https://accounts.cloud.databricks.com/)   . 3. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 4. Click  **Create Metastore**   .      1. Enter a name for the metastore.  2. Enter the region where the metastore will be deployed.            This must be the same region as the workspaces you want to use to access the data. Make sure that this matches the region of the cloud storage bucket you created earlier.  3. Enter the S3 bucket path (you can omit   `s3://`   ) and IAM role name that you created in step 1. 5. Click  **Create**   . 6. When prompted, select workspaces to link to the metastore.       For more information about linking workspaces to metastores, see  [Enable a workspace for Unity Catalog](enable-workspaces.html)  .     The user who creates a metastore is its original metastore admin. Databricks recommends that you reassign the original metastore admin to a group. See  [(Recommended) Transfer ownership of your metastore to a group](get-started.html#transfer-ownership)  .           Enable Databricks management for personal staging locations -------------------------------------------------------------     Databricks uses cross-origin resource sharing (CORS) to upload data to personal staging locations in Unity Catalog. See  [Configure Unity Catalog storage account for CORS](storage-cors.html)  .         Next steps ------------   * [Enable a workspace for Unity Catalog](enable-workspaces.html) * [Create and manage catalogs](create-catalogs.html) * [Create and manage schemas (databases)](create-schemas.html) * [Create tables](create-tables.html) * Learn more about  [Unity Catalog](index.html)         Delete a metastore --------------------     If you are closing your Databricks account or have another reason to delete access to data managed by your Unity Catalog metastore, you can delete the metastore.        Warning       All objects managed by the metastore will become inaccessible using Databricks workspaces. This action cannot be undone.      [Managed table](create-tables.html#managed-table)  data and metadata will be auto-deleted after 30 days. External table data in your cloud storage is not affected by metastore deletion.        To delete a metastore:     1. As a metastore admin, log in to the  [account console](https://accounts.cloud.databricks.com)   . 2. Click  ![Data Icon](../../_images/data-icon.png) **Data**   . 3. Click the metastore name. 4. On the  **Configuration**   tab, click the three-button menu at the far upper right and select  **Delete**   . 5. On the confirmation dialog, enter the name of the metastore and click  **Delete**   .	To use Unity Catalog to share metastores between workspaces, you need to create a metastore in Unity Catalog and link it to workspaces. To create a metastore, you must be a Databricks account admin, and your Databricks account must be on the Premium plan or above. You also need to configure a storage bucket and IAM role in AWS, and create the metastore in the same region as the workspaces you want to use to access the data. Once you have created the metastore, you can link workspaces to it. For more information about linking workspaces to metastores, see Enable a workspace for Unity Catalog.	1	1	1	True
195	no_reaction	https://docs.databricks.com/delta-live-tables/updates.html	update delta table based on temp view	Update (Delta Lake on Databricks) ===================================      Important       This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See  [UPDATE](../../sql/language-manual/delta-update.html)  .         ``` UPDATE [db\_name.]table\_name [AS alias] SET col1 = value1 [, col2 = value2 ...] [WHERE predicate]  ```      Update the column values for the rows that match a predicate. When no predicate is provided, update the column values for all rows.        Note       (Delta Lake on Databricks) If a column has a  `NOT     NULL` [constraint](create-table.html#create-table-delta)  , and an  `INSERT     INTO`  statement sets a column value to  `NULL`  , a  `SparkException`  is thrown.       **`WHERE`**      Filter rows by predicate.        Example ---------      ``` UPDATE events SET eventType = 'click' WHERE eventType = 'clk'  ```     `UPDATE`  supports subqueries in the  `WHERE`  predicate, including  `IN`  ,  `NOT     IN`  ,  `EXISTS`  ,  `NOT     EXISTS`  , and scalar subqueries.         Subquery Examples -------------------      ``` UPDATE all\_events  SET session\_time = 0, ignored = true  WHERE session\_time < (SELECT min(session\_time) FROM good\_events)  UPDATE orders AS t1  SET order\_status = 'returned'  WHERE EXISTS (SELECT oid FROM returned\_orders WHERE t1.oid = oid)  UPDATE events  SET category = 'undefined'  WHERE category NOT IN (SELECT category FROM events2 WHERE date > '2001-01-01')  ```       Note       The following types of subqueries are not supported:     * Nested subqueries, that is, a subquery inside another subquery * A  `NOT     IN`  subquery inside an  `OR`  , for example,  `a     =     3     OR     b     NOT     IN     (SELECT     c     from     t)`     In most cases, you can rewrite  `NOT     IN`  subqueries using  `NOT     EXISTS`  . We recommend using  `NOT     EXISTS`  whenever possible, as  `UPDATE`  with  `NOT     IN`  subqueries can be slow.	The context is about updating Delta tables in Databricks using the `UPDATE` command with the `WHERE` clause to filter rows by predicate. It also provides examples of using subqueries in the `WHERE` predicate. However, it does not provide information on how to update Delta tables based on temp view.	1	0	0	True
196	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-credential.html	how to create storage credential	DESCRIBE STORAGE CREDENTIAL =============================    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.3 and above  ![check marked yes](../../_images/check.png)  Unity Catalog only       Returns the metadata of an existing storage credential. The metadata information includes credential name, comment, owner and other metadata.       You must be account or metastore admin to execute this command.        Syntax --------      ``` DESCRIBE STORAGE CREDENTIAL credential\_name  ```        Parameters ------------   * **[credential\_name](sql-ref-names.html#credential-name)**      The name of an existing storage credential in the metastore. If the name does not exist, an exception is thrown.       Examples ----------      ``` > DESCRIBE CREDENTIAL good\_cred;  name owner created\_at created\_by credential  --------- ------ ------------------------ ------------ ---------------------------------------------  good\_cred admins 2022-01-01T08:00:00.0000 jane@doe.com AwsIamRole:arn:aws:iam:123456789012:roe/us....  ```        Related articles ------------------   * [ALTER STORAGE CREDENTIAL](sql-ref-syntax-ddl-alter-credential.html) * [DROP STORAGE CREDENTIAL](sql-ref-syntax-ddl-drop-credential.html) * [SHOW STORAGE CREDENTIALS](sql-ref-syntax-aux-show-credentials.html) * [INFORMATION\_SCHEMA.STORAGE\_CREDENTIALS](information-schema/storage_credentials.html)	To create a storage credential, you can use the `CREATE STORAGE CREDENTIAL` command in Databricks. Here is an example syntax:  ``` CREATE STORAGE CREDENTIAL <credential_name>   [COMMENT '<credential_comment>   [AWS_ACCESS_KEY_ID '<access_key_id>   [AWS_SECRET_ACCESS_KEY '<secret_access_key>   [AZURE_SAS_TOKEN '<sas_token>   [AZURE_STORAGE_ACCOUNT '<storage_account>   [AZURE_STORAGE_KEY '<storage_key>   [GOOGLE_APPLICATION_CREDENTIALS '<path_to_credentials_file>   [SCOPE '<scope> ```  You can replace the placeholders with the actual values for your storage credential. Note that the available options depend on the type of storage you are using.	1	1	1	True
197	no_reaction	https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-use-schema.html	what is the equivalent in sql of schema() to specify the schema to use ?	USE SCHEMA ============    **Applies to:**  ![check marked yes](../../_images/check.png)  Databricks SQL  ![check marked yes](../../_images/check.png)  Databricks Runtime 10.2 and above       Sets the current schema. After the current schema is set, unqualified references to objects such as tables, functions, and views that are referenced by SQLs are resolved from the current schema. The default schema name is  `default`  .       While usage of  `SCHEMA`  and  `DATABASE`  is interchangeable,  `SCHEMA`  is preferred.        Syntax --------      ``` USE [SCHEMA] schema\_name  ```        Parameter -----------   * **[schema\_name](sql-ref-names.html#schema-name)**      Name of the schema to use. If  `schema\_name`  is qualified the current catalog is also set to the specified catalog name. If the schema does not exist, an exception is thrown.       Examples ----------      ``` -- Use the 'userschema' which exists. > USE SCHEMA userschema;  -- Use the 'userschema1' which doesn't exist > USE SCHEMA userschema1;  Error: Database 'userschema1' not found;  -- Setting the catalog resets the schema to `default` > USE CATALOG some\_cat; > SELECT current\_schema(), current\_catalog();  some\_cat default  -- Setting the schema within the current catalog > USE SCHEMA some\_schem; > SELECT current\_schema(), current\_catalog();  some\_cat some\_schema  -- Resetting both catalog and schema > USE SCHEMA main.my\_schema; > SELECT current\_schema(), current\_catalog();  main my\_schema  -- Setting the catalog resets the schema to `default` again > USE CATALOG some\_cat; > SELECT current\_schema(), current\_catalog();  some\_cat default  ```        Related articles ------------------   * [CREATE SCHEMA](sql-ref-syntax-ddl-create-schema.html) * [DROP SCHEMA](sql-ref-syntax-ddl-drop-schema.html)	In SQL, the equivalent of `schema()` to specify the schema to use is `USE SCHEMA schema_name`.	1	1	1	True
