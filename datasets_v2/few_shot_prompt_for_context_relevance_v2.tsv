Question	Document	Label
How do I load files into Databricks?	"LOAD DATA =========== **Applies to:** ![check marked yes](../../_images/check.png) Databricks Runtime Loads the data into a Hive SerDe table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the `LOAD DATA` statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the table or the dependents are accessed the next time. Syntax -------- ``` LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table\_name [ PARTITION clause ] ``` Parameters ------------ * **path** Path of the file system. It can be either an absolute or a relative path. * **[table\_name](sql-ref-names.html#table-name)** Identifies the table to be inserted to. The name must not include a [temporal specification](sql-ref-names.html#table-name) . If the table cannot be found Databricks raises a [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html) error. * **[PARTITION clause](sql-ref-partition.html#partition)** An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition. * **LOCAL** If specified, it causes the `INPATH` to be resolved against the local file system, instead of the default file system, which is typically a distributed storage. * **OVERWRITE** By default, new data is appended to the table. If `OVERWRITE` is used, the table is instead overwritten with new data. Examples ---------- ``` -- Example without partition specification. -- Assuming the students table has already been created and populated. > SELECT \* FROM students; name address student\_id --------- ---------------------- ---------- Amy Smith 123 Park Ave, San Jose 111111 > CREATE TABLE test\_load (name VARCHAR(64), address VARCHAR(64), student\_id INT) USING HIVE; -- Assuming the students table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test\_load; > SELECT \* FROM test\_load; name address student\_id --------- ---------------------- ---------- Amy Smith 123 Park Ave, San Jose 111111 -- Example with partition specification. > CREATE TABLE test\_partition (c1 INT, c2 INT, c3 INT) PARTITIONED BY (c2, c3); > INSERT INTO test\_partition PARTITION (c2 = 2, c3 = 3) VALUES (1); > INSERT INTO test\_partition PARTITION (c2 = 5, c3 = 6) VALUES (4); > INSERT INTO test\_partition PARTITION (c2 = 8, c3 = 9) VALUES (7); > SELECT \* FROM test\_partition; c1 c2 c3 --- --- --- 1 2 3 4 5 6 7 8 9 > CREATE TABLE test\_load\_partition (c1 INT, c2 INT, c3 INT) USING HIVE PARTITIONED BY (c2, c3); -- Assuming the test\_partition table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/test\_partition/c2=2/c3=3' OVERWRITE INTO TABLE test\_load\_partition PARTITION (c2=2, c3=3); > SELECT \* FROM test\_load\_partition; c1 c2 c3 --- --- --- 1 2 3 ``` Related articles ------------------ * [INSERT INTO](sql-ref-syntax-dml-insert-into.html) * [COPY INTO](delta-copy-into.html)"	[[Yes]]
What is Databricks?	"What is Databricks? =====================  Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf. What is Databricks used for? ------------------------------  Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.  The Databricks workspace provides a unified interface and tools for most data tasks, including:  * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git  In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:  * REST API * CLI * Terraform  Managed integration with open source --------------------------------------  Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:  * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)  and [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)  Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:  * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)  How does Databricks work with AWS? ------------------------------------  The Databricks platform architecture comprises two primary parts:  * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.  Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.  Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.  Databricks workspaces meet the security and networking requirements of [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)  . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.  What are common use cases for Databricks? -------------------------------------------  Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.  Build an enterprise data lakehouse ------------------------------------  The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See [What is the Databricks Lakehouse?](../lakehouse/index.html) .  ETL and data engineering --------------------------  Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks. [Delta Live Tables](../delta-live-tables/index.html) simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.  Databricks provides a number of custom tools for [data ingestion](../ingestion/index.html) , including [Auto Loader](../ingestion/auto-loader/index.html) , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.  Machine learning, AI, and data science ----------------------------------------  Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including [MLflow](../mlflow/index.html) and the [Databricks Runtime for Machine Learning](../runtime/mlruntime.html) . See [Introduction to Databricks Machine Learning](../machine-learning/index.html) .  Data warehousing, analytics, and BI -------------------------------------  Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as [SQL warehouses](../sql/admin/create-sql-warehouse.html) , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the [SQL query editor](../sql/user/queries/queries.html) or in notebooks. [Notebooks](../notebooks/index.html) support Python, R, and Scala in addition to SQL, and allow users to embed the same [visualizations](../visualizations/index.html) available in [dashboards](../sql/user/dashboards/index.html) alongside links, images, and commentary written in markdown.  Data governance and secure data sharing -----------------------------------------  Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.  Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See [What is Unity Catalog?](../data-governance/unity-catalog/index.html) .  The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of [Delta Sharing](../data-sharing/index.html) .  DevOps, CI/CD, and task orchestration ---------------------------------------  The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations. [Workflows](../workflows/index.html) schedule Databricks notebooks, SQL queries, and other arbitrary code. [Repos](../repos/index.html) let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see [Developer tools and guidance](../dev-tools/index.html) .  Real-time and streaming analytics -----------------------------------  Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See [What is Apache Spark Structured Streaming?](../structured-streaming/index.html) ."	[[Yes]]
how do I put plotly charts in a dashboard	"Here is the Python code to call the Databricks REST API to start a cluster named ""test"": ``` import os from databricks_cli.sdk.api_client import ApiClient from databricks_cli.clusters.api import ClusterApi # Set up authentication host = os.getenv(""DATABRICKS_HOST"") token = os.getenv(""DATABRICKS_TOKEN"") client = ApiClient(host=host, token=token) # Start a cluster named ""test"" cluster_api = ClusterApi(client) cluster_api.start(cluster_name=""test"") ```"	[[No]]
how do I use vstudio	"Databricks extension for Visual Studio Code tutorial July 21, 2023 The Databricks extension for Visual Studio Code enables you to connect to your remote Databricks workspaces from the Visual Studio Code integrated development environment (IDE) running on your local development machine. Through these connections, you can: Synchronize local code that you develop in Visual Studio Code with code in your remote workspaces. Run local Python code files from Visual Studio Code on Databricks clusters in your remote workspaces. Run local Python code files (.py) and Python, R, Scala, and SQL notebooks (.py, .ipynb, .r, .scala, and .sql) from Visual Studio Code as automated Databricks jobs in your remote workspaces. Note The Databricks extension for Visual Studio Code supports running R, Scala, and SQL notebooks as automated jobs but does not provide any deeper support for these languages within Visual Studio Code. This article demonstrates how to quickly get started with the Databricks extension for Visual Studio Code by running a basic Python code file on a Databricks cluster in your remote workspace. This following hands-on tutorial assumes: You already have Visual Studio Code 1.69.1 or higher installed and configured for Python coding. See Setting up Visual Studio Code and Getting Started with Python in VS Code. Visual Studio Code is already running and has a local project opened. You have already generated a Databricks personal access token for your target Databricks workspace. See Databricks personal access token authentication. You have already added your Databricks personal access token as a token field along with your workspace instance URL, for example https://dbc-a1b2345c-d6e7.cloud.databricks.com, as a host field to the DEFAULT configuration profile in your local .databrickscfg file. See Databricks configuration profiles. To complete this tutorial, follow these steps: Install the extension: on the Databricks extension for Visual Studio Code page in the Visual Studio Code Marketplace, click Install. To complete the installation, follow the on-screen instructions. Open the extension: on the sidebar, click the Databricks logo. Start configuring the extension: in the Configuration pane, click Configure Databricks. Set the Databricks workspace: in the Command Palette, for Databricks Host, enter your workspace instance URL, for example https://dbc-a1b2345c-d6e7.cloud.databricks.com. Then press Enter. Click the entry DEFAULT: Authenticate using the DEFAULT profile. Set the Databricks cluster: in the Configuration pane, click Cluster, and then click the gear (Configure cluster) icon. Click the entry for the cluster that you want to use. Start the cluster, if it is not already started: in the Configuration pane, next to Cluster, click the play (Start Cluster) icon. Set the sync destination: in the Configuration pane, click Sync Destination, and then click the gear (Configure cluster) icon. In the Command Palette, click the sync destination name that is randomly generated by the extension. Create a basic, local Python code file to sync and run: on the sidebar, click the Explorer logo. On the main menu, click File > New File. Name the file demo.py and save it to the project root."	[[Yes]]
Whats the difference between managed and external tables?	"Managed tables Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly. By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the Delta table format. When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days. External tables External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses. When you run DROP TABLE on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a LOCATION path in your CREATE TABLE statement. External tables can use the following file formats: DELTA CSV JSON AVRO PARQUET ORC TEXT To manage access to the underlying cloud storage for an external table, you must set up storage credentials and external locations. To learn more, see Create an external table. Requirements You must have the CREATE TABLE privilege on the schema in which you want to create the table, as well as the USE SCHEMA privilege on the schema and the USE CATALOG privilege on the parent catalog. If you are creating an external table, see Create an external table for additional requirements."	[[Yes]]