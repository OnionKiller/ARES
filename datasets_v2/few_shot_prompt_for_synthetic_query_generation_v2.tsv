Question	Document
How do I load files into Databricks?	"LOAD DATA =========== **Applies to:** ![check marked yes](../../_images/check.png) Databricks Runtime Loads the data into a Hive SerDe table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the `LOAD DATA` statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the table or the dependents are accessed the next time. Syntax -------- ``` LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table\_name [ PARTITION clause ] ``` Parameters ------------ * **path** Path of the file system. It can be either an absolute or a relative path. * **[table\_name](sql-ref-names.html#table-name)** Identifies the table to be inserted to. The name must not include a [temporal specification](sql-ref-names.html#table-name) . If the table cannot be found Databricks raises a [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html) error. * **[PARTITION clause](sql-ref-partition.html#partition)** An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition. * **LOCAL** If specified, it causes the `INPATH` to be resolved against the local file system, instead of the default file system, which is typically a distributed storage. * **OVERWRITE** By default, new data is appended to the table. If `OVERWRITE` is used, the table is instead overwritten with new data. Examples ---------- ``` -- Example without partition specification. -- Assuming the students table has already been created and populated. > SELECT \* FROM students; name address student\_id --------- ---------------------- ---------- Amy Smith 123 Park Ave, San Jose 111111 > CREATE TABLE test\_load (name VARCHAR(64), address VARCHAR(64), student\_id INT) USING HIVE; -- Assuming the students table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test\_load; > SELECT \* FROM test\_load; name address student\_id --------- ---------------------- ---------- Amy Smith 123 Park Ave, San Jose 111111 -- Example with partition specification. > CREATE TABLE test\_partition (c1 INT, c2 INT, c3 INT) PARTITIONED BY (c2, c3); > INSERT INTO test\_partition PARTITION (c2 = 2, c3 = 3) VALUES (1); > INSERT INTO test\_partition PARTITION (c2 = 5, c3 = 6) VALUES (4); > INSERT INTO test\_partition PARTITION (c2 = 8, c3 = 9) VALUES (7); > SELECT \* FROM test\_partition; c1 c2 c3 --- --- --- 1 2 3 4 5 6 7 8 9 > CREATE TABLE test\_load\_partition (c1 INT, c2 INT, c3 INT) USING HIVE PARTITIONED BY (c2, c3); -- Assuming the test\_partition table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/test\_partition/c2=2/c3=3' OVERWRITE INTO TABLE test\_load\_partition PARTITION (c2=2, c3=3); > SELECT \* FROM test\_load\_partition; c1 c2 c3 --- --- --- 1 2 3 ``` Related articles ------------------ * [INSERT INTO](sql-ref-syntax-dml-insert-into.html) * [COPY INTO](delta-copy-into.html)"
how do I ensure the idempotency for streaming writes	"Enable idempotent writes across jobs ======================================     Sometimes a job that writes data to a Delta table is restarted due to various reasons (for example, job encounters a failure). The failed job may or may not have written the data to Delta table before terminating. In the case where the data is written to the Delta table, the restarted job writes the same data to the Delta table which results in duplicate data.       To address this, Delta tables support the following  `DataFrameWriter`  options to make the writes idempotent:     * `txnAppId`  : A unique string that you can pass on each  `DataFrame`  write. For example, this can be the name of the job. * `txnVersion`  : A monotonically increasing number that acts as transaction version. This number needs to be unique for data that is being written to the Delta table(s). For example, this can be the epoch seconds of the instant when the query is attempted for the first time. Any subsequent restarts of the same job needs to have the same value for  `txnVersion`  .     The above combination of options needs to be unique for each new data that is being ingested into the Delta table and the  `txnVersion`  needs to be higher than the last data that was ingested into the Delta table. For example:     * Last successfully written data contains option values as  `dailyETL:23423`  (  `txnAppId:txnVersion`  ). * Next write of data should have  `txnAppId     =     dailyETL`  and  `txnVersion`  as at least  `23424`  (one more than the last written data  `txnVersion`  ). * Any attempt to write data with  `txnAppId     =     dailyETL`  and  `txnVersion`  as  `23422`  or less is ignored because the  `txnVersion`  is less than the last recorded  `txnVersion`  in the table. * Attempt to write data with  `txnAppId:txnVersion`  as  `anotherETL:23424`  is successful writing data to the table as it contains a different  `txnAppId`  compared to the same option value in last ingested data.      Warning       This solution assumes that the data being written to Delta table(s) in multiple retries of the job is the same. If a write attempt to a Delta table succeeds but due to some downstream failure there is a second write attempt with same txn options but different data, then that second write attempt will be ignored. This can cause unexpected results.        See the following code for an example:         ``` app\_id = ... # A unique string that is used as an application ID. version = ... # A monotonically increasing number that acts as transaction version.  dataFrame.write.option(""txnVersion"", version).option(""txnAppId"", app\_id).save(...)  ```       ``` val appId = ... // A unique string that is used as an application ID. version = ... // A monotonically increasing number that acts as transaction version.  dataFrame.write.option(""txnVersion"", version).option(""txnAppId"", appId).save(...)  ```"
Where can I find a list of my frequently used queries?	"View frequent queries and users of a table ============================================     You can use the Insights tab in Data Explorer to view the most frequent recent queries and users of any table registered in Unity Catalog. The Insights tab reports on frequent queries and user access for the past 30 days.       This information can help you answer questions like:     * Can I trust this data? * What are some good ways to use this data? * Which users can answer my questions about this data?      Note       The queries and users listed on the Insights tab are limited to queries performed using Databricks SQL.         Before you begin ------------------     You must have the following permissions to view frequent queries and user data on the Insights tab.       In Unity Catalog:     * `SELECT`  privilege on the table. * `USE     SCHEMA`  privilege on the table’s parent schema. * `USE     CATALOG`  privilege on the table’s parent catalog.     Metastore admins have these privileges by default. See  [Manage privileges in Unity Catalog](../data-governance/unity-catalog/manage-privileges/index.html)  .       In Databricks SQL:     * `Can     View`  permissions on the queries. You will not see queries that you do not have permission to view. See  [Query access control](../security/auth-authz/access-control/query-acl.html)  .       View the Insights tab -----------------------   1. In your Databricks workspace, click  ![Data Icon](../_images/data-icon.png) **Data**   to open Data Explorer. 2. Search for or navigate to the table you want insights on.       See  [Search for workspace assets](../search/index.html)  and  [Explore tables](explore-tables.html)  . 3. On the table page, click the  **Insights**   tab.       Queries made on the table and users who accessed the table in the past 30 days are listed in order of frequency, with the most frequent on top.    ![Insights tab showing the most frequent recent queries and users on a table](../_images/insights-tab.png)"
how do I get unique values of a column	"pyspark.pandas.Index.unique  [¶](#pyspark-pandas-index-unique ""Permalink to this headline"") =============================================================================================  `Index.`  `unique`    (   *level     :     Union[int, Any, Tuple[Any, …], None]     =     None*   )    → pyspark.pandas.indexes.base.Index  [[source]](../../../_modules/pyspark/pandas/indexes/base.html#Index.unique) [¶](#pyspark.pandas.Index.unique ""Permalink to this definition"")    Return unique values in the index.     Be aware the order of unique values might be different than pandas.Index.unique     Parameters    **level**    int or str, optional, default is None     Returns     Index without duplicates     See also    [`Series.unique`](pyspark.pandas.Series.unique.html#pyspark.pandas.Series.unique ""pyspark.pandas.Series.unique"")  [`groupby.SeriesGroupBy.unique`](pyspark.pandas.groupby.SeriesGroupBy.unique.html#pyspark.pandas.groupby.SeriesGroupBy.unique ""pyspark.pandas.groupby.SeriesGroupBy.unique"")   Examples    ``` >>> ps.DataFrame({'a': ['a', 'b', 'c']}, index=[1, 1, 3]).index.unique().sort\_values() Int64Index([1, 3], dtype='int64')  ```  ``` >>> ps.DataFrame({'a': ['a', 'b', 'c']}, index=['d', 'e', 'e']).index.unique().sort\_values() Index(['d', 'e'], dtype='object')  ```   MultiIndex    ``` >>> ps.MultiIndex.from\_tuples([(""A"", ""X""), (""A"", ""Y""), (""A"", ""X"")]).unique() ...  MultiIndex([('A', 'X'),  ('A', 'Y')],  )  ```  [pyspark.pandas.Index.take](pyspark.pandas.Index.take.html ""previous page"")  [pyspark.pandas.Index.nunique](pyspark.pandas.Index.nunique.html ""next page"")"
how do I use vstudio	"Databricks extension for Visual Studio Code tutorial July 21, 2023 The Databricks extension for Visual Studio Code enables you to connect to your remote Databricks workspaces from the Visual Studio Code integrated development environment (IDE) running on your local development machine. Through these connections, you can: Synchronize local code that you develop in Visual Studio Code with code in your remote workspaces. Run local Python code files from Visual Studio Code on Databricks clusters in your remote workspaces. Run local Python code files (.py) and Python, R, Scala, and SQL notebooks (.py, .ipynb, .r, .scala, and .sql) from Visual Studio Code as automated Databricks jobs in your remote workspaces. Note The Databricks extension for Visual Studio Code supports running R, Scala, and SQL notebooks as automated jobs but does not provide any deeper support for these languages within Visual Studio Code. This article demonstrates how to quickly get started with the Databricks extension for Visual Studio Code by running a basic Python code file on a Databricks cluster in your remote workspace. This following hands-on tutorial assumes: You already have Visual Studio Code 1.69.1 or higher installed and configured for Python coding. See Setting up Visual Studio Code and Getting Started with Python in VS Code. Visual Studio Code is already running and has a local project opened. You have already generated a Databricks personal access token for your target Databricks workspace. See Databricks personal access token authentication. You have already added your Databricks personal access token as a token field along with your workspace instance URL, for example https://dbc-a1b2345c-d6e7.cloud.databricks.com, as a host field to the DEFAULT configuration profile in your local .databrickscfg file. See Databricks configuration profiles. To complete this tutorial, follow these steps: Install the extension: on the Databricks extension for Visual Studio Code page in the Visual Studio Code Marketplace, click Install. To complete the installation, follow the on-screen instructions. Open the extension: on the sidebar, click the Databricks logo. Start configuring the extension: in the Configuration pane, click Configure Databricks. Set the Databricks workspace: in the Command Palette, for Databricks Host, enter your workspace instance URL, for example https://dbc-a1b2345c-d6e7.cloud.databricks.com. Then press Enter. Click the entry DEFAULT: Authenticate using the DEFAULT profile. Set the Databricks cluster: in the Configuration pane, click Cluster, and then click the gear (Configure cluster) icon. Click the entry for the cluster that you want to use. Start the cluster, if it is not already started: in the Configuration pane, next to Cluster, click the play (Start Cluster) icon. Set the sync destination: in the Configuration pane, click Sync Destination, and then click the gear (Configure cluster) icon. In the Command Palette, click the sync destination name that is randomly generated by the extension. Create a basic, local Python code file to sync and run: on the sidebar, click the Explorer logo. On the main menu, click File > New File. Name the file demo.py and save it to the project root."
Whats the difference between managed and external tables?	"Managed tables Managed tables are the default way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. You should not use tools outside of Databricks to manipulate files in these tables directly. By default, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the Delta table format. When a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days. External tables External tables are tables whose data is stored outside of the managed storage location specified for the metastore, catalog, or schema. Use external tables only when you require direct access to the data outside of Databricks clusters or Databricks SQL warehouses. When you run DROP TABLE on an external table, Unity Catalog does not delete the underlying data. To drop a table you must be its owner. You can manage privileges on external tables and use them in queries in the same way as managed tables. To create an external table with SQL, specify a LOCATION path in your CREATE TABLE statement. External tables can use the following file formats: DELTA CSV JSON AVRO PARQUET ORC TEXT To manage access to the underlying cloud storage for an external table, you must set up storage credentials and external locations. To learn more, see Create an external table. Requirements You must have the CREATE TABLE privilege on the schema in which you want to create the table, as well as the USE SCHEMA privilege on the schema and the USE CATALOG privilege on the parent catalog. If you are creating an external table, see Create an external table for additional requirements."