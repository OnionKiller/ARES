Query	Document	Answer	Context_Relevance_Label	Answer_Faithfulness_Label	Answer_Relevance_Label
How do I load files into Databricks?	LOAD DATA =========== **Applies to:** ![check marked yes](../../_images/check.png) Databricks Runtime Loads the data into a Hive SerDe table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the `LOAD DATA` statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the table or the dependents are accessed the next time. Syntax -------- ``` LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table\_name [ PARTITION clause ] ``` Parameters ------------ * **path** Path of the file system. It can be either an absolute or a relative path. * **[table\_name](sql-ref-names.html#table-name)** Identifies the table to be inserted to. The name must not include a [temporal specification](sql-ref-names.html#table-name) . If the table cannot be found Databricks raises a [TABLE\_OR\_VIEW\_NOT\_FOUND](../../error-messages/table-or-view-not-found-error-class.html) error. * **[PARTITION clause](sql-ref-partition.html#partition)** An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition. * **LOCAL** If specified, it causes the `INPATH` to be resolved against the local file system, instead of the default file system, which is typically a distributed storage. * **OVERWRITE** By default, new data is appended to the table. If `OVERWRITE` is used, the table is instead overwritten with new data. Examples ---------- ``` -- Example without partition specification. -- Assuming the students table has already been created and populated. > SELECT \* FROM students; name address student\_id --------- ---------------------- ---------- Amy Smith 123 Park Ave, San Jose 111111 > CREATE TABLE test\_load (name VARCHAR(64), address VARCHAR(64), student\_id INT) USING HIVE; -- Assuming the students table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test\_load; > SELECT \* FROM test\_load; name address student\_id --------- ---------------------- ---------- Amy Smith 123 Park Ave, San Jose 111111 -- Example with partition specification. > CREATE TABLE test\_partition (c1 INT, c2 INT, c3 INT) PARTITIONED BY (c2, c3); > INSERT INTO test\_partition PARTITION (c2 = 2, c3 = 3) VALUES (1); > INSERT INTO test\_partition PARTITION (c2 = 5, c3 = 6) VALUES (4); > INSERT INTO test\_partition PARTITION (c2 = 8, c3 = 9) VALUES (7); > SELECT \* FROM test\_partition; c1 c2 c3 --- --- --- 1 2 3 4 5 6 7 8 9 > CREATE TABLE test\_load\_partition (c1 INT, c2 INT, c3 INT) USING HIVE PARTITIONED BY (c2, c3); -- Assuming the test\_partition table is in '/user/hive/warehouse/' > LOAD DATA LOCAL INPATH '/user/hive/warehouse/test\_partition/c2=2/c3=3' OVERWRITE INTO TABLE test\_load\_partition PARTITION (c2=2, c3=3); > SELECT \* FROM test\_load\_partition; c1 c2 c3 --- --- --- 1 2 3 ``` Related articles ------------------ * [INSERT INTO](sql-ref-syntax-dml-insert-into.html) * [COPY INTO](delta-copy-into.html)	To load files into Databricks, you can use the `LOAD DATA` statement to load data into a Hive SerDe table from the user-specified directory or file. If a directory is specified, then all the files from the directory are loaded. If a file is specified, then only the single file is loaded. Additionally, the `LOAD DATA` statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table.	[[Yes]]	[[Yes]]	[[Yes]]
What is Databricks?	What is Databricks? =====================  Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. The Databricks Lakehouse Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf. What is Databricks used for? ------------------------------  Our customers use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more.  The Databricks workspace provides a unified interface and tools for most data tasks, including:  * Data processing workflows scheduling and management * Working in SQL * Generating dashboards and visualizations * Data ingestion * Managing security, governance, and HA/DR * Data discovery, annotation, and exploration * Compute management * Machine learning (ML) modeling and tracking * ML model serving * Source control with Git  In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:  * REST API * CLI * Terraform  Managed integration with open source --------------------------------------  Databricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects founded by Databricks employees:  * [Delta Lake](https://delta.io/) * [Delta Sharing](https://delta.io/sharing) * [MLflow](https://mlflow.org/) * [Apache Spark](https://spark.apache.org/)  and [Structured Streaming](https://spark.apache.org/streaming/) * [Redash](https://redash.io/)  Databricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:  * [Workflows](../workflows/index.html) * [Unity Catalog](../data-governance/unity-catalog/index.html) * [Delta Live Tables](../delta-live-tables/index.html) * [Databricks SQL](../sql/index.html) * [Photon](../runtime/photon.html)  How does Databricks work with AWS? ------------------------------------  The Databricks platform architecture comprises two primary parts:  * The infrastructure used by Databricks to deploy, configure, and manage the platform and services. * The customer-owned infrastructure managed in collaboration by Databricks and your company.  Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.  Unity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.  Databricks workspaces meet the security and networking requirements of [some of the world’s largest and most security-minded companies](https://www.databricks.com/customers)  . Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.  What are common use cases for Databricks? -------------------------------------------  Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.  Build an enterprise data lakehouse ------------------------------------  The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See [What is the Databricks Lakehouse?](../lakehouse/index.html) .  ETL and data engineering --------------------------  Whether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks. [Delta Live Tables](../delta-live-tables/index.html) simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.  Databricks provides a number of custom tools for [data ingestion](../ingestion/index.html) , including [Auto Loader](../ingestion/auto-loader/index.html) , an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.  Machine learning, AI, and data science ----------------------------------------  Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including [MLflow](../mlflow/index.html) and the [Databricks Runtime for Machine Learning](../runtime/mlruntime.html) . See [Introduction to Databricks Machine Learning](../machine-learning/index.html) .  Data warehousing, analytics, and BI -------------------------------------  Databricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as [SQL warehouses](../sql/admin/create-sql-warehouse.html) , allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the [SQL query editor](../sql/user/queries/queries.html) or in notebooks. [Notebooks](../notebooks/index.html) support Python, R, and Scala in addition to SQL, and allow users to embed the same [visualizations](../visualizations/index.html) available in [dashboards](../sql/user/dashboards/index.html) alongside links, images, and commentary written in markdown.  Data governance and secure data sharing -----------------------------------------  Unity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.  Unity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See [What is Unity Catalog?](../data-governance/unity-catalog/index.html) .  The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of [Delta Sharing](../data-sharing/index.html) .  DevOps, CI/CD, and task orchestration ---------------------------------------  The development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations. [Workflows](../workflows/index.html) schedule Databricks notebooks, SQL queries, and other arbitrary code. [Repos](../repos/index.html) let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see [Developer tools and guidance](../dev-tools/index.html) .  Real-time and streaming analytics -----------------------------------  Databricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See [What is Apache Spark Structured Streaming?](../structured-streaming/index.html) .	Databricks is a unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. It is used to process, store, clean, share, analyze, model, and monetize datasets with solutions from BI to machine learning. The Databricks platform provides a unified interface and tools for most data tasks, including data processing workflows scheduling and management, working in SQL, generating dashboards and visualizations, data ingestion, managing security, governance, and HA/DR, data discovery, annotation, and exploration, compute management, machine learning (ML) modeling and tracking, ML model serving, and source control with Git. Databricks can be interacted with programmatically with REST API, CLI, and Terraform. It works with AWS by configuring a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control. Databricks has a variety of use cases, including building an enterprise data lakehouse, ETL and data engineering, machine learning, AI, and data science, data warehousing, analytics, and BI, data governance and secure data sharing, and DevOps, CI/CD, and task orchestration.	[[Yes]]	[[Yes]]	[[Yes]]
How do you deploy my model for serving?	Based on the context, it seems that the Delta table is not accessible due to corrupted CRC or JSON files. To recover the Delta table and make it readable, you can disable the validation config, identify which version of the table is corrupted, make a backup of the Parquet files that were added to the table in the corrupted versions, remove the corrupted CRC and JSON files, restore the Delta table to the most recent version that is not corrupted, and append the Parquet files that were previously backed up to avoid data loss.	The answer could not be found in the given context. I don't know if the answer is present.	[[No]]	[[No]]	[[No]]
how do I put plotly charts in a dashboard	Here is the Python code to call the Databricks REST API to start a cluster named "test": ``` import os from databricks_cli.sdk.api_client import ApiClient from databricks_cli.clusters.api import ClusterApi # Set up authentication host = os.getenv("DATABRICKS_HOST") token = os.getenv("DATABRICKS_TOKEN") client = ApiClient(host=host, token=token) # Start a cluster named "test" cluster_api = ClusterApi(client) cluster_api.start(cluster_name="test") ```	The answer could not be found in the given context. I don't know if the answer is present.	[[No]]	[[No]]	[[No]]
How to read local files in a notebook?	Workspace files basic usage June 01, 2023 You can use the workspace UI to perform basic tasks like creating, importing, and editing workspace files. Note All files present in a repository are synced as workspace files automatically when you clone a Git repository. Create a new file You can create a new file in any Databricks directory. Click the down arrow next to the directory name, and select Create > File from the menu. Import a file To import a file, click the down arrow next to the directory name, and select Import. The import dialog appears. You can drag files into the dialog or click browse to select files. Note Only notebooks can be imported from a URL. When you import a .zip file, Databricks automatically unzips the file and imports each file and notebook that is included in the .zip file. Edit a file To edit a file, click the filename in the workspace browser. The file opens and you can edit it. Changes are saved automatically. When you open a Markdown (.md) file, the rendered preview is displayed by default. To edit a cell, double-click in the cell. To return to preview mode, click anywhere outside the cell. The editor includes additional functionality such as autocomplete, multicursor support, and the ability to run code. For more information, see Use the Databricks notebook and file editor.	To read local files in a notebook, you can use the `dbutils.fs` module. You can use the `dbutils.fs.ls` command to list the files in a directory and the `dbutils.fs.head` command to read the contents of a file.	[[Yes]]	[[No]]	[[No]]
how to write excel file with Spark?	Save the content of SparkDataFrame in a text file at the specified path. Save the content of the SparkDataFrame in a text file at the specified path. The SparkDataFrame must have only one column of string type with the name "value". Each row becomes a new line in the output file. The text files will be encoded as UTF-8.  Usage write.text(x, path, ...)  # S4 method for SparkDataFrame,character write.text(x, path, mode = "error", ...) Arguments x A SparkDataFrame  path The directory where the file is saved  ... additional argument(s) passed to the method. You can find the text-specific options for writing text files in https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-optionData Source Option in the version you use.  mode one of 'append', 'overwrite', 'error', 'errorifexists', 'ignore' save mode (it is 'error' by default)  Note write.text since 2.0.0	To write an Excel file with Spark, you can use the `write.format()` method with the format set to "com.crealytics.spark.excel". This requires the "spark-excel" package to be added to your Spark application. You can specify the file path and other options using the `option()` method. For example:  ``` df.write.format("com.crealytics.spark.excel")   .option("header", "true")   .option("dataAddress", "'Sheet1'!A1")   .save("path/to/excel/file.xlsx") ```	[[Yes]]	[[No]]	[[No]]
How do I change the owner of a cluster?	Manage Unity Catalog object ownership in Data Explorer March 20, 2023  You can use Data Explorer to change the owner for securable objects in Unity Catalog-enabled Databricks workspaces. Object ownership can be transferred to other principals by either the current owner, a metastore admin, or the owner of the catalog or schema that contains the table. See Manage Unity Catalog object ownership.  Change owner Select the object, such as a catalog, schema, table, view, external location, or storage credential.  Click Permissions.  Click the blue pencil next to the Owner.  Select a group, user, or service principal from the dropdown list.  Click Save.	To change the owner of a cluster, you can't use Data Explorer. Instead, you can use the Databricks CLI or REST API to change the owner of a cluster.	[[Yes]]	[[Yes]]	[[No]]
how to remove partitions of a delta table?	Best practices for dropping a managed Delta Lake table Learn the best practices for dropping a managed Delta Lake table.   Written by Adam Pavlacka  Last published at: May 10th, 2022 Regardless of how you drop a managed table, it can take a significant amount of time, depending on the data size. Delta Lake managed tables in particular contain a lot of metadata in the form of transaction logs, and they can contain duplicate data files. If a Delta table has been in use for a long time, it can accumulate a very large amount of data.  In the Databricks environment, there are two ways to drop tables (AWS | Azure | GCP):  Run DROP TABLE in a notebook cell. Click Delete in the UI. Even though you can delete tables in the background without affecting workloads, it is always good to make sure that you run DELETE FROM (AWS | Azure | GCP) and VACUUM (AWS | Azure | GCP) before you start a drop command on any table. This ensures that the metadata and file sizes are cleaned up before you initiate the actual data deletion.  For example, if you are trying to delete the Delta table events, run the following commands before you start the DROP TABLE command:  Run DELETE FROM: DELETE FROM events Run VACUUM with an interval of zero: VACUUM events RETAIN 0 HOURS These two steps reduce the amount of metadata and number of uncommitted files that would otherwise increase the data deletion time. 	To remove partitions of a Delta table, you can use the DELETE FROM and VACUUM commands before initiating the DROP TABLE command. For example, if you want to delete the Delta table "events", you can run the following commands: DELETE FROM events and VACUUM events RETAIN 0 HOURS. These steps will reduce the amount of metadata and uncommitted files, which can help to decrease the time it takes to delete the data.	[[Yes]]	[[Yes]]	[[No]]